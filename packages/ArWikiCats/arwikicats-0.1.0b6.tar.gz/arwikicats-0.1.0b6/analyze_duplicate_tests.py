#!/usr/bin/env python3
"""
Ø£Ø¯Ø§Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙÙŠ Ù…Ù„ÙØ§Øª tests/event_lists

Ù‡Ø°Ø§ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª ÙŠÙ‚ÙˆÙ… Ø¨Ù€:
1. ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Python ÙÙŠ tests/event_lists
2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…ÙØ§ØªÙŠØ­-Ø§Ù„Ù‚ÙŠÙ… Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³
3. Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù„ÙƒÙ„ Ø²ÙˆØ¬
4. Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª
"""

import ast
import json
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Set, Tuple


class DuplicateTestAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""

    def __init__(self, base_path: str):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ù„Ù„

        Args:
            base_path: Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù…Ø¬Ù„Ø¯ tests/event_lists
        """
        self.base_path = Path(base_path)
        # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø£Ø²ÙˆØ§Ø¬: {(key, value): [(file_path, dict_name, line_number), ...]}
        self.test_pairs: Dict[Tuple[str, str], List[Tuple[str, str, int]]] = defaultdict(list)
        # ØªØ®Ø²ÙŠÙ† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„ÙØ§Øª
        self.file_contents: Dict[str, List[str]] = {}

    def extract_dict_from_node(self, node: ast.Dict, file_path: str, dict_name: str = "unknown") -> None:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…ÙØ§ØªÙŠØ­-Ø§Ù„Ù‚ÙŠÙ… Ù…Ù† Ø¹Ù‚Ø¯Ø© Ù‚Ø§Ù…ÙˆØ³ AST

        Args:
            node: Ø¹Ù‚Ø¯Ø© Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ ÙÙŠ AST
            file_path: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù
            dict_name: Ø§Ø³Ù… Ø§Ù„Ù‚Ø§Ù…ÙˆØ³
        """
        for key_node, value_node in zip(node.keys, node.values, strict=False):
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¹Ù†Ø§ØµØ± None (ÙÙŠ Ø­Ø§Ù„Ø© dictionary unpacking)
            if key_node is None or value_node is None:
                continue

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ø±ÙÙŠØ© ÙÙ‚Ø· (strings)
            if isinstance(key_node, ast.Constant) and isinstance(value_node, ast.Constant):
                if isinstance(key_node.value, str) and isinstance(value_node.value, str):
                    key = key_node.value
                    value = value_node.value
                    line_number = key_node.lineno

                    # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø²ÙˆØ¬ Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆÙ‚Ø¹Ù‡
                    self.test_pairs[(key, value)].append((file_path, dict_name, line_number))

    def analyze_file(self, file_path: Path) -> None:
        """
        ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Python ÙˆØ§Ø­Ø¯

        Args:
            file_path: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù
        """
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                lines = content.splitlines()

            # Ø­ÙØ¸ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ø§Ø­Ù‚Ø§Ù‹
            self.file_contents[str(file_path)] = lines

            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AST
            tree = ast.parse(content, filename=str(file_path))

            # ØªØªØ¨Ø¹ Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„ØªÙŠ ØªÙ… Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
            processed_nodes = set()

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³ ÙÙŠ Ø§Ù„Ù…Ù„Ù
            for node in ast.walk(tree):
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ¹ÙŠÙŠÙ†Ø§Øª Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³ (Ù…Ø«Ù„: data1 = {...})
                if isinstance(node, ast.Assign):
                    for target in node.targets:
                        if isinstance(target, ast.Name) and isinstance(node.value, ast.Dict):
                            # Ø§Ø³ØªØ®Ø¯Ø§Ù… id Ø§Ù„Ø¹Ù‚Ø¯Ø© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø©
                            if id(node.value) not in processed_nodes:
                                dict_name = target.id
                                self.extract_dict_from_node(node.value, str(file_path), dict_name)
                                processed_nodes.add(id(node.value))

        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù {file_path}: {e}")

    def scan_directory(self) -> None:
        """Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Python ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯"""
        print(f"ğŸ” Ø¬Ø§Ø±ÙŠ Ù…Ø³Ø­ Ø§Ù„Ù…Ø¬Ù„Ø¯: {self.base_path}")

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª .py
        python_files = list(self.base_path.rglob("*.py"))

        # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ù…Ù„ÙØ§Øª __pycache__ Ùˆ __init__.py
        python_files = [f for f in python_files if "__pycache__" not in str(f) and f.name != "__init__.py"]

        print(f"ğŸ“ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(python_files)} Ù…Ù„Ù Python")

        for file_path in python_files:
            self.analyze_file(file_path)

    def get_duplicates(self) -> Dict[Tuple[str, str], List[Tuple[str, str, int]]]:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙÙ‚Ø·

        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø¨Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙˆÙ…ÙˆØ§Ù‚Ø¹Ù‡Ø§
        """
        return {pair: locations for pair, locations in self.test_pairs.items() if len(locations) > 1}

    def print_statistics(self) -> None:
        """
        Print analysis statistics summarizing extracted key-value pairs and duplicates.

        Prints the total number of unique pairs, the number of pairs that appear in more than one location, the total duplicate occurrences across all files, and the percentage of unique pairs that are duplicated.
        """
        duplicates = self.get_duplicates()
        total_pairs = len(self.test_pairs)
        duplicate_pairs = len(duplicates)

        # Ø­Ø³Ø§Ø¨ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        total_occurrences = sum(len(locations) for locations in duplicates.values())

        print("\n" + "=" * 80)
        print("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„")
        print("=" * 80)
        print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„ÙØ±ÙŠØ¯Ø©: {total_pairs:,}")
        print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…ÙƒØ±Ø±Ø©: {duplicate_pairs:,}")
        print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {total_occurrences:,}")
        if total_pairs > 0:
            print(f"Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±: {(duplicate_pairs / total_pairs * 100):.2f}%")
        print("=" * 80 + "\n")

    def print_duplicates(self, limit: int = 20) -> None:
        """
        Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…ÙƒØ±Ø±Ø©

        Args:
            limit: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø©
        """
        duplicates = self.get_duplicates()

        if not duplicates:
            print("âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…ÙƒØ±Ø±Ø©!")
            return

        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª (Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹ Ø£ÙˆÙ„Ø§Ù‹)
        sorted_duplicates = sorted(duplicates.items(), key=lambda x: len(x[1]), reverse=True)

        print(f"\nğŸ” Ø¹Ø±Ø¶ Ø£ÙˆÙ„ {min(limit, len(sorted_duplicates))} Ø²ÙˆØ¬ Ù…ÙƒØ±Ø±:\n")

        for idx, ((key, value), locations) in enumerate(sorted_duplicates[:limit], 1):
            print(f"\n{idx}. ØªÙƒØ±Ø± {len(locations)} Ù…Ø±Ø©:")
            print(f"   Ø§Ù„Ù…ÙØªØ§Ø­: {key}")
            print(f"   Ø§Ù„Ù‚ÙŠÙ…Ø©: {value}")
            print("   Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹:")

            for file_path, dict_name, line_num in locations:
                rel_path = Path(file_path).relative_to(self.base_path.parent)
                print(f"      - {rel_path} (Ø§Ù„Ù‚Ø§Ù…ÙˆØ³: {dict_name}, Ø§Ù„Ø³Ø·Ø±: {line_num})")

        if len(sorted_duplicates) > limit:
            print(f"\n... Ùˆ {len(sorted_duplicates) - limit} Ø²ÙˆØ¬ Ù…ÙƒØ±Ø± Ø¢Ø®Ø±")

    def save_report(self, output_file: str = "duplicate_tests_report.json") -> None:
        """
        Write a JSON report of duplicate keyâ€“value pairs and their locations to a file.

        The report contains a "summary" with total unique pairs, number of duplicate pairs, and total occurrences, and a "duplicates" list where each entry includes the key, value, occurrence count, and a list of locations (file path relative to the analyzer's parent base path, dictionary name, and line number).

        Parameters:
            output_file (str): Path to the output JSON file (default: "duplicate_tests_report.json").
        """
        duplicates = self.get_duplicates()

        report = {
            "summary": {
                "total_unique_pairs": len(self.test_pairs),
                "duplicate_pairs": len(duplicates),
                "total_occurrences": sum(len(locs) for locs in duplicates.values()),
            },
            "duplicates": [],
        }

        for (key, value), locations in sorted(duplicates.items(), key=lambda x: len(x[1]), reverse=True):
            report["duplicates"].append(
                {
                    "key": key,
                    "value": value,
                    "count": len(locations),
                    "locations": [
                        {
                            "file": str(Path(fp).relative_to(self.base_path.parent)),
                            "dict_name": dn,
                            "line": ln,
                        }
                        for fp, dn, ln in locations
                    ],
                }
            )

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {output_file}")

    def remove_duplicates_interactive(self) -> None:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª Ø¨Ø´ÙƒÙ„ ØªÙØ§Ø¹Ù„ÙŠ"""
        duplicates = self.get_duplicates()

        if not duplicates:
            print("âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…ÙƒØ±Ø±Ø© Ù„Ù„Ø¥Ø²Ø§Ù„Ø©!")
            return

        print("\n" + "=" * 80)
        print("ğŸ—‘ï¸  ÙˆØ¶Ø¹ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª")
        print("=" * 80)
        print("Ø³ÙŠØªÙ… Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø£ÙˆÙ„ Ø¸Ù‡ÙˆØ± Ù„ÙƒÙ„ Ø²ÙˆØ¬ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰")

        response = input("\nÙ‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©ØŸ (Ù†Ø¹Ù…/Ù„Ø§): ").strip().lower()

        if response not in ["Ù†Ø¹Ù…", "yes", "y", "Ù†"]:
            print("âŒ ØªÙ… Ø§Ù„Ø¥Ù„ØºØ§Ø¡")
            return

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ù…Ø±Ø§Ø¯ Ø­Ø°ÙÙ‡Ø§ Ø­Ø³Ø¨ Ø§Ù„Ù…Ù„Ù
        lines_to_remove: Dict[str, Set[int]] = defaultdict(set)

        for (_key, _value), locations in duplicates.items():
            # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø£ÙˆÙ„ Ø¸Ù‡ÙˆØ±ØŒ Ø­Ø°Ù Ø§Ù„Ø¨Ø§Ù‚ÙŠ
            for file_path, _dict_name, line_num in locations[1:]:
                lines_to_remove[file_path].add(line_num)

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø³Ø·Ø± Ù…Ù† ÙƒÙ„ Ù…Ù„Ù
        files_modified = 0
        lines_removed = 0

        for file_path, line_numbers in lines_to_remove.items():
            try:
                lines = self.file_contents[file_path]

                # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­ØªÙˆÙ‰ Ø¬Ø¯ÙŠØ¯ Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ù…ÙƒØ±Ø±Ø©
                new_lines = []
                for idx, line in enumerate(lines, 1):
                    if idx not in line_numbers:
                        new_lines.append(line)
                    else:
                        lines_removed += 1

                # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø¯Ø«
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write("\n".join(new_lines))
                    if new_lines and not new_lines[-1].endswith("\n"):
                        f.write("\n")

                files_modified += 1
                rel_path = Path(file_path).relative_to(self.base_path.parent)
                print(f"âœ… ØªÙ… ØªØ­Ø¯ÙŠØ«: {rel_path} (Ø­Ø°Ù {len(line_numbers)} Ø³Ø·Ø±)")

            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« {file_path}: {e}")

        print("\nâœ¨ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡!")
        print(f"   - Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¹Ø¯Ù„Ø©: {files_modified}")
        print(f"   - Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {lines_removed}")


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    # Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    base_path = Path(__file__).parent / "tests"

    if not base_path.exists():
        print(f"âŒ Ø§Ù„Ù…Ø³Ø§Ø± ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {base_path}")
        return

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ù„Ù„
    analyzer = DuplicateTestAnalyzer(str(base_path))

    # Ù…Ø³Ø­ Ø§Ù„Ù…Ù„ÙØ§Øª
    analyzer.scan_directory()

    # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    analyzer.print_statistics()

    # Ø¹Ø±Ø¶ Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª
    analyzer.print_duplicates(limit=30)

    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    analyzer.save_report()

    # Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ù† Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª
    print("\n" + "=" * 80)
    response = input("Ù‡Ù„ ØªØ±ÙŠØ¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©ØŸ (Ù†Ø¹Ù…/Ù„Ø§): ").strip().lower()

    if response in ["Ù†Ø¹Ù…", "yes", "y", "Ù†"]:
        analyzer.remove_duplicates_interactive()
    else:
        print("âœ… ØªÙ… Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ù…Ù„ÙØ§Øª ÙƒÙ…Ø§ Ù‡ÙŠ")


if __name__ == "__main__":
    main()
