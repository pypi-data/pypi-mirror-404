import json
import sys
import linecache
import random
import numpy as np
import pandas as pd
import pyarrow.parquet as pq
import torch
import tqdm
import hashlib
import os
import shutil
import concurrent.futures
from pathlib import Path
from torch.utils.data import Dataset, IterableDataset, ConcatDataset
from transformers import AutoTokenizer


class SequentialDataset(Dataset):
    ''' é€šç”¨é¡ºåºæ•°æ®é›† (æ”¯æŒ Parquet & JSONL æ··åˆè¯»å–)
    åŠŸèƒ½ï¼š
    1. é€’å½’æ‰«æç›®å½•ä¸‹æ‰€æœ‰ .parquet å’Œ .jsonl æ–‡ä»¶ã€‚
    2. æ ¹æ® data_config ä¸­çš„å…³é”®å­—åŒ¹é…è·¯å¾„ï¼Œè‡ªåŠ¨åº”ç”¨åˆ—æ˜ å°„ã€‚
    3. æ”¯æŒ FIMã€CoT è‡ªåŠ¨æå– (æ”¯æŒ startswith åŒ¹é…)ã€å·¥å…·è°ƒç”¨è§£æã€‚
    4. æ”¯æŒå…¨é‡å†…å­˜åŠ è½½ (In-Memory)ã€‚
    Args:
        root_dir (str | Path): æ•°æ®é›†æ ¹ç›®å½•ã€‚
        tokenizer: åˆ†è¯å™¨ã€‚
        config (dict): æ–‡ä»¶å¤¹ååˆ°åˆ—åçš„æ˜ å°„é…ç½®ã€‚
        cot_delimiter (tuple, optional): CoT æå–ç¬¦å· ('<think>', '</think>')ã€‚
        fim_rate (float): FIM æ¦‚ç‡ã€‚
        max_length (int, optional): æœ€å¤§é•¿åº¦ã€‚
        in_memory (bool): æ˜¯å¦é¢„åŠ è½½åˆ°å†…å­˜ã€‚
    '''
    def __init__(
        self, 
        root_dir, 
        tokenizer, 
        config=None, 
        cot_delimiter=None, 
        fim_rate=0.5, 
        max_length=None, 
        in_memory=False
    ):
        self.root_dir = Path(root_dir)
        
        if isinstance(tokenizer, str):
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, trust_remote_code=True)
        else:
            self.tokenizer = tokenizer
            
        self.config = config or {}
        
        self.cot_start = None
        self.cot_end = None
        if cot_delimiter and len(cot_delimiter) == 2:
            self.cot_start, self.cot_end = cot_delimiter
            
        self.fim_rate = fim_rate
        self.max_length = max_length
        self.in_memory = in_memory
        
        self.data_entries = []
        self.total_rows = 0
        self._build_index()
    def _get_mapping_for_path(self, file_path):
        ''' æ ¹æ®è·¯å¾„å…³é”®å­—åŒ¹é…é…ç½® '''
        path_str = str(file_path).replace('\\', '/')
        for folder_key, mapping in self.config.items():
            if folder_key in path_str:
                return mapping
        
        return {
            'system': 'system',
            'train': 'text',
            'user': 'user',
            'model': 'model',
            'reasoning': 'thought',
            'tool_calls': 'tool_calls',
            'tool_result': 'tool_output'
        }
    def _get_file_type(self, path):
        ext = path.suffix.lower()
        if ext == '.parquet': return 'parquet'
        if ext == '.jsonl': return 'jsonl'
        return None
    
    def _build_index(self):
        print(f'[SequentialDataset] Scanning {self.root_dir} (In-Memory: {self.in_memory})...')
        
        files = []
        
        if self.config:
            print(f"   - Targeted Scan: {list(self.config.keys())}")
            
            for folder_name in self.config.keys():
                target_path = self.root_dir / folder_name
                
                if not target_path.exists():
                    print(f"   âš ï¸ Warning: Config path '{folder_name}' not found under root.")
                    continue
                
                if target_path.is_dir():
                    for ext in ['*.parquet', '*.jsonl']:
                        files.extend(list(target_path.rglob(ext)))
                elif target_path.is_file():
                     files.append(target_path)
                     
        else:
            print(f"   - Full Scan (No config provided)")
            for ext in ['*.parquet', '*.jsonl']:
                files.extend(list(self.root_dir.rglob(ext)))
        
        files = sorted(list(set(files)))

        try:
            from tqdm import tqdm
            iterator = tqdm(files, desc="Indexing Files")
        except ImportError:
            iterator = files

        for file_path in iterator:
            ftype = self._get_file_type(file_path)
            mapping = self._get_mapping_for_path(file_path)
            if not mapping: continue
            
            cols = [v for v in mapping.values() if v]
            num_rows = 0
            cached_data = None

            try:
                if ftype == 'parquet':
                    if self.in_memory and cols:
                        cached_data = pd.read_parquet(file_path, columns=cols, engine='pyarrow')
                        num_rows = len(cached_data)
                    else:
                        meta = pq.read_metadata(file_path)
                        num_rows = meta.num_rows

                elif ftype == 'jsonl':
                    if self.in_memory:
                        cached_data = pd.read_json(file_path, lines=True)
                        num_rows = len(cached_data)
                    else:
                        count = 0
                        with open(file_path, 'rb') as f:
                            while True:
                                buffer = f.read(1024*1024)
                                if not buffer: break
                                count += buffer.count(b'\n')
                        num_rows = count

            except Exception as e:
                print(f"âš ï¸ Error reading {file_path}: {e}")
                continue

            if num_rows == 0: continue

            self.data_entries.append({
                'path': str(file_path),
                'type': ftype,
                'start_idx': self.total_rows,
                'end_idx': self.total_rows + num_rows,
                'mapping': mapping,
                'cache': cached_data
            })
            self.total_rows += num_rows
            
        print(f'[SequentialDataset] Indexed {len(self.data_entries)} files, {self.total_rows} rows.')

    def __len__(self):
        return self.total_rows
    def _read_row(self, entry, local_idx):
        if self.in_memory and entry['cache'] is not None:
            try: return entry['cache'].iloc[local_idx]
            except: return {}
        ftype = entry['type']
        cols = [v for v in entry['mapping'].values() if v]
        if ftype == 'parquet':
            try:
                return pd.read_parquet(entry['path'], columns=cols, engine='pyarrow').iloc[local_idx]
            except: return {}
        elif ftype == 'jsonl':
            try:
                line = linecache.getline(entry['path'], local_idx + 1)
                if line: return json.loads(line)
            except: return {}
        
        return {}
    def _create_fim_dict(self, text):
        if len(text) < 50: return None
        total_len = len(text)
        span_len = int(total_len * np.random.uniform(0.1, 0.3))
        if span_len < 5: return None
        start = np.random.randint(0, total_len - span_len)
        return {
            'role': 'fim', 'prefix': text[:start], 'middle': text[start:start+span_len], 'suffix': text[start+span_len:]
        }
    def _safe_json_load(self, content):
        if not content: return None
        if isinstance(content, (list, dict)): return content
        try: return json.loads(str(content))
        except: return None
        
    def _extract_embedded_cot(self, text):
        ''' æ ¹æ® startswith åˆ¤æ–­å¹¶æå– CoT '''
        if not self.cot_start or not text:
            return text, None
        
        processed_text = text.lstrip()
        
        if processed_text.startswith(self.cot_start):
            start_offset = len(self.cot_start)
            e_idx = processed_text.find(self.cot_end, start_offset)
            
            if e_idx != -1:
                cot_content = processed_text[start_offset : e_idx].strip()
                response_content = processed_text[e_idx + len(self.cot_end) :].strip()
                return response_content, cot_content
        
        return text, None
    
    def __getitem__(self, idx):
        entry = next(e for e in self.data_entries if e['start_idx'] <= idx < e['end_idx'])
        mapping = entry['mapping']
        
        row_data = self._read_row(entry, idx - entry['start_idx'])
        def get(key):
            col_name = mapping.get(key)
            if not col_name: return None
            
            val = None
            if hasattr(row_data, 'get'):
                val = row_data.get(col_name)
            elif hasattr(row_data, 'index') and col_name in row_data.index:
                val = row_data[col_name]
            
            if pd.notna(val) and val is not None:
                s = str(val).strip()
                return s if len(s) > 0 else None
            return None
        messages = []
        sys_text = get('system')
        if sys_text: messages.append({'role': 'system', 'content': sys_text})
        train_text = get('train')
        if train_text:
            if self.fim_rate > 0 and np.random.rand() < self.fim_rate:
                fim_msg = self._create_fim_dict(train_text)
                messages.append(fim_msg if fim_msg else {'role': 'train', 'content': train_text})
            else:
                messages.append({'role': 'train', 'content': train_text})
        user_text = get('user')
        if user_text: messages.append({'role': 'user', 'content': user_text})
        model_text = get('model')
        reasoning_text = get('reasoning')
        
        if model_text and self.cot_start:
            parsed_model, parsed_cot = self._extract_embedded_cot(model_text)
            if parsed_cot:
                model_text = parsed_model
                if not reasoning_text: reasoning_text = parsed_cot
        
        tool_calls_raw = get('tool_calls')
        if model_text or reasoning_text or tool_calls_raw:
            msg = {
                'role': 'model',
                'content': model_text if model_text else '',
                'reasoning_content': reasoning_text
            }
            if tool_calls_raw:
                parsed_tools = self._safe_json_load(tool_calls_raw)
                if parsed_tools: msg['tool_calls'] = parsed_tools
            messages.append(msg)
        tool_res = get('tool_result')
        if tool_res: messages.append({'role': 'tool', 'content': tool_res})
        if not messages:
            return {
                'input_ids': torch.tensor([], dtype=torch.long),
                'labels': torch.tensor([], dtype=torch.long)
            }
        tokenized = self.tokenizer.apply_chat_template(
            messages,
            truncation=False,
            padding=False,
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return tokenized[0]


class CachedDataset(Dataset):
    ''' ç¼“å­˜åŠ é€Ÿæ•°æ®é›† (æ”¯æŒå¢é‡æ›´æ–° & æ–­ç‚¹ç»­ä¼ )
    '''
    def __init__(
        self, 
        source_dataset, 
        cache_dir, 
        max_length=2048, 
        chunk_size=1024*1024*1024, # 1GB
        rebuild=False, 
        num_workers=8,
        save_interval=10000 # æ¯å¤„ç†å¤šå°‘æ¡ä¿å­˜ä¸€æ¬¡ Index
    ):
        self.source_dataset = source_dataset
        self.cache_dir = cache_dir
        self.max_length = max_length
        self.chunk_size = chunk_size
        self.num_workers = num_workers
        self.save_interval = save_interval
        
        self.meta_path = os.path.join(cache_dir, "meta.json")
        self.idx_path = os.path.join(cache_dir, "index.npy")
        self.data_prefix = "data_"
        if rebuild:
            print("ğŸ”„ [CachedDataset] Rebuild requested. Clearing old cache...")
            if os.path.exists(cache_dir):
                shutil.rmtree(cache_dir)
            self._build_cache(resume_from=0)
        else:
            processed_count = self._get_processed_count()
            total_count = len(self.source_dataset)
            
            if processed_count < total_count:
                print(f"ğŸ”„ [CachedDataset] Incremental Update: {processed_count} -> {total_count}")
                self._build_cache(resume_from=processed_count)
            elif processed_count > total_count:
                print(f"âš ï¸ [CachedDataset] Cache ({processed_count}) > Source ({total_count}). Source may have shrunk? Rebuilding...")
                shutil.rmtree(cache_dir)
                self._build_cache(resume_from=0)
            else:
                print(f"âœ… [CachedDataset] Cache up-to-date ({processed_count} samples).")
    
        self._load_cache()

    def _get_processed_count(self):
        ''' è·å–å½“å‰å·²ç¼“å­˜çš„æ ·æœ¬æ•°é‡ '''
        if not os.path.exists(self.idx_path) or not os.path.exists(self.meta_path):
            return 0
        try:
            indices = np.load(self.idx_path, mmap_mode='r')
            return len(indices)
        except:
            return 0
        
    def _process_sample(self, idx):
        ''' å­çº¿ç¨‹ï¼šå¤„ç†å•ä¸ªæ ·æœ¬ '''
        try:
            raw_sample = self.source_dataset[idx]
            
            ids = raw_sample
            if hasattr(raw_sample, 'ids'): ids = raw_sample.ids
            elif isinstance(raw_sample, dict) and 'input_ids' in raw_sample: ids = raw_sample['input_ids']
            
            if isinstance(ids, torch.Tensor): ids = ids.tolist()
            
            if not ids or len(ids) == 0: return None
            if len(ids) > self.max_length: ids = ids[:self.max_length]
            return np.array(ids, dtype=self.dtype).tobytes()
        except Exception as e:
            return f"Error: {str(e)[:100]}"
        
    def _build_cache(self, resume_from=0):
        ''' æ„å»º/æ›´æ–°ç¼“å­˜çš„æ ¸å¿ƒé€»è¾‘ '''
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        os.makedirs(self.cache_dir, exist_ok=True)
        
        vocab_size = getattr(getattr(self.source_dataset, 'tokenizer', None), "vocab_size", 65536)
        self.dtype = np.uint16 if vocab_size < 65535 else np.uint32
        dtype_name = np.dtype(self.dtype).name
        
        total_samples = len(self.source_dataset)
        
        indices = []
        current_file_id = 0
        current_file_ptr = 0
        total_tokens = 0
        
        if resume_from > 0:
            indices = np.load(self.idx_path).tolist()
            
            with open(self.meta_path, 'r') as f:
                meta = json.load(f)
                total_tokens = meta.get('total_tokens', 0)
            
            if indices:
                last_entry = indices[-1]
                current_file_id = int(last_entry[0])
                current_file_ptr = int(last_entry[1]) + int(last_entry[2]) * np.dtype(self.dtype).itemsize
                
                last_bin_path = os.path.join(self.cache_dir, f"{self.data_prefix}{current_file_id}.bin")
                if os.path.exists(last_bin_path):
                    real_size = os.path.getsize(last_bin_path)
                    if real_size > current_file_ptr:
                        print(f"ğŸ”§ Repairing bin file: Truncating {last_bin_path} from {real_size} to {current_file_ptr}")
                        with open(last_bin_path, "r+b") as f:
                            f.truncate(current_file_ptr)
                            
        print(f"ğŸ“¦ [CachedDataset] {'Resuming' if resume_from > 0 else 'Starting'} Cache Build...")
        print(f"   - Progress: {resume_from} -> {total_samples}")
        print(f"   - File ID:  {current_file_id} (Offset: {current_file_ptr})")
        sys.stdout.flush()
        bin_path = os.path.join(self.cache_dir, f"{self.data_prefix}{current_file_id}.bin")
        mode = "ab" if os.path.exists(bin_path) else "wb"
        f_current = open(bin_path, mode)
        
        try:
            if self.num_workers <= 1:
                iterable = map(self._process_sample, range(resume_from, total_samples))
            else:
                executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.num_workers)
                iterable = executor.map(self._process_sample, range(resume_from, total_samples), chunksize=1)

            pbar = tqdm.tqdm(iterable, total=total_samples - resume_from, desc="Caching", unit="it", ncols=100)
            
            step_count = 0
            
            for res in pbar:
                step_count += 1
                
                if res is None: continue
                if isinstance(res, str) and res.startswith("Error"): continue
                
                byte_len = len(res)
                item_len = byte_len // np.dtype(self.dtype).itemsize
                
                if (current_file_ptr + byte_len > self.chunk_size) and (current_file_ptr > 0):
                    f_current.close()
                    current_file_id += 1
                    current_file_ptr = 0
                    path = os.path.join(self.cache_dir, f"{self.data_prefix}{current_file_id}.bin")
                    f_current = open(path, "wb")
                
                f_current.write(res)
                indices.append((current_file_id, current_file_ptr, item_len))
                
                current_file_ptr += byte_len
                total_tokens += item_len
                
                if step_count % 1000 == 0:
                    pbar.set_postfix({"f": current_file_id, "tok": f"{total_tokens/1e6:.1f}M"})
                
                if step_count % self.save_interval == 0:
                    self._save_metadata(indices, total_tokens, dtype_name, current_file_id)
            
            if self.num_workers > 1:
                executor.shutdown()
        finally:
            if f_current: f_current.close()
            
        print("\nğŸ“ Saving final index...")
        self._save_metadata(indices, total_tokens, dtype_name, current_file_id)
        print(f"âœ… Build Complete! Total: {len(indices)} samples.")

    def _save_metadata(self, indices, total_tokens, dtype_name, current_file_id):
        ''' ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ® (Helper) '''
        np_indices = np.array(indices, dtype=np.uint64)
        np.save(self.idx_path, np_indices)
        
        meta = {
            "dtype": dtype_name,
            "total_samples": len(indices),
            "total_tokens": int(total_tokens),
            "max_length": self.max_length,
            "num_chunks": current_file_id + 1
        }

        temp_path = self.meta_path + ".tmp"
        with open(temp_path, "w") as f:
            json.dump(meta, f)
        os.replace(temp_path, self.meta_path)

    def _load_cache(self):
        with open(self.meta_path, "r") as f:
            self.meta = json.load(f)
        
        self.dtype = np.dtype(self.meta["dtype"])
        self.num_chunks = self.meta.get("num_chunks", 1)
        
        idx_candidates = ["index.npy", "index.bin", "index.bin.npy"]
        real_idx_path = next((os.path.join(self.cache_dir, p) for p in idx_candidates if os.path.exists(os.path.join(self.cache_dir, p))), None)
        
        if not real_idx_path: raise FileNotFoundError(f"Missing index in {self.cache_dir}")
        self.indices = np.load(real_idx_path, mmap_mode='r')
        self.num_samples = len(self.indices)
        
        self.mmaps = []
        for i in range(self.num_chunks):
            path = os.path.join(self.cache_dir, f"{self.data_prefix}{i}.bin")
            if os.path.exists(path):
                self.mmaps.append(np.memmap(path, dtype=self.dtype, mode='r'))
            else:
                self.mmaps.append(None)

    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        file_id, byte_offset, length = self.indices[idx]
        item_offset = int(byte_offset // np.dtype(self.dtype).itemsize)
        mmap = self.mmaps[int(file_id)]
        if mmap is None: return torch.tensor([], dtype=torch.long)
        
        np_array = np.array(mmap[item_offset : item_offset + int(length)], copy=True)
        return torch.from_numpy(np_array.astype(np.int64))


class AutoMixedDataset(ConcatDataset):
    ''' è‡ªåŠ¨æ··åˆæ•°æ®é›† (è§£å†³å¢é‡ç¼“å­˜é—®é¢˜)
    
    åŸç†ï¼š
    ä¸å°†æ‰€æœ‰æ•°æ®æ··åœ¨ä¸€èµ·ç¼“å­˜ï¼Œè€Œæ˜¯æ ¹æ® config çš„ key ä¸ºæ¯ä¸ªå­æ•°æ®é›†
    åˆ›å»ºç‹¬ç«‹çš„ CachedDatasetã€‚
    
    ä¼˜åŠ¿ï¼š
    1. ä¿®æ”¹ config åˆ é™¤æŸä¸ªæ•°æ®é›†æ—¶ï¼Œä¸éœ€è¦é‡å»ºå…¶ä»–æ•°æ®é›†çš„ç¼“å­˜ã€‚
    2. æ”¯æŒå•ç‹¬å¤ç”¨æŸä¸ªå­æ•°æ®é›†çš„ç¼“å­˜ã€‚
    '''
    def __init__(
        self, 
        root_dir, 
        tokenizer, 
        data_config, 
        cache_root_dir,
        cot_delimiter=None,
        fim_rate=0.5,
        in_memory=True,
        max_length=4096,
        num_workers=8
    ):
        self.datasets = []
        self.configs = []
        
        for name, sub_mapping in data_config.items():
            
            print(f"ğŸ”— [AutoMixed] Preparing subset: {name}")
            
            sub_config = {name: sub_mapping}
            
            config_str = json.dumps(sub_config, sort_keys=True)
            config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
            
            sub_cache_dir = os.path.join(cache_root_dir, f"{name}_{config_hash}")
            
            raw_dataset = SequentialDataset(
                root_dir=root_dir,
                tokenizer=tokenizer,
                config=sub_config,
                cot_delimiter=cot_delimiter,
                fim_rate=fim_rate,
                max_length=max_length,
                in_memory=in_memory
            )
            
            cached_ds = CachedDataset(
                source_dataset=raw_dataset,
                cache_dir=sub_cache_dir,
                max_length=max_length,
                num_workers=num_workers,
                save_interval=10000
            )
            
            self.datasets.append(cached_ds)
        
        super().__init__(self.datasets)


class PackedSeqDataset(IterableDataset):
    ''' æ‰“åŒ…åºåˆ—æ•°æ®é›† (Sequence Packing)
    å°† SequentialParquetDataset ä¸­å˜é•¿çš„æ ·æœ¬æ‹¼æ¥èµ·æ¥ï¼Œ
    å¹¶æŒ‰ç…§å›ºå®šçš„ max_length è¿›è¡Œåˆ‡å‰²ï¼Œä»¥å‡å°‘ Padding æµªè´¹ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚
    
    å·¥ä½œåŸç†ï¼š
    Buffer: [æ ·æœ¬A][æ ·æœ¬B][æ ·æœ¬C...]
    å¦‚æœ Buffer >= max_lengthï¼Œåˆ‡å‡ºå‰ max_length ä¸ª Token è¿”å›ï¼Œ
    å‰©ä½™éƒ¨åˆ†ä¿ç•™åœ¨ Buffer ä¸­ç­‰å¾…ä¸‹ä¸€ä¸ªæ ·æœ¬æ‹¼æ¥ã€‚
    Args:
        dataset (Dataset): åŸå§‹çš„ SequentialParquetDataset å®ä¾‹ã€‚
        max_length (int): ç›®æ ‡åºåˆ—é•¿åº¦ (context window)ã€‚
        shuffle (bool): æ˜¯å¦æ‰“ä¹±åŸå§‹æ•°æ®é›†çš„è¯»å–é¡ºåºã€‚é»˜è®¤ä¸º Trueã€‚
        seed (int): éšæœºç§å­ã€‚
    '''
    def __init__(self, dataset: SequentialDataset|CachedDataset, max_length:int, shuffle=True, seed=42):
        self.dataset = dataset
        self.max_length = max_length
        self.shuffle = shuffle
        self.seed = seed
        self.epoch = 0
    
    def set_epoch(self, epoch):
        ''' è®¾ç½®å½“å‰ Epochï¼Œç¡®ä¿æ¯è½® Shuffle é¡ºåºä¸åŒ

        Args:
            epoch (int): å½“å‰çš„ epoch æ•°ã€‚
        '''
        self.epoch = epoch
    
    def __iter__(self):
        ''' è¿­ä»£æ•°æ®é›†ï¼Œè¿”å›æ‰“åŒ…åçš„åºåˆ—

        åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸‹ï¼Œæ ¹æ® worker_id åˆ†é…æ•°æ®åˆ†ç‰‡ã€‚
        å¦‚æœå¯ç”¨ shuffleï¼Œä¼šåœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶åŸºäº seed + epoch æ‰“ä¹±ç´¢å¼•ã€‚
        å°†å˜é•¿æ ·æœ¬æ‹¼æ¥åˆ° buffer ä¸­ï¼Œå½“ buffer é•¿åº¦è¾¾åˆ° max_length æ—¶åˆ‡ç‰‡è¿”å›ã€‚

        Yields:
            torch.Tensor: å½¢çŠ¶ä¸º (max_length,) çš„é•¿æ•´å‹å¼ é‡ (input_ids)ã€‚
        '''
        worker_info = torch.utils.data.get_worker_info()
        
        num_samples = len(self.dataset)
        indices = list(range(num_samples))
        
        if self.shuffle:
            rng = random.Random(self.seed + self.epoch)
            rng.shuffle(indices)
        
        if worker_info is not None:
            per_worker = int(np.ceil(num_samples / float(worker_info.num_workers)))
            worker_id = worker_info.id
            iter_start = worker_id * per_worker
            iter_end = min(iter_start + per_worker, num_samples)
            indices = indices[iter_start:iter_end]
            
        buffer_ids = []
        
        for idx in indices:
            new_ids = self.dataset[idx]
            
            if len(new_ids) == 0: continue
            
            buffer_ids.extend(new_ids)
            
            while len(buffer_ids) >= self.max_length:
                chunk = buffer_ids[:self.max_length]
                buffer_ids = buffer_ids[self.max_length:]
                input_ids = torch.tensor(chunk, dtype=torch.long)
                
                yield input_ids