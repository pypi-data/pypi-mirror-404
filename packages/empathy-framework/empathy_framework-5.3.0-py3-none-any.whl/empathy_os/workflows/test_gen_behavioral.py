"""Behavioral Test Generation Workflow.

Automatically generates behavioral test templates for modules to improve coverage.
This is a meta-workflow that helps developers achieve high test coverage systematically.

Usage:
    from empathy_os.workflows import BehavioralTestGenerationWorkflow

    workflow = BehavioralTestGenerationWorkflow()
    result = await workflow.execute(
        module_path="src/empathy_os/config.py",
        output_dir="tests/behavioral/generated"
    )

    # Or batch mode:
    result = await workflow.execute(
        batch=True,
        top_n=50,
        min_lines=50
    )

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import ast
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any

from ..workflows.base import BaseWorkflow, ModelTier, WorkflowStage
from ..workflows.llm_base import LLMWorkflowGenerator


@dataclass
class FunctionInfo:
    """Information about a function to test."""

    name: str
    is_async: bool
    args: list[str]
    returns: str | None
    docstring: str | None
    line_number: int


@dataclass
class ClassInfo:
    """Information about a class to test."""

    name: str
    methods: list[dict]  # List of FunctionInfo dicts
    is_abstract: bool
    bases: list[str]
    line_number: int


@dataclass
class ModuleInfo:
    """Information about a module to test."""

    file_path: str
    classes: list[dict]  # List of ClassInfo dicts
    functions: list[dict]  # List of FunctionInfo dicts
    imports: list[str]
    total_lines: int


class BehavioralTestLLMGenerator(LLMWorkflowGenerator):
    """LLM-enhanced behavioral test generator.

    Generates comprehensive, runnable tests instead of placeholders.
    Falls back to template generation if LLM fails.
    """

    def __init__(self, model_tier: str = "capable", use_llm: bool = True):
        """Initialize generator.

        Args:
            model_tier: Model tier for LLM (cheap, capable, premium)
            use_llm: Whether to use LLM (if False, always use templates)
        """
        super().__init__(model_tier=model_tier, enable_cache=True)
        self.use_llm = use_llm

    def generate_test_file(
        self, module_info: ModuleInfo, output_path: Path, source_code: str
    ) -> str:
        """Generate complete test file.

        Args:
            module_info: Module analysis information
            output_path: Path where test file will be saved
            source_code: Full source code of module

        Returns:
            Complete test file content
        """
        if not self.use_llm:
            # Skip LLM, go straight to template
            return self._generate_template(module_info, output_path)

        # Use LLM generation via base class
        context = {
            "module_info": asdict(module_info),
            "output_path": str(output_path),
            "source_code": source_code,
        }

        prompt = self._build_prompt(module_info, source_code)

        return self.generate(context, prompt)

    def _build_prompt(self, module_info: ModuleInfo, source_code: str) -> str:
        """Build LLM prompt for test generation.

        Args:
            module_info: Module analysis
            source_code: Source code

        Returns:
            Formatted prompt for LLM
        """
        module_name = Path(module_info.file_path).stem
        import_path = (
            module_info.file_path.replace("src/", "").replace(".py", "").replace("/", ".")
        )

        # Build context about what to test
        classes_info = "\n".join(
            [
                f"  - {cls['name']}: {len(cls['methods'])} methods"
                for cls in module_info.classes
            ]
        )
        functions_info = "\n".join(
            [f"  - {func['name']}()" for func in module_info.functions[:10]]
        )

        prompt = f"""Generate comprehensive behavioral tests for this Python module.

MODULE: {module_info.file_path}
IMPORT PATH: {import_path}

CLASSES:
{classes_info if classes_info else "  (none)"}

FUNCTIONS:
{functions_info if functions_info else "  (none)"}

SOURCE CODE (first 3000 chars):
```python
{source_code[:3000]}{"..." if len(source_code) > 3000 else ""}
```

Generate a complete, runnable test file that:
1. Uses Given/When/Then behavioral test structure
2. Tests all public classes and functions identified above
3. Includes edge cases, error handling, and success paths
4. Uses proper mocking for external dependencies (APIs, databases, file I/O)
5. Includes pytest fixtures where appropriate
6. Has descriptive test names and docstrings
7. Targets 80%+ code coverage

Requirements:
- Import from {import_path} (not from src/)
- Use pytest conventions (test_ prefix, assert statements)
- Mock external dependencies appropriately
- Include both positive and negative test cases
- Add docstrings explaining what each test verifies

Start with this copyright header:
\"\"\"Behavioral tests for {module_name}.

Generated by LLM-enhanced test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
\"\"\"

Return ONLY the complete Python test file content, no explanations."""

        return prompt

    def _generate_with_template(self, context: dict[str, Any]) -> str:
        """Fallback template generation.

        Args:
            context: Must contain 'module_info', 'output_path', 'source_code'

        Returns:
            Template test file
        """
        module_info_dict = context["module_info"]
        output_path = Path(context["output_path"])

        # Reconstruct ModuleInfo from dict
        module_info = ModuleInfo(**module_info_dict)

        return self._generate_template(module_info, output_path)

    def _generate_template(self, module_info: ModuleInfo, output_path: Path) -> str:
        """Generate template (old method, used as fallback).

        Args:
            module_info: Module analysis
            output_path: Output file path

        Returns:
            Template test file content
        """
        module_name = Path(module_info.file_path).stem
        import_path = (
            module_info.file_path.replace("src/", "").replace(".py", "").replace("/", ".")
        )

        lines = [
            f'"""Behavioral tests for {module_name}.py - GENERATED BY EMPATHY FRAMEWORK.',
            "",
            "Generated by: empathy workflow run test-gen-behavioral",
            "Framework: https://github.com/Smart-AI-Memory/empathy-framework",
            "",
            "INSTRUCTIONS:",
            "1. Review and customize tests below",
            "2. Add proper test data and assertions",
            "3. Test success AND error paths",
            f"4. Run: pytest {output_path} --cov={module_info.file_path} -v",
            '"""',
            "",
            "import pytest",
            "from unittest.mock import Mock, AsyncMock, patch",
            "",
            f"from {import_path} import ...",  # User fills this in
            "",
        ]

        # Add test templates
        for class_dict in module_info.classes:
            lines.append(f"class Test{class_dict['name']}:")
            lines.append(f'    """Tests for {class_dict["name"]}."""')
            lines.append("")
            lines.append(f"    def test_{class_dict['name'].lower()}_init(self):")
            lines.append('        """Test initialization."""')
            lines.append("        pass  # TODO: Implement")
            lines.append("")

        lines.append(
            "# Generated by Empathy Framework - https://github.com/Smart-AI-Memory/empathy-framework"
        )

        return "\n".join(lines)

    def _validate(self, result: str) -> bool:
        """Validate test file has proper structure.

        Args:
            result: Generated test file content

        Returns:
            True if valid
        """
        # Check for basic test file structure
        required = ["import pytest", "def test_", '"""']
        has_required = all(req in result for req in required)

        # Check minimum length
        long_enough = len(result) > 200

        # Check no TODOs (means it's a real implementation, not template)
        no_todos = "# TODO:" not in result and "pass  # TODO" not in result

        return has_required and long_enough and no_todos


class ModuleAnalyzer(ast.NodeVisitor):
    """Analyze Python module to extract testable elements."""

    def __init__(self, source_code: str):
        self.source_code = source_code
        self.classes: list[ClassInfo] = []
        self.functions: list[FunctionInfo] = []
        self.imports: list[str] = []

    def visit_ClassDef(self, node: ast.ClassDef):
        """Extract class information."""
        methods = []
        is_abstract = any(
            isinstance(base, ast.Name) and base.id in ("ABC", "ABCMeta") for base in node.bases
        )

        for item in node.body:
            if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                method_info = self._extract_function_info(item)
                methods.append(asdict(method_info))

        class_info = ClassInfo(
            name=node.name,
            methods=methods,
            is_abstract=is_abstract,
            bases=[self._get_base_name(base) for base in node.bases],
            line_number=node.lineno,
        )
        self.classes.append(class_info)
        self.generic_visit(node)

    def visit_FunctionDef(self, node: ast.FunctionDef):
        """Extract top-level function information."""
        func_info = self._extract_function_info(node)
        self.functions.append(func_info)
        self.generic_visit(node)

    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):
        """Extract async function information."""
        func_info = self._extract_function_info(node, is_async=True)
        self.functions.append(func_info)
        self.generic_visit(node)

    def _extract_function_info(
        self, node: ast.FunctionDef | ast.AsyncFunctionDef, is_async: bool = False
    ) -> FunctionInfo:
        """Extract information from function node."""
        args = [arg.arg for arg in node.args.args if arg.arg != "self"]
        returns = ast.unparse(node.returns) if node.returns else None
        docstring = ast.get_docstring(node)
        is_async = isinstance(node, ast.AsyncFunctionDef) or is_async

        return FunctionInfo(
            name=node.name,
            is_async=is_async,
            args=args,
            returns=returns,
            docstring=docstring,
            line_number=node.lineno,
        )

    def _get_base_name(self, base: ast.expr) -> str:
        """Get base class name."""
        return base.id if isinstance(base, ast.Name) else ast.unparse(base)


class BehavioralTestGenerationWorkflow(BaseWorkflow):
    """Workflow to generate behavioral test templates automatically.

    Now enhanced with LLM generation for comprehensive, runnable tests!
    """

    def __init__(self, use_llm: bool = True, model_tier: str = "capable"):
        """Initialize workflow.

        Args:
            use_llm: Whether to use LLM generation (default: True)
            model_tier: Model tier for LLM (cheap, capable, premium)
        """
        super().__init__(
            name="behavioral-test-generation",
            description="Generate behavioral test templates for modules",
            stages={
                "analyze": WorkflowStage(
                    name="analyze",
                    description="Analyze module structure",
                    tier_hint=ModelTier.CHEAP,
                    system_prompt="Analyze Python module structure",
                    task_type="analysis",
                ),
                "generate": WorkflowStage(
                    name="generate",
                    description="Generate comprehensive behavioral tests",
                    tier_hint=ModelTier.CAPABLE,
                    system_prompt="Generate comprehensive behavioral tests using LLM",
                    task_type="code_generation",
                ),
            },
        )
        self.llm_generator = BehavioralTestLLMGenerator(
            model_tier=model_tier, use_llm=use_llm
        )

    def analyze_module(self, file_path: Path) -> ModuleInfo:
        """Analyze a Python module."""
        source_code = file_path.read_text()
        tree = ast.parse(source_code)

        analyzer = ModuleAnalyzer(source_code)
        analyzer.visit(tree)

        return ModuleInfo(
            file_path=str(file_path),
            classes=[asdict(c) for c in analyzer.classes],
            functions=[asdict(f) for c in analyzer.functions],
            imports=analyzer.imports,
            total_lines=len(source_code.splitlines()),
        )

    def generate_test_template(
        self, module_info: ModuleInfo, output_path: Path, source_code: str = ""
    ) -> str:
        """Generate comprehensive behavioral tests using LLM.

        Args:
            module_info: Module analysis information
            output_path: Path where test file will be saved
            source_code: Full source code of module (optional, will be read if not provided)

        Returns:
            Complete test file content (runnable tests, not placeholders!)
        """
        # Read source code if not provided
        if not source_code:
            source_code = Path(module_info.file_path).read_text()

        # Use LLM generator to create comprehensive tests
        return self.llm_generator.generate_test_file(
            module_info=module_info, output_path=output_path, source_code=source_code
        )

    async def execute(
        self,
        module_path: str | None = None,
        batch: bool = False,
        top_n: int = 50,
        output_dir: str = "tests/behavioral/generated",
        **kwargs,
    ) -> dict[str, Any]:
        """Execute behavioral test generation.

        Args:
            module_path: Path to module to generate tests for
            batch: If True, generate tests for multiple low-coverage modules
            top_n: Number of modules to process in batch mode
            output_dir: Where to save generated tests

        Returns:
            Results with paths to generated files
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)

        generated_files = []

        if batch:
            # Batch mode: find low-coverage modules
            # TODO: Implement batch processing
            pass
        elif module_path:
            # Single module mode
            src_path = Path(module_path)
            source_code = src_path.read_text()
            module_info = self.analyze_module(src_path)

            test_filename = f"test_{src_path.stem}_behavioral.py"
            test_path = output_path / test_filename

            # Generate comprehensive tests with LLM
            template = self.generate_test_template(
                module_info, test_path, source_code=source_code
            )
            test_path.write_text(template)

            generated_files.append(str(test_path))

            # Print stats
            stats = self.llm_generator.get_stats()
            print("\nðŸ“Š Generation Stats:")
            print(f"  LLM Success Rate: {stats['llm_success_rate']:.1%}")
            print(f"  Template Fallback Rate: {stats['template_fallback_rate']:.1%}")
            print(f"  Cache Hit Rate: {stats['cache_hit_rate']:.1%}")
            print(f"  Estimated Cost: ${stats['total_cost_usd']:.4f}")

        return {
            "generated_files": generated_files,
            "count": len(generated_files),
            "output_dir": str(output_path),
        }


# Export for discovery
__all__ = ["BehavioralTestGenerationWorkflow"]
