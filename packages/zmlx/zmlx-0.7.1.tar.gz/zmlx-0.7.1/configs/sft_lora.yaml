# ZMLX SFT with LoRA
# Usage: zmlx train --config configs/sft_lora.yaml

model: "mlx-community/Llama-3.2-1B-Instruct"
dataset: "mlx-community/WikiSQL"

# LoRA
lora: true
lora_rank: 8
lora_alpha: 16.0
lora_dropout: 0.0
lora_target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj

# Training
iters: 1000
batch_size: 4
learning_rate: 1.0e-4
lr_schedule: cosine
warmup_steps: 100
max_seq_length: 2048

# Evaluation
eval_interval: 100
save_interval: 500
output_dir: adapters/sft_lora

# ZMLX
patch: true
patch_compute_dtype: float32
patch_threadgroup: 256
