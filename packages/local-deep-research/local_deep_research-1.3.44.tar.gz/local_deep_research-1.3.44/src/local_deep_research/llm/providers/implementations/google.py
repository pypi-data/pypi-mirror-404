"""Google/Gemini LLM provider for Local Deep Research."""

from loguru import logger

from ...llm_registry import register_llm
from ..openai_base import OpenAICompatibleProvider


class GoogleProvider(OpenAICompatibleProvider):
    """Google Gemini provider using OpenAI-compatible endpoint.

    This uses Google's OpenAI-compatible API endpoint to access Gemini models,
    which automatically supports all current and future Gemini models without
    needing to update the code.
    """

    provider_name = "Google Gemini"
    api_key_setting = "llm.google.api_key"
    default_base_url = "https://generativelanguage.googleapis.com/v1beta/openai"
    default_model = "gemini-1.5-flash"

    # Metadata for auto-discovery
    provider_key = "GOOGLE"
    company_name = "Google"
    region = "US"
    country = "United States"
    data_location = "Worldwide"  # Google has data centers worldwide
    is_cloud = True

    @classmethod
    def requires_auth_for_models(cls):
        """Google requires authentication for listing models.

        Note: Google's OpenAI-compatible /models endpoint has a bug (returns 401).
        The native Gemini API endpoint requires an API key.
        """
        return True

    @classmethod
    def list_models_for_api(cls, api_key=None, base_url=None):
        """List available models using Google's native API.

        Args:
            api_key: Google API key
            base_url: Not used - Google uses a fixed endpoint

        Google's OpenAI-compatible /models endpoint returns 401 (bug),
        so we use the native Gemini API endpoint instead.
        """
        if not api_key:
            logger.debug("Google Gemini requires API key for listing models")
            return []

        try:
            from ....security import safe_get

            # Use the native Gemini API endpoint (not OpenAI-compatible)
            # Note: Google's API requires the key as a query parameter, not in headers
            # This is their documented approach: https://ai.google.dev/api/rest
            url = f"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}"

            response = safe_get(url, timeout=10)

            if response.status_code == 200:
                data = response.json()
                models = []

                for model in data.get("models", []):
                    model_name = model.get("name", "")
                    # Extract just the model ID from "models/gemini-1.5-flash"
                    if model_name.startswith("models/"):
                        model_id = model_name[7:]  # Remove "models/" prefix
                    else:
                        model_id = model_name

                    # Only include generative models (not embedding models)
                    supported_methods = model.get(
                        "supportedGenerationMethods", []
                    )
                    if "generateContent" in supported_methods and model_id:
                        models.append(
                            {
                                "value": model_id,
                                "label": model_id,
                            }
                        )

                logger.info(
                    f"Found {len(models)} generative models from Google Gemini API"
                )
                return models
            else:
                logger.warning(
                    f"Google Gemini API returned status {response.status_code}"
                )
                return []

        except Exception:
            logger.exception("Error fetching Google Gemini models")
            return []


# Keep the standalone functions for backward compatibility and registration
def create_google_llm(model_name=None, temperature=0.7, **kwargs):
    """Factory function for Google/Gemini LLMs using OpenAI-compatible endpoint.

    Args:
        model_name: Name of the model to use (e.g., "gemini-1.5-flash", "gemini-2.0-flash-exp", etc.)
        temperature: Model temperature (0.0-1.0)
        **kwargs: Additional arguments including settings_snapshot

    Returns:
        A configured ChatOpenAI instance pointing to Google's API

    Raises:
        ValueError: If Google API key is not configured
    """
    return GoogleProvider.create_llm(model_name, temperature, **kwargs)


def is_google_available(settings_snapshot=None):
    """Check if Google/Gemini is available.

    Args:
        settings_snapshot: Optional settings snapshot to use

    Returns:
        True if Google API key is configured, False otherwise
    """
    return GoogleProvider.is_available(settings_snapshot)


def register_google_provider():
    """Register the Google/Gemini provider with the LLM registry."""
    register_llm("google", create_google_llm)
    logger.info("Registered Google/Gemini LLM provider")
