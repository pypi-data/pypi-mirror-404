"""Tests for IONOS AI Model Hub LLM provider."""

import pytest
from unittest.mock import Mock, patch

from local_deep_research.llm.providers.implementations.ionos import (
    IONOSProvider,
    create_ionos_llm,
    is_ionos_available,
    register_ionos_provider,
)


class TestIONOSProviderMetadata:
    """Tests for IONOSProvider class metadata."""

    def test_provider_name(self):
        """Provider name is correct."""
        assert IONOSProvider.provider_name == "IONOS AI Model Hub"

    def test_provider_key(self):
        """Provider key is correct."""
        assert IONOSProvider.provider_key == "IONOS"

    def test_is_cloud(self):
        """IONOS is a cloud provider."""
        assert IONOSProvider.is_cloud is True

    def test_region(self):
        """Region is EU."""
        assert IONOSProvider.region == "EU"

    def test_country(self):
        """Country is Germany."""
        assert IONOSProvider.country == "Germany"

    def test_data_location(self):
        """Data location is Germany."""
        assert IONOSProvider.data_location == "Germany"

    def test_company_name(self):
        """Company name is IONOS."""
        assert IONOSProvider.company_name == "IONOS"

    def test_gdpr_compliant(self):
        """IONOS is GDPR compliant."""
        assert IONOSProvider.gdpr_compliant is True

    def test_api_key_setting(self):
        """API key setting is correct."""
        assert IONOSProvider.api_key_setting == "llm.ionos.api_key"

    def test_default_model(self):
        """Default model is set to open model."""
        assert IONOSProvider.default_model is not None
        assert "llama" in IONOSProvider.default_model.lower()

    def test_default_base_url(self):
        """Default base URL is correct."""
        assert "ionos.com" in IONOSProvider.default_base_url


class TestIONOSCreateLLM:
    """Tests for create_llm method."""

    def test_create_llm_raises_without_api_key(self):
        """Raises ValueError when API key not configured."""
        with patch(
            "local_deep_research.llm.providers.openai_base.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = None

            with pytest.raises(ValueError) as exc_info:
                IONOSProvider.create_llm()

            assert "api key" in str(exc_info.value).lower()

    def test_create_llm_with_valid_api_key(self):
        """Successfully creates ChatOpenAI instance with valid API key."""

        def mock_get_setting_side_effect(key, default=None, *args, **kwargs):
            settings_map = {
                "llm.ionos.api_key": "test-ionos-key",
                "llm.max_tokens": None,
                "llm.streaming": None,
                "llm.max_retries": None,
                "llm.request_timeout": None,
            }
            return settings_map.get(key, default)

        with patch(
            "local_deep_research.llm.providers.openai_base.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.side_effect = mock_get_setting_side_effect

            with patch(
                "local_deep_research.llm.providers.openai_base.ChatOpenAI"
            ) as mock_chat:
                mock_llm = Mock()
                mock_chat.return_value = mock_llm

                result = IONOSProvider.create_llm()

                assert result is mock_llm
                mock_chat.assert_called_once()

    def test_create_llm_uses_default_model_when_none(self):
        """Uses default model when none specified."""

        def mock_get_setting_side_effect(key, default=None, *args, **kwargs):
            settings_map = {
                "llm.ionos.api_key": "test-key",
                "llm.max_tokens": None,
                "llm.streaming": None,
                "llm.max_retries": None,
                "llm.request_timeout": None,
            }
            return settings_map.get(key, default)

        with patch(
            "local_deep_research.llm.providers.openai_base.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.side_effect = mock_get_setting_side_effect

            with patch(
                "local_deep_research.llm.providers.openai_base.ChatOpenAI"
            ) as mock_chat:
                IONOSProvider.create_llm()

                call_kwargs = mock_chat.call_args[1]
                assert call_kwargs["model"] == IONOSProvider.default_model

    def test_create_llm_with_custom_model(self):
        """Uses custom model when specified."""

        def mock_get_setting_side_effect(key, default=None, *args, **kwargs):
            settings_map = {
                "llm.ionos.api_key": "test-key",
                "llm.max_tokens": None,
                "llm.streaming": None,
                "llm.max_retries": None,
                "llm.request_timeout": None,
            }
            return settings_map.get(key, default)

        with patch(
            "local_deep_research.llm.providers.openai_base.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.side_effect = mock_get_setting_side_effect

            with patch(
                "local_deep_research.llm.providers.openai_base.ChatOpenAI"
            ) as mock_chat:
                IONOSProvider.create_llm(
                    model_name="meta-llama/llama-3.2-70b-instruct"
                )

                call_kwargs = mock_chat.call_args[1]
                assert (
                    call_kwargs["model"] == "meta-llama/llama-3.2-70b-instruct"
                )

    def test_create_llm_passes_temperature(self):
        """Passes temperature parameter."""

        def mock_get_setting_side_effect(key, default=None, *args, **kwargs):
            settings_map = {
                "llm.ionos.api_key": "test-key",
                "llm.max_tokens": None,
                "llm.streaming": None,
                "llm.max_retries": None,
                "llm.request_timeout": None,
            }
            return settings_map.get(key, default)

        with patch(
            "local_deep_research.llm.providers.openai_base.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.side_effect = mock_get_setting_side_effect

            with patch(
                "local_deep_research.llm.providers.openai_base.ChatOpenAI"
            ) as mock_chat:
                IONOSProvider.create_llm(temperature=0.3)

                call_kwargs = mock_chat.call_args[1]
                assert call_kwargs["temperature"] == 0.3

    def test_create_llm_uses_ionos_base_url(self):
        """Uses IONOS's base URL."""

        def mock_get_setting_side_effect(key, default=None, *args, **kwargs):
            settings_map = {
                "llm.ionos.api_key": "test-key",
                "llm.max_tokens": None,
                "llm.streaming": None,
                "llm.max_retries": None,
                "llm.request_timeout": None,
            }
            return settings_map.get(key, default)

        with patch(
            "local_deep_research.llm.providers.openai_base.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.side_effect = mock_get_setting_side_effect

            with patch(
                "local_deep_research.llm.providers.openai_base.ChatOpenAI"
            ) as mock_chat:
                IONOSProvider.create_llm()

                call_kwargs = mock_chat.call_args[1]
                assert "ionos.com" in call_kwargs["base_url"]


class TestIONOSIsAvailable:
    """Tests for is_available method."""

    def test_is_available_true_when_key_exists(self):
        """Returns True when API key is configured."""
        with patch(
            "local_deep_research.llm.providers.openai_base.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = "test-key"

            result = IONOSProvider.is_available()
            assert result is True

    def test_is_available_false_when_no_key(self):
        """Returns False when API key is not configured."""
        with patch(
            "local_deep_research.llm.providers.openai_base.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = None

            result = IONOSProvider.is_available()
            assert result is False

    def test_is_available_false_when_empty_key(self):
        """Returns False when API key is empty string."""
        with patch(
            "local_deep_research.llm.providers.openai_base.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = ""

            result = IONOSProvider.is_available()
            assert result is False


class TestIONOSRequiresAuth:
    """Tests for requires_auth_for_models method."""

    def test_requires_auth_for_models(self):
        """IONOS requires authentication for listing models."""
        assert IONOSProvider.requires_auth_for_models() is True


class TestIONOSBackwardCompatibility:
    """Tests for backward compatibility functions."""

    def test_create_ionos_llm_function(self):
        """create_ionos_llm() delegates to IONOSProvider."""
        with patch.object(IONOSProvider, "create_llm") as mock_create:
            mock_llm = Mock()
            mock_create.return_value = mock_llm

            result = create_ionos_llm(model_name="test-model", temperature=0.5)

            mock_create.assert_called_once_with("test-model", 0.5)
            assert result is mock_llm

    def test_is_ionos_available_function(self):
        """is_ionos_available() delegates to IONOSProvider."""
        with patch.object(IONOSProvider, "is_available") as mock_available:
            mock_available.return_value = True

            result = is_ionos_available()

            mock_available.assert_called_once()
            assert result is True

    def test_register_ionos_provider_function(self):
        """register_ionos_provider() registers with registry."""
        with patch(
            "local_deep_research.llm.providers.implementations.ionos.register_llm"
        ) as mock_register:
            register_ionos_provider()

            mock_register.assert_called_once_with("ionos", create_ionos_llm)
