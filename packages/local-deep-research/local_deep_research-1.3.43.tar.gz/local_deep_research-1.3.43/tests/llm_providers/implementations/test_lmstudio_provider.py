"""Tests for LM Studio LLM provider."""

from unittest.mock import Mock, patch

from local_deep_research.llm.providers.implementations.lmstudio import (
    LMStudioProvider,
    create_lmstudio_llm,
    is_lmstudio_available,
    register_lmstudio_provider,
)


class TestLMStudioProviderMetadata:
    """Tests for LMStudioProvider class metadata."""

    def test_provider_name(self):
        """Provider name is correct."""
        assert LMStudioProvider.provider_name == "LM Studio"

    def test_provider_key(self):
        """Provider key is correct."""
        assert LMStudioProvider.provider_key == "LMSTUDIO"

    def test_is_not_cloud(self):
        """LM Studio is a local provider."""
        assert LMStudioProvider.is_cloud is False

    def test_region_is_local(self):
        """Region is Local."""
        assert LMStudioProvider.region == "Local"

    def test_country_is_local(self):
        """Country is Local."""
        assert LMStudioProvider.country == "Local"

    def test_data_location_is_local(self):
        """Data location is Local."""
        assert LMStudioProvider.data_location == "Local"

    def test_company_name(self):
        """Company name is LM Studio."""
        assert LMStudioProvider.company_name == "LM Studio"

    def test_api_key_setting_is_none(self):
        """LM Studio doesn't require API key."""
        assert LMStudioProvider.api_key_setting is None

    def test_url_setting(self):
        """URL setting is defined."""
        assert LMStudioProvider.url_setting == "llm.lmstudio.url"

    def test_default_model(self):
        """Default model is set."""
        assert LMStudioProvider.default_model is not None
        assert LMStudioProvider.default_model == "local-model"

    def test_default_base_url(self):
        """Default base URL is localhost."""
        assert "localhost" in LMStudioProvider.default_base_url
        assert "1234" in LMStudioProvider.default_base_url


class TestLMStudioCreateLLM:
    """Tests for create_llm method."""

    def test_create_llm_success(self):
        """Successfully creates ChatOpenAI instance."""
        with patch(
            "local_deep_research.config.thread_settings.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = "http://localhost:1234/v1"

            with patch(
                "local_deep_research.llm.providers.openai_base.ChatOpenAI"
            ) as mock_chat:
                mock_llm = Mock()
                mock_chat.return_value = mock_llm

                result = LMStudioProvider.create_llm()

                assert result is mock_llm
                mock_chat.assert_called_once()

    def test_create_llm_uses_default_url(self):
        """Uses default URL when not configured."""
        with patch(
            "local_deep_research.config.thread_settings.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = LMStudioProvider.default_base_url

            with patch(
                "local_deep_research.llm.providers.openai_base.ChatOpenAI"
            ) as mock_chat:
                LMStudioProvider.create_llm()

                call_kwargs = mock_chat.call_args[1]
                assert "localhost" in call_kwargs["base_url"]
                assert "1234" in call_kwargs["base_url"]

    def test_create_llm_uses_custom_url(self):
        """Uses custom URL from settings."""
        with patch(
            "local_deep_research.config.thread_settings.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = "http://custom:5000/v1"

            with patch(
                "local_deep_research.llm.providers.openai_base.ChatOpenAI"
            ) as mock_chat:
                LMStudioProvider.create_llm()

                call_kwargs = mock_chat.call_args[1]
                assert "custom" in call_kwargs["base_url"]
                assert "5000" in call_kwargs["base_url"]

    def test_create_llm_uses_placeholder_api_key(self):
        """Uses placeholder API key for LM Studio."""
        with patch(
            "local_deep_research.config.thread_settings.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = "http://localhost:1234/v1"

            with patch(
                "local_deep_research.llm.providers.openai_base.ChatOpenAI"
            ) as mock_chat:
                LMStudioProvider.create_llm()

                call_kwargs = mock_chat.call_args[1]
                # Should use a placeholder key like "not-required"
                assert call_kwargs["api_key"] == "not-required"

    def test_create_llm_with_custom_model(self):
        """Uses custom model when specified."""
        with patch(
            "local_deep_research.config.thread_settings.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = "http://localhost:1234/v1"

            with patch(
                "local_deep_research.llm.providers.openai_base.ChatOpenAI"
            ) as mock_chat:
                LMStudioProvider.create_llm(model_name="my-local-model")

                call_kwargs = mock_chat.call_args[1]
                assert call_kwargs["model"] == "my-local-model"

    def test_create_llm_with_custom_temperature(self):
        """Uses custom temperature."""
        with patch(
            "local_deep_research.config.thread_settings.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = "http://localhost:1234/v1"

            with patch(
                "local_deep_research.llm.providers.openai_base.ChatOpenAI"
            ) as mock_chat:
                LMStudioProvider.create_llm(temperature=0.3)

                call_kwargs = mock_chat.call_args[1]
                assert call_kwargs["temperature"] == 0.3


class TestLMStudioIsAvailable:
    """Tests for is_available method."""

    def test_is_available_true_when_server_responds(self):
        """Returns True when LM Studio server responds."""
        with patch(
            "local_deep_research.config.thread_settings.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = "http://localhost:1234/v1"

            with patch("local_deep_research.security.safe_get") as mock_get:
                mock_response = Mock()
                mock_response.status_code = 200
                mock_get.return_value = mock_response

                result = LMStudioProvider.is_available()
                assert result is True

    def test_is_available_false_when_server_error(self):
        """Returns False when server returns error."""
        with patch(
            "local_deep_research.config.thread_settings.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = "http://localhost:1234/v1"

            with patch("local_deep_research.security.safe_get") as mock_get:
                mock_response = Mock()
                mock_response.status_code = 500
                mock_get.return_value = mock_response

                result = LMStudioProvider.is_available()
                assert result is False

    def test_is_available_false_on_connection_error(self):
        """Returns False when connection fails."""
        with patch(
            "local_deep_research.config.thread_settings.get_setting_from_snapshot"
        ) as mock_get_setting:
            mock_get_setting.return_value = "http://localhost:1234/v1"

            with patch("local_deep_research.security.safe_get") as mock_get:
                mock_get.side_effect = Exception("Connection refused")

                result = LMStudioProvider.is_available()
                assert result is False


class TestLMStudioRequiresAuth:
    """Tests for requires_auth_for_models method."""

    def test_does_not_require_auth_for_models(self):
        """LM Studio doesn't require authentication for listing models."""
        assert LMStudioProvider.requires_auth_for_models() is False


class TestLMStudioBackwardCompatibility:
    """Tests for backward compatibility functions."""

    def test_create_lmstudio_llm_function(self):
        """create_lmstudio_llm() delegates to LMStudioProvider."""
        with patch.object(LMStudioProvider, "create_llm") as mock_create:
            mock_llm = Mock()
            mock_create.return_value = mock_llm

            result = create_lmstudio_llm(model_name="test", temperature=0.5)

            mock_create.assert_called_once_with("test", 0.5)
            assert result is mock_llm

    def test_is_lmstudio_available_function(self):
        """is_lmstudio_available() delegates to LMStudioProvider."""
        with patch.object(LMStudioProvider, "is_available") as mock_available:
            mock_available.return_value = True

            result = is_lmstudio_available()

            mock_available.assert_called_once()
            assert result is True

    def test_register_lmstudio_provider_function(self):
        """register_lmstudio_provider() registers with registry."""
        with patch(
            "local_deep_research.llm.providers.implementations.lmstudio.register_llm"
        ) as mock_register:
            register_lmstudio_provider()

            mock_register.assert_called_once_with(
                "lmstudio", create_lmstudio_llm
            )
