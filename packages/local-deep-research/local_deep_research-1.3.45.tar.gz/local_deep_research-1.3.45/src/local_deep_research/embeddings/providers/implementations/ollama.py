"""Ollama embedding provider."""

from typing import Any, Dict, List, Optional

from langchain_community.embeddings import OllamaEmbeddings
from langchain_core.embeddings import Embeddings
from loguru import logger

from ....config.thread_settings import get_setting_from_snapshot
from ....utilities.llm_utils import get_ollama_base_url
from ..base import BaseEmbeddingProvider
from ....security import safe_get


class OllamaEmbeddingsProvider(BaseEmbeddingProvider):
    """
    Ollama embedding provider.

    Uses Ollama API for local embedding models.
    No API key required, runs locally.
    """

    provider_name = "Ollama"
    provider_key = "OLLAMA"
    requires_api_key = False
    supports_local = True
    default_model = "nomic-embed-text"

    @classmethod
    def create_embeddings(
        cls,
        model: Optional[str] = None,
        settings_snapshot: Optional[Dict[str, Any]] = None,
        **kwargs,
    ) -> Embeddings:
        """
        Create Ollama embeddings instance.

        Args:
            model: Model name (defaults to nomic-embed-text)
            settings_snapshot: Optional settings snapshot
            **kwargs: Additional parameters (base_url, etc.)

        Returns:
            OllamaEmbeddings instance
        """
        # Get model from settings if not specified
        if model is None:
            model = get_setting_from_snapshot(
                "embeddings.ollama.model",
                default=cls.default_model,
                settings_snapshot=settings_snapshot,
            )

        # Get Ollama URL
        base_url = kwargs.get("base_url")
        if base_url is None:
            base_url = get_ollama_base_url(settings_snapshot)

        logger.info(
            f"Creating OllamaEmbeddings with model={model}, base_url={base_url}"
        )

        return OllamaEmbeddings(
            model=model,
            base_url=base_url,
        )

    @classmethod
    def is_available(
        cls, settings_snapshot: Optional[Dict[str, Any]] = None
    ) -> bool:
        """Check if Ollama is available."""
        try:
            import requests

            # Get Ollama URL
            base_url = get_ollama_base_url(settings_snapshot)

            # Check if Ollama is running
            try:
                response = safe_get(
                    f"{base_url}/api/tags",
                    timeout=3.0,
                    allow_localhost=True,
                    allow_private_ips=True,
                )
                return response.status_code == 200
            except requests.exceptions.RequestException:
                return False

        except Exception:
            logger.exception("Error checking Ollama availability")
            return False

    @classmethod
    def get_available_models(
        cls, settings_snapshot: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, str]]:
        """Get list of available Ollama embedding models."""
        from ....utilities.llm_utils import fetch_ollama_models

        base_url = get_ollama_base_url(settings_snapshot)
        return fetch_ollama_models(base_url, timeout=3.0)
