# Copyright (c) 2015-2024 by the parties listed in the AUTHORS file.
# All rights reserved.  Use of this source code is governed by
# a BSD-style license that can be found in the LICENSE file.

import datetime
import gc
import glob
import hashlib
import importlib
import numbers
import os
import re
import sqlite3
import time
from collections import UserDict
from tempfile import TemporaryDirectory
from contextlib import contextmanager

import astropy.io.misc.hdf5 as aspy5
from astropy import units as u
import h5py
import numpy as np
from astropy.table import meta as aspymeta
from astropy.table import Table
from wurlitzer import pipes, STDOUT

from ._libtoast import (
    AlignedF32,
    AlignedF64,
    AlignedI8,
    AlignedI16,
    AlignedI32,
    AlignedI64,
    AlignedU8,
    AlignedU16,
    AlignedU32,
    AlignedU64,
    Environment,
    Logger,
    threading_state,
    vatan2,
    vcos,
    vexp,
    vfast_atan2,
    vfast_cos,
    vfast_erfinv,
    vfast_exp,
    vfast_log,
    vfast_rsqrt,
    vfast_sin,
    vfast_sincos,
    vfast_sqrt,
    vlog,
    vrsqrt,
    vsin,
    vsincos,
    vsqrt,
)
from .mpi import MPI, use_mpi


def _create_log_rank(level):
    def log_rank(self, msg, comm=None, rank=0, timer=None):
        """Log a message on one process with optional timing.

        A common use case when logging from high-level code is to print a message on
        only a single process, and also to include some timing information.

        If a timer is specified, then it must be specified on all processes.  In this
        case the method is collective, and a barrier() is used to ensure accurate
        timing results.

        Args:
            msg (str):  The log message (only used on specified rank).
            comm (MPI.Comm):  The communicator, or None.
            rank (int):  The rank of the process that should do the logging.
            timer (Timer):  The optional timer.

        """
        log = Logger.get()
        my_rank = 0
        if comm is not None:
            my_rank = comm.rank

        if timer is not None:
            if not timer.is_running():
                err = f"Called {level}_rank with a timer that is not running.  "
                err += f"Did you forget to start it?"
                raise RuntimeError(err)
            if comm is not None:
                comm.barrier()
            timer.stop()

        if my_rank == rank:
            if timer is not None:
                msg = f"{msg} {timer.seconds():0.2f} s"
            if level == "VERBOSE":
                log.verbose(msg)
            elif level == "DEBUG":
                log.debug(msg)
            elif level == "INFO":
                log.info(msg)
            elif level == "WARNING":
                log.warning(msg)
            elif level == "ERROR":
                log.error(msg)
            elif level == "CRITICAL":
                log.critical(msg)
            else:
                raise RuntimeError(f"invalid log level {level}")

        if timer is not None:
            timer.clear()
            timer.start()

    return log_rank


# Add rank logging for all log levels
Logger.verbose_rank = _create_log_rank("VERBOSE")
Logger.debug_rank = _create_log_rank("DEBUG")
Logger.info_rank = _create_log_rank("INFO")
Logger.warning_rank = _create_log_rank("WARNING")
Logger.error_rank = _create_log_rank("ERROR")
Logger.critical_rank = _create_log_rank("CRITICAL")


@contextmanager
def stdouterr_redirected(to=None, comm=None, overwrite=False):
    """Redirect stdout and stderr to a file.

    This uses wurlitzer to redirect I/O to separate files per process.  Then the
    per-process files are concatenated into the final output.

    Args:
        to (str): The output file name.
        comm (mpi4py.MPI.Comm): The optional MPI communicator.
        overwrite (bool): if True overwrite file, otherwise backup to to.N first

    """
    log = Logger.get()
    nproc = 1
    rank = 0
    if comm is not None:
        nproc = comm.size
        rank = comm.rank

    def _rank_filename(basename, rank):
        """Return standard filename for output file of individual rank.

        Args:
            basename (str): base filename with path of final output log
            rank (int or str): MPI rank

        Returns:
            rank_filename (str): filename to use for temporary log of individual rank

        `rank` can be a wildcard '*' to generate a glob string.
        """
        return f"{basename}-rank{rank}"

    def _backup_filename(filename):
        """Rename filename to next available filename.N

        If filename == '/dev/null' or filename doesn't exist, just return filename.

        Args:
            filename (str): full path to original filename

        Returns:
            (str): New filename.N, or filename if original file didn't already exist

        """
        if filename == "/dev/null" or not os.path.exists(filename):
            return filename
        n = 0
        while True:
            altfile = f"{filename}.{n}"
            if os.path.exists(altfile):
                n += 1
            else:
                break
        os.rename(filename, altfile)
        return altfile

    def _combine_individual_outputs(to, nproc=None):
        """Combine individual {to}_{n} files into a single {to} file

        If nproc is specified, assume there are exactly that many per-rank
        files; otherwise glob to find out what is there.
        """
        if nproc is None:
            individual_files = sorted(glob.glob(_rank_filename(to, "*")))
        else:
            individual_files = [_rank_filename(to, p) for p in range(nproc)]

        with open(to, "w") as outfile:
            for p, fname in enumerate(individual_files):
                outfile.write(
                    f"================ Start of Process {p} ================\n"
                )
                with open(fname) as infile:
                    outfile.write(infile.read())
                outfile.write(
                    f"================= End of Process {p} =================\n\n"
                )

        # Only remove input files after successfully finishing merging
        for fname in individual_files:
            os.remove(fname)

    # Redirect both stdout and stderr to the same file

    if to is None:
        to = "/dev/null"
    if rank == 0:
        log.info(f"Begin log redirection to {to} at {time.asctime()}")

    # Determine individual per-rank output filenames and check if there are
    # leftover per-rank files from a previous crash  that need to be merged
    # before proceeding. Leftovers might have come from a run with a different
    # nproc, so don't enforce that here.
    pto = to
    if to != "/dev/null":
        pto = _rank_filename(to, rank)
        if rank == 0 and os.path.exists(pto):
            _combine_individual_outputs(to)

    # Backup previous output file if needed
    if rank == 0 and not overwrite:
        _backup_filename(to)

    # All ranks wait for logfile backup
    if comm is not None:
        comm.barrier()

    try:
        with open(pto, "w") as f, pipes(f, stderr=STDOUT):
            yield  # Allow code to be run with the redirected output
    finally:
        if nproc > 1:
            comm.barrier()

        # Concatenate per-process files
        if rank == 0 and to != "/dev/null":
            _combine_individual_outputs(to, nproc)

        if nproc > 1:
            comm.barrier()
        if rank == 0:
            log.info(f"End log redirection to {to} at {time.asctime()}")


# This function sets the numba threading layer to (hopefully) be compatible with TOAST.
# The TOAST threading concurrency is used to attempt to set the numba threading.  We
# try to use the OpenMP backend for numba and then TBB.  The "workqueue" backend (which
# is process based).  May not be compatible with all systems, so we use that as a
# last resort.  This function should be called by any operators that use numba.

numba_threading_layer = None


def set_numba_threading():
    """Set the numba threading layer.

    For parallel numba jit blocks, the backend threading layer is selected at runtime
    based on an order set inside the numba package.  We would like to change the
    order of selection to prefer one of the thread-based backends (omp or tbb).  We also
    set the maximum number of threads used by numba to be the same as the number of
    threads used by TOAST.  Since TOAST does not use numba, it means that there will
    be a consistent maximum number of threads in use at all times and no
    oversubscription.

    Args:
        None

    Returns:
        None

    """
    global numba_threading_layer
    if numba_threading_layer is not None:
        # Already set.
        return

    # Get the number of threads used by TOAST at runtime.
    env = Environment.get()
    log = Logger.get()
    toastthreads = env.max_threads()

    rank = 0
    if use_mpi:
        rank = MPI.COMM_WORLD.rank

    threading = "default"
    have_numba_omp = False
    try:
        # New style package layout
        from numba.np.ufunc import omppool

        have_numba_omp = True
        if rank == 0:
            log.debug("Numba has OpenMP threading support")
    except ImportError:
        try:
            # Old style
            from numba.npyufunc import omppool

            have_numba_omp = True
            if rank == 0:
                log.debug("Numba has OpenMP threading support")
        except ImportError:
            # no OpenMP support
            if rank == 0:
                log.debug("Numba does not support OpenMP")
    have_numba_tbb = False
    try:
        # New style package layout
        from numba.np.ufunc import tbbpool

        have_numba_tbb = True
        if rank == 0:
            log.debug("Numba has TBB threading support")
    except ImportError:
        try:
            # Old style
            from numba.npyufunc import tbbpool

            have_numba_tbb = True
            if rank == 0:
                log.debug("Numba has TBB threading support")
        except ImportError:
            # no TBB
            if rank == 0:
                log.debug("Numba does not support TBB")

    # Prefer OMP backend
    if have_numba_omp:
        threading = "omp"
    elif have_numba_tbb:
        threading = "tbb"

    try:
        from numba import config, threading_layer, vectorize

        # Set threading layer and number of threads.  Note that this still
        # does not always work.  The conf structure is repopulated from the
        # environment on every compilation if any of the NUMBA_* variables
        # have changed.
        config.THREADING_LAYER = threading
        config.NUMBA_DEFAULT_NUM_THREADS = toastthreads
        config.NUMBA_NUM_THREADS = toastthreads
        os.environ["NUMBA_THREADING_LAYER"] = threading
        os.environ["NUMBA_DEFAULT_NUM_THREADS"] = "{:d}".format(toastthreads)
        os.environ["NUMBA_NUM_THREADS"] = "{:d}".format(toastthreads)

        # In order to get numba to actually select a threading layer, we must
        # trigger compilation of a parallel function.
        @vectorize("float64(float64)", target="parallel")
        def force_thread_launch(x):
            return x + 1

        force_thread_launch(np.zeros(1))

        # Log the layer that was selected
        numba_threading_layer = threading_layer()
        if rank == 0:
            log.debug("Numba threading layer set to {}".format(numba_threading_layer))
            log.debug(
                "Numba max threads now forced to {}".format(config.NUMBA_NUM_THREADS)
            )
    except ImportError:
        # Numba not available at all
        if rank == 0:
            log.debug("Cannot import numba- ignoring threading layer.")


def astropy_control(max_future=None, offline=False, node_local=False):
    """This function attempts to trigger any astropy downloads.

    The astropy package will automatically download external data files on demand.
    This can be a problem if multiple processes on a distributed system are using the
    same astropy installation.  This function will trigger IERS downloads in a
    controlled way on one process (or one per node if astropy is node-local).

    When requesting Alt / Az coordinate system transforms, times outside of the IERS
    data range will produce a warning on every process.  If the max_future time is set,
    we check if that will trigger a warning, print the warning once, and then disable
    it to avoid spewing the warning on every process of a parallel job.

    Args:
        None

    Returns:
        None

    """
    from astropy.utils import iers

    log = Logger.get()

    rank = 0
    if use_mpi:
        rank = MPI.COMM_WORLD.rank

    # If auto_download is False, the bundled IERS-B table is always used, even if a
    # previously downloaded IERS-A table exists.
    if offline:
        iers.conf.auto_download = False
        log.warning(
            "Disabling downloaded astropy IERS- coordinate transforms will be less accurate"
        )
    else:
        iers.conf.auto_download = True

    # Check future time range

    now_time = datetime.datetime.now(tz=datetime.timezone.utc)
    if max_future is not None:
        if max_future > now_time + datetime.timedelta(days=365):
            msg = f"Maximum future time {max_future} is more than one year in future.\n"
            msg += f"Coordinate transforms using IERS will be less accurate."
            if rank == 0:
                log.warning(msg)

    # Disable future warnings
    # log.info("Disabling astropy IERSRangeError warnings")
    # warnings.filterwarnings("ignore", category=iers.IERSRangeError, module=iers)

    if node_local:
        # Every node has a local astropy installation (i.e. it is not installed to a
        # network / shared filesystem).  In this case one process per node does the
        # trigger.
        pass
    else:
        # Only one global process triggers
        pass


try:
    import psutil

    def memreport(msg="", comm=None, silent=False):
        """Gather and report the amount of allocated, free and swapped system memory"""
        if psutil is None:
            return
        vmem = psutil.virtual_memory()._asdict()
        gc.collect()
        vmem2 = psutil.virtual_memory()._asdict()
        memstr = None
        if comm is None or comm.rank == 0:
            memstr = "Memory usage {}\n".format(msg)
        for key, value in vmem.items():
            value2 = vmem2[key]
            vlist = None
            vlist2 = None
            if comm is None:
                vlist = [value]
                vlist2 = [value2]
            else:
                vlist = comm.gather(value, root=0)
                vlist2 = comm.gather(value2, root=0)
            if comm is None or comm.rank == 0:
                vlist = np.array(vlist, dtype=np.float64)
                vlist2 = np.array(vlist2, dtype=np.float64)
                if key != "percent":
                    # From bytes to better units
                    if np.amax(vlist) < 2**20:
                        vlist /= 2**10
                        vlist2 /= 2**10
                        unit = "kB"
                    elif np.amax(vlist) < 2**30:
                        vlist /= 2**20
                        vlist2 /= 2**20
                        unit = "MB"
                    else:
                        vlist /= 2**30
                        vlist2 /= 2**30
                        unit = "GB"
                else:
                    unit = "% "
                if comm is None or comm.size == 1:
                    memstr += "{:>12} : {:8.3f} {}\n".format(key, vlist[0], unit)
                    if vlist[0] > 0 and np.abs(vlist2[0] - vlist[0]) / vlist[0] > 1e-3:
                        memstr += "{:>12} : {:8.3f} {} (after GC)\n".format(
                            key, vlist2[0], unit
                        )
                else:
                    med1 = np.median(vlist)
                    memstr += (
                        "{:>12} : {:8.3f} {}  < {:8.3f} +- {:8.3f} {}  "
                        "< {:8.3f} {}\n".format(
                            key,
                            np.amin(vlist),
                            unit,
                            med1,
                            np.std(vlist),
                            unit,
                            np.amax(vlist),
                            unit,
                        )
                    )
                    med2 = np.median(vlist2)
                    if med1 > 0 and np.abs(med2 - med1) / med1 > 1e-3:
                        memstr += (
                            "{:>12} : {:8.3f} {}  < {:8.3f} +- {:8.3f} {}  "
                            "< {:8.3f} {} (after GC)\n".format(
                                key,
                                np.amin(vlist2),
                                unit,
                                med2,
                                np.std(vlist2),
                                unit,
                                np.amax(vlist2),
                                unit,
                            )
                        )
        if comm is None or comm.rank == 0:
            if not silent:
                print(memstr, flush=True)
        if comm is not None:
            comm.Barrier()
        return memstr

except ImportError:

    def memreport(msg="", comm=None):
        return


def object_ndim(x):
    """Get the number of dimension of an object.

    Scalars return 0.  Numpy arrays return their actual ndim value.
    Objects that support the buffer protocol return the ndim value of the
    corresponding memoryview.  Nested lists are traversed to compute the
    effective number of dimensions.

    Args:
        x (object): some stuff.

    Returns:
        (int): the number of dimensions of the stuff.

    """
    try:
        nd = x.ndim
        return nd
    except AttributeError:
        # Not a numpy array...
        try:
            view = memoryview(x)
            nd = view.ndim
            return nd
        except TypeError:
            # Does not support buffer protocol...
            try:
                lg = len(x)
                # It's a list!
                nd = 1
                cur = x[0]
                try:
                    lg = len(cur)
                    nd += 1
                    cur = cur[0]
                    try:
                        lg = len(cur)
                        nd += 1
                        # Using lists of more than 3 dimensions (rather than
                        # a numpy array) is kind of crazy...
                    except TypeError:
                        pass
                except TypeError:
                    pass
                return nd
            except TypeError:
                # Must be a scalar.
                return 0


def ensure_buffer_i64(data):
    """Return a flattened object that supports the buffer protocol.

    Numpy arrays and objects supporting the buffer protocol are flattened and
    copied to contiguous memory if needed.  Scalars and lists are converted
    to numpy arrays.

    Args:
        data (object):  A scalar or iterable.

    Returns:
        (array):  The input unmodified or converted to an array.

    """
    return np.ascontiguousarray(data, dtype=np.int64).flatten()


def ensure_buffer_f64(data):
    """Return a flattened object that supports the buffer protocol.

    Numpy arrays and objects supporting the buffer protocol are flattened and
    copied to contiguous memory if needed.  Scalars and lists are converted
    to numpy arrays.

    Args:
        data (object):  A scalar or iterable.

    Returns:
        (array):  The input unmodified or converted to an array.

    """
    return np.ascontiguousarray(data, dtype=np.float64).flatten()
    # if isinstance(data, np.ndarray):
    #     print("ensure: found numpy array, shape=", data.shape, flush=True)
    #     return np.ascontiguousarray(data.flatten(), dtype=np.float64)
    # try:
    #     view = memoryview(data)
    #     if view.ndim == 0:
    #         # A single element...
    #         print("ensure: converting scalar buffer", flush=True)
    #         return np.array([view], dtype=np.float64)
    #     elif (not view.c_contiguous) or (view.ndim != 1):
    #         print("ensure: found non-contiguous memory view shape=", view.shape, flush=True)
    #         return np.ascontiguousarray(view, dtype=np.float64)
    #     else:
    #         print("ensure: returning original data shape=", view.shape, flush=True)
    #         return data
    # except TypeError:
    #     # Does not support buffer protocol
    #     print("ensure: converting non-buffer object ", data, flush=True)
    #     return np.ascontiguousarray(data, dtype=np.float64)


def name_UID(name, int64=False):
    """Return a unique integer for a specified name string."""
    bdet = name.encode("utf-8")
    dhash = hashlib.md5()
    dhash.update(bdet)
    bdet = dhash.digest()
    uid = None
    try:
        ind = int.from_bytes(bdet, byteorder="little")
        if int64:
            uid = int(ind & 0x7FFFFFFFFFFFFFFF)
        else:
            # FIXME:  This commented out line is the correct thing to use
            # for signed integers.  However it will change the random seed
            # values everywhere.  Make this change sometime when it is less
            # disruptive.
            # uid = int(ind & 0x7FFFFFFF)
            uid = int(ind & 0xFFFFFFFF)
    except:
        raise RuntimeError(
            "Cannot convert detector name {} to a unique integer-\
            maybe it is too long?".format(name)
        )
    return uid


def rate_from_times(timestamps, mean=False):
    """Compute effective sample rate in Hz from timestamps.

    There are many cases when we want to apply algorithms that require a fixed
    sample rate.  We want to compute that from timestamps while also checking for
    any outliers that could compromise the results.

    By default this function uses the median delta_t, under the assumption that
    variations in the timing is due to small numerical / bit noise effects.  For larger
    variations using the mean may be more appropriate (set mean=True).

    This returns the sample rate and also the statistics of the time deltas between
    samples.

    Args:
        timestamps (array):  The array of timestamps.

    Returns:
        (tuple):  The (rate, dt, dt_min, dt_max, dt_std) values.

    """
    tdiff = np.array(np.diff(timestamps))
    dt_min = np.min(tdiff)
    dt_max = np.max(tdiff)
    dt_std = np.std(tdiff)
    dt = None
    if mean:
        dt = np.mean(tdiff)
    else:
        dt = np.median(tdiff)
    return (1.0 / dt, dt, dt_min, dt_max, dt_std)


def dtype_to_aligned(dt):
    """For a numpy dtype, return the equivalent internal Aligned storage class.

    Args:
        dt (dtype):  The numpy dtype.

    Returns:
        (tuple):  The (storage class, item size).

    """
    log = Logger.get()
    itemsize = None
    storage_class = None
    ttype = np.dtype(dt)
    if ttype.char == "b":
        storage_class = AlignedI8
        itemsize = 1
    elif ttype.char == "B":
        storage_class = AlignedU8
        itemsize = 1
    elif ttype.char == "h":
        storage_class = AlignedI16
        itemsize = 2
    elif ttype.char == "H":
        storage_class = AlignedU16
        itemsize = 2
    elif ttype.char == "i":
        storage_class = AlignedI32
        itemsize = 4
    elif ttype.char == "I":
        storage_class = AlignedU32
        itemsize = 4
    elif (ttype.char == "q") or (ttype.char == "l"):
        storage_class = AlignedI64
        itemsize = 8
    elif (ttype.char == "Q") or (ttype.char == "L"):
        storage_class = AlignedU64
        itemsize = 8
    elif ttype.char == "f":
        storage_class = AlignedF32
        itemsize = 4
    elif ttype.char == "d":
        storage_class = AlignedF64
        itemsize = 8
    elif ttype.char == "F":
        raise NotImplementedError("No support yet for complex numbers")
    elif ttype.char == "D":
        raise NotImplementedError("No support yet for complex numbers")
    else:
        msg = "Unsupported data typecode '{}'".format(ttype.char)
        log.error(msg)
        raise ValueError(msg)
    return (storage_class, itemsize)


def array_dot(u, v):
    """Dot product of each row of two 2D arrays"""
    return np.sum(u * v, axis=1).reshape((-1, 1))


def object_fullname(o):
    """Return the fully qualified name of an object."""
    module = o.__module__
    if module is None or module == str.__module__:
        return o.__qualname__
    return "{}.{}".format(module, o.__qualname__)


def import_from_name(name):
    """Import a class from its full name."""
    cls_parts = name.split(".")
    cls_name = cls_parts.pop()
    cls_mod_name = ".".join(cls_parts)
    cls = None
    try:
        cls_mod = importlib.import_module(cls_mod_name)
        cls = getattr(cls_mod, cls_name)
    except:
        msg = f"Cannot import class '{cls_name}' from module '{cls_mod_name}'"
        raise RuntimeError(msg)
    return cls


def system_state(comm=None):
    """Print a snapshot of the current system state across the job."""
    log = Logger.get()
    max, curmax = threading_state()
    msg = f"Threading snapshot:  Overall max = {max}, Current max = {curmax}\n"
    memstr = memreport(msg="system snapshot", comm=comm, silent=True)
    if comm is None or comm.rank == 0:
        msg += memstr
        log.info(msg)


# Test whether h5py supports parallel I/O

hdf5_is_parallel = None


def have_hdf5_parallel():
    global hdf5_is_parallel
    if hdf5_is_parallel is not None:
        # Already checked
        return hdf5_is_parallel

    # Do we even have MPI?
    if not use_mpi:
        hdf5_is_parallel = False
        return hdf5_is_parallel

    # Try to open a temp file on each process with the mpio driver but using
    # COMM_SELF.  This lets us test the presence of the driver without actually
    # doing any communication
    try:
        with TemporaryDirectory() as tempdir:
            tempfile = os.path.join(tempdir, f"test_hdf5_mpio_{MPI.COMM_WORLD.rank}.h5")
            with h5py.File(tempfile, "w", driver="mpio", comm=MPI.COMM_SELF) as f:
                # Yay!
                hdf5_is_parallel = True
    except (ValueError, AssertionError, AttributeError) as e:
        # Nope...
        hdf5_is_parallel = False
    return hdf5_is_parallel


def hdf5_use_serial(handle, comm):
    """Check if all processes in a communicator have access to the file"""
    if comm is None:
        return True
    if comm.size == 1:
        return True
    # Have to check...
    total = comm.allreduce((1 if handle is not None else 0), op=MPI.SUM)
    if total != comm.size:
        return True
    else:
        return False


def table_write_parallel_hdf5(table, root, name, comm=None):
    """Write astropy table to HDF5 with parallel support.

    The astropy.io.misc.hdf5.write_table_hdf5() does not support situations
    where the h5py package is using parallel HDF5 under the hood.  In this
    case, we need to create datasets on all processes but only write to them
    from one process.

    Args:
        table (Table):  The data table.
        root (h5py.Group):  The group to use for creating datasets.
        name (str):  The data set name
        comm (MPI.Comm):  The communicator of processes containing duplicates
            of the table.

    Returns:
        None

    """
    rank = 0
    if comm is not None:
        rank = comm.rank

    # Encode any mixin columns as plain columns + appropriate metadata
    table = aspy5._encode_mixins(table)

    # Table with numpy unicode strings can't be written in HDF5 so
    # to write such a table a copy of table is made containing columns as
    # bytestrings.  Now this copy of the table can be written in HDF5.
    safe_table = replace_unicode_arrays(table)

    # Write the table to the root group
    tarray = safe_table.as_array()
    dset_shape = tarray.shape
    dset_type = tarray.dtype
    dset = None
    if root is not None:
        # This process is participating
        dset = root.create_dataset(name, dset_shape, dtype=dset_type)
        if rank == 0:
            # Only one process writes the data
            dset[:] = tarray
    del dset

    # Serialize metadata
    header_yaml = aspymeta.get_yaml_from_table(safe_table)
    header_encoded = np.array([h.encode("utf-8") for h in header_yaml])
    mdset_shape = header_encoded.shape
    mdset_type = header_encoded.dtype
    mdset = None
    if root is not None:
        mdset = root.create_dataset(
            aspy5.meta_path(name), mdset_shape, dtype=mdset_type
        )
        if rank == 0:
            mdset[:] = header_encoded
    del mdset


def unit_conversion(source, target):
    """Get the multiplicative factor to convert data.

    Given data in source units, return the scale factor needed to convert
    that data into target units.

    Args:
        source (Unit):  The source units.
        target (Unit):  The target units.

    Returns:
        (float):  The conversion factor.

    """
    scale = 1.0 * source
    return scale.to_value(target)


class SetDict(UserDict):
    """
    Utility class representing a dictionary of sets with some inplace operations.
    """

    def __setitem__(self, key, value):
        """
        insures that values are stored as sets
        this will be used by the `__init__` function
        NOTE: values must be iterable
        """
        super().__setitem__(key, set(value))

    def __isub__(self, other):
        """
        -= operation performing set difference on all keys
        `other` can be a normal dict
        """
        for key, value in other.items():
            self[key] -= set(value)
        return self

    def __ior__(self, other):
        """
        |= operation performing set union on all keys
        `other` can be a normal dict
        """
        for key, value in other.items():
            self[key] |= set(value)
        return self

    def __iand__(self, other):
        """
        &= operation performing set intersection on all keys
        `other` can be a normal dict
        """
        for key, value in other.items():
            self[key] &= set(value)
        return self

    def __str__(self):
        """prints only the non-empty/None sets for brevity sake"""
        result = "{ "
        for k, v in self.items():
            if (len(v) > 0) and not all(x is None for x in v):
                result += f"{k}:{list(v)} "
        result += "}"
        return result

    def is_empty(self):
        """returns True if the container is empty or contains only None"""
        for k, v in self.items():
            if (len(v) > 0) and not all(x is None for x in v):
                return False
        return True


def flagged_noise_fill(data, flags, buffer, poly_order=1):
    """Fill flagged samples with noise.

    This finds contiguous flagged samples and fills each gap with a polynomial
    trend using nearby samples plus gaussian white noise.

    Args:
        data (array):  The local data buffer to process.
        flags (array):  The array of sample flags.
        buffer (int):  Number of samples to use on either side of flagged regions.
        poly_order (int):  The polynomial order to fit across the gap.

    Returns:
        None

    """
    n_samp = len(data)
    if len(flags) != n_samp:
        msg = "Data and flag array lengths should be the same"
        raise RuntimeError(msg)

    if buffer <= 0 or buffer > n_samp // 4:
        msg = "The buffer size around flagged regions should be large enough"
        msg += " to estimate nearby noise properties, but small enough to fit"
        msg += " within the buffer"
        raise RuntimeError(msg)

    if np.sum(flags == 0) == 0:
        raise RuntimeError(f"Cannot gapfill a buffer without any good data")

    flag_indx = np.arange(n_samp, dtype=np.int64)[flags != 0]
    flag_groups = np.split(flag_indx, np.where(np.diff(flag_indx) != 1)[0] + 1)
    nfgroup = len(flag_groups)

    # Merge groups that are closer than the buffer length
    groups = list()
    igrp = 0
    while igrp < nfgroup:
        grp = flag_groups[igrp]
        if len(grp) == 0:
            igrp += 1
            continue
        first = grp[0]
        last = grp[-1] + 1
        while igrp + 1 < nfgroup and last + buffer > flag_groups[igrp + 1][0]:
            igrp += 1
            last = flag_groups[igrp][-1] + 1
        groups.append((int(first), int(last)))
        igrp += 1

    for igrp, (bad_first, bad_last) in enumerate(groups):
        full_first = bad_first - buffer
        if full_first < 0:
            full_first = 0
        full_last = bad_last + buffer
        if full_last > n_samp:
            full_last = n_samp
        fit_n_samps = full_last - full_first
        fit_samps = np.arange(fit_n_samps)
        fit_flags = flags[full_first:full_last]
        fit_good = fit_flags == 0
        fit_bad = np.logical_not(fit_good)
        in_fit_x = fit_samps[fit_good]
        in_fit_y = data[full_first:full_last][fit_good]
        fit_poly = np.polynomial.polynomial.Polynomial.fit(
            in_fit_x, in_fit_y, poly_order
        )
        fit_curve = fit_poly(in_fit_x)

        rms = np.std(in_fit_y - fit_curve)

        # Fill the gaps with noise plus the fit polynomial
        full_fit = fit_poly(fit_samps)
        n_bad = np.count_nonzero(fit_bad)
        data[full_first:full_last][fit_bad] = full_fit[fit_bad] + np.random.normal(
            scale=rms, size=n_bad
        )


def sqlite_connect(filename=None, mode="w"):
    """Utility function for connecting to an sqlite3 DB.

    This provides a single function for opening an sqlite connection
    consistently across the code base.  When connecting to a file on
    disk we try to use options which will be more performant in the
    situation where the DB is on a networked / shared filesystem and
    being accessed from multiple processes.

    Args:
        filename (str):  The path on disk or None if using an in-memory DB.
        mode (str):  Either "r" or "w".

    Returns:
        (sqlite3.Connection):  The database connection.

    """
    if filename is None or filename == ":memory:":
        # Memory-backed DB
        if mode == "r":
            raise ValueError("Cannot open memory DB in read-only mode")
        return sqlite3.connect(":memory:")

    # This timeout is in seconds.  If multiple processes are writing, they
    # might be blocked for a while until they get their turn.  This prevents
    # them from giving up too soon if other processes have a write lock.
    # https://www.sqlite.org/pragma.html#pragma_busy_timeout
    busy_time = 100

    # Journaling options

    # Persistent journaling mode.  File creation / deletion can be expensive
    # on some filesystems.  This just writes some zeros to the header and
    # leaves the file.  This journal is a "side car" file next to the original
    # DB file and is safe to delete manually if one is sure that no processes
    # are accessing the DB.
    # https://www.sqlite.org/pragma.html#pragma_journal_mode
    if mode == "r":
        journal_mode = "off"
    else:
        journal_mode = "persist"

    # Max size of the journal.  Although it is being overwritten repeatedly,
    # if it gets too large we purge it and recreate.  This should not happen
    # for most normal operations.
    # https://www.sqlite.org/pragma.html#pragma_journal_size_limit
    journal_size = f"{10 * 1024 * 1024}"

    # Disk synchronization options

    # Using "normal" instead of the default "full" can avoid potentially expensive
    # (on network filesystems) sync operations.
    # https://www.sqlite.org/pragma.html#pragma_synchronous
    if mode == "r":
        sync_mode = "off"
    else:
        sync_mode = "normal"

    # Memory caching

    # The default page size in modern sqlite is 4096 bytes, and should be fine.
    # We set this explicitly to allow easy changing in the future or keeping it
    # fixed if the default changes.
    # https://www.sqlite.org/pragma.html#pragma_page_size
    page_size = 4096

    # The number of pages to cache in memory.  Setting this to a few MB of RAM
    # can have substantial performance benefits.  Total will be number of pages
    # times page size.
    # https://www.sqlite.org/pragma.html#pragma_cache_size
    n_cache_pages = 4000

    # Open connection
    if mode == "r":
        connstr = f"file:{filename}?mode=ro"
    else:
        connstr = f"file:{filename}?mode=rwc"
    conn = sqlite3.connect(connstr, uri=True, timeout=busy_time)

    # Set cache sizes
    conn.execute(f"pragma page_size={page_size}")
    conn.execute(f"pragma cache_size={n_cache_pages}")

    # Set journaling / sync options
    conn.execute(f"pragma journal_mode={journal_mode}")
    conn.execute(f"pragma journal_size_limit={journal_size}")
    conn.execute(f"pragma synchronous={sync_mode}")

    # Other tuning options

    # Hold temporary tables in memory.
    # https://www.sqlite.org/pragma.html#pragma_temp_store
    conn.execute("pragma temp_store=memory")

    if mode == "r":
        conn.execute("pragma query_only=true")
    return conn


def sqlite_scalar(input):
    """Ensure that any scalars are one of the sqlite types."""
    if isinstance(input, numbers.Integral):
        return int(input)
    elif isinstance(input, u.Quantity):
        return float(input.value)
    elif isinstance(input, numbers.Number):
        # We already checked integers above, so this must be float
        return float(input)
    else:
        # Must be a string
        return input


def array_equal(left, right, f32=False, log_prefix=""):
    log = Logger.get()
    if isinstance(left, u.Quantity):
        # Floating point values with units
        if not isinstance(right, u.Quantity):
            msg = f"{log_prefix}: Lefthand array is a Quantity, Righthand is not"
            log.verbose(msg)
            return False
        if left.unit != right.unit:
            msg = f"{log_prefix}: Lefthand unit ({left.unit}) != "
            msg += f"Righthand unit ({right.unit})"
            log.verbose(msg)
            return False
        lval = left.value
        rval = right.value
    else:
        lval = left
        rval = right
    if lval.dtype != rval.dtype:
        msg = f"{log_prefix}: Lefthand dtype ({lval.dtype}) != "
        msg += f"Righthand dtype ({rval.dtype})"
        log.verbose(msg)
        return False
    if lval.dtype == np.dtype(np.float64) or lval.dtype == np.dtype(np.float32):
        # Float data
        drange = np.nanmax(lval) - np.nanmin(lval)
        if lval.dtype == np.dtype(np.float32) or f32:
            rtol = 1.0e-6
            atol = 10.0 * drange * 1.0e-6
        else:
            rtol = 1.0e-12
            atol = 10.0 * drange * 1.0e-15
        if not np.allclose(lval, rval, rtol=rtol, atol=atol, equal_nan=True):
            msg = f"{log_prefix}: (atol={atol}, rtol={rtol}) "
            msg += f"Lefthand data {lval} != "
            msg += f"Righthand data {rval}"
            log.verbose(msg)
            return False
    else:
        # Some other type of data
        if not np.array_equal(lval, rval):
            msg = f"{log_prefix}: "
            msg += f"Lefthand data {lval} != "
            msg += f"Righthand data {rval}"
            log.verbose(msg)
            return False
    return True


def table_equal(left, right):
    """Improved table comparison beyond astropy.Table.values_equal()

    Do an elementwise comparison with extra care for boolean columns.

    Args:
        left (Table):  The first table to compare.
        right (Table):  The second table to compare.

    Returns:
        (bool):  True if all elements are equal.

    """
    log = Logger.get()
    left_names = set(left.colnames)
    right_names = set(right.colnames)
    if left_names != right_names:
        msg = f"Table colnames {left_names} != {right_names}"
        log.verbose(msg)
        return False

    for nm in left_names:
        left_col = left[nm]
        right_col = right[nm]
        if isinstance(left_col, u.Quantity):
            # floating point values with units
            if not isinstance(right_col, u.Quantity):
                msg = f"Column {nm} is a Quantity on left, but not right"
                log.verbose(msg)
                return False
            if left_col.unit != right_col.unit:
                msg = f"Column {nm} unit mismatch {left_col.unit} != {right_col.unit}"
                log.verbose(msg)
                return False
            check = array_equal(
                left_col.value, right_col.value, log_prefix=f"Column {nm} Quantities"
            )
            if not check:
                return False
        else:
            # Normal arrays
            check = array_equal(left_col, right_col, log_prefix=f"Column {nm}")
            if not check:
                return False
    return True


def unicode_array_to_bytes(input):
    """Convert numpy arrays with Unicode strings to fixed-length bytes.

    Args:
        input (str):  The input array.

    Returns:
        (array):  The input converted to 'S' bytes, or the original if not
            an array of strings.

    """
    utype_pat = re.compile(r"[><|]*U(\d+)$")
    if np.issubdtype(input.dtype, np.str_):
        # Unicode string
        mat = utype_pat.match(str(input.dtype))
        if mat is not None:
            maxlen = mat.group(1)
            stype = f"S{maxlen}"
            return input.astype(np.dtype(stype))
        else:
            msg = f"unicode to bytes string type has dtype {str(input.dtype)}"
            raise RuntimeError(msg)
    else:
        return input


def byte_array_to_unicode(input):
    """Convert fixed length byte arrays to unicode dtype.

    Args:
        input (str):  The input array.

    Returns:
        (array):  The input converted to 'U' dtype, or the original if not
            an array of type 'S'.

    """
    stype_pat = re.compile(r".*S(\d+)$")
    if np.issubdtype(input.dtype, np.bytes_):
        # Unicode string
        mat = stype_pat.match(str(input.dtype))
        if mat is not None:
            maxlen = mat.group(1)
            utype = f"U{maxlen}"
            return input.astype(np.dtype(utype))
        else:
            msg = f"bytes to unicode bytes type has dtype {str(input.dtype)}"
            raise RuntimeError(msg)
    else:
        return input


def replace_unicode_arrays(obj):
    """Recursively replace unicode numpy arrays.

    Descend the object container recursively and replace numpy unicode arrays with
    fixed-length byte arrays.

    Args:
        obj (object):  A container or array

    Returns:
        (object):  The same style container as the input, with unicode arrays replaced

    """
    if isinstance(obj, Table):
        # Modify table columns as needed.
        new_obj = obj.copy()
        tnames = new_obj.colnames
        for nm in tnames:
            old_col = new_obj[nm]
            if np.issubdtype(old_col.dtype, np.str_):
                # Replace it
                new_col = replace_unicode_arrays(old_col)
                new_obj.replace_column(nm, new_col)
    elif isinstance(obj, dict):
        new_obj = dict()
        for k, v in obj.items():
            new_obj[k] = replace_unicode_arrays(v)
    elif isinstance(obj, tuple):
        new_obj = list()
        for val in obj:
            new_obj.append(replace_unicode_arrays(val))
        new_obj = tuple(new_obj)
    elif isinstance(obj, list):
        new_obj = list()
        for val in obj:
            new_obj.append(replace_unicode_arrays(val))
    elif isinstance(obj, np.ndarray):
        new_obj = unicode_array_to_bytes(obj)
    else:
        new_obj = obj
    return new_obj


def replace_byte_arrays(obj):
    """Recursively replace fixed-length numpy byte arrays.

    Descend the object container recursively and replace numpy byte arrays with
    Unicode arrays.

    Args:
        obj (object):  A container or array

    Returns:
        (object):  The same style container as the input, with byte arrays replaced

    """
    if isinstance(obj, Table):
        # Modify table columns as needed.
        new_obj = obj.copy()
        tnames = new_obj.colnames
        for nm in tnames:
            old_col = new_obj[nm]
            if np.issubdtype(old_col.dtype, np.bytes_):
                # Replace it
                new_col = replace_byte_arrays(old_col)
                new_obj.replace_column(nm, new_col)
    elif isinstance(obj, dict):
        new_obj = dict()
        for k, v in obj.items():
            new_obj[k] = replace_byte_arrays(v)
    elif isinstance(obj, tuple):
        new_obj = list()
        for val in obj:
            new_obj.append(replace_byte_arrays(val))
        new_obj = tuple(new_obj)
    elif isinstance(obj, list):
        new_obj = list()
        for val in obj:
            new_obj.append(replace_byte_arrays(val))
    elif isinstance(obj, np.ndarray):
        new_obj = byte_array_to_unicode(obj)
    else:
        new_obj = obj
    return new_obj


def count_string_arrays(obj, prefix="", verbose=False):
    """Recursively count the number of unicode and byte arrays.

    This is intended for debugging, and prints to stdout.

    Args:
        obj (object):  A container or array
        prefix (str):  Prefix to prepend to print statements
        verbose (bool):  If True, print info for every object

    Returns:
        (tuple):  The number of (unicode, byte) arrays (dtype 'U' and 'S') found.

    """
    ufound = 0
    sfound = 0
    stype_pat = re.compile(r".*S(\d+)$")
    utype_pat = re.compile(r"[><|]*U(\d+)$")
    if isinstance(obj, Table):
        # Modify table columns as needed.
        tnames = obj.colnames
        for nm in tnames:
            col = obj[nm]
            if np.issubdtype(col.dtype, np.bytes_):
                # Count it
                if verbose:
                    print(f"{prefix}found byte array in table column {nm}", flush=True)
                sfound += 1
            if np.issubdtype(col.dtype, np.str_):
                # Count it
                if verbose:
                    print(
                        f"{prefix}found unicode array in table column {nm}", flush=True
                    )
                ufound += 1
    elif isinstance(obj, dict):
        for k, v in obj.items():
            dufound, dsfound = count_string_arrays(v)
            ufound += dufound
            sfound += dsfound
    elif isinstance(obj, tuple):
        for val in obj:
            dufound, dsfound = count_string_arrays(val)
            ufound += dufound
            sfound += dsfound
    elif isinstance(obj, list):
        for val in obj:
            dufound, dsfound = count_string_arrays(val)
            ufound += dufound
            sfound += dsfound
    elif isinstance(obj, np.ndarray):
        smat = stype_pat.match(str(obj.dtype))
        umat = utype_pat.match(str(obj.dtype))
        if smat is not None:
            if verbose:
                print(f"{prefix}found S array {obj}", flush=True)
            sfound += 1
        elif umat is not None:
            if verbose:
                print(f"{prefix}found U array {obj}", flush=True)
            ufound += 1
    else:
        pass
    return (ufound, sfound)
