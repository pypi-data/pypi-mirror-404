{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a08767",
   "metadata": {},
   "source": [
    "# ðŸ–¼ï¸ polars-cv: Comprehensive Demo\n",
    "\n",
    "This notebook provides a complete demonstration of the **polars-cv** plugin - a high-performance vision/array processing plugin for Polars DataFrames.\n",
    "\n",
    "## What is polars-cv?\n",
    "\n",
    "polars-cv enables:\n",
    "- **Lazy, zero-copy image processing** on DataFrame columns\n",
    "- **Composable pipelines** with automatic fused execution via DAG-based graphs\n",
    "- **Multi-domain operations** - seamlessly move between images, masks, contours, and scalars\n",
    "- **Multi-source & multi-output** - read from multiple columns, output multiple named results\n",
    "- **Named nodes with aliases** - define reusable pipeline checkpoints\n",
    "- **Common Subexpression Elimination (CSE)** - automatic optimization of shared operations\n",
    "- **Dynamic parameters** using Polars expressions for per-row customization\n",
    "- **Binary operations** between pipelines (add, subtract, multiply, blend, mask)\n",
    "- **Native metric functions** - `mask_iou()`, `mask_dice()`, `hamming_distance()`, `hash_similarity()`\n",
    "- **Seamless ML integration** with NumPy, PyTorch, and other frameworks\n",
    "\n",
    "The plugin leverages **view-buffer**, a Rust crate providing stride-aware tensor operations with automatic kernel fusion.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Pipeline** | Define source â†’ operations â†’ sink for image processing |\n",
    "| **Lazy Composition** | Use `.cv.pipe()` to create composable `LazyPipelineExpr` |\n",
    "| **Named Nodes** | Use `.alias(name)` to create checkpoints for multi-output |\n",
    "| **Multi-Output** | Use `.merge_pipe()` + dict `.sink()` for Struct output |\n",
    "| **CSE Optimization** | Shared prefixes automatically extracted and reused |\n",
    "| **Domain Transitions** | Seamlessly move between buffer/contour/scalar domains |\n",
    "| **Native Functions** | `mask_iou()`, `mask_dice()`, `hamming_distance()`, `hash_similarity()` |\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Imports](#1-setup--imports)\n",
    "2. [Basic Pipeline Operations](#2-basic-pipeline-operations)\n",
    "3. [DType Promotion & Normalization](#3-dtype-promotion--normalization)\n",
    "4. [Dynamic Parameters with Expressions](#4-dynamic-parameters-with-expressions)\n",
    "5. [Geometry Operations](#5-geometry-operations)\n",
    "6. [Composable Pipelines](#6-composable-pipelines-the-core-of-polars-cv) - *The core of polars-cv*\n",
    "7. [Binary Operations & Mask Application](#7-binary-operations--mask-application)\n",
    "8. [Multi-Source Pipelines](#8-multi-source-pipelines)\n",
    "9. [Multi-Output with CSE Optimization](#9-multi-output-with-cse-optimization)\n",
    "10. [Reusable Pipeline Patterns](#10-reusable-pipeline-patterns)\n",
    "11. [Domain Transitions: Images â†” Contours â†” Scalars](#11-domain-transitions-images--contours--scalars)\n",
    "12. [ML Workflow: Segmentation Pipeline](#12-ml-workflow-segmentation-pipeline)\n",
    "13. [PyTorch Integration](#13-pytorch-integration)\n",
    "14. [Perceptual Image Hashing](#14-perceptual-image-hashing)\n",
    "15. [Conclusion](#15-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ba412",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "First, let's import the necessary packages and set up helper functions for displaying images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up non-interactive matplotlib backend for script execution\n",
    "# This prevents plt.show() from blocking when running as a script\n",
    "import os\n",
    "\n",
    "if os.environ.get(\"MPLBACKEND\") is None and not hasattr(os, \"_called_from_jupyter\"):\n",
    "    import matplotlib\n",
    "\n",
    "    matplotlib.use(\"Agg\")\n",
    "\n",
    "# Core imports\n",
    "import io\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from PIL import Image\n",
    "\n",
    "# polars-cv imports\n",
    "from polars_cv import (\n",
    "    BBOX_SCHEMA,\n",
    "    CONTOUR_SCHEMA,\n",
    "    POINT_SCHEMA,\n",
    "    HashAlgorithm,\n",
    "    Pipeline,\n",
    "    hamming_distance,\n",
    "    hash_similarity,\n",
    "    mask_dice,\n",
    "    mask_iou,\n",
    "    numpy_from_struct,\n",
    ")\n",
    "from polars_cv.geometry.schemas import contour_from_points\n",
    "\n",
    "# Display settings\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 4]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "print(f\"âœ… Polars version: {pl.__version__}\")\n",
    "print(\"âœ… polars-cv loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b725bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for displaying images\n",
    "\n",
    "\n",
    "def bytes_to_image(data: bytes) -> Image.Image:\n",
    "    \"\"\"Convert image bytes (PNG/JPEG) to PIL Image.\"\"\"\n",
    "    return Image.open(io.BytesIO(data))\n",
    "\n",
    "\n",
    "# Note: numpy_from_struct is imported from polars_cv\n",
    "# It parses the struct output (data, dtype, shape) from numpy/torch sink to numpy array\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: list[Any], titles: list[str] | None = None, cmap: str | None = None\n",
    ") -> None:\n",
    "    \"\"\"Display multiple images side by side.\"\"\"\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(4 * n, 4))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, (ax, img) in enumerate(zip(axes, images)):\n",
    "        if isinstance(img, bytes):\n",
    "            img = bytes_to_image(img)\n",
    "        ax.imshow(img, cmap=cmap)\n",
    "        ax.axis(\"off\")\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_arrays(\n",
    "    arrays: list[np.ndarray], titles: list[str] | None = None, cmap: str = \"viridis\"\n",
    ") -> None:\n",
    "    \"\"\"Display multiple numpy arrays as heatmaps.\"\"\"\n",
    "    n = len(arrays)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(4 * n, 4))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, (ax, arr) in enumerate(zip(axes, arrays)):\n",
    "        im = ax.imshow(arr, cmap=cmap)\n",
    "        ax.axis(\"off\")\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00205ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample test images for the demo\n",
    "\n",
    "\n",
    "def create_test_image(\n",
    "    width: int = 256, height: int = 256, pattern: str = \"gradient\"\n",
    ") -> bytes:\n",
    "    \"\"\"Create a test image with various patterns.\"\"\"\n",
    "    if pattern == \"gradient\":\n",
    "        # RGB gradient pattern\n",
    "        r = np.linspace(0, 255, width, dtype=np.uint8)\n",
    "        g = np.linspace(0, 255, height, dtype=np.uint8)\n",
    "        img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        img[:, :, 0] = r[np.newaxis, :]  # Red gradient horizontal\n",
    "        img[:, :, 1] = g[:, np.newaxis]  # Green gradient vertical\n",
    "        img[:, :, 2] = 128  # Blue constant\n",
    "    elif pattern == \"checkerboard\":\n",
    "        block_size = 32\n",
    "        img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        for i in range(0, height, block_size):\n",
    "            for j in range(0, width, block_size):\n",
    "                if ((i // block_size) + (j // block_size)) % 2 == 0:\n",
    "                    img[i : i + block_size, j : j + block_size] = [255, 255, 255]\n",
    "                else:\n",
    "                    img[i : i + block_size, j : j + block_size] = [50, 50, 50]\n",
    "    elif pattern == \"circles\":\n",
    "        # Concentric circles\n",
    "        y, x = np.ogrid[:height, :width]\n",
    "        cx, cy = width // 2, height // 2\n",
    "        r = np.sqrt((x - cx) ** 2 + (y - cy) ** 2)\n",
    "        img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        img[:, :, 0] = ((np.sin(r / 10) + 1) * 127.5).astype(np.uint8)\n",
    "        img[:, :, 1] = ((np.cos(r / 15) + 1) * 127.5).astype(np.uint8)\n",
    "        img[:, :, 2] = 100\n",
    "    elif pattern == \"heatmap\":\n",
    "        # Gaussian heatmap for ML demo\n",
    "        y, x = np.ogrid[:height, :width]\n",
    "        cx, cy = width // 2 + 30, height // 2 - 20\n",
    "        sigma = 50\n",
    "        gaussian = np.exp(-((x - cx) ** 2 + (y - cy) ** 2) / (2 * sigma**2))\n",
    "        img = (gaussian * 255).astype(np.uint8)\n",
    "        img = np.stack([img, img, img], axis=-1)  # Grayscale as RGB\n",
    "    elif pattern == \"segmentation\":\n",
    "        # Multi-region segmentation mask style\n",
    "        img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        # Region 1: circle\n",
    "        y, x = np.ogrid[:height, :width]\n",
    "        cx1, cy1 = width // 4, height // 3\n",
    "        r1 = np.sqrt((x - cx1) ** 2 + (y - cy1) ** 2)\n",
    "        img[r1 < 40] = [200, 50, 50]  # Red\n",
    "        # Region 2: ellipse\n",
    "        cx2, cy2 = 3 * width // 4, height // 2\n",
    "        ellipse = ((x - cx2) / 50) ** 2 + ((y - cy2) / 30) ** 2\n",
    "        img[ellipse < 1] = [50, 200, 50]  # Green\n",
    "        # Region 3: rectangle\n",
    "        img[height // 2 : height // 2 + 60, width // 3 : width // 3 + 80] = [\n",
    "            50,\n",
    "            50,\n",
    "            200,\n",
    "        ]\n",
    "    else:\n",
    "        # Random noise\n",
    "        rng = np.random.default_rng(42)\n",
    "        img = rng.integers(0, 256, (height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Convert to PNG bytes\n",
    "    pil_img = Image.fromarray(img)\n",
    "    buffer = io.BytesIO()\n",
    "    pil_img.save(buffer, format=\"PNG\")\n",
    "    return buffer.getvalue()\n",
    "\n",
    "\n",
    "# Create test images\n",
    "test_images = {\n",
    "    \"gradient\": create_test_image(256, 256, \"gradient\"),\n",
    "    \"checkerboard\": create_test_image(256, 256, \"checkerboard\"),\n",
    "    \"circles\": create_test_image(256, 256, \"circles\"),\n",
    "    \"heatmap\": create_test_image(256, 256, \"heatmap\"),\n",
    "    \"segmentation\": create_test_image(256, 256, \"segmentation\"),\n",
    "    \"noise\": create_test_image(256, 256, \"noise\"),\n",
    "}\n",
    "\n",
    "# Display them\n",
    "display_images(\n",
    "    [\n",
    "        test_images[\"gradient\"],\n",
    "        test_images[\"checkerboard\"],\n",
    "        test_images[\"circles\"],\n",
    "        test_images[\"segmentation\"],\n",
    "    ],\n",
    "    [\"Gradient\", \"Checkerboard\", \"Circles\", \"Segmentation\"],\n",
    ")\n",
    "print(f\"Created {len(test_images)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965a5cf3",
   "metadata": {},
   "source": [
    "## 2. Basic Pipeline Operations\n",
    "\n",
    "polars-cv uses a fluent **Pipeline** API to define image processing operations. A complete pipeline has three parts:\n",
    "\n",
    "1. **Source**: How to interpret input data (`image_bytes`, `blob`, `raw`, `file_path`, `contour`)\n",
    "2. **Operations**: The transformations to apply (resize, grayscale, normalize, etc.)\n",
    "3. **Sink**: The output format (`numpy`, `torch`, `png`, `jpeg`, `blob`, `native`)\n",
    "\n",
    "### 2.1 Your First Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07913c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple resize pipeline\n",
    "resize_pipe = (\n",
    "    Pipeline()\n",
    "    .source(\"image_bytes\")  # Input is PNG/JPEG bytes\n",
    "    .resize(height=128, width=128)  # Resize to 128x128\n",
    "    .sink(\"png\")  # Output as PNG bytes\n",
    ")\n",
    "\n",
    "# Print the pipeline structure\n",
    "print(\"Pipeline specification:\")\n",
    "print(resize_pipe)\n",
    "print()\n",
    "\n",
    "# Create a DataFrame with images\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"name\": [\"gradient\", \"checkerboard\", \"circles\"],\n",
    "        \"image\": [\n",
    "            test_images[\"gradient\"],\n",
    "            test_images[\"checkerboard\"],\n",
    "            test_images[\"circles\"],\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Apply the pipeline using .cv.pipeline()\n",
    "result = df.with_columns(resized=pl.col(\"image\").cv.pipeline(resize_pipe))\n",
    "\n",
    "print(f\"Original DataFrame schema: {df.schema}\")\n",
    "print(f\"Result DataFrame schema: {result.schema}\")\n",
    "\n",
    "# Display original vs resized\n",
    "row = result.row(0, named=True)\n",
    "display_images(\n",
    "    [row[\"image\"], row[\"resized\"]], [\"Original (256x256)\", \"Resized (128x128)\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9818243",
   "metadata": {},
   "source": [
    "### 2.2 Resize Filter Types\n",
    "\n",
    "polars-cv supports three resize filter types:\n",
    "- **nearest**: Fastest, best for pixel art or binary masks\n",
    "- **bilinear**: Good balance of speed and quality\n",
    "- **lanczos3**: Best quality, slower (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ebb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare resize filters\n",
    "filters = [\"nearest\", \"bilinear\", \"lanczos3\"]\n",
    "resized_images = []\n",
    "\n",
    "for filter_type in filters:\n",
    "    pipe = (\n",
    "        Pipeline()\n",
    "        .source(\"image_bytes\")\n",
    "        .resize(height=64, width=64, filter=filter_type)\n",
    "        .sink(\"png\")\n",
    "    )\n",
    "    result = pl.DataFrame({\"img\": [test_images[\"checkerboard\"]]}).with_columns(\n",
    "        out=pl.col(\"img\").cv.pipeline(pipe)\n",
    "    )\n",
    "    resized_images.append(result[\"out\"][0])\n",
    "\n",
    "display_images(\n",
    "    [test_images[\"checkerboard\"]] + resized_images,\n",
    "    [\"Original\"] + [f\"{f} (64x64)\" for f in filters],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675d181",
   "metadata": {},
   "source": [
    "### 2.3 Common Image Operations\n",
    "\n",
    "Let's explore common image operations with intermediate outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grayscale conversion\n",
    "gray_pipe = Pipeline().source(\"image_bytes\").grayscale().sink(\"png\")\n",
    "\n",
    "# Threshold (binary)\n",
    "threshold_pipe = Pipeline().source(\"image_bytes\").grayscale().threshold(128).sink(\"png\")\n",
    "\n",
    "# Blur\n",
    "blur_pipe = Pipeline().source(\"image_bytes\").blur(sigma=3.0).sink(\"png\")\n",
    "\n",
    "# Apply all to gradient image\n",
    "test_df = pl.DataFrame({\"img\": [test_images[\"gradient\"]]})\n",
    "ops_result = test_df.with_columns(\n",
    "    gray=pl.col(\"img\").cv.pipeline(gray_pipe),\n",
    "    threshold=pl.col(\"img\").cv.pipeline(threshold_pipe),\n",
    "    blur=pl.col(\"img\").cv.pipeline(blur_pipe),\n",
    ")\n",
    "\n",
    "row = ops_result.row(0, named=True)\n",
    "display_images(\n",
    "    [row[\"img\"], row[\"gray\"], row[\"threshold\"], row[\"blur\"]],\n",
    "    [\"Original\", \"Grayscale\", \"Threshold (128)\", \"Blur (Ïƒ=3)\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50264820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip operations and cropping\n",
    "flip_h_pipe = Pipeline().source(\"image_bytes\").flip_h().sink(\"png\")\n",
    "flip_v_pipe = Pipeline().source(\"image_bytes\").flip_v().sink(\"png\")\n",
    "crop_pipe = (\n",
    "    Pipeline()\n",
    "    .source(\"image_bytes\")\n",
    "    .crop(top=50, left=50, height=100, width=150)\n",
    "    .sink(\"png\")\n",
    ")\n",
    "\n",
    "test_df = pl.DataFrame({\"img\": [test_images[\"gradient\"]]})\n",
    "flip_result = test_df.with_columns(\n",
    "    flip_h=pl.col(\"img\").cv.pipeline(flip_h_pipe),\n",
    "    flip_v=pl.col(\"img\").cv.pipeline(flip_v_pipe),\n",
    "    crop=pl.col(\"img\").cv.pipeline(crop_pipe),\n",
    ")\n",
    "\n",
    "row = flip_result.row(0, named=True)\n",
    "display_images(\n",
    "    [row[\"img\"], row[\"flip_h\"], row[\"flip_v\"], row[\"crop\"]],\n",
    "    [\"Original\", \"Flip Horizontal\", \"Flip Vertical\", \"Cropped (100x150)\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed503b5e",
   "metadata": {},
   "source": [
    "### 2.4 Chained Operations\n",
    "\n",
    "Pipeline operations can be chained together. The operations are executed in a single pass through the Rust backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex chained pipeline - common preprocessing for ML\n",
    "ml_preprocess_pipe = (\n",
    "    Pipeline()\n",
    "    .source(\"image_bytes\")\n",
    "    .resize(height=256, width=256)  # Resize to standard size\n",
    "    .crop(top=16, left=16, height=224, width=224)  # Center crop\n",
    "    .flip_h()  # Data augmentation\n",
    "    .sink(\"png\")\n",
    ")\n",
    "\n",
    "print(\"ML Preprocessing Pipeline:\")\n",
    "print(ml_preprocess_pipe)\n",
    "\n",
    "# Apply to test image\n",
    "result = pl.DataFrame({\"img\": [test_images[\"circles\"]]}).with_columns(\n",
    "    processed=pl.col(\"img\").cv.pipeline(ml_preprocess_pipe)\n",
    ")\n",
    "\n",
    "row = result.row(0, named=True)\n",
    "display_images(\n",
    "    [row[\"img\"], row[\"processed\"]],\n",
    "    [\"Original (256x256)\", \"After ML Preprocessing (224x224)\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b0a04",
   "metadata": {},
   "source": [
    "## 3. DType Promotion & Normalization\n",
    "\n",
    "polars-cv implements an automatic **DType Promotion System** that handles type conversions seamlessly. Operations like `normalize` accept any numeric input and automatically promote integers to floats.\n",
    "\n",
    "### Key Concepts:\n",
    "- **MinMax normalization**: Scales values to [0, 1] range\n",
    "- **ZScore normalization**: Centers data around 0 with unit standard deviation\n",
    "- Outputs are float32 by default (configurable with `out_dtype`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102731e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization pipelines\n",
    "# Note: When using 'numpy' sink, we get raw bytes that can be converted back to arrays\n",
    "\n",
    "# MinMax normalization - outputs float32 in [0, 1]\n",
    "minmax_pipe = (\n",
    "    Pipeline()\n",
    "    .source(\"image_bytes\")\n",
    "    .grayscale()  # Convert to single channel for easier visualization\n",
    "    .normalize(method=\"minmax\")\n",
    "    .sink(\"numpy\")\n",
    ")\n",
    "\n",
    "# ZScore normalization - outputs float32 with mean=0, std=1\n",
    "zscore_pipe = (\n",
    "    Pipeline()\n",
    "    .source(\"image_bytes\")\n",
    "    .grayscale()\n",
    "    .normalize(method=\"zscore\")\n",
    "    .sink(\"numpy\")\n",
    ")\n",
    "\n",
    "# Apply both\n",
    "result = pl.DataFrame({\"img\": [test_images[\"gradient\"]]}).with_columns(\n",
    "    minmax=pl.col(\"img\").cv.pipeline(minmax_pipe),\n",
    "    zscore=pl.col(\"img\").cv.pipeline(zscore_pipe),\n",
    ")\n",
    "\n",
    "# Convert back to arrays for visualization using numpy_from_struct\n",
    "# It automatically parses the header with shape/dtype info\n",
    "minmax_arr = numpy_from_struct(result[\"minmax\"][0])\n",
    "zscore_arr = numpy_from_struct(result[\"zscore\"][0])\n",
    "\n",
    "print(f\"MinMax range: [{minmax_arr.min():.3f}, {minmax_arr.max():.3f}]\")\n",
    "print(f\"ZScore mean: {zscore_arr.mean():.3f}, std: {zscore_arr.std():.3f}\")\n",
    "\n",
    "display_arrays(\n",
    "    [minmax_arr.squeeze(), zscore_arr.squeeze()],\n",
    "    [\"MinMax Normalized [0,1]\", \"ZScore Normalized (Î¼=0, Ïƒ=1)\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85540f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and clamp operations\n",
    "# These also support automatic dtype promotion\n",
    "\n",
    "# Scale by factor\n",
    "scale_pipe = (\n",
    "    Pipeline()\n",
    "    .source(\"image_bytes\")\n",
    "    .grayscale()\n",
    "    .scale(factor=0.5)  # Halve all values\n",
    "    .sink(\"numpy\")\n",
    ")\n",
    "\n",
    "# Clamp to range\n",
    "clamp_pipe = (\n",
    "    Pipeline()\n",
    "    .source(\"image_bytes\")\n",
    "    .grayscale()\n",
    "    .normalize(method=\"minmax\")  # [0, 1]\n",
    "    .clamp(min_val=0.2, max_val=0.8)  # Clip to [0.2, 0.8]\n",
    "    .sink(\"numpy\")\n",
    ")\n",
    "\n",
    "result = pl.DataFrame({\"img\": [test_images[\"gradient\"]]}).with_columns(\n",
    "    scaled=pl.col(\"img\").cv.pipeline(scale_pipe),\n",
    "    clamped=pl.col(\"img\").cv.pipeline(clamp_pipe),\n",
    ")\n",
    "\n",
    "# scale output is f32 (promoted from u8), clamp is also f32\n",
    "scaled_arr = numpy_from_struct(result[\"scaled\"][0])\n",
    "clamped_arr = numpy_from_struct(result[\"clamped\"][0])\n",
    "\n",
    "print(f\"Scaled range: [{scaled_arr.min():.1f}, {scaled_arr.max():.1f}]\")\n",
    "print(f\"Clamped range: [{clamped_arr.min():.2f}, {clamped_arr.max():.2f}]\")\n",
    "\n",
    "display_arrays(\n",
    "    [scaled_arr.squeeze(), clamped_arr.squeeze()],\n",
    "    [\"Scaled (Ã—0.5)\", \"Clamped [0.2, 0.8]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86f9a8",
   "metadata": {},
   "source": [
    "## 4. Dynamic Parameters with Expressions\n",
    "\n",
    "One of polars-cv's most powerful features is **dynamic parameters**. Any pipeline parameter can be a Polars expression (`pl.col(...)`) that gets resolved per-row at execution time.\n",
    "\n",
    "This enables:\n",
    "- Per-image resize dimensions based on metadata\n",
    "- Adaptive thresholding based on image statistics\n",
    "- Dynamic cropping based on detected regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic resize - each row gets different dimensions!\n",
    "dynamic_resize_pipe = (\n",
    "    Pipeline()\n",
    "    .source(\"image_bytes\")\n",
    "    .resize(\n",
    "        height=pl.col(\"target_h\"), width=pl.col(\"target_w\")\n",
    "    )  # Expression parameters!\n",
    "    .sink(\"png\")\n",
    ")\n",
    "\n",
    "# Create DataFrame with per-row dimensions\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"name\": [\"small\", \"medium\", \"large\"],\n",
    "        \"image\": [test_images[\"circles\"]] * 3,\n",
    "        \"target_h\": [64, 128, 200],\n",
    "        \"target_w\": [64, 128, 200],\n",
    "    }\n",
    ")\n",
    "\n",
    "result = df.with_columns(resized=pl.col(\"image\").cv.pipeline(dynamic_resize_pipe))\n",
    "\n",
    "print(\"Each image resized to different dimensions:\")\n",
    "print(\n",
    "    result.select(\n",
    "        \"name\",\n",
    "        \"target_h\",\n",
    "        \"target_w\",\n",
    "        pl.col(\"resized\").bin.size().alias(\"output_bytes\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display all resized images\n",
    "display_images(\n",
    "    [result[\"resized\"][i] for i in range(3)],\n",
    "    [\n",
    "        f\"{row['name']} ({row['target_h']}x{row['target_w']})\"\n",
    "        for row in result.iter_rows(named=True)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic crop based on bounding box columns\n",
    "dynamic_crop_pipe = (\n",
    "    Pipeline()\n",
    "    .source(\"image_bytes\")\n",
    "    .crop(\n",
    "        top=pl.col(\"bbox_y\"),\n",
    "        left=pl.col(\"bbox_x\"),\n",
    "        height=pl.col(\"bbox_h\"),\n",
    "        width=pl.col(\"bbox_w\"),\n",
    "    )\n",
    "    .sink(\"png\")\n",
    ")\n",
    "\n",
    "# Simulate detected bounding boxes\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"image\": [test_images[\"gradient\"]] * 3,\n",
    "        \"region\": [\"top-left\", \"center\", \"bottom-right\"],\n",
    "        \"bbox_x\": [10, 80, 150],\n",
    "        \"bbox_y\": [10, 80, 150],\n",
    "        \"bbox_w\": [80, 100, 90],\n",
    "        \"bbox_h\": [80, 100, 90],\n",
    "    }\n",
    ")\n",
    "\n",
    "result = df.with_columns(cropped=pl.col(\"image\").cv.pipeline(dynamic_crop_pipe))\n",
    "\n",
    "display_images(\n",
    "    [result[\"cropped\"][i] for i in range(3)],\n",
    "    [f\"Crop: {row['region']}\" for row in result.iter_rows(named=True)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3f25a",
   "metadata": {},
   "source": [
    "## 5. Geometry Operations\n",
    "\n",
    "polars-cv provides a comprehensive geometry module for working with **contours**, **points**, and **bounding boxes**. This is essential for computer vision tasks like:\n",
    "- Object detection and segmentation\n",
    "- Annotation processing\n",
    "- IoU/Dice metrics calculation\n",
    "\n",
    "### 5.1 Contour Schema\n",
    "\n",
    "Contours are stored as Polars Struct columns with the following schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the contour schema\n",
    "print(\"CONTOUR_SCHEMA:\")\n",
    "print(CONTOUR_SCHEMA)\n",
    "print()\n",
    "print(\"POINT_SCHEMA:\")\n",
    "print(POINT_SCHEMA)\n",
    "print()\n",
    "print(\"BBOX_SCHEMA:\")\n",
    "print(BBOX_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contours using the helper function\n",
    "contours = [\n",
    "    # Square contour\n",
    "    contour_from_points([(50, 50), (50, 150), (150, 150), (150, 50)]),\n",
    "    # Triangle contour\n",
    "    contour_from_points([(100, 30), (30, 170), (170, 170)]),\n",
    "    # Irregular polygon (L-shape)\n",
    "    contour_from_points(\n",
    "        [(20, 20), (20, 180), (100, 180), (100, 100), (180, 100), (180, 20)]\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create DataFrame with contours\n",
    "contour_df = pl.DataFrame(\n",
    "    {\n",
    "        \"name\": [\"square\", \"triangle\", \"L-shape\"],\n",
    "        \"contour\": contours,\n",
    "    }\n",
    ").cast({\"contour\": CONTOUR_SCHEMA})\n",
    "\n",
    "print(\"Contour DataFrame:\")\n",
    "print(contour_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329cfd1",
   "metadata": {},
   "source": [
    "### 5.2 Geometric Measures\n",
    "\n",
    "The `.contour` namespace provides operations for computing geometric properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158328ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute geometric measures using the .contour namespace\n",
    "measures_df = contour_df.with_columns(\n",
    "    area=pl.col(\"contour\").contour.area(),\n",
    "    perimeter=pl.col(\"contour\").contour.perimeter(),\n",
    "    winding=pl.col(\"contour\").contour.winding(),\n",
    "    is_convex=pl.col(\"contour\").contour.is_convex(),\n",
    ")\n",
    "\n",
    "print(\"Geometric Measures:\")\n",
    "print(measures_df.select(\"name\", \"area\", \"perimeter\", \"winding\", \"is_convex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f3754",
   "metadata": {},
   "source": [
    "### 5.3 Rasterizing Contours to Masks\n",
    "\n",
    "The `source(\"contour\")` operation rasterizes contours to binary masks.\n",
    "You specify the output dimensions with width/height parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize contours to masks using polars-cv pipeline\n",
    "print(\"Contour rasterization with Pipeline().source('contour'):\")\n",
    "\n",
    "# Create a pipeline that rasterizes contour to a 200x200 mask\n",
    "contour_pipe = Pipeline().source(\"contour\", width=200, height=200).sink(\"numpy\")\n",
    "\n",
    "# Apply to contour DataFrame\n",
    "contour_raster_result = contour_df.with_columns(\n",
    "    mask=pl.col(\"contour\").cv.pipeline(contour_pipe)\n",
    ")\n",
    "print(contour_raster_result.select(\"name\", \"mask\"))\n",
    "\n",
    "# Verify the mask shape and visualize\n",
    "masks = []\n",
    "for i in range(len(contour_raster_result)):\n",
    "    mask_bytes = contour_raster_result[\"mask\"][i]\n",
    "    mask_arr = numpy_from_struct(mask_bytes)\n",
    "    masks.append(mask_arr.squeeze())\n",
    "\n",
    "print(f\"âœ… Rasterized mask shape: {masks[0].shape}, dtype: {masks[0].dtype}\")\n",
    "\n",
    "display_arrays(\n",
    "    masks,\n",
    "    [f\"{name} mask\" for name in contour_raster_result[\"name\"].to_list()],\n",
    "    cmap=\"gray\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c540b0a",
   "metadata": {},
   "source": [
    "## 6. Composable Pipelines (The Core of polars-cv)\n",
    "\n",
    "This is the **most powerful feature** of polars-cv. Instead of applying pipelines one at a time,\n",
    "you can compose them into a single DAG (Directed Acyclic Graph) that executes in one optimized pass.\n",
    "\n",
    "### Why Composable Pipelines?\n",
    "\n",
    "1. **Efficiency**: Multiple operations fused into a single Rust call\n",
    "2. **Reusability**: Define pipeline fragments once, reuse everywhere\n",
    "3. **Multi-output**: Extract multiple intermediate results from one execution\n",
    "4. **Automatic optimization**: CSE (Common Subexpression Elimination) shares common prefixes\n",
    "\n",
    "### Two Modes:\n",
    "\n",
    "| Mode | Syntax | Returns | Use Case |\n",
    "|------|--------|---------|----------|\n",
    "| **Eager** | `pl.col(\"x\").cv.pipeline(pipe)` | `pl.Expr` | Simple, single-output pipelines |\n",
    "| **Lazy** | `pl.col(\"x\").cv.pipe(pipe)` | `LazyPipelineExpr` | Composition, multi-output |\n",
    "\n",
    "The key insight: **Use `.cv.pipe()` for composition, call `.sink()` at the end to materialize.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy mode example - compose pipelines before execution\n",
    "\n",
    "# Define pipelines WITHOUT sinks (for lazy composition)\n",
    "img_pipe = Pipeline().source(\"image_bytes\").resize(height=200, width=200)\n",
    "\n",
    "# Create lazy expressions using .cv.pipe()\n",
    "img_expr = pl.col(\"image\").cv.pipe(img_pipe)  # Returns LazyPipelineExpr\n",
    "\n",
    "print(f\"img_expr type: {type(img_expr)}\")\n",
    "print(f\"img_expr: {img_expr}\")\n",
    "print()\n",
    "print(\"These are NOT Polars expressions yet - they need .sink() to materialize!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the lazy pipeline with .sink()\n",
    "\n",
    "# Create test data\n",
    "compose_df = pl.DataFrame({\"image\": [test_images[\"circles\"]]})\n",
    "\n",
    "# Method 1: Simple lazy composition\n",
    "result = compose_df.with_columns(\n",
    "    resized=pl.col(\"image\").cv.pipe(img_pipe).sink(\"png\"),\n",
    ")\n",
    "\n",
    "display_images([result[\"resized\"][0]], [\"Resized via Lazy Composition (200x200)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff482a3",
   "metadata": {},
   "source": [
    "### 6.1 Pipeline Chaining with `.pipe()`\n",
    "\n",
    "The `.pipe()` method allows chaining additional operations onto an existing `LazyPipelineExpr`.\n",
    "When the chained pipeline has **no source**, it continues from the upstream node's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base processing and chain additional operations\n",
    "base_pipe = Pipeline().source(\"image_bytes\").resize(height=128, width=128)\n",
    "gray_ops = Pipeline().grayscale()  # No source - will continue from upstream\n",
    "thresh_ops = Pipeline().threshold(128)  # No source - chains further\n",
    "\n",
    "# Chain operations using .pipe()\n",
    "base = pl.col(\"image\").cv.pipe(base_pipe)\n",
    "gray = base.pipe(gray_ops)  # Continues from 'base'\n",
    "thresh = gray.pipe(thresh_ops)  # Continues from 'gray'\n",
    "\n",
    "print(\"Chained LazyPipelineExpr:\")\n",
    "print(f\"  base: {base}\")\n",
    "print(f\"  gray: {gray}\")\n",
    "print(f\"  thresh: {thresh}\")\n",
    "\n",
    "# Execute the final result\n",
    "result = compose_df.with_columns(binary=thresh.sink(\"png\"))\n",
    "display_images([result[\"binary\"][0]], [\"Chained: resize â†’ grayscale â†’ threshold\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0568b752",
   "metadata": {},
   "source": [
    "## 7. Binary Operations & Mask Application\n",
    "\n",
    "polars-cv supports element-wise binary operations between two `LazyPipelineExpr` instances.\n",
    "These operations use **type-based semantics**:\n",
    "\n",
    "| Operation | u8/u16 Behavior | f32/f64 Behavior |\n",
    "|-----------|-----------------|------------------|\n",
    "| `add` | Saturating (clamps to 255) | Standard |\n",
    "| `subtract` | Saturating (clamps to 0) | Standard |\n",
    "| `multiply` | Saturating | Standard |\n",
    "| `blend` | Normalized: (a/255)*(b/255)*255 | Standard |\n",
    "| `divide` | Integer division | Standard |\n",
    "| `ratio` | Scaled: (a/b)*255 | Standard |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94383202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary operations work between two pipelines with the same output shape\n",
    "# We'll process the same image with different operations to demonstrate\n",
    "\n",
    "df_binary = pl.DataFrame(\n",
    "    {\n",
    "        \"image\": [test_images[\"circles\"]],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define two pipelines that process the same image differently\n",
    "# Both output 128x128 RGB images\n",
    "pipe_original = Pipeline().source(\"image_bytes\").resize(height=128, width=128)\n",
    "pipe_blurred = (\n",
    "    Pipeline().source(\"image_bytes\").resize(height=128, width=128).blur(sigma=5.0)\n",
    ")\n",
    "\n",
    "# Create lazy expressions from the same source\n",
    "img_original = pl.col(\"image\").cv.pipe(pipe_original)\n",
    "img_blurred = pl.col(\"image\").cv.pipe(pipe_blurred)\n",
    "\n",
    "# Binary operations (demonstrating with same-shape outputs)\n",
    "add_result = img_original.add(img_blurred).sink(\"png\")\n",
    "subtract_result = img_original.subtract(img_blurred).sink(\"png\")\n",
    "blend_result = img_original.blend(img_blurred).sink(\"png\")\n",
    "\n",
    "result = df_binary.with_columns(\n",
    "    original=img_original.sink(\"png\"),\n",
    "    blurred=img_blurred.sink(\"png\"),\n",
    "    added=add_result,\n",
    "    subtracted=subtract_result,\n",
    "    blended=blend_result,\n",
    ")\n",
    "\n",
    "row = result.row(0, named=True)\n",
    "display_images(\n",
    "    [row[\"original\"], row[\"blurred\"]],\n",
    "    [\"Original\", \"Blurred (Ïƒ=5)\"],\n",
    ")\n",
    "display_images(\n",
    "    [row[\"added\"], row[\"subtracted\"], row[\"blended\"]],\n",
    "    [\"Add (saturating)\", \"Subtract (edge detect)\", \"Blend (normalized)\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3fafae",
   "metadata": {},
   "source": [
    "### 7.1 Mask Application\n",
    "\n",
    "The `apply_mask()` method applies a binary mask to an image. Where the mask is zero,\n",
    "the output is zero; where the mask is non-zero, the original values are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e670d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a circular mask using a contour\n",
    "circle_contour = contour_from_points(\n",
    "    [\n",
    "        (64 + 50 * np.cos(a), 64 + 50 * np.sin(a))\n",
    "        for a in np.linspace(0, 2 * np.pi, 32, endpoint=False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_mask = pl.DataFrame(\n",
    "    {\n",
    "        \"image\": [test_images[\"gradient\"]],\n",
    "        \"mask_contour\": [circle_contour],\n",
    "    }\n",
    ").cast({\"mask_contour\": CONTOUR_SCHEMA})\n",
    "\n",
    "# Image pipeline\n",
    "img_pipe = Pipeline().source(\"image_bytes\").resize(height=128, width=128)\n",
    "img = pl.col(\"image\").cv.pipe(img_pipe)\n",
    "\n",
    "# Contour source with explicit dimensions (rasterizes to mask)\n",
    "mask_pipe = Pipeline().source(\"contour\", width=128, height=128)\n",
    "mask = pl.col(\"mask_contour\").cv.pipe(mask_pipe)\n",
    "\n",
    "# Apply mask to image\n",
    "masked_result = img.apply_mask(mask).sink(\"png\")\n",
    "\n",
    "result = df_mask.with_columns(masked=masked_result)\n",
    "\n",
    "row = result.row(0, named=True)\n",
    "display_images(\n",
    "    [row[\"image\"], row[\"masked\"]],\n",
    "    [\"Original\", \"Masked with Circular Contour\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd05fa",
   "metadata": {},
   "source": [
    "### 7.2 Shape Inference for Contour Sources\n",
    "\n",
    "When working with composed pipelines, you can use `shape=` to infer contour rasterization\n",
    "dimensions from another `LazyPipelineExpr`. This ensures the mask matches the image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adecc1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape inference example - mask dimensions match image automatically\n",
    "df_shape = pl.DataFrame(\n",
    "    {\n",
    "        \"image\": [test_images[\"circles\"]],\n",
    "        \"contour\": [\n",
    "            contour_from_points([(30, 30), (30, 90), (90, 90), (90, 30)])\n",
    "        ],  # Square\n",
    "    }\n",
    ").cast({\"contour\": CONTOUR_SCHEMA})\n",
    "\n",
    "# Define image pipeline with specific dimensions\n",
    "img_pipe = Pipeline().source(\"image_bytes\").resize(height=100, width=150)  # Non-square!\n",
    "img = pl.col(\"image\").cv.pipe(img_pipe)\n",
    "\n",
    "# Contour source with shape= to infer dimensions from image\n",
    "mask_pipe = Pipeline().source(\"contour\", shape=img)  # Auto-infers 150x100\n",
    "mask = pl.col(\"contour\").cv.pipe(mask_pipe)\n",
    "\n",
    "# Apply mask\n",
    "result = df_shape.with_columns(\n",
    "    masked=img.apply_mask(mask).sink(\"png\"),\n",
    ")\n",
    "\n",
    "print(\"Shape inference: contour mask auto-matched to 150x100 image\")\n",
    "display_images([result[\"masked\"][0]], [\"Masked with Auto-Sized Contour\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3dee1",
   "metadata": {},
   "source": [
    "### 7.3 Convenience: `apply_contour_mask()`\n",
    "\n",
    "For the common case of applying a contour as a mask, use the convenience method\n",
    "`apply_contour_mask()` which auto-infers dimensions from the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c6d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply_contour_mask() convenience method\n",
    "result = df_shape.with_columns(\n",
    "    # This auto-infers dimensions from img's output shape\n",
    "    masked=img.apply_contour_mask(mask).sink(\"png\"),\n",
    ")\n",
    "\n",
    "display_images([result[\"masked\"][0]], [\"apply_contour_mask() convenience\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64cd54d",
   "metadata": {},
   "source": [
    "## 8. Multi-Source Pipelines\n",
    "\n",
    "polars-cv supports **multi-source** pipelines where different branches read from\n",
    "different DataFrame columns. This is essential for workflows like:\n",
    "- Comparing two images\n",
    "- Applying a separately-loaded mask\n",
    "- Computing difference between prediction and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-source example: combine data from two different columns\n",
    "# This demonstrates how to read from different DataFrame columns in one pipeline\n",
    "\n",
    "df_multi = pl.DataFrame(\n",
    "    {\n",
    "        \"base_image\": [test_images[\"gradient\"]],\n",
    "        \"overlay_image\": [\n",
    "            test_images[\"gradient\"]\n",
    "        ],  # Use same source for compatible shapes\n",
    "    }\n",
    ")\n",
    "\n",
    "# Two separate pipelines reading from different columns\n",
    "# Note: Both must produce same-shape outputs for binary operations\n",
    "base_pipe = Pipeline().source(\"image_bytes\").resize(height=128, width=128)\n",
    "overlay_pipe = Pipeline().source(\"image_bytes\").resize(height=128, width=128).flip_h()\n",
    "\n",
    "# Create lazy expressions from different columns\n",
    "base = pl.col(\"base_image\").cv.pipe(base_pipe)\n",
    "overlay = pl.col(\"overlay_image\").cv.pipe(overlay_pipe)\n",
    "\n",
    "# Blend them together - multi-source composition!\n",
    "result = df_multi.with_columns(\n",
    "    base_out=base.sink(\"png\"),\n",
    "    overlay_out=overlay.sink(\"png\"),\n",
    "    blended=base.blend(overlay).sink(\"png\"),\n",
    ")\n",
    "\n",
    "row = result.row(0, named=True)\n",
    "display_images(\n",
    "    [row[\"base_out\"], row[\"overlay_out\"], row[\"blended\"]],\n",
    "    [\"Base (gradient)\", \"Overlay (flipped)\", \"Blended\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824ab46",
   "metadata": {},
   "source": [
    "## 9. Multi-Output with CSE Optimization\n",
    "\n",
    "polars-cv supports **multi-output pipelines** using `.alias()` and dict-based `.sink()`.\n",
    "Combined with automatic **Common Subexpression Elimination (CSE)**, this enables highly\n",
    "efficient pipelines where shared operations are computed only once.\n",
    "\n",
    "### The Pattern:\n",
    "1. Mark intermediate points with `.alias(name)` - creates a named checkpoint\n",
    "2. Branch from checkpoints using `.pipe()` for different outputs\n",
    "3. Merge branches with `.merge_pipe()` to include all in the graph\n",
    "4. Sink multiple outputs with `.sink({alias: format, ...})`\n",
    "\n",
    "### Benefits:\n",
    "- **Shared operations computed once** - CSE automatically extracts common prefixes\n",
    "- **Single plugin call** - entire graph executes in one optimized pass\n",
    "- **Struct output** - all results returned in a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b419080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-output pipeline with aliases\n",
    "df_multi_out = pl.DataFrame({\"image\": [test_images[\"circles\"]]})\n",
    "\n",
    "# Build a branching pipeline with named checkpoints\n",
    "base = (\n",
    "    pl.col(\"image\")\n",
    "    .cv.pipe(Pipeline().source(\"image_bytes\").resize(height=128, width=128))\n",
    "    .alias(\"resized\")  # Checkpoint 1\n",
    ")\n",
    "\n",
    "# Branch 1: grayscale\n",
    "gray = base.pipe(Pipeline().grayscale()).alias(\"gray\")  # Checkpoint 2\n",
    "\n",
    "# Branch 2: threshold (from grayscale)\n",
    "thresh = gray.pipe(Pipeline().threshold(128)).alias(\"thresh\")  # Checkpoint 3\n",
    "\n",
    "# Branch 3: blur (from grayscale)\n",
    "blur = gray.pipe(Pipeline().blur(sigma=3.0)).alias(\"blur\")  # Checkpoint 4\n",
    "\n",
    "# Merge branches and sink multiple outputs\n",
    "merged = thresh.merge_pipe(blur)  # Combine branches for multi-output\n",
    "\n",
    "# Sink with dict: returns Struct column with named Binary fields\n",
    "result = df_multi_out.with_columns(\n",
    "    outputs=merged.sink(\n",
    "        {\n",
    "            \"resized\": \"png\",\n",
    "            \"gray\": \"png\",\n",
    "            \"thresh\": \"png\",\n",
    "            \"blur\": \"png\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Multi-output result schema:\", result.schema)\n",
    "print()\n",
    "print(\"Outputs column contains a Struct with named fields:\")\n",
    "print(result[\"outputs\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract individual outputs from the Struct column\n",
    "extracted = result.select(\n",
    "    pl.col(\"outputs\").struct.field(\"resized\").alias(\"resized_png\"),\n",
    "    pl.col(\"outputs\").struct.field(\"gray\").alias(\"gray_png\"),\n",
    "    pl.col(\"outputs\").struct.field(\"thresh\").alias(\"thresh_png\"),\n",
    "    pl.col(\"outputs\").struct.field(\"blur\").alias(\"blur_png\"),\n",
    ")\n",
    "\n",
    "# Display all outputs\n",
    "display_images(\n",
    "    [\n",
    "        extracted[\"resized_png\"][0],\n",
    "        extracted[\"gray_png\"][0],\n",
    "        extracted[\"thresh_png\"][0],\n",
    "        extracted[\"blur_png\"][0],\n",
    "    ],\n",
    "    [\"Resized\", \"Grayscale\", \"Threshold\", \"Blur\"],\n",
    ")\n",
    "\n",
    "print(\"âœ… All 4 outputs computed from a single fused pipeline execution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483355c5",
   "metadata": {},
   "source": [
    "### 9.1 Common Subexpression Elimination (CSE)\n",
    "\n",
    "When multiple branches share common operations, polars-cv automatically detects\n",
    "and extracts shared prefixes. This optimization is transparent - you don't need to\n",
    "change your code!\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Before CSE:\n",
    "  gray_pipe: source â†’ resize â†’ grayscale\n",
    "  mask_pipe: source â†’ resize â†’ grayscale â†’ threshold â†’ extract\n",
    "\n",
    "After CSE:\n",
    "  _shared:   source â†’ resize â†’ grayscale  (computed once)\n",
    "  gray_pipe: (empty) â† upstream: _shared\n",
    "  mask_pipe: threshold â†’ extract â† upstream: _shared\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSE example: two branches with shared prefix\n",
    "print(\"CSE Optimization Example:\")\n",
    "print()\n",
    "print(\n",
    "    \"When we define two pipelines that both start with resizeâ†’grayscale,\"\n",
    "    \" CSE automatically shares that prefix.\"\n",
    ")\n",
    "print()\n",
    "\n",
    "# Both branches share: resize â†’ grayscale\n",
    "base = pl.col(\"image\").cv.pipe(\n",
    "    Pipeline().source(\"image_bytes\").resize(height=100, width=100)\n",
    ")\n",
    "gray = base.pipe(Pipeline().grayscale()).alias(\"gray\")\n",
    "\n",
    "# Branch 1: blur\n",
    "branch1 = gray.pipe(Pipeline().blur(2.0)).alias(\"blurred\")\n",
    "\n",
    "# Branch 2: threshold\n",
    "branch2 = gray.pipe(Pipeline().threshold(128)).alias(\"thresholded\")\n",
    "\n",
    "# Merge and execute - CSE will share the gray computation\n",
    "merged = branch1.merge_pipe(branch2)\n",
    "result = df_multi_out.with_columns(\n",
    "    outputs=merged.sink({\"gray\": \"png\", \"blurred\": \"png\", \"thresholded\": \"png\"})\n",
    ")\n",
    "\n",
    "extracted = result.select(\n",
    "    pl.col(\"outputs\").struct.field(\"gray\").alias(\"gray\"),\n",
    "    pl.col(\"outputs\").struct.field(\"blurred\").alias(\"blurred\"),\n",
    "    pl.col(\"outputs\").struct.field(\"thresholded\").alias(\"thresholded\"),\n",
    ")\n",
    "\n",
    "display_images(\n",
    "    [extracted[\"gray\"][0], extracted[\"blurred\"][0], extracted[\"thresholded\"][0]],\n",
    "    [\"Gray (shared)\", \"Branch 1: Blur\", \"Branch 2: Threshold\"],\n",
    ")\n",
    "\n",
    "print(\"âœ… Grayscale computed once and shared between both branches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b5d61",
   "metadata": {},
   "source": [
    "## 10. Reusable Pipeline Patterns\n",
    "\n",
    "The composition system enables powerful **software engineering patterns** for pipelines:\n",
    "\n",
    "| Pattern | Description |\n",
    "|---------|-------------|\n",
    "| **Fragments** | Define operation groups as variables, chain with `.pipe()` |\n",
    "| **Factories** | Functions that return configured pipelines |\n",
    "| **Config-driven** | Build pipelines from dictionaries/configs |\n",
    "\n",
    "These patterns make pipelines testable, maintainable, and reusable across projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3fded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Reusable pipeline fragments\n",
    "preprocessing_ops = Pipeline().resize(height=128, width=128).flip_h()\n",
    "augmentation_ops = Pipeline().blur(sigma=1.5)\n",
    "normalization_ops = Pipeline().grayscale().normalize(method=\"minmax\")\n",
    "\n",
    "\n",
    "# Pattern 2: Parameterized pipeline factory\n",
    "def create_resize_pipeline(size: int) -> Pipeline:\n",
    "    \"\"\"Create a resize pipeline with specified size.\"\"\"\n",
    "    return Pipeline().source(\"image_bytes\").resize(height=size, width=size)\n",
    "\n",
    "\n",
    "def create_augmentation_chain(flip: bool = True, blur_sigma: float = 0.0) -> Pipeline:\n",
    "    \"\"\"Create an augmentation pipeline with configurable options.\"\"\"\n",
    "    ops = Pipeline()\n",
    "    if flip:\n",
    "        ops = ops.flip_h()\n",
    "    if blur_sigma > 0:\n",
    "        ops = ops.blur(sigma=blur_sigma)\n",
    "    return ops\n",
    "\n",
    "\n",
    "# Use the factories\n",
    "df_reuse = pl.DataFrame({\"image\": [test_images[\"gradient\"]]})\n",
    "\n",
    "# Compose reusable fragments\n",
    "base = pl.col(\"image\").cv.pipe(create_resize_pipeline(100))\n",
    "augmented = base.pipe(create_augmentation_chain(flip=True, blur_sigma=2.0))\n",
    "final = augmented.pipe(normalization_ops)\n",
    "\n",
    "result = df_reuse.with_columns(processed=final.sink(\"numpy\"))\n",
    "\n",
    "arr = numpy_from_struct(result[\"processed\"][0])\n",
    "print(f\"Final output shape: {arr.shape}, dtype: {arr.dtype}\")\n",
    "print(f\"Value range: [{arr.min():.3f}, {arr.max():.3f}]\")\n",
    "\n",
    "display_arrays([arr.squeeze()], [\"Reusable Pipeline Composition\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d023ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 3: Configuration-driven pipeline creation\n",
    "\n",
    "\n",
    "def build_ml_pipeline(config: dict[str, Any]) -> Pipeline:\n",
    "    \"\"\"Build an ML preprocessing pipeline from configuration.\"\"\"\n",
    "    pipe = Pipeline().source(\"image_bytes\")\n",
    "\n",
    "    # Resize if specified\n",
    "    if \"target_size\" in config:\n",
    "        size = config[\"target_size\"]\n",
    "        pipe = pipe.resize(height=size, width=size)\n",
    "\n",
    "    # Apply augmentations\n",
    "    if config.get(\"flip_horizontal\", False):\n",
    "        pipe = pipe.flip_h()\n",
    "    if config.get(\"flip_vertical\", False):\n",
    "        pipe = pipe.flip_v()\n",
    "\n",
    "    # Color/normalize\n",
    "    if config.get(\"grayscale\", False):\n",
    "        pipe = pipe.grayscale()\n",
    "    if config.get(\"normalize\", False):\n",
    "        pipe = pipe.normalize(method=config.get(\"normalize_method\", \"minmax\"))\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# Example configurations\n",
    "train_config = {\n",
    "    \"target_size\": 224,\n",
    "    \"flip_horizontal\": True,\n",
    "    \"normalize\": True,\n",
    "    \"normalize_method\": \"minmax\",\n",
    "}\n",
    "\n",
    "inference_config = {\n",
    "    \"target_size\": 224,\n",
    "    \"grayscale\": True,\n",
    "    \"normalize\": True,\n",
    "}\n",
    "\n",
    "# Build pipelines from config\n",
    "train_pipe = build_ml_pipeline(train_config)\n",
    "inference_pipe = build_ml_pipeline(inference_config)\n",
    "\n",
    "print(\"Train pipeline:\", train_pipe)\n",
    "print(\"Inference pipeline:\", inference_pipe)\n",
    "\n",
    "# Apply both - note: normalized output is float32, use 'numpy' sink\n",
    "# PNG requires U8 dtype, so we use numpy for float data\n",
    "result = df_reuse.with_columns(\n",
    "    train=pl.col(\"image\").cv.pipe(train_pipe).sink(\"numpy\"),\n",
    "    inference=pl.col(\"image\").cv.pipe(inference_pipe).sink(\"numpy\"),\n",
    ")\n",
    "\n",
    "train_output = numpy_from_struct(result[\"train\"][0])\n",
    "inference_output = numpy_from_struct(result[\"inference\"][0])\n",
    "\n",
    "print(f\"\\nTrain output shape: {train_output.shape}, dtype: {train_output.dtype}\")\n",
    "print(\n",
    "    f\"Inference output shape: {inference_output.shape}, dtype: {inference_output.dtype}\"\n",
    ")\n",
    "\n",
    "# Display both as arrays since they're float32\n",
    "display_arrays(\n",
    "    [train_output[:, :, 0], inference_output.squeeze()],  # Take first channel for train\n",
    "    [\"Train Preprocessing (channel 0)\", \"Inference Preprocessing\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b29951",
   "metadata": {},
   "source": [
    "## 11. Domain Transitions: Images â†” Contours â†” Scalars\n",
    "\n",
    "polars-cv supports **multi-domain pipelines** with seamless transitions:\n",
    "\n",
    "| Domain | Description | Example Operations |\n",
    "|--------|-------------|-------------------|\n",
    "| **buffer** | Image/array data | resize, grayscale, threshold, blur |\n",
    "| **contour** | Polygon geometry | area, perimeter, translate, scale |\n",
    "| **scalar** | Single number | (output of area, perimeter, etc.) |\n",
    "| **vector** | Multiple numbers | (output of centroid, bbox, etc.) |\n",
    "\n",
    "### Domain Transitions:\n",
    "- `buffer â†’ contour`: `extract_contours()` - Extract polygons from binary mask\n",
    "- `contour â†’ buffer`: `rasterize()` or `source(\"contour\")` - Draw polygon to mask\n",
    "- `contour â†’ scalar`: `area()`, `perimeter()` - Compute measurements\n",
    "- `contour â†’ vector`: `centroid()`, `bounding_box()` - Return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d455f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete domain transition example: Image â†’ Contour â†’ Scalar\n",
    "\n",
    "# Create a binary mask image\n",
    "binary_pipe = (\n",
    "    Pipeline()\n",
    "    .source(\"image_bytes\")\n",
    "    .grayscale()\n",
    "    .threshold(128)  # Creates binary mask\n",
    "    .sink(\"png\")\n",
    ")\n",
    "\n",
    "# Apply to segmentation test image\n",
    "df_domain = pl.DataFrame({\"image\": [test_images[\"segmentation\"]]})\n",
    "result = df_domain.with_columns(binary=pl.col(\"image\").cv.pipeline(binary_pipe))\n",
    "\n",
    "display_images(\n",
    "    [test_images[\"segmentation\"], result[\"binary\"][0]],\n",
    "    [\"Original Segmentation\", \"Binary Threshold\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract contours and compute properties using the .contour namespace\n",
    "# (This uses the DataFrame-level contour operations)\n",
    "\n",
    "# First, create contours directly and compute their properties\n",
    "shapes = [\n",
    "    (\n",
    "        \"circle\",\n",
    "        contour_from_points(\n",
    "            [\n",
    "                (100 + 40 * np.cos(a), 85 + 40 * np.sin(a))\n",
    "                for a in np.linspace(0, 2 * np.pi, 32, endpoint=False)\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    (\"rectangle\", contour_from_points([(85, 100), (85, 160), (165, 160), (165, 100)])),\n",
    "    (\"triangle\", contour_from_points([(200, 80), (160, 140), (240, 140)])),\n",
    "]\n",
    "\n",
    "df_shapes = pl.DataFrame(\n",
    "    {\"name\": [s[0] for s in shapes], \"contour\": [s[1] for s in shapes]}\n",
    ").cast({\"contour\": CONTOUR_SCHEMA})\n",
    "\n",
    "# Compute geometric properties\n",
    "result_props = df_shapes.with_columns(\n",
    "    area=pl.col(\"contour\").contour.area(),\n",
    "    perimeter=pl.col(\"contour\").contour.perimeter(),\n",
    "    is_convex=pl.col(\"contour\").contour.is_convex(),\n",
    ")\n",
    "\n",
    "print(\"Shape Properties (contour â†’ scalar domain):\")\n",
    "print(result_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabb91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize contours back to masks (contour â†’ buffer)\n",
    "raster_pipe = Pipeline().source(\"contour\", width=200, height=200).sink(\"numpy\")\n",
    "\n",
    "df_raster = df_shapes.with_columns(mask=pl.col(\"contour\").cv.pipeline(raster_pipe))\n",
    "\n",
    "masks = [numpy_from_struct(df_raster[\"mask\"][i]).squeeze() for i in range(3)]\n",
    "display_arrays(\n",
    "    masks, [f\"{name} mask\" for name in df_raster[\"name\"].to_list()], cmap=\"gray\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d1e71",
   "metadata": {},
   "source": [
    "## 12. ML Workflow: Segmentation Pipeline\n",
    "\n",
    "Let's build a complete **ML-style segmentation workflow** that demonstrates:\n",
    "\n",
    "1. Processing input images through a preprocessing pipeline\n",
    "2. Generating fake predictions (simulating model output)\n",
    "3. Processing ground truth contour annotations\n",
    "4. Computing **IoU** and **Dice** metrics using native `mask_iou()` and `mask_dice()`\n",
    "5. Visualizing predictions vs ground truth with overlays\n",
    "\n",
    "**Key advantage**: The native `mask_iou()` and `mask_dice()` functions compute metrics\n",
    "directly on `LazyPipelineExpr` objects - no Python loops required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57bd5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic ML data\n",
    "\n",
    "\n",
    "def create_heatmap_prediction(cx: int, cy: int, sigma: float, size: int = 200) -> bytes:\n",
    "    \"\"\"Create a fake heatmap prediction (simulating model output).\"\"\"\n",
    "    y, x = np.ogrid[:size, :size]\n",
    "    gaussian = np.exp(-((x - cx) ** 2 + (y - cy) ** 2) / (2 * sigma**2))\n",
    "    # Convert to 8-bit grayscale PNG\n",
    "    img = (gaussian * 255).astype(np.uint8)\n",
    "    pil_img = Image.fromarray(img, mode=\"L\")\n",
    "    buffer = io.BytesIO()\n",
    "    pil_img.save(buffer, format=\"PNG\")\n",
    "    return buffer.getvalue()\n",
    "\n",
    "\n",
    "def create_ground_truth_contour(\n",
    "    cx: int, cy: int, radius: int, n_points: int = 32\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Create a circular ground truth contour.\"\"\"\n",
    "    angles = np.linspace(0, 2 * np.pi, n_points, endpoint=False)\n",
    "    points = [(cx + radius * np.cos(a), cy + radius * np.sin(a)) for a in angles]\n",
    "    return contour_from_points(points)\n",
    "\n",
    "\n",
    "# Create dataset with predictions and ground truth\n",
    "np.random.seed(42)\n",
    "n_samples = 5\n",
    "\n",
    "data: dict[str, list[Any]] = {\n",
    "    \"sample_id\": list(range(n_samples)),\n",
    "    \"prediction\": [],\n",
    "    \"ground_truth\": [],\n",
    "}\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Ground truth center and radius\n",
    "    gt_cx, gt_cy = 100 + np.random.randint(-20, 20), 100 + np.random.randint(-20, 20)\n",
    "    gt_radius = 40 + np.random.randint(-10, 10)\n",
    "\n",
    "    # Prediction center (with some error - small offset to ensure overlap)\n",
    "    pred_cx = gt_cx + np.random.randint(-8, 8)\n",
    "    pred_cy = gt_cy + np.random.randint(-8, 8)\n",
    "    # Sigma chosen so thresholded area roughly matches GT radius\n",
    "    # For threshold at 128 (50% of 255), radius â‰ˆ 1.18 * sigma\n",
    "    pred_sigma = gt_radius / 1.18  # Spread to match GT area\n",
    "\n",
    "    data[\"prediction\"].append(create_heatmap_prediction(pred_cx, pred_cy, pred_sigma))\n",
    "    data[\"ground_truth\"].append(create_ground_truth_contour(gt_cx, gt_cy, gt_radius))\n",
    "\n",
    "ml_df = pl.DataFrame(data).cast({\"ground_truth\": CONTOUR_SCHEMA})\n",
    "print(f\"ML DataFrame schema: {ml_df.schema}\")\n",
    "print(ml_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process predictions and ground truth using multi-source lazy composition\n",
    "\n",
    "# Prediction pipeline: grayscale then threshold to binary mask\n",
    "# Note: grayscale() is needed before threshold() to ensure correct processing\n",
    "pred_pipe = Pipeline().source(\"image_bytes\").grayscale().threshold(128)\n",
    "pred_expr = pl.col(\"prediction\").cv.pipe(pred_pipe)\n",
    "\n",
    "# Ground truth pipeline: rasterize contours to masks\n",
    "gt_pipe = Pipeline().source(\"contour\", width=200, height=200)\n",
    "gt_expr = pl.col(\"ground_truth\").cv.pipe(gt_pipe)\n",
    "\n",
    "# Process both and get outputs\n",
    "processed = ml_df.with_columns(\n",
    "    pred_mask=pred_expr.sink(\"png\"),\n",
    "    gt_mask=gt_expr.sink(\"png\"),\n",
    ")\n",
    "\n",
    "# Visualize first sample\n",
    "row = processed.row(0, named=True)\n",
    "display_images(\n",
    "    [row[\"prediction\"], row[\"pred_mask\"], row[\"gt_mask\"]],\n",
    "    [\"Raw Heatmap\", \"Thresholded Prediction\", \"Ground Truth Mask\"],\n",
    "    cmap=\"gray\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IoU and Dice using polars-cv native functions\n",
    "# This avoids Python loops and is much more efficient for large datasets!\n",
    "\n",
    "# Contour-based IoU (comparing ground truth contours with themselves - should be 1.0)\n",
    "contour_metrics = ml_df.select(\n",
    "    \"sample_id\",\n",
    "    iou_self=pl.col(\"ground_truth\").contour.iou(pl.col(\"ground_truth\")),\n",
    "    dice_self=pl.col(\"ground_truth\").contour.dice(pl.col(\"ground_truth\")),\n",
    "    gt_area=pl.col(\"ground_truth\").contour.area(),\n",
    ")\n",
    "\n",
    "print(\"Contour-based Metrics (comparing GT with itself):\")\n",
    "print(contour_metrics)\n",
    "\n",
    "# Pixel-based IoU/Dice using native mask_iou() and mask_dice() functions\n",
    "# These operate directly on LazyPipelineExpr - no Python loops needed!\n",
    "pred_pipe_metrics = Pipeline().source(\"image_bytes\").grayscale().threshold(128)\n",
    "gt_pipe_metrics = Pipeline().source(\"contour\", width=200, height=200)\n",
    "\n",
    "# Create lazy pipeline expressions\n",
    "pred_expr = pl.col(\"prediction\").cv.pipe(pred_pipe_metrics)\n",
    "gt_expr = pl.col(\"ground_truth\").cv.pipe(gt_pipe_metrics)\n",
    "\n",
    "# Compute metrics using native functions in a single optimized pass\n",
    "pixel_metrics_df = ml_df.select(\n",
    "    \"sample_id\",\n",
    "    iou=mask_iou(pred_expr, gt_expr),\n",
    "    dice=mask_dice(pred_expr, gt_expr),\n",
    ")\n",
    "\n",
    "print(\"\\nPixel-based Segmentation Metrics (pred vs GT) using native functions:\")\n",
    "print(pixel_metrics_df)\n",
    "print(f\"\\nMean IoU: {pixel_metrics_df['iou'].mean():.3f}\")\n",
    "print(f\"Mean Dice: {pixel_metrics_df['dice'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2152048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overlay of predictions vs ground truth\n",
    "\n",
    "\n",
    "def create_overlay(pred_bytes: bytes, gt_bytes: bytes) -> np.ndarray:\n",
    "    \"\"\"Create RGB overlay: green=GT, red=pred, yellow=overlap.\"\"\"\n",
    "    pred = np.array(Image.open(io.BytesIO(pred_bytes)).convert(\"L\")) > 128\n",
    "    gt = np.array(Image.open(io.BytesIO(gt_bytes)).convert(\"L\")) > 128\n",
    "\n",
    "    h, w = pred.shape\n",
    "    overlay = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    # Red channel: prediction\n",
    "    overlay[:, :, 0] = pred.astype(np.uint8) * 255\n",
    "    # Green channel: ground truth\n",
    "    overlay[:, :, 1] = gt.astype(np.uint8) * 255\n",
    "    # Yellow where both overlap (R+G)\n",
    "\n",
    "    return overlay\n",
    "\n",
    "\n",
    "# Create overlays for first 3 samples\n",
    "overlays = []\n",
    "titles = []\n",
    "# Get IoU values from our metrics DataFrame\n",
    "iou_values = pixel_metrics_df[\"iou\"].to_list()\n",
    "for i, row in enumerate(processed.head(3).iter_rows(named=True)):\n",
    "    overlay = create_overlay(row[\"pred_mask\"], row[\"gt_mask\"])\n",
    "    overlays.append(overlay)\n",
    "    titles.append(f\"Sample {i} (IoU={iou_values[i]:.2f})\")\n",
    "\n",
    "print(\"Overlay: Green=GT, Red=Pred, Yellow=Overlap\")\n",
    "display_images(overlays, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb8d7a9",
   "metadata": {},
   "source": [
    "### 12.1 Multi-Output ML Pipeline\n",
    "\n",
    "Let's create a more comprehensive ML pipeline that outputs:\n",
    "- Original image (normalized)\n",
    "- Predicted mask\n",
    "- Ground truth mask\n",
    "- Overlay visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive multi-output ML pipeline\n",
    "df_ml = ml_df.head(1)  # Use first sample\n",
    "\n",
    "# Create base expressions\n",
    "pred_base = (\n",
    "    pl.col(\"prediction\").cv.pipe(Pipeline().source(\"image_bytes\")).alias(\"pred_raw\")\n",
    ")\n",
    "\n",
    "# Note: grayscale() is needed before threshold() for proper binary mask creation\n",
    "pred_thresh = pred_base.pipe(Pipeline().grayscale().threshold(128)).alias(\"pred_mask\")\n",
    "\n",
    "gt_mask = (\n",
    "    pl.col(\"ground_truth\")\n",
    "    .cv.pipe(Pipeline().source(\"contour\", width=200, height=200))\n",
    "    .alias(\"gt_mask\")\n",
    ")\n",
    "\n",
    "# For multi-output, merge the branches\n",
    "merged = pred_thresh.merge_pipe(gt_mask)\n",
    "\n",
    "# Sink all outputs\n",
    "result = df_ml.with_columns(\n",
    "    outputs=merged.sink(\n",
    "        {\n",
    "            \"pred_raw\": \"png\",\n",
    "            \"pred_mask\": \"png\",\n",
    "            \"gt_mask\": \"png\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Extract and display\n",
    "extracted = result.select(\n",
    "    pl.col(\"outputs\").struct.field(\"pred_raw\").alias(\"pred_raw\"),\n",
    "    pl.col(\"outputs\").struct.field(\"pred_mask\").alias(\"pred_mask\"),\n",
    "    pl.col(\"outputs\").struct.field(\"gt_mask\").alias(\"gt_mask\"),\n",
    ")\n",
    "\n",
    "display_images(\n",
    "    [extracted[\"pred_raw\"][0], extracted[\"pred_mask\"][0], extracted[\"gt_mask\"][0]],\n",
    "    [\"Prediction (raw)\", \"Prediction (threshold)\", \"Ground Truth\"],\n",
    ")\n",
    "\n",
    "print(\"âœ… Multi-output ML pipeline with 3 outputs from single execution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399aa53a",
   "metadata": {},
   "source": [
    "## 13. PyTorch Integration\n",
    "\n",
    "polars-cv can output directly to **torch format** for seamless ML integration.\n",
    "The `torch` sink produces bytes that can be converted to PyTorch tensors.\n",
    "\n",
    "### Architecture Considerations\n",
    "\n",
    "**polars-cv** is optimized for **batch-columnar processing**:\n",
    "- Processes entire columns/Series at once\n",
    "- Leverages Rust parallelism and SIMD optimizations\n",
    "- Best performance when processing many rows together\n",
    "\n",
    "**PyTorch DataLoader** is designed for **sample-wise processing**:\n",
    "- Calls `__getitem__(idx)` for individual samples\n",
    "- Batches samples *after* individual retrieval\n",
    "\n",
    "The recommended pattern is to **preprocess all images with polars-cv** in batch,\n",
    "then use PyTorch transforms for per-sample augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c993b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if PyTorch is available\n",
    "try:\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"âš ï¸ PyTorch not installed - skipping torch integration demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3201d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # Pipeline that outputs torch-compatible format\n",
    "    # ImageNet-style preprocessing\n",
    "    torch_pipe = (\n",
    "        Pipeline()\n",
    "        .source(\"image_bytes\")\n",
    "        .resize(height=224, width=224)  # ImageNet size\n",
    "        .normalize(method=\"minmax\")  # Scale to [0, 1]\n",
    "        .sink(\"torch\")  # Output as torch-compatible bytes\n",
    "    )\n",
    "\n",
    "    # Process batch of images\n",
    "    batch_df = pl.DataFrame(\n",
    "        {\n",
    "            \"image\": [\n",
    "                test_images[\"gradient\"],\n",
    "                test_images[\"circles\"],\n",
    "                test_images[\"checkerboard\"],\n",
    "            ],\n",
    "            \"label\": [0, 1, 2],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    processed = batch_df.with_columns(\n",
    "        tensor_bytes=pl.col(\"image\").cv.pipeline(torch_pipe)\n",
    "    )\n",
    "\n",
    "    print(f\"Processed {len(processed)} images\")\n",
    "    print(f\"Tensor bytes column dtype: {processed['tensor_bytes'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e66f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # Convert bytes to PyTorch tensors using numpy_from_struct\n",
    "    def bytes_to_torch(data: bytes) -> torch.Tensor:\n",
    "        \"\"\"Convert torch-format bytes to PyTorch tensor using numpy_from_struct.\"\"\"\n",
    "        arr = numpy_from_struct(data)\n",
    "        return torch.from_numpy(arr.copy())\n",
    "\n",
    "    # Create tensor batch\n",
    "    tensors = []\n",
    "    labels = []\n",
    "\n",
    "    for row in processed.iter_rows(named=True):\n",
    "        # Shape after processing: (224, 224, 3) for RGB, float32\n",
    "        tensor = bytes_to_torch(row[\"tensor_bytes\"])\n",
    "        # Transpose to PyTorch format: (C, H, W)\n",
    "        tensor = tensor.permute(2, 0, 1)\n",
    "        tensors.append(tensor)\n",
    "        labels.append(row[\"label\"])\n",
    "\n",
    "    # Stack into batch\n",
    "    batch_tensor = torch.stack(tensors)\n",
    "    batch_labels = torch.tensor(labels)\n",
    "\n",
    "    print(f\"Batch tensor shape: {batch_tensor.shape}\")\n",
    "    print(f\"Batch tensor dtype: {batch_tensor.dtype}\")\n",
    "    print(f\"Batch labels: {batch_labels}\")\n",
    "    print(f\"Value range: [{batch_tensor.min():.3f}, {batch_tensor.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # Create a Dataset class for DataLoader integration\n",
    "    #\n",
    "    # KEY PATTERN: Batch preprocessing with polars-cv\n",
    "    # - All images are preprocessed in __init__ using Polars' batch processing\n",
    "    # - This leverages Polars' parallelism and SIMD optimizations\n",
    "    # - __getitem__ only retrieves already-processed data\n",
    "    # - Optional PyTorch transforms apply per-sample augmentations\n",
    "\n",
    "    class PreprocessedPolarsDataset(Dataset):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset with batch preprocessing.\n",
    "\n",
    "        polars-cv preprocesses ALL images in __init__ using batch processing.\n",
    "        The DataLoader then retrieves already-processed samples efficiently.\n",
    "        Per-sample augmentations are applied in __getitem__ via PyTorch transforms.\n",
    "\n",
    "        This pattern leverages each framework's strengths:\n",
    "        - Polars: Heavy batch preprocessing (resize, normalize, decode)\n",
    "        - PyTorch: Per-sample random augmentations (flips, rotations)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            df: pl.DataFrame,\n",
    "            image_col: str,\n",
    "            label_col: str,\n",
    "            pipeline: Pipeline,\n",
    "            transform: \"callable | None\" = None,  # PyTorch augmentations\n",
    "        ) -> None:\n",
    "            \"\"\"\n",
    "            Initialize dataset with batch preprocessing.\n",
    "\n",
    "            Args:\n",
    "                df: Source DataFrame with image bytes\n",
    "                image_col: Column containing image bytes\n",
    "                label_col: Column containing labels\n",
    "                pipeline: polars-cv Pipeline for preprocessing\n",
    "                transform: Optional PyTorch transform for augmentation\n",
    "            \"\"\"\n",
    "            # Batch preprocess ALL images with polars-cv\n",
    "            # This leverages Polars' parallel execution and SIMD optimizations\n",
    "            self.df = df.with_columns(_tensor=pl.col(image_col).cv.pipeline(pipeline))\n",
    "            self.label_col = label_col\n",
    "            self.transform = transform  # Per-sample augmentation (PyTorch)\n",
    "\n",
    "        def __len__(self) -> int:\n",
    "            \"\"\"Return dataset size.\"\"\"\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx: int) -> tuple[torch.Tensor, int]:\n",
    "            \"\"\"Get a single sample (already preprocessed by Polars).\"\"\"\n",
    "            row = self.df.row(idx, named=True)\n",
    "            tensor = bytes_to_torch(row[\"_tensor\"])\n",
    "            tensor = tensor.permute(2, 0, 1)  # (C, H, W)\n",
    "\n",
    "            # Apply PyTorch augmentations (varies per-epoch if random)\n",
    "            if self.transform:\n",
    "                tensor = self.transform(tensor)\n",
    "\n",
    "            label = row[self.label_col]\n",
    "            return tensor, label\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = PreprocessedPolarsDataset(batch_df, \"image\", \"label\", torch_pipe)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    # Iterate through batches\n",
    "    print(\"DataLoader iteration:\")\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        print(\n",
    "            f\"  Batch {batch_idx}: images shape={images.shape}, labels={labels.tolist()}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nâœ… Batch preprocessing with polars-cv + PyTorch DataLoader!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6c036",
   "metadata": {},
   "source": [
    "### Augmentation: Division of Responsibilities\n",
    "\n",
    "For optimal results, divide preprocessing between Polars and PyTorch:\n",
    "\n",
    "**Use polars-cv for:**\n",
    "- Heavy preprocessing (decode, resize, normalize)\n",
    "- Operations that benefit from batch processing\n",
    "- Deterministic operations (same every epoch)\n",
    "\n",
    "**Use PyTorch transforms for:**\n",
    "- Random augmentations (flips, rotations, crops)\n",
    "- Per-sample variations that should differ each epoch\n",
    "- Operations that should vary during training\n",
    "\n",
    "This hybrid approach respects each framework's architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f47f2",
   "metadata": {},
   "source": [
    "## 14. Perceptual Image Hashing\n",
    "\n",
    "polars-cv provides **perceptual image hashing** for finding similar images.\n",
    "Unlike cryptographic hashes (MD5, SHA), perceptual hashes produce similar\n",
    "fingerprints for visually similar images, even after transformations.\n",
    "\n",
    "### Key Features:\n",
    "- **Robust to small changes**: Resize, blur, format conversion produce similar hashes\n",
    "- **Distinguishes different images**: Structurally different images have different hashes\n",
    "- **Multiple algorithms**: Average, Difference, Perceptual (DCT), and Blockhash\n",
    "- **Native comparison**: `hamming_distance()` and `hash_similarity()` functions\n",
    "\n",
    "### Use Cases:\n",
    "- **Duplicate detection**: Find near-duplicate images in large datasets\n",
    "- **Image similarity search**: Find visually similar images\n",
    "- **Content deduplication**: Identify copies with minor edits\n",
    "\n",
    "**Key advantage**: The native `hamming_distance()` and `hash_similarity()` functions\n",
    "work on entire DataFrames without Python loops - essential for large-scale deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polars-cv provides native functions for hash comparison\n",
    "# These functions work directly on LazyPipelineExpr objects for efficient batch processing:\n",
    "# - hamming_distance(hash1, hash2) -> Polars expression returning distance\n",
    "# - hash_similarity(hash1, hash2) -> Polars expression returning similarity %\n",
    "#\n",
    "# These are designed for efficient batch operations on DataFrames, not individual\n",
    "# hash comparisons. We'll demonstrate both approaches in this section.\n",
    "\n",
    "print(\"âœ… Native hash comparison functions available:\")\n",
    "print(\"   â€¢ hamming_distance(hash1, hash2) - returns Polars expression\")\n",
    "print(\"   â€¢ hash_similarity(hash1, hash2) - returns Polars expression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d1577",
   "metadata": {},
   "source": [
    "### 14.1 Basic Perceptual Hash Usage\n",
    "\n",
    "Let's compute perceptual hashes for our test images and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b03bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a perceptual hash pipeline\n",
    "phash_pipe = Pipeline().source(\"image_bytes\").perceptual_hash().sink(\"list\")\n",
    "\n",
    "# Compute hashes for test images\n",
    "hash_df = pl.DataFrame(\n",
    "    {\n",
    "        \"name\": [\"gradient\", \"checkerboard\", \"circles\", \"noise\"],\n",
    "        \"image\": [\n",
    "            test_images[\"gradient\"],\n",
    "            test_images[\"checkerboard\"],\n",
    "            test_images[\"circles\"],\n",
    "            test_images[\"noise\"],\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "hash_result = hash_df.with_columns(hash=pl.col(\"image\").cv.pipeline(phash_pipe))\n",
    "\n",
    "print(\"Perceptual hashes for test images:\")\n",
    "for row in hash_result.iter_rows(named=True):\n",
    "    hash_hex = \"\".join(f\"{b:02x}\" for b in row[\"hash\"])\n",
    "    print(f\"  {row['name']:12s}: {hash_hex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81dbcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare hashes between all pairs using native hash_similarity() function\n",
    "# This uses a cross-join approach for efficient batch computation\n",
    "\n",
    "# Create cross-join for pairwise comparison\n",
    "left_hash = hash_df.select(\n",
    "    pl.col(\"name\").alias(\"name_a\"), pl.col(\"image\").alias(\"image_a\")\n",
    ")\n",
    "right_hash = hash_df.select(\n",
    "    pl.col(\"name\").alias(\"name_b\"), pl.col(\"image\").alias(\"image_b\")\n",
    ")\n",
    "cross_hash = left_hash.join(right_hash, how=\"cross\")\n",
    "\n",
    "# Define hash pipelines for both columns\n",
    "pipe_a = Pipeline().source(\"image_bytes\").perceptual_hash()\n",
    "pipe_b = Pipeline().source(\"image_bytes\").perceptual_hash()\n",
    "\n",
    "# Compute similarity using native function\n",
    "similarity_matrix = cross_hash.with_columns(\n",
    "    similarity=hash_similarity(\n",
    "        pl.col(\"image_a\").cv.pipe(pipe_a),\n",
    "        pl.col(\"image_b\").cv.pipe(pipe_b),\n",
    "        hash_bits=64,\n",
    "    )\n",
    ").select(\"name_a\", \"name_b\", \"similarity\")\n",
    "\n",
    "# Display as a pivot table\n",
    "print(\"\\nHash similarity matrix (%) using native hash_similarity():\")\n",
    "pivot = similarity_matrix.pivot(on=\"name_b\", index=\"name_a\", values=\"similarity\")\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057c278",
   "metadata": {},
   "source": [
    "### 14.2 Robustness to Small Processing Changes\n",
    "\n",
    "Perceptual hashes are designed to be **robust to common image transformations**:\n",
    "- Resizing (downscale/upscale)\n",
    "- Blur/smoothing\n",
    "- Format conversion (PNG â†’ JPEG)\n",
    "- Minor color adjustments\n",
    "\n",
    "Let's demonstrate this robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f144a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variations of the same image\n",
    "original_img = test_images[\"circles\"]\n",
    "\n",
    "# Apply various transformations using polars-cv pipelines\n",
    "# 1. Resize to smaller then back to original size (lossy operation)\n",
    "resize_pipe = (\n",
    "    Pipeline()\n",
    "    .source(\"image_bytes\")\n",
    "    .resize(height=64, width=64)  # Downscale\n",
    "    .resize(height=256, width=256)  # Upscale back\n",
    "    .sink(\"png\")\n",
    ")\n",
    "\n",
    "# 2. Blur the image\n",
    "blur_pipe = Pipeline().source(\"image_bytes\").blur(sigma=2.0).sink(\"png\")\n",
    "\n",
    "# 3. Convert to JPEG with compression (lossy)\n",
    "jpeg_pipe = Pipeline().source(\"image_bytes\").sink(\"jpeg\")\n",
    "\n",
    "# Apply transformations\n",
    "transform_df = pl.DataFrame({\"image\": [original_img]})\n",
    "transformed = transform_df.with_columns(\n",
    "    resized=pl.col(\"image\").cv.pipeline(resize_pipe),\n",
    "    blurred=pl.col(\"image\").cv.pipeline(blur_pipe),\n",
    "    jpeg=pl.col(\"image\").cv.pipeline(jpeg_pipe),\n",
    ")\n",
    "\n",
    "# Now compute perceptual hashes for all versions\n",
    "variants_df = pl.DataFrame(\n",
    "    {\n",
    "        \"variant\": [\"original\", \"resized\", \"blurred\", \"jpeg\"],\n",
    "        \"image\": [\n",
    "            original_img,\n",
    "            transformed[\"resized\"][0],\n",
    "            transformed[\"blurred\"][0],\n",
    "            transformed[\"jpeg\"][0],\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "variants_hashed = variants_df.with_columns(hash=pl.col(\"image\").cv.pipeline(phash_pipe))\n",
    "\n",
    "# Display images side by side\n",
    "display_images(\n",
    "    [variants_hashed[\"image\"][i] for i in range(4)],\n",
    "    variants_hashed[\"variant\"].to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0d8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all variants to the original using native functions\n",
    "# We'll compare each variant against the original image\n",
    "\n",
    "# Create a DataFrame with original paired against each variant\n",
    "original_bytes = variants_hashed.filter(pl.col(\"variant\") == \"original\")[\"image\"][0]\n",
    "\n",
    "variants_comparison = variants_hashed.with_columns(\n",
    "    pl.lit(original_bytes).alias(\"original_image\")\n",
    ")\n",
    "\n",
    "# Define pipelines\n",
    "orig_pipe = Pipeline().source(\"image_bytes\").perceptual_hash()\n",
    "var_pipe = Pipeline().source(\"image_bytes\").perceptual_hash()\n",
    "\n",
    "# Compute similarity and distance using native functions\n",
    "variants_with_metrics = variants_comparison.with_columns(\n",
    "    similarity=hash_similarity(\n",
    "        pl.col(\"original_image\").cv.pipe(orig_pipe),\n",
    "        pl.col(\"image\").cv.pipe(var_pipe),\n",
    "        hash_bits=64,\n",
    "    ),\n",
    "    distance=hamming_distance(\n",
    "        pl.col(\"original_image\").cv.pipe(orig_pipe),\n",
    "        pl.col(\"image\").cv.pipe(var_pipe),\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Similarity of transformed images to original:\")\n",
    "print(\"-\" * 50)\n",
    "for row in variants_with_metrics.iter_rows(named=True):\n",
    "    sim = row[\"similarity\"]\n",
    "    dist = int(row[\"distance\"])\n",
    "    hash_hex = \"\".join(f\"{b:02x}\" for b in row[\"hash\"])\n",
    "    status = \"âœ…\" if sim >= 90 else (\"âš ï¸\" if sim >= 75 else \"âŒ\")\n",
    "    print(f\"{status} {row['variant']:12s}: {sim:5.1f}% similar (distance: {dist} bits)\")\n",
    "    print(f\"   Hash: {hash_hex}\")\n",
    "\n",
    "print()\n",
    "print(\"âœ… Small transformations produce similar hashes (high similarity)\")\n",
    "print(\"   This demonstrates robustness to resize, blur, and JPEG compression!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2913623",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### 14.3 Different Images Produce Different Hashes\n",
    "\n",
    "While perceptual hashes are robust to small changes, they **correctly distinguish**\n",
    "structurally different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d595ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some structurally very different images\n",
    "def create_pattern_image(pattern: str, size: int = 256) -> bytes:\n",
    "    \"\"\"Create images with different patterns for comparison.\"\"\"\n",
    "    img = np.zeros((size, size, 3), dtype=np.uint8)\n",
    "\n",
    "    if pattern == \"solid_red\":\n",
    "        img[:, :] = [255, 0, 0]\n",
    "    elif pattern == \"solid_blue\":\n",
    "        img[:, :] = [0, 0, 255]\n",
    "    elif pattern == \"horizontal_stripes\":\n",
    "        for i in range(0, size, 32):\n",
    "            img[i : i + 16, :] = [255, 255, 255]\n",
    "    elif pattern == \"vertical_stripes\":\n",
    "        for i in range(0, size, 32):\n",
    "            img[:, i : i + 16] = [255, 255, 255]\n",
    "    elif pattern == \"diagonal\":\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                if (i + j) % 32 < 16:\n",
    "                    img[i, j] = [255, 255, 255]\n",
    "\n",
    "    pil_img = Image.fromarray(img)\n",
    "    buffer = io.BytesIO()\n",
    "    pil_img.save(buffer, format=\"PNG\")\n",
    "    return buffer.getvalue()\n",
    "\n",
    "\n",
    "# Create different pattern images\n",
    "different_images = {\n",
    "    \"circles\": test_images[\"circles\"],\n",
    "    \"checkerboard\": test_images[\"checkerboard\"],\n",
    "    \"solid_red\": create_pattern_image(\"solid_red\"),\n",
    "    \"solid_blue\": create_pattern_image(\"solid_blue\"),\n",
    "    \"h_stripes\": create_pattern_image(\"horizontal_stripes\"),\n",
    "    \"v_stripes\": create_pattern_image(\"vertical_stripes\"),\n",
    "}\n",
    "\n",
    "# Display them\n",
    "display_images(\n",
    "    list(different_images.values())[:4],\n",
    "    list(different_images.keys())[:4],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca50d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hashes and compare using native functions\n",
    "diff_df = pl.DataFrame(\n",
    "    {\n",
    "        \"name\": list(different_images.keys()),\n",
    "        \"image\": list(different_images.values()),\n",
    "    }\n",
    ")\n",
    "\n",
    "diff_hashed = diff_df.with_columns(hash=pl.col(\"image\").cv.pipeline(phash_pipe))\n",
    "\n",
    "# Compare circles (our reference) with all other images using native functions\n",
    "reference_name = \"circles\"\n",
    "reference_bytes = diff_hashed.filter(pl.col(\"name\") == reference_name)[\"image\"][0]\n",
    "\n",
    "# Add reference image for comparison\n",
    "diff_comparison = diff_hashed.filter(pl.col(\"name\") != reference_name).with_columns(\n",
    "    pl.lit(reference_bytes).alias(\"reference_image\")\n",
    ")\n",
    "\n",
    "# Compute metrics using native functions\n",
    "ref_pipe = Pipeline().source(\"image_bytes\").perceptual_hash()\n",
    "img_pipe = Pipeline().source(\"image_bytes\").perceptual_hash()\n",
    "\n",
    "diff_with_metrics = diff_comparison.with_columns(\n",
    "    similarity=hash_similarity(\n",
    "        pl.col(\"reference_image\").cv.pipe(ref_pipe),\n",
    "        pl.col(\"image\").cv.pipe(img_pipe),\n",
    "        hash_bits=64,\n",
    "    ),\n",
    "    distance=hamming_distance(\n",
    "        pl.col(\"reference_image\").cv.pipe(ref_pipe),\n",
    "        pl.col(\"image\").cv.pipe(img_pipe),\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Comparing '{reference_name}' with other images:\")\n",
    "print(\"-\" * 55)\n",
    "for row in diff_with_metrics.iter_rows(named=True):\n",
    "    sim = row[\"similarity\"]\n",
    "    dist = int(row[\"distance\"])\n",
    "    # Different images should have low similarity\n",
    "    status = \"âœ…\" if sim < 75 else \"âš ï¸\"\n",
    "    print(f\"{status} {row['name']:12s}: {sim:5.1f}% similar (distance: {dist} bits)\")\n",
    "\n",
    "print()\n",
    "print(\"âœ… Different images correctly produce different hashes (low similarity)\")\n",
    "print(\"   This shows the hash distinguishes structurally different content!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f01b06",
   "metadata": {},
   "source": [
    "### 14.4 Hash Algorithm Comparison\n",
    "\n",
    "polars-cv supports multiple perceptual hash algorithms. Each has different\n",
    "characteristics:\n",
    "\n",
    "| Algorithm | Speed | Robustness | Best For |\n",
    "|-----------|-------|------------|----------|\n",
    "| **Average** | Fastest | Lower | Quick approximate matching |\n",
    "| **Difference** | Fast | Medium | General purpose |\n",
    "| **Perceptual** | Medium | High | Most use cases (default) |\n",
    "| **Blockhash** | Medium | High | Crop-resistant matching |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9830b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different hash algorithms on the same image transformation\n",
    "test_image = test_images[\"circles\"]\n",
    "\n",
    "# Create a transformed version (resize)\n",
    "resized_test = pl.DataFrame({\"image\": [test_image]}).with_columns(\n",
    "    resized=pl.col(\"image\").cv.pipeline(resize_pipe)\n",
    ")[\"resized\"][0]\n",
    "\n",
    "algorithms = [\n",
    "    HashAlgorithm.AVERAGE,\n",
    "    HashAlgorithm.DIFFERENCE,\n",
    "    HashAlgorithm.PERCEPTUAL,\n",
    "    HashAlgorithm.BLOCKHASH,\n",
    "]\n",
    "\n",
    "print(\"Algorithm comparison: Original vs Resized image\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for algo in algorithms:\n",
    "    # Create hash pipelines for this algorithm\n",
    "    orig_algo_pipe = Pipeline().source(\"image_bytes\").perceptual_hash(algorithm=algo)\n",
    "    resized_algo_pipe = Pipeline().source(\"image_bytes\").perceptual_hash(algorithm=algo)\n",
    "\n",
    "    # Compare original vs resized using native functions\n",
    "    algo_df = pl.DataFrame(\n",
    "        {\"original\": [test_image], \"resized\": [resized_test]}\n",
    "    ).with_columns(\n",
    "        similarity=hash_similarity(\n",
    "            pl.col(\"original\").cv.pipe(orig_algo_pipe),\n",
    "            pl.col(\"resized\").cv.pipe(resized_algo_pipe),\n",
    "            hash_bits=64,\n",
    "        ),\n",
    "        distance=hamming_distance(\n",
    "            pl.col(\"original\").cv.pipe(orig_algo_pipe),\n",
    "            pl.col(\"resized\").cv.pipe(resized_algo_pipe),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    sim = algo_df[\"similarity\"][0]\n",
    "    dist = int(algo_df[\"distance\"][0])\n",
    "\n",
    "    status = \"âœ…\" if sim >= 85 else (\"âš ï¸\" if sim >= 70 else \"âŒ\")\n",
    "    print(f\"{status} {algo.value:12s}: {sim:5.1f}% similar (distance: {dist} bits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e639ac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### 14.5 Practical Use Case: Finding Duplicates in a Dataset\n",
    "\n",
    "Here's how you might use perceptual hashing to find near-duplicate images\n",
    "in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0123256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a dataset with some duplicates (slightly modified versions)\n",
    "def add_noise(img_bytes: bytes, intensity: float = 0.05) -> bytes:\n",
    "    \"\"\"Add slight random noise to an image.\"\"\"\n",
    "    img = np.array(Image.open(io.BytesIO(img_bytes)))\n",
    "    rng = np.random.default_rng(42)\n",
    "    noise = (rng.random(img.shape) * intensity * 255).astype(np.int16)\n",
    "    noisy = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    pil_img = Image.fromarray(noisy)\n",
    "    buffer = io.BytesIO()\n",
    "    pil_img.save(buffer, format=\"PNG\")\n",
    "    return buffer.getvalue()\n",
    "\n",
    "\n",
    "# Create a \"dataset\" with some near-duplicates\n",
    "dataset_images = [\n",
    "    (\"image_001\", test_images[\"gradient\"]),\n",
    "    (\"image_002\", test_images[\"circles\"]),\n",
    "    (\"image_003\", add_noise(test_images[\"gradient\"], 0.02)),  # Near-dup of 001\n",
    "    (\"image_004\", test_images[\"checkerboard\"]),\n",
    "    (\"image_005\", test_images[\"circles\"]),  # Exact dup of 002\n",
    "    (\"image_006\", add_noise(test_images[\"circles\"], 0.03)),  # Near-dup of 002\n",
    "]\n",
    "\n",
    "dataset_df = pl.DataFrame(\n",
    "    {\n",
    "        \"id\": [img[0] for img in dataset_images],\n",
    "        \"image\": [img[1] for img in dataset_images],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Find potential duplicates using native functions (no Python loops!)\n",
    "# This is the scalable approach for large datasets\n",
    "SIMILARITY_THRESHOLD = 85.0\n",
    "\n",
    "# Create cross-join for pairwise comparison\n",
    "dup_left = dataset_df.select(\n",
    "    pl.col(\"id\").alias(\"id_a\"), pl.col(\"image\").alias(\"image_a\")\n",
    ")\n",
    "dup_right = dataset_df.select(\n",
    "    pl.col(\"id\").alias(\"id_b\"), pl.col(\"image\").alias(\"image_b\")\n",
    ")\n",
    "dup_cross = dup_left.join(dup_right, how=\"cross\")\n",
    "\n",
    "# Filter to only compare where id_a < id_b (avoid duplicates and self-comparison)\n",
    "dup_cross = dup_cross.filter(pl.col(\"id_a\") < pl.col(\"id_b\"))\n",
    "\n",
    "# Define hash pipelines\n",
    "dup_pipe_a = Pipeline().source(\"image_bytes\").perceptual_hash()\n",
    "dup_pipe_b = Pipeline().source(\"image_bytes\").perceptual_hash()\n",
    "\n",
    "# Compute similarity using native function\n",
    "dup_result = dup_cross.with_columns(\n",
    "    similarity=hash_similarity(\n",
    "        pl.col(\"image_a\").cv.pipe(dup_pipe_a),\n",
    "        pl.col(\"image_b\").cv.pipe(dup_pipe_b),\n",
    "        hash_bits=64,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Find duplicates above threshold\n",
    "duplicates = dup_result.filter(pl.col(\"similarity\") >= SIMILARITY_THRESHOLD).sort(\n",
    "    \"similarity\", descending=True\n",
    ")\n",
    "\n",
    "print(\"Finding potential duplicates (similarity > 85%):\")\n",
    "print(\"-\" * 55)\n",
    "for row in duplicates.iter_rows(named=True):\n",
    "    print(f\"  {row['id_a']} â†” {row['id_b']}: {row['similarity']:.1f}% similar\")\n",
    "\n",
    "if len(duplicates) == 0:\n",
    "    print(\"  No duplicates found above threshold\")\n",
    "\n",
    "print()\n",
    "print(f\"âœ… Found {len(duplicates)} potential duplicate pairs\")\n",
    "print(\"   This technique scales well to large datasets - no Python loops required!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ac7b2",
   "metadata": {},
   "source": [
    "### 14.6 Native Hash Comparison Functions\n",
    "\n",
    "For efficient batch processing, polars-cv provides **native functions**\n",
    "that work directly with pipeline expressions:\n",
    "\n",
    "- `hamming_distance(hash1, hash2)` - Returns Polars expression with bit distance\n",
    "- `hash_similarity(hash1, hash2)` - Returns Polars expression with similarity %\n",
    "\n",
    "These are much faster for large datasets as they leverage the full pipeline\n",
    "optimization and avoid Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a686022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate native hash comparison on a cross-join of images\n",
    "# This efficiently compares ALL pairs in a single optimized operation\n",
    "\n",
    "# Create cross-join for pairwise comparison\n",
    "left = dataset_df.select(pl.col(\"id\").alias(\"id_a\"), pl.col(\"image\").alias(\"image_a\"))\n",
    "right = dataset_df.select(pl.col(\"id\").alias(\"id_b\"), pl.col(\"image\").alias(\"image_b\"))\n",
    "cross = left.join(right, how=\"cross\")\n",
    "\n",
    "# Filter to only compare where id_a < id_b (avoid duplicates and self-comparison)\n",
    "cross = cross.filter(pl.col(\"id_a\") < pl.col(\"id_b\"))\n",
    "\n",
    "# Define hash pipelines for both columns\n",
    "hash_pipe_a = Pipeline().source(\"image_bytes\").perceptual_hash()\n",
    "hash_pipe_b = Pipeline().source(\"image_bytes\").perceptual_hash()\n",
    "\n",
    "# Create lazy pipeline expressions\n",
    "hash_a = pl.col(\"image_a\").cv.pipe(hash_pipe_a)\n",
    "hash_b = pl.col(\"image_b\").cv.pipe(hash_pipe_b)\n",
    "\n",
    "# Use native hash comparison functions - fully optimized!\n",
    "result = cross.with_columns(\n",
    "    distance=hamming_distance(hash_a, hash_b),\n",
    "    similarity=hash_similarity(hash_a, hash_b, hash_bits=64),\n",
    ")\n",
    "\n",
    "print(\"Pairwise comparison using native hamming_distance() and hash_similarity():\")\n",
    "print(\"-\" * 65)\n",
    "similar_pairs = result.filter(pl.col(\"similarity\") >= SIMILARITY_THRESHOLD).sort(\n",
    "    \"similarity\", descending=True\n",
    ")\n",
    "for row in similar_pairs.iter_rows(named=True):\n",
    "    print(\n",
    "        f\"  {row['id_a']} â†” {row['id_b']}: \"\n",
    "        f\"{row['similarity']:.1f}% similar (distance: {int(row['distance'])} bits)\"\n",
    "    )\n",
    "\n",
    "print()\n",
    "print(f\"âœ… Native function found {len(similar_pairs)} similar pairs\")\n",
    "print(\"   This approach is highly scalable - no Python loops required!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7a3b2",
   "metadata": {},
   "source": [
    "## 15. Conclusion\n",
    "\n",
    "This notebook demonstrated the key capabilities of **polars-cv**:\n",
    "\n",
    "### âœ… What We Covered\n",
    "\n",
    "| Section | Highlights |\n",
    "|---------|------------|\n",
    "| **Basic Pipelines** | Source/sink architecture, image operations, chained processing |\n",
    "| **DType Promotion** | Automatic type conversion, normalization methods |\n",
    "| **Dynamic Parameters** | Per-row customization using Polars expressions |\n",
    "| **Geometry** | Contour schemas, geometric measures, rasterization |\n",
    "| **Composable Pipelines** | `.cv.pipe()`, `.pipe()` chaining, fused execution |\n",
    "| **Binary Operations** | add, subtract, blend, mask application |\n",
    "| **Multi-Source** | Different columns feeding different branches |\n",
    "| **Multi-Output + CSE** | `.alias()`, `.merge_pipe()`, automatic optimization |\n",
    "| **Reusable Patterns** | Fragments, factories, config-driven pipelines |\n",
    "| **Domain Transitions** | Image â†” Contour â†” Scalar conversions |\n",
    "| **ML Workflow** | `mask_iou()`, `mask_dice()` for segmentation metrics |\n",
    "| **PyTorch** | Direct tensor output, DataLoader integration |\n",
    "| **Perceptual Hashing** | `hamming_distance()`, `hash_similarity()` for duplicate detection |\n",
    "\n",
    "### ðŸ”‘ Key Takeaways\n",
    "\n",
    "1. **Use `.cv.pipe()` for composition** - enables multi-output and CSE optimization\n",
    "2. **Native functions avoid Python loops** - `mask_iou()`, `mask_dice()`, `hamming_distance()`, `hash_similarity()`\n",
    "3. **CSE is automatic** - shared prefixes are computed once, no manual optimization needed\n",
    "4. **Multi-domain is seamless** - images, contours, and scalars in one pipeline\n",
    "\n",
    "### ðŸ”— Resources\n",
    "\n",
    "- **Repository**: [polars-cv](https://github.com/heshamdar/polars-cv)\n",
    "- **view-buffer**: The underlying Rust tensor orchestration library\n",
    "- **Polars Documentation**: [pola.rs](https://pola.rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2011c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰ Demo complete! polars-cv provides:\")\n",
    "print(\"   â€¢ High-performance image processing in Polars\")\n",
    "print(\"   â€¢ Zero-copy operations where possible\")\n",
    "print(\"   â€¢ Composable, reusable pipelines with named nodes\")\n",
    "print(\"   â€¢ Multi-source and multi-output support\")\n",
    "print(\"   â€¢ Automatic CSE optimization\")\n",
    "print(\"   â€¢ Perceptual image hashing for similarity detection\")\n",
    "print(\"   â€¢ Seamless ML framework integration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
