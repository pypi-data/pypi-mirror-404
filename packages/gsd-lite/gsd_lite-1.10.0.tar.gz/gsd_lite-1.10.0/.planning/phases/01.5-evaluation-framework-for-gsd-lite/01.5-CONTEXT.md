# Phase 1.5: Evaluation Framework for GSD-lite - Context

**Gathered:** 2026-01-26
**Status:** Ready for planning

<domain>
## Phase Boundary

Build evaluation sequence in `./test` with simulated repo, manual prompts, and `EVAL_LOG.md` framework. Focus is on enabling iterative QC of GSD-lite templates through manual human execution, not automated testing infrastructure.

</domain>

<decisions>
## Implementation Decisions

### Simulation Complexity
- **Location:** `./test` directory in the project root (persistent, not ephemeral).
- **Content:** Functional subset of a Python module-based data engineering pipeline (simple working code to test understanding).
- **Reset:** Manual revert after each run (no script automation required).

### Evaluation Reporting
- **File:** Single `EVAL_LOG.md` file (continuous history).
- **Format:** Markdown table for manual entry.
- **Columns:** Date, Scenario, Expected, Actual, Rating (1-5), Pass/Fail, Notes.
- **Granularity:** "Expected vs Actual" comparison is key.

### Execution Mode
- **Prompts:** Manual driver (instructions in a Markdown file, user copy-pastes to agent).
- **Verification:** Manual human judgment (User compares result to mental "Gold Standard").
- **Logging:** User manually fills the spreadsheet/table after the run.

### Claude's Discretion
- Exact folder structure within `./test`.
- Specifics of the dummy Python code (the "functional subset").
- Visual layout of the `EVAL_LOG.md` table.

</decisions>

<specifics>
## Specific Ideas

- "Let us synthesize one under ./test dir"
- "Log down in a spreadsheet / markdown notebook"
- "Functional Subset" of a module-based program

</specifics>

<deferred>
## Deferred Ideas

- Automated test runners / CI integration (Phase 1.5 is explicitly manual).
- "Gold Standard" automated comparison (LLM-as-judge) - future.

</deferred>

---

*Phase: 01.5-evaluation-framework-for-gsd-lite*
*Context gathered: 2026-01-26*
