{
    "configurations": [
        {
            "name": "TinyLlama 1.1B (CPU-friendly)",
            "description": "Compact chat model optimized for CPU",
            "config": {
                "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "max_model_len": 2048,
                "cpu_kvcache_space": 4,
                "dtype": "bfloat16"
            }
        },
        {
            "name": "Llama 3.2 1B (Gated)",
            "description": "Latest Llama 3.2 1B model - requires HF token",
            "config": {
                "model": "meta-llama/Llama-3.2-1B",
                "max_model_len": 2048,
                "cpu_kvcache_space": 4,
                "dtype": "bfloat16",
                "hf_token": "YOUR_HF_TOKEN_HERE"
            }
        },
        {
            "name": "Gemma 2 2B (Gated)",
            "description": "Google Gemma 2 2B - requires HF token",
            "config": {
                "model": "google/gemma-2-2b",
                "max_model_len": 2048,
                "cpu_kvcache_space": 6,
                "dtype": "bfloat16",
                "hf_token": "YOUR_HF_TOKEN_HERE"
            }
        },
        {
            "name": "OPT 125M (Testing)",
            "description": "Tiny model for quick testing",
            "config": {
                "model": "facebook/opt-125m",
                "max_model_len": 1024,
                "cpu_kvcache_space": 2,
                "dtype": "bfloat16"
            }
        },
        {
            "name": "OPT 1.3B",
            "description": "Medium OPT model",
            "config": {
                "model": "facebook/opt-1.3b",
                "max_model_len": 2048,
                "cpu_kvcache_space": 8,
                "dtype": "bfloat16"
            }
        },
        {
            "name": "Llama 2 7B (GPU)",
            "description": "Production-ready 7B chat model (slow on CPU)",
            "config": {
                "model": "meta-llama/Llama-2-7b-chat-hf",
                "tensor_parallel_size": 1,
                "gpu_memory_utilization": 0.9,
                "dtype": "auto",
                "enable_prefix_caching": true,
                "hf_token": "YOUR_HF_TOKEN_HERE"
            }
        },
        {
            "name": "Mistral 7B (GPU)",
            "description": "High-quality instruct model",
            "config": {
                "model": "mistralai/Mistral-7B-Instruct-v0.2",
                "tensor_parallel_size": 1,
                "gpu_memory_utilization": 0.9,
                "dtype": "auto",
                "enable_prefix_caching": true
            }
        }
    ],
    "generation_presets": [
        {
            "name": "Precise",
            "temperature": 0.1,
            "max_tokens": 512
        },
        {
            "name": "Balanced",
            "temperature": 0.7,
            "max_tokens": 512
        },
        {
            "name": "Creative",
            "temperature": 1.2,
            "max_tokens": 1024
        },
        {
            "name": "Long Form",
            "temperature": 0.7,
            "max_tokens": 2048
        }
    ]
}
