{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3f0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import atdata\n",
    "from atdata.local import LocalDatasetEntry, S3DataStore, Index\n",
    "import webdataset as wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7ea651",
   "metadata": {},
   "outputs": [],
   "source": [
    "@atdata.packable\n",
    "class TrainingSample:\n",
    "    \"\"\"A sample containing features and label for training.\"\"\"\n",
    "    features: NDArray\n",
    "    label: int\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@atdata.packable\n",
    "class TextSample( atdata.PackableSample ):\n",
    "    \"\"\"A sample containing text data.\"\"\"\n",
    "    text: str\n",
    "    category: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55549f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TextSample(\n",
    "    text = 'Hello',\n",
    "    category = 'test',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2679d94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x82\\xa4text\\xa5Hello\\xa8category\\xa4test'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.packed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a780b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0821b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: analysis-hive\n",
      "Supports streaming: True\n",
      "LocalIndex connected\n"
     ]
    }
   ],
   "source": [
    "# Connect to S3\n",
    "store = S3DataStore( '.credentials/r2-analysis-hive.env',\n",
    "    bucket = \"analysis-hive\"\n",
    ")\n",
    "print(f\"Bucket: {store.bucket}\")\n",
    "print(f\"Supports streaming: {store.supports_streaming()}\")\n",
    "\n",
    "# Connect to Redis\n",
    "index = Index(\n",
    "    data_store = store,\n",
    "    auto_stubs = True,\n",
    ")\n",
    "print( \"LocalIndex connected\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd2229f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'TrainingSample',\n",
       "  'version': '1.0.0',\n",
       "  'fields': [{'name': 'features',\n",
       "    'fieldType': {'$type': 'local#ndarray', 'dtype': 'float32'},\n",
       "    'optional': False},\n",
       "   {'name': 'label',\n",
       "    'fieldType': {'$type': 'local#primitive', 'primitive': 'int'},\n",
       "    'optional': False}],\n",
       "  '$ref': 'atdata://local/sampleSchema/TrainingSample@1.0.0',\n",
       "  'description': 'A sample containing features and label for training.',\n",
       "  'createdAt': '2026-01-28T21:47:19.019250+00:00'},\n",
       " {'name': 'TextSample',\n",
       "  'version': '1.0.1',\n",
       "  'fields': [{'name': 'text',\n",
       "    'fieldType': {'$type': 'local#primitive', 'primitive': 'str'},\n",
       "    'optional': False},\n",
       "   {'name': 'category',\n",
       "    'fieldType': {'$type': 'local#primitive', 'primitive': 'str'},\n",
       "    'optional': False}],\n",
       "  '$ref': 'atdata://local/sampleSchema/TextSample@1.0.1',\n",
       "  'description': 'A sample containing text data.',\n",
       "  'createdAt': '2026-01-28T19:02:34.955392+00:00'},\n",
       " {'name': 'TextSample',\n",
       "  'version': '1.1',\n",
       "  'fields': [{'name': 'text',\n",
       "    'fieldType': {'$type': 'local#primitive', 'primitive': 'str'},\n",
       "    'optional': False},\n",
       "   {'name': 'category',\n",
       "    'fieldType': {'$type': 'local#primitive', 'primitive': 'str'},\n",
       "    'optional': False}],\n",
       "  '$ref': 'atdata://local/sampleSchema/TextSample@1.1',\n",
       "  'description': 'A sample containing text data.',\n",
       "  'createdAt': '2026-01-28T21:50:25.171656+00:00'},\n",
       " {'name': 'TrainingSample',\n",
       "  'version': '1.1',\n",
       "  'fields': [{'name': 'features',\n",
       "    'fieldType': {'$type': 'local#ndarray', 'dtype': 'float32'},\n",
       "    'optional': False},\n",
       "   {'name': 'label',\n",
       "    'fieldType': {'$type': 'local#primitive', 'primitive': 'int'},\n",
       "    'optional': False},\n",
       "   {'name': 'confidence',\n",
       "    'fieldType': {'$type': 'local#primitive', 'primitive': 'float'},\n",
       "    'optional': False}],\n",
       "  '$ref': 'atdata://local/sampleSchema/TrainingSample@1.1',\n",
       "  'description': 'A sample containing features and label for training.',\n",
       "  'createdAt': '2026-01-28T21:50:04.707637+00:00'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list( index.list_schemas() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23765ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = next( index.schemas )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4be08f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atdata://local/sampleSchema/TrainingSample@1.0.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51829873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published schema: atdata://local/sampleSchema/TrainingSample@1.0.0\n",
      "  - TrainingSample v1.0.0\n",
      "  - TextSample v1.0.1\n",
      "  - TextSample v1.1\n",
      "  - TrainingSample v1.1\n",
      "Schema fields: ['features', 'label']\n",
      "Decoded type: TrainingSample\n"
     ]
    }
   ],
   "source": [
    "# Publish a schema\n",
    "schema_ref = index.publish_schema( TrainingSample, version=\"1.0.0\")\n",
    "print(f\"Published schema: {schema_ref}\")\n",
    "\n",
    "# List all schemas\n",
    "for schema in index.list_schemas():\n",
    "    print(f\"  - {schema.get('name', 'Unknown')} v{schema.get('version', '?')}\")\n",
    "\n",
    "# Get schema record\n",
    "schema_record = index.get_schema(schema_ref)\n",
    "print(f\"Schema fields: {[f['name'] for f in schema_record.get('fields', [])]}\")\n",
    "\n",
    "# Decode schema back to a PackableSample class\n",
    "decoded_type = index.decode_schema(schema_ref)\n",
    "print(f\"Decoded type: {decoded_type.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fadbddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published schema: atdata://local/sampleSchema/TextSample@1.0.1\n",
      "  - TrainingSample v1.0.0\n",
      "  - TextSample v1.0.1\n",
      "  - TextSample v1.1\n",
      "  - TrainingSample v1.1\n",
      "Schema fields: ['text', 'category']\n",
      "Decoded type: TextSample\n"
     ]
    }
   ],
   "source": [
    "# Publish a schema\n",
    "schema_ref_2 = index.publish_schema(TextSample, version=\"1.0.1\")\n",
    "print(f\"Published schema: {schema_ref_2}\")\n",
    "\n",
    "# List all schemas\n",
    "for schema in index.list_schemas():\n",
    "    print(f\"  - {schema.get('name', 'Unknown')} v{schema.get('version', '?')}\")\n",
    "\n",
    "# Get schema record\n",
    "schema_record = index.get_schema(schema_ref_2)\n",
    "print(f\"Schema fields: {[f['name'] for f in schema_record.get('fields', [])]}\")\n",
    "\n",
    "# Decode schema back to a PackableSample class\n",
    "decoded_type = index.decode_schema(schema_ref_2)\n",
    "print(f\"Decoded type: {decoded_type.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18e14e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, TypeAlias, Generic, Callable, Any\n",
    "\n",
    "S = TypeVar( 'S', bound = atdata.PackableSample )\n",
    "V = TypeVar( 'V', bound = atdata.PackableSample )\n",
    "\n",
    "FromAnyTo = Callable[[Any], V]\n",
    "\n",
    "def make_local_lens( f: FromAnyTo[V], remote: type[S], local: type[V] ) -> atdata.Lens[S, V]:\n",
    "    \"\"\"TODO\"\"\"\n",
    "    @atdata.lens\n",
    "    def _to_local( s: S ) -> V:\n",
    "        return f( s )\n",
    "    return _to_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fede400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[atdata] Generated schema module in: /Users/max/.atdata/stubs\n",
      "[atdata] For IDE support, add this path to your type checker:\n",
      "[atdata]   VS Code/Pylance: Add to python.analysis.extraPaths\n",
      "[atdata]   PyCharm: Mark as Sources Root\n",
      "[atdata]   mypy: Add to mypy_path in mypy.ini\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index.load_schema( 'atdata://local/sampleSchema/TextSample@1.1' )\n",
    "TextSampleRemote = index.types.TextSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53979bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TextSampleRemote(\n",
    "    text = 'hello',\n",
    "    category = 'test',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a3122bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_text_sample( s: Any ) -> TextSample:\n",
    "    return TextSample(\n",
    "        text = s.text,\n",
    "        category = s.category,\n",
    "    )\n",
    "\n",
    "l = make_local_lens( _to_text_sample, TextSampleRemote, TextSample )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a730c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = l( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b8d647",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55d944d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing data/TextSample_test-000000.tar 0 0.0 GB 0\n",
      "# writing data/TextSample_test-000001.tar 1000 0.0 GB 1000\n",
      "# writing data/TextSample_test-000002.tar 1000 0.0 GB 2000\n",
      "# writing data/TextSample_test-000003.tar 1000 0.0 GB 3000\n",
      "# writing data/TextSample_test-000004.tar 1000 0.0 GB 4000\n",
      "# writing data/TextSample_test-000005.tar 1000 0.0 GB 5000\n",
      "# writing data/TextSample_test-000006.tar 1000 0.0 GB 6000\n",
      "# writing data/TextSample_test-000007.tar 1000 0.0 GB 7000\n",
      "# writing data/TextSample_test-000008.tar 1000 0.0 GB 8000\n",
      "# writing data/TextSample_test-000009.tar 1000 0.0 GB 9000\n"
     ]
    }
   ],
   "source": [
    "import webdataset as wds\n",
    "from uuid import uuid4\n",
    "\n",
    "data_pattern = 'data/TextSample_test-%06d.tar'\n",
    "\n",
    "with wds.writer.ShardWriter( data_pattern, maxcount = 1_000 ) as sink:\n",
    "    for i in range( 10_000 ):\n",
    "        new_sample = TextSample(\n",
    "            text = str( uuid4() ),\n",
    "            category = 'test',\n",
    "        )\n",
    "        sink.write( new_sample.as_wds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5978b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atdata import load_dataset\n",
    "\n",
    "ds = (\n",
    "    load_dataset( 'data/TextSample_test-{000000..000009}.tar',\n",
    "        split = 'test'\n",
    "    )\n",
    "    .as_type( TextSample )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc81a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next( iter( ds.ordered() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3beac49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextSample(text='7298401e-7183-4bf6-8f9f-feeab0145d88', category='test')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ebfcc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing analysis-hive/prototyping/data--97e6a2ab-f3ee-4f31-967f-2c0b1bc8f1e1--000000.tar 0 0.0 GB 0\n"
     ]
    }
   ],
   "source": [
    "entry = index.insert_dataset( ds, \n",
    "    name = 'proto-text-samples-10',\n",
    "    prefix = 'prototyping',\n",
    "    schema_ref = 'atdata://local/sampleSchema/TextSample@1.1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e74d68f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalDatasetEntry(name='proto-text-samples-10', schema_ref='atdata://local/sampleSchema/TextSample@1.1', data_urls=['s3://analysis-hive/prototyping/data--97e6a2ab-f3ee-4f31-967f-2c0b1bc8f1e1--000000.tar'], metadata=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51090c3",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* We should make sure that the `s3` URI-scheme here is properly used\n",
    "    * Should we be using the `https` URI since actually this is doing data streaming with `wds`? Or does this indicate that we should think more deeply about the `Dataset` API design and generalizing how we're setting up the `wds` data streaming ...\n",
    "    * No matter what, we're definitely going to want to make sure that we incorporate the actual host details of the `LocalIndex`'s `S3DataStore` for this, since the S3 host is definitely not local.\n",
    "    * Should there be underscores here? These feel like public properties ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90872fe7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a2736f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atdata import load_dataset\n",
    "\n",
    "# Load from local index\n",
    "ds = load_dataset( \"@local/proto-text-samples-10\", TextSample,\n",
    "    split = 'train',\n",
    "    #\n",
    "    index = index,\n",
    ")\n",
    "\n",
    "# The index resolves the dataset name to URLs and schema\n",
    "for batch in ds.shuffled( batch_size = 32 ):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30bb49",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* This is also getting linting errors on `load_dataset` that there are no matching overloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fbe0e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['490364ef-cbb3-4ae6-9d1e-e77b23d93179',\n",
       " '9d8eada3-c8be-4bf2-bf30-b05cb732881e',\n",
       " '12d7f7c4-142b-45a1-8094-2358ec9e95d0',\n",
       " 'fc8fb9a3-4aa0-4446-a90e-7575840cf28b',\n",
       " '7cef4d2b-2775-4118-b420-c57f098eb6fc',\n",
       " '1b98a97b-c6bf-40a9-b2a8-e94f9384b632',\n",
       " '028f4549-096e-46c1-8d01-286086d2edc5',\n",
       " 'ba3858be-da4b-47c1-be68-644edf0bf354',\n",
       " '69e9fc0f-7047-4aca-aac5-d5cecbfc6d44',\n",
       " '8147fd45-b1cd-4945-b021-e97666f54dbd',\n",
       " 'a1ba76bc-71fb-49af-87e2-ff62b850224b',\n",
       " '84e58647-fd44-4fd7-9feb-dc6bfde2aef3',\n",
       " 'f923b9bc-b999-4ae2-b21d-b407d1fc7e32',\n",
       " '29c3055b-73fb-48b9-a662-26ea1f697f0c',\n",
       " '4e624f21-785f-4f87-87c4-371598a93e79',\n",
       " 'a285cbaa-7a5c-4518-a5a6-f4a1f70b4fa8',\n",
       " 'd9a8973b-8430-420b-9b93-0ddde1e6fb1f',\n",
       " '59eb6822-572c-4597-b929-a7dadb06770a',\n",
       " 'd300982a-0d4e-41e4-a849-b27fb93e5333',\n",
       " '53f0edee-e060-49fb-bcc9-4fa29bf75747',\n",
       " '3a301280-3fdf-49a3-aad1-8285b9182e74',\n",
       " 'a9d8afac-84b8-4ef0-992c-7611980dda9a',\n",
       " '1d06edc6-1276-4460-aed8-d6974878fd1f',\n",
       " '45642c2e-d9c2-499e-9d2e-a89b6b77dc42',\n",
       " 'f035dddd-7c4e-420d-ac77-14e1bbf77f99',\n",
       " '4c89ac85-8bb2-4d12-b32b-41fba0434b48',\n",
       " '9d42f346-c799-474f-97fd-1ed127e7df6d',\n",
       " 'ed6fcbc4-06d4-4198-9123-60a0fdf78cff',\n",
       " '57541ab5-29b3-411f-9f54-03ffd63dfcc7',\n",
       " 'c449c2cb-04cd-4f5e-b217-0cd9a905bacb',\n",
       " '4d602158-2146-4c32-aaf1-6004ebabe49a',\n",
       " 'b850607e-0d6a-41d1-b171-4dff19f8ae65']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c2afcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://analysis-hive/prototyping/data--4a5ff662-803b-4700-81f4-45f288f6e565--000000.tar'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4238ba",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* We're getting linting errors because of the protocol use for `AbstractIndex`; better to subclass, or is there a way for this to get the protocol adherence?\n",
    "* The S3 URI error is showing up here now because of how dataset loading works! The data is uploaded correctly on my end, but it can't be accessed because of this URI not being the correct way to access the data for `wds` streaming over `https`; we should think of how best to encode this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbedcd2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
