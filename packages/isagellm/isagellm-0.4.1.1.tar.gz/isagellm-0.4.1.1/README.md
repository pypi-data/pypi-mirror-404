# sageLLM

## Protocol Compliance (Mandatory)

- MUST follow Protocol v0.1:
  https://github.com/intellistream/sagellm-docs/blob/main/docs/specs/protocol_v0.1.md
- Any globally shared definitions (fields, error codes, metrics, IDs, schemas) MUST be added to
  Protocol first.

<p align="center">
  <strong>ğŸš€ Modular LLM Inference Engine for Domestic Computing Power</strong>
</p>

<p align="center">
  Ollama-like experience for Chinese hardware ecosystems (Huawei Ascend, NVIDIA)
</p>

______________________________________________________________________

## âœ¨ Features

- ğŸ¯ **One-Click Install** - `pip install isagellm` gets you started immediately
- ğŸ§  **CPU-First** - Default CPU engine, no GPU required
- ğŸ‡¨ğŸ‡³ **Domestic Hardware** - First-class support for Huawei Ascend NPU
- ğŸ“Š **Observable** - Built-in metrics (TTFT, TBT, throughput, KV usage)
- ğŸ§© **Plugin System** - Extend with custom backends and engines

## ğŸ“¦ Quick Install

```bash
# Install sageLLM (CPU-first, no GPU required)
pip install isagellm

# With Control Plane (request routing & scheduling)
pip install 'isagellm[control-plane]'

# With API Gateway (OpenAI-compatible REST API)
pip install 'isagellm[gateway]'

# Full server (Control Plane + Gateway)
pip install 'isagellm[server]'

# With CUDA support
pip install 'isagellm[cuda]'

# All features
pip install 'isagellm[all]'
```

### ğŸš€ å›½å†…åŠ é€Ÿå®‰è£… PyTorchï¼ˆæ¨èï¼‰

ç”±äº PyTorch CUDA ç‰ˆæœ¬ä»å®˜æ–¹æºä¸‹è½½è¾ƒæ…¢ï¼ˆ~800MBï¼‰ï¼Œæˆ‘ä»¬åœ¨ GitHub Releases æä¾›é¢„å…ˆä¸‹è½½çš„ wheelsï¼š

```bash
# æ–¹æ³• 1ï¼šä½¿ç”¨ sagellm CLI (æ¨èï¼Œæœ€ç®€å•)
pip install isagellm
sage-llm install cuda --github     # ä» GitHub ä¸‹è½½ï¼Œå¿«é€Ÿ
sage-llm install cuda              # ä»å®˜æ–¹æºä¸‹è½½ï¼ˆé»˜è®¤ï¼‰

# æ–¹æ³• 2ï¼šç›´æ¥ä½¿ç”¨ pip --find-links
pip install torch==2.5.1+cu121 torchvision torchaudio \
  --find-links https://github.com/intellistream/sagellm-pytorch-wheels/releases/download/v2.5.1-cu121/ \
  --trusted-host github.com
```

**å…¶ä»–æ”¯æŒçš„åç«¯**ï¼š

- `sage-llm install ascend` - åä¸ºæ˜‡è…¾ NPU
- `sage-llm install kunlun` - ç™¾åº¦æ˜†ä»‘ XPU
- `sage-llm install haiguang` - æµ·å…‰ DCU
- `sage-llm install cpu` - CPU-onlyï¼ˆæœ€å°ä¸‹è½½ï¼‰

ğŸ’¡ **ä¸ºä»€ä¹ˆä½¿ç”¨ GitHub åŠ é€Ÿï¼Ÿ**

- âœ… å›½å†…è®¿é—®é€Ÿåº¦å¿«ï¼ˆGitHub CDNï¼‰
- âœ… æ— éœ€é…ç½®é•œåƒæº
- âœ… å®˜æ–¹ wheelsï¼Œ100% å¯ä¿¡

ğŸ“¦ **Wheels ä»“åº“**: https://github.com/intellistream/sagellm-pytorch-wheels

## ğŸš€ Quick Start

### CLI (åƒ vLLM/Ollama ä¸€æ ·ç®€å•)

```bash
# ä¸€é”®å¯åŠ¨ï¼ˆå®Œæ•´æ ˆï¼šGateway + Engineï¼‰
pip install 'isagellm[gateway]'
sage-llm serve --model Qwen2-7B

# âœ… OpenAI API è‡ªåŠ¨å¯ç”¨
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2-7B",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'

# æŸ¥çœ‹ç³»ç»Ÿä¿¡æ¯
sage-llm info

# å•æ¬¡æ¨ç†ï¼ˆä¸å¯åŠ¨æœåŠ¡å™¨ï¼‰
sage-llm run -p "What is LLM inference?"

# é«˜çº§ç”¨æ³•ï¼šåˆ†å¸ƒå¼éƒ¨ç½²ï¼ˆåˆ†åˆ«å¯åŠ¨å„ç»„ä»¶ï¼‰
sage-llm serve --engine-only --port 9000   # ä»…å¼•æ“
sage-llm gateway --port 8000                # ä»… Gateway
```

### Python API (Control Plane - Recommended)

```python
import asyncio

from sagellm import ControlPlaneManager, BackendConfig, EngineConfig

# Install with: pip install 'isagellm[control-plane]'
async def main() -> None:
    manager = ControlPlaneManager(
        backend_config=BackendConfig(kind="cpu", device="cpu"),
        engine_configs=[
            EngineConfig(
                kind="cpu",
                model="sshleifer/tiny-gpt2",
                model_path="sshleifer/tiny-gpt2"
            )
        ]
    )

    await manager.start()
    try:
        # Requests are automatically routed to available engines
        response = await manager.execute_request(
            prompt="Hello, world!",
            max_tokens=128
        )
        print(response.output_text)
        print(f"TTFT: {response.metrics.ttft_ms:.2f} ms")
        print(f"Throughput: {response.metrics.throughput_tps:.2f} tokens/s")
    finally:
        await manager.stop()


asyncio.run(main())
```

**âš ï¸ Important:** Direct engine creation (`create_engine()`) is not exported from the umbrella
package. All production code must use `ControlPlaneManager` for proper request routing, scheduling,
and lifecycle management.

### Configuration

```yaml
# ~/.sage-llm/config.yaml
backend:
  kind: cpu  # Options: cpu, pytorch-cuda, pytorch-ascend
  device: cpu

engine:
  kind: cpu
  model: sshleifer/tiny-gpt2

control_plane:
  endpoint: "localhost:8080"
```

## ğŸ“Š Metrics & Validation

sageLLM provides comprehensive performance metrics:

```json
{
  "ttft_ms": 45.2,
  "tbt_ms": 12.5,
  "throughput_tps": 80.0,
  "peak_mem_mb": 24576,
  "kv_used_tokens": 4096,
  "prefix_hit_rate": 0.85
}
```

Run benchmarks:

```bash
sage-llm demo --workload year1 --output metrics.json
```

## ğŸ—ï¸ Architecture

```
isagellm (umbrella package)
â”œâ”€â”€ isagellm-protocol       # Protocol v0.1 types
â”‚   â””â”€â”€ Request, Response, Metrics, Error, StreamEvent
â”œâ”€â”€ isagellm-backend        # Hardware abstraction (L1 - Foundation)
â”‚   â””â”€â”€ BackendProvider, CPUBackend, (CUDABackend, AscendBackend)
â”œâ”€â”€ isagellm-comm           # Communication primitives (L2 - Infrastructure)
â”‚   â””â”€â”€ Topology, CollectiveOps (all_reduce/gather), P2P (send/recv), Overlap
â”œâ”€â”€ isagellm-kv-cache       # KV cache management (L2 - Optional)
â”‚   â””â”€â”€ PrefixCache, MemoryPool, EvictionPolicies, Predictor, KV Transfer
â”œâ”€â”€ isagellm-compression    # Inference acceleration (quantization, sparsity, etc.) (L2 - Optional)
â”‚   â””â”€â”€ Quantization, Sparsity, SpeculativeDecoding, Fusion
â”œâ”€â”€ isagellm-core           # Engine core & runtime (L3)
â”‚   â””â”€â”€ Config, Engine, Factory, DemoRunner, Adapters (vLLM/LMDeploy)
â”œâ”€â”€ isagellm-control-plane  # Request routing & scheduling (L4 - Optional)
â”‚   â””â”€â”€ ControlPlaneManager, Router, Policies, Lifecycle
â””â”€â”€ isagellm-gateway        # OpenAI-compatible REST API (L5 - Optional)
    â””â”€â”€ FastAPI server, /v1/chat/completions, Session management
```

## ğŸ”§ Development

### Quick Setup (Development Mode)

```bash
# Clone all repositories
./scripts/clone-all-repos.sh

# Install all packages in editable mode
./quickstart.sh

# Open all repos in VS Code Multi-root Workspace
code sagellm.code-workspace
```

**ğŸ“– See [WORKSPACE_GUIDE.md](WORKSPACE_GUIDE.md) for Multi-root Workspace usage.**

### Testing

```bash
# Clone and setup
git clone https://github.com/IntelliStream/sagellm.git
cd sagellm
pip install -e ".[dev]"

# Run tests
pytest -v

# Format & lint
ruff format .
ruff check . --fix

# Type check
mypy src/sagellm/

# Verify dependency hierarchy
python scripts/verify_dependencies.py
```

### ğŸ“– Development Resources

- **[DEPLOYMENT_GUIDE.md](docs/DEPLOYMENT_GUIDE.md)** - å®Œæ•´éƒ¨ç½²ä¸é…ç½®æŒ‡å—
- **[TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)** - æ•…éšœæ’æŸ¥å¿«é€Ÿå‚è€ƒ
- **[ENVIRONMENT_VARIABLES.md](docs/ENVIRONMENT_VARIABLES.md)** - ç¯å¢ƒå˜é‡å®Œæ•´å‚è€ƒ
- **[DEVELOPER_GUIDE.md](docs/DEVELOPER_GUIDE.md)** - å¼€å‘è€…æŒ‡å—
- **[WORKSPACE_GUIDE.md](docs/WORKSPACE_GUIDE.md)** - Multi-root Workspace ä½¿ç”¨
- **[INFERENCE_FLOW.md](docs/INFERENCE_FLOW.md)** - æ¨ç†æµç¨‹è¯¦è§£
- **[PR_CHECKLIST.md](docs/PR_CHECKLIST.md)** - Pull Request æ£€æŸ¥æ¸…å•

______________________________________________________________________

## ğŸ“š Documentation Index

### ç”¨æˆ·æ–‡æ¡£

- [å¿«é€Ÿå¼€å§‹](README.md#-quick-start) - 5 åˆ†é’Ÿä¸Šæ‰‹
- [éƒ¨ç½²æŒ‡å—](docs/DEPLOYMENT_GUIDE.md) - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [é…ç½®å‚è€ƒ](docs/DEPLOYMENT_GUIDE.md#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E) - å®Œæ•´é…ç½®é€‰é¡¹
- [ç¯å¢ƒå˜é‡](docs/ENVIRONMENT_VARIABLES.md) - ç¯å¢ƒå˜é‡å‚è€ƒ
- [æ•…éšœæ’æŸ¥](docs/TROUBLESHOOTING.md) - å¸¸è§é—®é¢˜è§£å†³

### å¼€å‘è€…æ–‡æ¡£

- [å¼€å‘æŒ‡å—](docs/DEVELOPER_GUIDE.md) - è´¡çŒ®ä»£ç 
- [æ¶æ„è®¾è®¡](README.md#-architecture) - ç³»ç»Ÿæ¶æ„
- [Workspace ä½¿ç”¨](docs/WORKSPACE_GUIDE.md) - Multi-root å·¥ä½œåŒº
- [PR æ£€æŸ¥æ¸…å•](docs/PR_CHECKLIST.md) - æäº¤å‰æ£€æŸ¥

### API æ–‡æ¡£

- OpenAI å…¼å®¹ API - å‚è§ [sagellm-gateway](https://github.com/intellistream/sagellm-gateway)
- Python API - å‚è§ [API_REFERENCE.md](docs/API_REFERENCE.md)ï¼ˆå¾…è¡¥å……ï¼‰

### å­åŒ…æ–‡æ¡£

- [sagellm-protocol](https://github.com/intellistream/sagellm-protocol) - åè®®å®šä¹‰

- [sagellm-backend](https://github.com/intellistream/sagellm-backend) - åç«¯æŠ½è±¡

- [sagellm-core](https://github.com/intellistream/sagellm-core) - å¼•æ“æ ¸å¿ƒ

- [sagellm-control-plane](https://github.com/intellistream/sagellm-control-plane) - æ§åˆ¶é¢

- [sagellm-gateway](https://github.com/intellistream/sagellm-gateway) - API ç½‘å…³

- [sagellm-benchmark](https://github.com/intellistream/sagellm-benchmark) - åŸºå‡†æµ‹è¯•

- [**DEVELOPER_GUIDE.md**](DEVELOPER_GUIDE.md) - æ¶æ„è§„èŒƒä¸å¼€å‘æŒ‡å—

- [**PR_CHECKLIST.md**](PR_CHECKLIST.md) - Pull Request å®¡æŸ¥æ¸…å•

- [**scripts/verify_dependencies.py**](scripts/verify_dependencies.py) - ä¾èµ–å±‚æ¬¡éªŒè¯

## ï¿½ è´¡çŒ®æŒ‡å—

### å·¥ä½œæµç¨‹ï¼ˆå¿…é¡»éµå¾ªï¼‰

åœ¨æäº¤ä»£ç å‰ï¼Œ**å¿…é¡»**ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

#### 1ï¸âƒ£ åˆ›å»º Issue

æè¿°ä½ è¦è§£å†³çš„é—®é¢˜ã€å®ç°çš„åŠŸèƒ½æˆ–æ”¹è¿›ï¼š

```bash
gh issue create \
  --title "[Category] ç®€çŸ­æè¿°" \
  --label "bug,sagellm-core" \
  --body "è¯¦ç»†æè¿°..."
```

**Issue ç±»å‹**ï¼š

- `[Bug]` - Bug ä¿®å¤
- `[Feature]` - æ–°åŠŸèƒ½
- `[Performance]` - æ€§èƒ½ä¼˜åŒ–
- `[Integration]` - ä¸å…¶ä»–æ¨¡å—é›†æˆ
- `[Docs]` - æ–‡æ¡£æ”¹è¿›

#### 2ï¸âƒ£ åœ¨æœ¬åœ°åˆ†æ”¯å¼€å‘

åˆ›å»ºå¼€å‘åˆ†æ”¯å¹¶è§£å†³é—®é¢˜ï¼š

```bash
# ä» main-dev åˆ›å»ºåˆ†æ”¯ï¼ˆä¸æ˜¯ mainï¼ï¼‰
git fetch origin main-dev
git checkout -b fix/#123-short-description origin/main-dev

# è¿›è¡Œå¼€å‘
# ...

# ç¡®ä¿é€šè¿‡æ‰€æœ‰æ£€æŸ¥
ruff format .
ruff check . --fix
pytest -v
```

**åˆ†æ”¯å‘½åçº¦å®š**ï¼š

- Bug ä¿®å¤ï¼š`bugfix/#123-xxx`
- æ–°åŠŸèƒ½ï¼š`feature/#456-xxx`
- æ–‡æ¡£ï¼š`docs/#789-xxx`
- æ€§èƒ½ï¼š`perf/#101-xxx`

#### 3ï¸âƒ£ å‘èµ· Pull Request

æäº¤ä»£ç ä¾›å®¡æŸ¥ï¼š

```bash
git push origin fix/#123-short-description
gh pr create \
  --base main-dev \
  --head fix/#123-short-description \
  --title "Fix: [ç®€çŸ­æè¿°]" \
  --body "è§£å†³ #123

## æ”¹åŠ¨
- æ”¹åŠ¨ 1
- æ”¹åŠ¨ 2

## æµ‹è¯•
- æ–°å¢å•å…ƒæµ‹è¯•
- æ‰€æœ‰æµ‹è¯•é€šè¿‡ âœ“"
```

**PR å¿…é¡»åŒ…å«**ï¼š

- æ¸…æ™°çš„æ ‡é¢˜ï¼ˆFix/Feature/Docs/Perfï¼‰
- å…³è” issue å·ï¼š`Closes #123`
- æ”¹åŠ¨åˆ—è¡¨å’Œæµ‹è¯•è¯´æ˜
- é€šè¿‡æ‰€æœ‰ CI æ£€æŸ¥

#### 4ï¸âƒ£ ä»£ç å®¡æŸ¥ä¸åˆå¹¶

ç­‰å¾…å®¡æ‰¹ååˆå¹¶åˆ° `main-dev`ï¼š

```bash
# åœ¨ GitHub ç•Œé¢ç‚¹å‡»"Merge"æŒ‰é’®
# åˆå¹¶åˆ° main-devï¼ˆä¸æ˜¯ mainï¼ï¼‰
```

**åˆå¹¶å‰æ¡ä»¶**ï¼š

- âœ… è‡³å°‘ä¸€åç»´æŠ¤è€…å®¡æ‰¹
- âœ… CI æ£€æŸ¥å…¨éƒ¨é€šè¿‡ï¼ˆpytest, ruffï¼‰
- âœ… åˆå¹¶åˆ° `main-dev` åˆ†æ”¯

### å¿«é€Ÿæ£€æŸ¥æ¸…å•

åœ¨å‘èµ· PR å‰æ£€æŸ¥ï¼š

- [ ] ä» `main-dev` åˆ†æ”¯åˆ›å»ºå¼€å‘åˆ†æ”¯
- [ ] æ›´æ–°äº† `CHANGELOG.md`
- [ ] `ruff format .` æ ¼å¼åŒ–ä»£ç 
- [ ] `ruff check . --fix` é€šè¿‡ lint
- [ ] `pytest -v` é€šè¿‡æ‰€æœ‰æµ‹è¯•
- [ ] å…³è”äº†ç›¸å…³ issueï¼š`Closes #123`

### åé¢ä¾‹å­ âŒ

- âŒ ç›´æ¥åœ¨ `main` åˆ†æ”¯æäº¤
- âŒ PR ä¸­æ²¡æœ‰å…³è” issue
- âŒ ä¿®æ”¹äº†ä»£ç ä½†æ²¡æœ‰æ›´æ–° CHANGELOG
- âŒ ä»£ç æ²¡æœ‰é€šè¿‡ lint æ£€æŸ¥
- âŒ æäº¤å‰æ²¡æœ‰è¿è¡Œæµ‹è¯•

### ç›¸å…³èµ„æº

- **Issue Labels**ï¼š`bug`, `enhancement`, `documentation`, `sagellm-core`, `sagellm-backend` ç­‰
- **GitHub CLI**ï¼š`gh issue create`, `gh pr create`
- **æ›´å¤šä¿¡æ¯**ï¼šè§ `.github/copilot-instructions.md`

## ï¿½ğŸ“š Package Details

| Package          | PyPI Name           | Import Name        | Description                     |
| ---------------- | ------------------- | ------------------ | ------------------------------- |
| sagellm          | `isagellm`          | `sagellm`          | Umbrella package (install this) |
| sagellm-protocol | `isagellm-protocol` | `sagellm_protocol` | Protocol v0.1 types             |
| sagellm-core     | `isagellm-core`     | `sagellm_core`     | Runtime & config                |
| sagellm-backend  | `isagellm-backend`  | `sagellm_backend`  | Hardware abstraction            |

## ğŸ“„ License

Proprietary - IntelliStream. Internal use only.

______________________________________________________________________

<p align="center">
  <sub>Built with â¤ï¸ by IntelliStream Team for domestic AI infrastructure</sub>
</p>
# test
