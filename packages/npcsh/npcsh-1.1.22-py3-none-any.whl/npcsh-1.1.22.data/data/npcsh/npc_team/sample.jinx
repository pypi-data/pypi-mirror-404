jinx_name: "sample"
description: "Send a prompt directly to the LLM."
inputs:
- prompt: ""
- model: ""
- provider: ""
steps:
  - name: "send_prompt_to_llm"
    engine: "python"
    code: |
      import traceback
      from npcpy.llm_funcs import get_llm_response
      
      prompt = context.get('prompt')
      llm_model = context.get('model')
      llm_provider = context.get('provider')
      output_messages = context.get('messages', [])
      current_npc = context.get('npc')

      if not prompt or not prompt.strip():
          context['output'] = "Usage: /sample <your prompt> [-m --model] model  [-p --provider] provider"
          context['messages'] = output_messages
          exit()
      
      # Fallback for model/provider if not explicitly set in Jinx inputs
      if not llm_model and current_npc and current_npc.model:
          llm_model = current_npc.model
      if not llm_provider and current_npc and current_npc.provider:
          llm_provider = current_npc.provider
      
      # Final fallbacks from state
      if not llm_model: llm_model = state.chat_model if state else "llama3.2"
      if not llm_provider: llm_provider = state.chat_provider if state else "ollama"

      try:
          result = get_llm_response(
              prompt=prompt,
              model=llm_model,
              provider=llm_provider,
              npc=current_npc,
          )
          
          if isinstance(result, dict):
              context['output'] = result.get('response')
              context['messages'] = result.get('messages', output_messages)
              context['model'] = llm_model
              context['provider'] = llm_provider
          else:
              context['output'] = str(result)
              context['messages'] = output_messages
      
      except Exception as e:
          traceback.print_exc()
          context['output'] = f"Error sampling LLM: {e}"
          context['messages'] = output_messages