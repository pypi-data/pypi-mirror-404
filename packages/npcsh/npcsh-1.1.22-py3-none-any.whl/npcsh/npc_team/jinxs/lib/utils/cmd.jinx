jinx_name: cmd
description: Command mode - LLM generates and executes shell commands
inputs:
- query: null
- model: null
- provider: null
- stream: true

steps:
  - name: cmd_execute
    engine: python
    code: |
      from npcpy.llm_funcs import execute_llm_command

      npc = context.get('npc')
      messages = context.get('messages', [])
      query = context.get('query', '')
      stream = context.get('stream', True)

      model = context.get('model') or (npc.model if npc else None)
      provider = context.get('provider') or (npc.provider if npc else None)

      if not query:
          context['output'] = ''
          context['messages'] = messages
      else:
          response = execute_llm_command(
              query,
              model=model,
              provider=provider,
              npc=npc,
              stream=stream,
              messages=messages
          )

          context['output'] = response.get('response', '')
          context['messages'] = response.get('messages', messages)

          # Track usage
          if 'usage' in response and npc and hasattr(npc, 'shared_context'):
              usage = response['usage']
              npc.shared_context['session_input_tokens'] += usage.get('input_tokens', 0)
              npc.shared_context['session_output_tokens'] += usage.get('output_tokens', 0)
              npc.shared_context['turn_count'] += 1
