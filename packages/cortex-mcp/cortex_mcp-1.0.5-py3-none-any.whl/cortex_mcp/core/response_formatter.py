"""
Response Formatter - Trust UX

Cortex Phase 7: Trust UX
LLM ì‘ë‹µì— ì‹ ë¢°ë„ ì§€í‘œë¥¼ ì¶”ê°€í•˜ì—¬ ì‚¬ìš©ìê°€ ì‘ë‹µì˜ ì‹ ë¢°ì„±ì„ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- Trust Prefix ì¶”ê°€ (grounding score ê¸°ë°˜)
- Verification Summary í¬ë§·íŒ…
- Claim Assessment í¬ë§·íŒ…
- Evidence List í¬ë§·íŒ…
"""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from .alpha_logger import AlphaLogger, LogModule


class TrustLevel(Enum):
    """ì‹ ë¢°ë„ ë ˆë²¨"""

    HIGH = "high"  # grounding >= 0.7
    MEDIUM = "medium"  # grounding >= 0.4
    LOW = "low"  # grounding < 0.4


class ResponseFormatter:
    """
    Trust UXë¥¼ ìœ„í•œ ì‘ë‹µ í¬ë§·í„°

    LLM ì‘ë‹µì— ì‹ ë¢°ë„ ì§€í‘œë¥¼ ì¶”ê°€í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ
    ì‘ë‹µì˜ ì‹ ë¢°ì„±ì„ ëª…í™•íˆ ì „ë‹¬í•©ë‹ˆë‹¤.
    """

    # ì‹ ë¢°ë„ ë ˆë²¨ë³„ ì´ëª¨ì§€
    TRUST_EMOJIS = {
        TrustLevel.HIGH: "âœ…",
        TrustLevel.MEDIUM: "âš ï¸",
        TrustLevel.LOW: "ğŸš¨",
    }

    # ì‹ ë¢°ë„ ë ˆë²¨ë³„ ë¼ë²¨
    TRUST_LABELS = {
        TrustLevel.HIGH: "High confidence",
        TrustLevel.MEDIUM: "Medium confidence",
        TrustLevel.LOW: "Low confidence",
    }

    def __init__(self, project_id: Optional[str] = None):
        """
        Response Formatter ì´ˆê¸°í™”

        Args:
            project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì (Optional)
        """
        self.project_id = project_id
        self.logger = AlphaLogger()

    def add_trust_prefix(self, response: str, verification_result: Dict[str, Any]) -> str:
        """
        LLM ì‘ë‹µì— ì‹ ë¢°ë„ ì§€í‘œ Prefix ì¶”ê°€

        Args:
            response: ì›ë³¸ LLM ì‘ë‹µ
            verification_result: ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
                - grounding_score: Grounding ì ìˆ˜ (0.0 ~ 1.0)
                - evidence_count: Evidence íŒŒì¼ ê°œìˆ˜
                - claim_count: Claim ê°œìˆ˜ (Optional)
                - referenced_contexts: ì°¸ì¡°ëœ Context ëª©ë¡ (Optional)

        Returns:
            Trust prefixê°€ ì¶”ê°€ëœ ì‘ë‹µ
        """
        grounding_score = verification_result.get("grounding_score", 0.0)
        evidence_count = verification_result.get("evidence_count", 0)
        claim_count = verification_result.get("claim_count", 0)

        # ì‹ ë¢°ë„ ë ˆë²¨ ê²°ì •
        trust_level = self._get_trust_level(grounding_score)

        # Prefix êµ¬ì„±
        emoji = self.TRUST_EMOJIS[trust_level]
        label = self.TRUST_LABELS[trust_level]

        prefix = (
            f"{emoji} {label} (grounding: {grounding_score:.2f}, {evidence_count} evidence files"
        )

        if claim_count > 0:
            prefix += f", {claim_count} claims"

        prefix += ")\n\n"

        # Low confidenceì¼ ë•Œ ê²½ê³  ì¶”ê°€
        if trust_level == TrustLevel.LOW:
            prefix += "âš ï¸ This response relies on weak evidence. Please verify before using.\n\n"

        # Referenced contexts ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        referenced_contexts = verification_result.get("referenced_contexts", [])
        if referenced_contexts:
            prefix += f"ğŸ“ Cortex loaded context from: {', '.join(referenced_contexts[:3])}"
            if len(referenced_contexts) > 3:
                prefix += f" (+{len(referenced_contexts) - 3} more)"
            prefix += "\n\n"

        # ë¡œê¹…
        self.logger.log(
            module=LogModule.GENERAL,
            action="add_trust_prefix",
            metadata={
                "trust_level": trust_level.value,
                "grounding_score": grounding_score,
                "evidence_count": evidence_count,
                "claim_count": claim_count,
                "context_count": len(referenced_contexts),
            },
        )

        return prefix + response

    def format_verification_summary(self, verification_result: Dict[str, Any]) -> str:
        """
        ê²€ì¦ ê²°ê³¼ ìš”ì•½ í¬ë§·íŒ…

        Args:
            verification_result: ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

        Returns:
            í¬ë§·íŒ…ëœ ê²€ì¦ ìš”ì•½
        """
        grounding_score = verification_result.get("grounding_score", 0.0)
        evidence_count = verification_result.get("evidence_count", 0)
        claim_count = verification_result.get("claim_count", 0)
        verified_claims = verification_result.get("verified_claims", 0)

        trust_level = self._get_trust_level(grounding_score)
        emoji = self.TRUST_EMOJIS[trust_level]

        summary = f"{emoji} Verification Summary\n"
        summary += f"{'=' * 40}\n"
        summary += f"Grounding Score: {grounding_score:.2f}\n"
        summary += f"Evidence Files: {evidence_count}\n"

        if claim_count > 0:
            verification_rate = (verified_claims / claim_count * 100) if claim_count > 0 else 0
            summary += (
                f"Claims: {verified_claims}/{claim_count} verified ({verification_rate:.1f}%)\n"
            )

        summary += f"Trust Level: {self.TRUST_LABELS[trust_level]}\n"

        return summary

    def format_claim_assessment(self, claim_text: str, assessment_result: Dict[str, Any]) -> str:
        """
        Claim í‰ê°€ ê²°ê³¼ í¬ë§·íŒ…

        Args:
            claim_text: Claim í…ìŠ¤íŠ¸
            assessment_result: í‰ê°€ ê²°ê³¼ (fuzzy_claim_analyzer.assess_claim ê²°ê³¼)

        Returns:
            í¬ë§·íŒ…ëœ í‰ê°€ ê²°ê³¼
        """
        decision = assessment_result.get("decision", "UNKNOWN")
        final_confidence = assessment_result.get("final_confidence", 0.0)
        linguistic_confidence = assessment_result.get("linguistic_confidence", 0.0)
        evidence_confidence = assessment_result.get("evidence_confidence", 0.0)

        # Decisionì— ë”°ë¥¸ ì´ëª¨ì§€
        decision_emoji = {
            "ACCEPT": "âœ…",
            "CAUTION": "âš ï¸",
            "WARN": "ğŸš¨",
        }.get(decision, "â“")

        formatted = f"{decision_emoji} Claim Assessment: {decision}\n"
        formatted += f'Claim: "{claim_text}"\n'
        formatted += f"Final Confidence: {final_confidence:.2f}\n"
        formatted += f"  â””â”€ Linguistic: {linguistic_confidence:.2f}\n"
        formatted += f"  â””â”€ Evidence: {evidence_confidence:.2f}\n"

        return formatted

    def format_evidence_list(self, evidence_list: List[Dict[str, Any]], max_items: int = 5) -> str:
        """
        Evidence ëª©ë¡ í¬ë§·íŒ…

        Args:
            evidence_list: Evidence ëª©ë¡
            max_items: í‘œì‹œí•  ìµœëŒ€ í•­ëª© ìˆ˜

        Returns:
            í¬ë§·íŒ…ëœ Evidence ëª©ë¡
        """
        if not evidence_list:
            return "ğŸ“­ No evidence found.\n"

        formatted = f"ğŸ“š Evidence ({len(evidence_list)} items):\n"

        for i, evidence in enumerate(evidence_list[:max_items]):
            evidence_type = evidence.get("type", "unknown")
            evidence_id = evidence.get("id", "unknown")
            relevance_score = evidence.get("relevance_score", 0.0)

            formatted += (
                f"  {i + 1}. [{evidence_type}] {evidence_id} (relevance: {relevance_score:.2f})\n"
            )

        if len(evidence_list) > max_items:
            formatted += f"  ... and {len(evidence_list) - max_items} more\n"

        return formatted

    def format_context_summary(self, contexts: List[str], max_items: int = 5) -> str:
        """
        Context ëª©ë¡ í¬ë§·íŒ…

        Args:
            contexts: Context ID ëª©ë¡
            max_items: í‘œì‹œí•  ìµœëŒ€ í•­ëª© ìˆ˜

        Returns:
            í¬ë§·íŒ…ëœ Context ëª©ë¡
        """
        if not contexts:
            return "ğŸ“‚ No contexts loaded.\n"

        formatted = f"ğŸ“‚ Loaded Contexts ({len(contexts)}):\n"

        for i, context_id in enumerate(contexts[:max_items]):
            formatted += f"  {i + 1}. {context_id}\n"

        if len(contexts) > max_items:
            formatted += f"  ... and {len(contexts) - max_items} more\n"

        return formatted

    def format_complete_report(
        self,
        response: str,
        verification_result: Dict[str, Any],
        claim_assessments: Optional[List[Dict[str, Any]]] = None,
        evidence_list: Optional[List[Dict[str, Any]]] = None,
    ) -> str:
        """
        ì™„ì „í•œ ì‹ ë¢°ë„ ë³´ê³ ì„œ ìƒì„±

        Args:
            response: ì›ë³¸ LLM ì‘ë‹µ
            verification_result: ê²€ì¦ ê²°ê³¼
            claim_assessments: Claim í‰ê°€ ëª©ë¡ (Optional)
            evidence_list: Evidence ëª©ë¡ (Optional)

        Returns:
            ì™„ì „í•œ ë³´ê³ ì„œ
        """
        report = "=" * 60 + "\n"
        report += "CORTEX TRUST REPORT\n"
        report += "=" * 60 + "\n\n"

        # Verification Summary
        report += self.format_verification_summary(verification_result) + "\n"

        # Referenced Contexts
        referenced_contexts = verification_result.get("referenced_contexts", [])
        if referenced_contexts:
            report += self.format_context_summary(referenced_contexts) + "\n"

        # Claim Assessments
        if claim_assessments:
            report += "ğŸ” Claim Assessments:\n"
            report += "-" * 60 + "\n"
            for assessment in claim_assessments[:3]:  # Top 3ë§Œ í‘œì‹œ
                claim_text = assessment.get("claim_text", "")
                report += self.format_claim_assessment(claim_text, assessment) + "\n"

            if len(claim_assessments) > 3:
                report += f"... and {len(claim_assessments) - 3} more claims\n\n"

        # Evidence List
        if evidence_list:
            report += self.format_evidence_list(evidence_list) + "\n"

        # Original Response
        report += "=" * 60 + "\n"
        report += "ORIGINAL RESPONSE\n"
        report += "=" * 60 + "\n"
        report += response + "\n"

        return report

    def _get_trust_level(self, grounding_score: float) -> TrustLevel:
        """
        Grounding Scoreì—ì„œ ì‹ ë¢°ë„ ë ˆë²¨ ê²°ì •

        Args:
            grounding_score: Grounding ì ìˆ˜ (0.0 ~ 1.0)

        Returns:
            ì‹ ë¢°ë„ ë ˆë²¨
        """
        if grounding_score >= 0.7:
            return TrustLevel.HIGH
        elif grounding_score >= 0.4:
            return TrustLevel.MEDIUM
        else:
            return TrustLevel.LOW

    def export_trust_metrics(self, verification_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Trust Metrics ë‚´ë³´ë‚´ê¸° (ì—°êµ¬/ë¶„ì„ìš©)

        Args:
            verification_result: ê²€ì¦ ê²°ê³¼

        Returns:
            Trust metrics ë”•ì…”ë„ˆë¦¬
        """
        grounding_score = verification_result.get("grounding_score", 0.0)
        trust_level = self._get_trust_level(grounding_score)

        return {
            "timestamp": datetime.now().isoformat(),
            "trust_level": trust_level.value,
            "grounding_score": grounding_score,
            "evidence_count": verification_result.get("evidence_count", 0),
            "claim_count": verification_result.get("claim_count", 0),
            "verified_claims": verification_result.get("verified_claims", 0),
            "context_count": len(verification_result.get("referenced_contexts", [])),
        }
