"""
Cortex Phase 9.1 - ìë™ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ ë° ì¬ìˆ˜í–‰ ì‹œìŠ¤í…œ

í•µì‹¬ ê¸°ëŠ¥:
1. ì˜ë¯¸ ê¸°ë°˜ í™•ì‹ ë„ ê°ì§€ (fuzzy_claim_analyzer)
2. ìë™ ê²€ì¦ (claim_verifier + grounding_scorer)
3. í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ ì‹œ ìë™ ì¬ìˆ˜í–‰
4. ëŒ€í™” ì™„ë£Œ ì‹œ ì „ì²´ ê²€ì¦

ì‚¬ìš© íë¦„:
- AI ë‹µë³€ ìƒì„± â†’ í™•ì‹ ë„ ë¶„ì„ â†’ ê²€ì¦ â†’ ì¬ìˆ˜í–‰ â†’ ê²€ì¦ëœ ê²°ê³¼
"""

import logging
import time
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from core.claim_extractor import Claim, ClaimExtractor
from core.claim_verifier import ClaimVerifier
from core.fuzzy_claim_analyzer import FuzzyClaimAnalyzer
from core.grounding_scorer import GroundingScorer

# Phase 9.5: Advanced Hallucination Detection
from core.hardcode_detector import HardcodeDetector
from core.method_existence_checker import MethodExistenceChecker

# Phase 9.7: ì¤‘ì•™ ìƒìˆ˜ í†µì¼
from core.hallucination_constants import (
    CONFIDENCE_SCORES,
    HIGH_CONFIDENCE_THRESHOLD,
    VERIFICATION_PASS_THRESHOLD,
    VERIFICATION_TIMEOUT_SECONDS,
    PER_CLAIM_TIMEOUT_SECONDS,
)

logger = logging.getLogger(__name__)


@dataclass
class VerificationResult:
    """ê²€ì¦ ê²°ê³¼"""

    verified: bool  # ê²€ì¦ í†µê³¼ ì—¬ë¶€
    grounding_score: float  # ê·¼ê±° ì ìˆ˜ (0.0-1.0)
    confidence_level: str  # í™•ì‹ ë„ ë ˆë²¨ (very_high, high, medium, low, none)
    claims: List[Claim]  # ì¶”ì¶œëœ ì£¼ì¥ ëª©ë¡
    unverified_claims: List[Dict[str, Any]]  # ê²€ì¦ ì‹¤íŒ¨í•œ ì£¼ì¥
    requires_retry: bool  # ì¬ìˆ˜í–‰ í•„ìš” ì—¬ë¶€
    retry_reason: Optional[str] = None  # ì¬ìˆ˜í–‰ ì‚¬ìœ 
    referenced_contexts: List[str] = None  # ì°¸ì¡°í•œ ë§¥ë½ ëª©ë¡ (íŒŒì¼ ê²½ë¡œ ë“±) - ì „ì²´ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (í•˜ìœ„ í˜¸í™˜)
    claim_evidence_map: Dict[str, List[str]] = field(default_factory=dict)  # ì‹ ê·œ: Claimë³„ Evidence ë§¤í•‘ (claim_id -> file_paths)

    # ULTRATHINK MODE: í™•ì‹ ë„ë³„ ë¶„ë¥˜ ê²€ì¦ (Phase 9.6)
    verified_claims: List[Dict[str, Any]] = field(default_factory=list)  # ê²€ì¦ í†µê³¼í•œ Claim (í™•ì‹ ë„ ë†’ìŒ + grounding í†µê³¼)
    pending_claims: List[Dict[str, Any]] = field(default_factory=list)  # ê²€ì¦ ë³´ë¥˜ Claim (í™•ì‹ ë„ ë‚®ìŒ - ì‘ì—… ë¯¸ì™„ë£Œ ê°€ëŠ¥)
    claim_grounding_scores: Dict[str, float] = field(default_factory=dict)  # Claim IDë³„ grounding score

    # Phase 9 ê°œì„ : ì„±ëŠ¥ ì£¼ì¥ ë³„ë„ ì²˜ë¦¬ (ì •ë³´ ì œê³µìš© - ê²€ì¦ ëŒ€ìƒ ì•„ë‹˜)
    performance_claims: Dict[str, Any] = field(default_factory=dict)  # ì„±ëŠ¥ ê´€ë ¨ ì£¼ì¥ (ì˜ˆ: 300x í–¥ìƒ, 0.0001s ì˜¤ë²„í—¤ë“œ)


class AutoVerifier:
    """
    ìë™ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ ì‹œìŠ¤í…œ

    AI ë‹µë³€ì˜ í™•ì‹ ë„ë¥¼ ë¶„ì„í•˜ê³ , ë†’ì€ í™•ì‹ ë„ ê°ì§€ ì‹œ ìë™ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.
    í• ë£¨ì‹œë„¤ì´ì…˜ ë°œê²¬ ì‹œ ì¬ìˆ˜í–‰ì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.

    Note:
        Phase 9.7: ëª¨ë“  ìƒìˆ˜ëŠ” hallucination_constants.pyì—ì„œ import
        - HIGH_CONFIDENCE_THRESHOLD
        - VERIFICATION_PASS_THRESHOLD
        - VERIFICATION_TIMEOUT_SECONDS
        - PER_CLAIM_TIMEOUT_SECONDS
        - CONFIDENCE_SCORES
    """

    def __init__(self):
        self.claim_extractor = ClaimExtractor()
        self.fuzzy_analyzer = FuzzyClaimAnalyzer()

        # Phase 9.5: Advanced Detection
        self.hardcode_detector = HardcodeDetector()
        self.method_checker = None  # Lazy initialization (project_path í•„ìš”)

        # CACHE REMOVED: ClaimVerifierì™€ GroundingScorer ìºì‹œ ì œê±°ë¨ (ì •í™•ì„± > ì„±ëŠ¥)
        # - íŒŒì¼ ìˆ˜ì • í›„ Evidence Graphê°€ ë³€ê²½ë˜ë¯€ë¡œ ìºì‹œ ì‚¬ìš© ë¶ˆê°€
        # - í•­ìƒ ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±í•˜ì—¬ ìµœì‹  Evidence Graph ì‚¬ìš©

    def verify_response(
        self, response_text: str, context: Optional[Dict[str, Any]] = None
    ) -> VerificationResult:
        """
        AI ì‘ë‹µ ìë™ ê²€ì¦

        Args:
            response_text: AI ì‘ë‹µ í…ìŠ¤íŠ¸
            context: ê²€ì¦ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ (íŒŒì¼ ê²½ë¡œ, í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë“±)

        Returns:
            VerificationResult: ê²€ì¦ ê²°ê³¼

        ê²€ì¦ ì ˆì°¨:
        1. í™•ì‹ ë„ ë¶„ì„ (fuzzy_claim_analyzer)
        2. í™•ì‹ ë„ >= 0.8ì´ë©´ Claim ì¶”ì¶œ
        3. Claimë³„ ê²€ì¦ ë° referenced_contexts ìˆ˜ì§‘ (claim_verifier)
        4. ì „ì²´ ì‘ë‹µì— ëŒ€í•œ Grounding Score ê³„ì‚° (grounding_scorer)
        5. Score < 0.7ì´ë©´ ì¬ìˆ˜í–‰ íŠ¸ë¦¬ê±°
        """
        # BUG FIX: íƒ€ì„ì•„ì›ƒ ë°©ì§€ë¥¼ ìœ„í•œ ì‹œì‘ ì‹œê°„ ê¸°ë¡
        verification_start_time = time.time()

        def _check_timeout(operation_name: str = "verification") -> bool:
            """íƒ€ì„ì•„ì›ƒ ì²´í¬ - Trueë©´ íƒ€ì„ì•„ì›ƒ ì´ˆê³¼"""
            elapsed = time.time() - verification_start_time
            if elapsed > VERIFICATION_TIMEOUT_SECONDS:
                logger.warning(
                    f"[TIMEOUT] {operation_name} íƒ€ì„ì•„ì›ƒ ì´ˆê³¼: {elapsed:.2f}s > {VERIFICATION_TIMEOUT_SECONDS}s"
                )
                return True
            return False

        # Phase 9.5.1: Hardcode Detection (answer.md Line 113 ë¬¸ì œ í•´ê²°)
        # BUG FIX: í™•ì‹ ë„ì™€ ë¬´ê´€í•˜ê²Œ í•­ìƒ ë¨¼ì € ì‹¤í–‰
        hardcode_detections = self.hardcode_detector.detect_in_response(response_text)

        if hardcode_detections:
            # í•˜ë“œì½”ë”© íŒ¨í„´ ë°œê²¬ â†’ ê²€ì¦ ì‹¤íŒ¨
            high_severity_detections = [
                d for d in hardcode_detections
                if d.severity in ["CRITICAL", "HIGH"]
            ]

            if high_severity_detections:
                logger.warning(
                    f"[PHASE 9.5.1] í•˜ë“œì½”ë”© íŒ¨í„´ ê°ì§€: {len(high_severity_detections)}ê°œ (CRITICAL/HIGH)"
                )

                return VerificationResult(
                    verified=False,
                    grounding_score=0.0,
                    confidence_level="very_high",  # HardcodeëŠ” í•­ìƒ í™•ì‹ ë„ ë†’ì€ ìœ„ë°˜
                    claims=[],
                    unverified_claims=[
                        {
                            "claim_type": "hardcoded_value",
                            "text": d.line_content,
                            "reason": f"Hardcoded pattern detected: {d.pattern_name} ({d.description})",
                        }
                        for d in high_severity_detections
                    ],
                    requires_retry=True,
                    retry_reason=f"Hardcoded test values detected ({len(high_severity_detections)} patterns)",
                    referenced_contexts=[],
                )

        # 1. í™•ì‹ ë„ ë¶„ì„
        analysis_result = self.fuzzy_analyzer.analyze_response(response_text)
        confidence_level = analysis_result["overall_confidence_level"]
        confidence_score = self._confidence_to_score(confidence_level)

        logger.info(f"í™•ì‹ ë„ ë¶„ì„: {confidence_level} (score: {confidence_score})")

        # 2. Claim ì¶”ì¶œ
        claims = self.claim_extractor.extract_claims(response_text)
        logger.info(f"ì¶”ì¶œëœ Claim ìˆ˜: {len(claims)}")

        # ULTRATHINK MODE (Phase 9.6): í™•ì‹ ë„ë³„ ë¶„ë¥˜ ê²€ì¦
        # ì „ì²´ í‰ê·  í™•ì‹ ë„ê°€ ë‚®ì•„ë„, ê°œë³„ Claim ì¤‘ í™•ì‹ ë„ ë†’ì€ ê²ƒì€ ê²€ì¦
        logger.info("[ULTRATHINK] í™•ì‹ ë„ë³„ Claim ë¶„ë¥˜ ì‹œì‘")

        # ê° Claimì˜ í™•ì‹ ë„ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        claim_analyses = analysis_result.get("claim_analyses", [])

        # Claimì„ í™•ì‹ ë„ë³„ë¡œ ë¶„ë¥˜
        high_confidence_claims = []  # fuzzy_score >= 0.8
        medium_confidence_claims = []  # 0.5 <= fuzzy_score < 0.8
        low_confidence_claims = []  # fuzzy_score < 0.5

        for i, claim in enumerate(claims):
            if i < len(claim_analyses):
                claim_analysis = claim_analyses[i]
                fuzzy_score = claim_analysis.get("fuzzy_score", 0.5)
                conf_level = claim_analysis.get("confidence_level", "none")
            else:
                # ë¶„ì„ ê²°ê³¼ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
                fuzzy_score = 0.5
                conf_level = "none"

            claim_info = {
                "claim": claim,
                "fuzzy_score": fuzzy_score,
                "confidence_level": conf_level,
            }

            if fuzzy_score >= 0.8:
                high_confidence_claims.append(claim_info)
            elif fuzzy_score >= 0.5:
                medium_confidence_claims.append(claim_info)
            else:
                low_confidence_claims.append(claim_info)

        logger.info(f"[ULTRATHINK] HIGH: {len(high_confidence_claims)}, "
                   f"MEDIUM: {len(medium_confidence_claims)}, "
                   f"LOW: {len(low_confidence_claims)}")

        # Phase 9.5.2: Method Existence Deep Check (answer.md Line 148 ë¬¸ì œ í•´ê²°)
        # BUG FIX: claim ì˜ì¡´ì„± ì œê±° - í•­ìƒ method checker ì‹¤í–‰
        if context and "project_path" in context:
            # MethodExistenceChecker ì´ˆê¸°í™” (Lazy)
            if self.method_checker is None:
                self.method_checker = MethodExistenceChecker(context["project_path"])

            # ì „ì²´ response_textì—ì„œ ë©”ì„œë“œ í˜¸ì¶œ ê²€ì¦ (claim ì˜ì¡´ì„± ì œê±°!)
            method_check_result = self.method_checker.verify_claim_method_calls(response_text)

            # ë©”ì„œë“œ í˜¸ì¶œì´ ê°ì§€ë˜ì—ˆê³ , ê²€ì¦ ì‹¤íŒ¨í•œ ê²½ìš°ë§Œ ì²˜ë¦¬
            if method_check_result["method_calls"] and not method_check_result["verified"]:
                # ë©”ì„œë“œ ì¡´ì¬í•˜ì§€ ì•ŠìŒ â†’ ê²€ì¦ ì‹¤íŒ¨
                missing_methods = [
                    call_result
                    for call_result in method_check_result["method_calls"]
                    if not call_result["exists"]
                ]

                logger.warning(
                    f"[PHASE 9.5.2] ë©”ì„œë“œ ì¡´ì¬ í™•ì¸ ì‹¤íŒ¨: {len(missing_methods)}ê°œ ë©”ì„œë“œ"
                )

                return VerificationResult(
                    verified=False,
                    grounding_score=0.0,
                    confidence_level=confidence_level,
                    claims=claims,
                    unverified_claims=[
                        {
                            "claim_type": "missing_method",
                            "text": f"{call_result['method_call'].object_name}.{call_result['method_call'].method_name}()",
                            "reason": call_result["reason"],
                        }
                        for call_result in missing_methods
                    ],
                    requires_retry=True,
                    retry_reason=f"Missing methods detected ({len(missing_methods)} methods)",
                    referenced_contexts=[],
                )

        if not claims:
            # Claim ì—†ìœ¼ë©´ ê²€ì¦ ë¶ˆí•„ìš”
            logger.info("[ULTRATHINK] Claim ì—†ìŒ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return VerificationResult(
                verified=True,
                grounding_score=0.5,  # Bug Fix: Claim ì—†ìŒ = ê²€ì¦ ë¶ˆê°€ = ì¤‘ê°„ ì ìˆ˜ (0.5)
                confidence_level=confidence_level,
                claims=[],
                unverified_claims=[],
                requires_retry=False,
                referenced_contexts=[],
                verified_claims=[],
                pending_claims=[],
                claim_grounding_scores={},
            )

        # 3. ULTRATHINK: MEDIUM/LOW ê·¸ë£¹ì€ pending_claimsë¡œ ì¶”ê°€
        pending_claims = []
        for claim_info in medium_confidence_claims + low_confidence_claims:
            pending_claims.append({
                "claim_type": claim_info["claim"].claim_type,
                "text": claim_info["claim"].text,
                "fuzzy_score": claim_info["fuzzy_score"],
                "confidence_level": claim_info["confidence_level"],
                "reason": "í™•ì‹ ë„ ë‚®ìŒ - ì‘ì—… ë¯¸ì™„ë£Œ ë˜ëŠ” ë¶ˆí™•ì‹¤",
            })

        logger.info(f"[ULTRATHINK] {len(pending_claims)}ê°œ Claim ê²€ì¦ ë³´ë¥˜ (í™•ì‹ ë„ ë‚®ìŒ)")

        # 4. ULTRATHINK: HIGH ê·¸ë£¹ë§Œ ê²€ì¦ ìˆ˜í–‰
        unverified_claims = []
        verified_claims = []
        claim_grounding_scores = {}

        # ULTRATHINK: HIGH ê·¸ë£¹ë§Œ ê²€ì¦ (high_confidence_claims ì§ì ‘ ë°˜ë³µ)
        logger.info(f"[ULTRATHINK] {len(high_confidence_claims)}ê°œ HIGH confidence Claim ê²€ì¦ ì‹œì‘")

        # CRITICAL FIX: contextì—ì„œ referenced_contexts ë¨¼ì € í™•ì¸ (Phase 9.4 í†µí•©)
        referenced_contexts = (context or {}).get("referenced_contexts", [])
        if referenced_contexts:
            logger.info(f"âœ… Evidence Graphì—ì„œ referenced_contexts ì‚¬ìš©: {len(referenced_contexts)}ê°œ")
        else:
            logger.info("âš ï¸ Evidence Graph ì—†ìŒ - _collect_evidenceë¡œ ëŒ€ì²´")
            referenced_contexts = []

        # ClaimVerifierì™€ GroundingScorer ì´ˆê¸°í™” (í•„ìš” ì‹œ)
        claim_verifier = self._get_claim_verifier(context or {})
        grounding_scorer = self._get_grounding_scorer(context or {})

        # BUG FIX: contextì—ì„œ file_contentsë¥¼ Evidence Graphì— ì¶”ê°€
        self._populate_evidence_graph_from_context(context or {}, claim_verifier)

        # referenced_contextsê°€ ì—†ì„ ë•Œë§Œ _collect_evidence í˜¸ì¶œ (Fallback)
        if not referenced_contexts:
            for claim_info in high_confidence_claims:
                claim = claim_info["claim"]
                # ì¦ê±° ìˆ˜ì§‘ (contextì—ì„œ)
                evidence = self._collect_evidence(claim, context or {})

                # ì¦ê±°ë¥¼ referenced_contextsë¡œ ë³€í™˜ (íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ)
                for ev in evidence:
                    if ev not in referenced_contexts:
                        referenced_contexts.append(ev)

        # Claim ê²€ì¦ (referenced_contextsê°€ ìˆë“  ì—†ë“  ìˆ˜í–‰)
        # ì‹ ê·œ: Claimë³„ Evidence ë§¤í•‘ ì´ˆê¸°í™”
        claim_evidence_map = {}

        # ULTRATHINK: HIGH ê·¸ë£¹ claim_info ë°˜ë³µ (fuzzy_score, confidence_level ì ‘ê·¼ ê°€ëŠ¥)
        timeout_occurred = False  # BUG FIX: íƒ€ì„ì•„ì›ƒ í”Œë˜ê·¸
        for claim_info in high_confidence_claims:
            # BUG FIX: ë£¨í”„ ì‹œì‘ ì‹œ íƒ€ì„ì•„ì›ƒ ì²´í¬
            if _check_timeout("claim_verification_loop"):
                timeout_occurred = True
                logger.warning(f"[TIMEOUT] Claim ê²€ì¦ ë£¨í”„ ì¤‘ë‹¨ - ë‚¨ì€ claims: {len(high_confidence_claims) - high_confidence_claims.index(claim_info)}ê°œ")
                break

            claim = claim_info["claim"]
            # Claim ID ìƒì„±
            claim_id = f"{claim.claim_type}:{claim.start}:{claim.end}"
            claim_files = []  # ì´ Claimë§Œì˜ íŒŒì¼ ëª©ë¡

            # Evidence Graph ê¸°ë°˜ ê²€ì¦
            if claim_verifier:
                # DEBUG: context íŒŒë¼ë¯¸í„° í™•ì¸ (íƒ€ì„ì•„ì›ƒ ì‹œ ìŠ¤í‚µ)
                if not _check_timeout("debug_logging"):
                    print(f"[DEBUG-AUTO_VERIFIER] claim_verifier.verify_claim í˜¸ì¶œ ì§ì „")
                    print(f"[DEBUG-AUTO_VERIFIER]   - context type: {type(context)}")
                    print(f"[DEBUG-AUTO_VERIFIER]   - context is None: {context is None}")
                    if context:
                        print(f"[DEBUG-AUTO_VERIFIER]   - context keys: {context.keys()}")
                        print(f"[DEBUG-AUTO_VERIFIER]   - 'files_modified' in context: {'files_modified' in context}")
                    print(f"[DEBUG-AUTO_VERIFIER]   - context or {{}} ê²°ê³¼: {type(context or {})}")

                # BUG FIX: verify_claimì˜ ë‘ ë²ˆì§¸ íŒŒë¼ë¯¸í„°ëŠ” context_history (Dict)
                verify_result = claim_verifier.verify_claim(claim, context or {})

                # BUG FIX: ê²€ì¦ ê²°ê³¼ì—ì„œ evidenceë¥¼ ì¶”ì¶œí•˜ì—¬ referenced_contextsì— ì¶”ê°€
                if verify_result.get("evidence"):
                    for evidence_item in verify_result["evidence"]:
                        # Evidence dictì—ì„œ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
                        if isinstance(evidence_item, dict):
                            # file_specific_diff íƒ€ì…: verified_files í•„ë“œì— íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
                            if evidence_item.get("type") == "file_specific_diff":
                                verified_files = evidence_item.get("verified_files", [])
                                claim_files.extend(verified_files)  # ì‹ ê·œ: Claimë³„ íŒŒì¼ ì¶”ì 
                                for file_path in verified_files:
                                    if file_path not in referenced_contexts:
                                        referenced_contexts.append(file_path)
                                        logger.info(f"[Evidence Matching] referenced_contextsì— ì¶”ê°€: {file_path}")
                            # evidence_graph_files, codebase_verified, content_matched_files íƒ€ì…: files í•„ë“œì— íŒŒì¼ ëª©ë¡
                            elif evidence_item.get("type") in ["evidence_graph_files", "codebase_verified", "evidence_graph_diff", "git_diff", "content_matched_files"]:
                                files_list = evidence_item.get("files", [])
                                if files_list:
                                    claim_files.extend(files_list)  # ì‹ ê·œ: Claimë³„ íŒŒì¼ ì¶”ì 
                                    for file_path in files_list:
                                        if file_path not in referenced_contexts:
                                            referenced_contexts.append(file_path)
                                            logger.info(f"[Evidence Matching] referenced_contextsì— ì¶”ê°€: {file_path} (from {evidence_item.get('type')})")
                            # Phase 9.6 FIX: indirect_reference íƒ€ì… ì²˜ë¦¬ ì¶”ê°€
                            elif evidence_item.get("type") == "indirect_reference":
                                # indirect_referenceëŠ” original_evidence í•„ë“œì— ì‹¤ì œ evidence í¬í•¨
                                original_evidence = evidence_item.get("original_evidence", {})
                                files_list = original_evidence.get("files", [])
                                if files_list:
                                    claim_files.extend(files_list)  # ì‹ ê·œ: Claimë³„ íŒŒì¼ ì¶”ì 
                                    for file_path in files_list:
                                        if file_path not in referenced_contexts:
                                            referenced_contexts.append(file_path)
                                            logger.info(f"[Evidence Matching] referenced_contextsì— ì¶”ê°€: {file_path} (from indirect_reference)")
                            # ê¸°íƒ€ íƒ€ì…: file_path í•„ë“œ ì§ì ‘ í™•ì¸
                            elif "file_path" in evidence_item:
                                file_path = evidence_item["file_path"]
                                claim_files.append(file_path)  # ì‹ ê·œ: Claimë³„ íŒŒì¼ ì¶”ì 
                                if file_path not in referenced_contexts:
                                    referenced_contexts.append(file_path)
                                    logger.info(f"[Evidence Matching] referenced_contextsì— ì¶”ê°€: {file_path}")

                # ULTRATHINK: ê²€ì¦ ê²°ê³¼ ì²˜ë¦¬
                if verify_result.get("verified", False):
                    # ê²€ì¦ ì„±ê³µ ì‹œ verified_claimsì— ì¶”ê°€
                    evidence_count = len(verify_result.get("evidence", []))

                    # ê°œë³„ Claimì˜ grounding score ê³„ì‚°
                    # (Evidence ê°œìˆ˜ ê¸°ë°˜ ê°„ì´ ì ìˆ˜: ì¦ê±° ë§ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
                    individual_score = min(1.0, evidence_count * 0.3)  # ìµœëŒ€ 1.0

                    verified_claims.append({
                        "claim_type": claim.claim_type,
                        "text": claim.text,
                        "fuzzy_score": claim_info["fuzzy_score"],
                        "confidence_level": claim_info["confidence_level"],
                        "evidence_count": evidence_count,
                        "grounding_score": individual_score,
                    })

                    claim_grounding_scores[claim_id] = individual_score
                    logger.info(f"[ULTRATHINK] Claim ê²€ì¦ ì„±ê³µ: {claim.text[:50]}... (score: {individual_score:.2f})")
                else:
                    # ê²€ì¦ ì‹¤íŒ¨ ì‹œ unverified_claimsì— ì¶”ê°€
                    unverified_claims.append(
                        {
                            "claim_type": claim.claim_type,
                            "text": claim.text,
                            "reason": verify_result.get("reason", "ì¦ê±° ë¶€ì¡±"),
                        }
                    )
            else:
                # ClaimVerifier ì—†ìœ¼ë©´ ê²€ì¦ ë¶ˆê°€
                unverified_claims.append(
                    {
                        "claim_type": claim.claim_type,
                        "text": claim.text,
                        "reason": "ClaimVerifier ì—†ìŒ",
                    }
                )

            # ì‹ ê·œ: Claimë³„ ë§¤í•‘ ì €ì¥
            claim_evidence_map[claim_id] = claim_files

        # BUG FIX: íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ ì¡°ê¸° ë°˜í™˜
        if timeout_occurred or _check_timeout("before_grounding_scorer"):
            logger.warning("[TIMEOUT] íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•´ ê²€ì¦ ì¡°ê¸° ì¢…ë£Œ")
            return VerificationResult(
                verified=False,
                grounding_score=0.0,
                confidence_level=confidence_level,
                claims=claims,
                unverified_claims=[{
                    "claim_type": "timeout",
                    "text": "ê²€ì¦ íƒ€ì„ì•„ì›ƒ",
                    "reason": f"ê²€ì¦ ì‹œê°„ ì´ˆê³¼ ({VERIFICATION_TIMEOUT_SECONDS}ì´ˆ)",
                }],
                requires_retry=False,  # íƒ€ì„ì•„ì›ƒì€ ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
                retry_reason="ê²€ì¦ íƒ€ì„ì•„ì›ƒ - ê²°ê³¼ ì—†ì´ ì¢…ë£Œ",
                referenced_contexts=referenced_contexts,
                claim_evidence_map=claim_evidence_map,
                verified_claims=verified_claims,
                pending_claims=pending_claims,
                claim_grounding_scores=claim_grounding_scores,
            )

        # 4. Grounding Score ê³„ì‚° (ì „ì²´ ì‘ë‹µì— ëŒ€í•´ í•œ ë²ˆë§Œ)
        if grounding_scorer:
            # [DEBUG] claim_evidence_map ìƒíƒœ ë¡œê¹…
            print(f"\n[DEBUG-AUTO_VERIFIER] claim_evidence_map before grounding_scorer:")
            print(f"  - Map keys: {list(claim_evidence_map.keys())}")
            print(f"  - Map values: {claim_evidence_map}")
            print(f"  - Total claims: {len(claims)}")

            # GroundingScorerê°€ ìˆìœ¼ë©´ í•­ìƒ ì‚¬ìš© (referenced_contextsê°€ ë¹„ì–´ìˆì–´ë„)
            grounding_result = grounding_scorer.calculate_score(
                response_text=response_text,
                claims=claims,
                referenced_contexts=referenced_contexts,
                context_history=context,
                claim_evidence_map=claim_evidence_map  # ì‹ ê·œ: Claimë³„ ë§¤í•‘ ì „ë‹¬
            )
            avg_score = grounding_result["grounding_score"]

            # Phase 9 ê°œì„ : ì„±ëŠ¥ ì£¼ì¥ ì •ë³´ ì¶”ì¶œ (ì •ë³´ ì œê³µìš©)
            performance_info = grounding_result.get("performance_claims", {})

            # [DEBUG] grounding_result ë¡œê¹…
            print(f"[DEBUG-AUTO_VERIFIER] grounding_result:")
            print(f"  - grounding_score: {avg_score}")
            print(f"  - verified_claims: {grounding_result.get('verified_claims', 'N/A')}")
            print(f"  - total_claims: {grounding_result.get('total_claims', 'N/A')}")
            print(f"  - performance_claims: {performance_info}")
            print(f"  - mode: {grounding_result.get('mode', 'N/A')}")
        else:
            # GroundingScorer ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ê³„ì‚°: contexts / claims
            if claims:
                avg_score = len(referenced_contexts) / len(claims)
            else:
                avg_score = 1.0 if referenced_contexts else 0.0
            performance_info = {}  # GroundingScorer ì—†ìœ¼ë©´ ë¹ˆ dict

        # 5. ì¬ìˆ˜í–‰ í•„ìš” ì—¬ë¶€ íŒë‹¨
        requires_retry = avg_score < VERIFICATION_PASS_THRESHOLD
        retry_reason = None

        if requires_retry:
            retry_reason = (
                f"ê·¼ê±° ë¶€ì¡± (Grounding Score: {avg_score:.2f} < {VERIFICATION_PASS_THRESHOLD}). "
                f"ê²€ì¦ ì‹¤íŒ¨í•œ ì£¼ì¥: {len(unverified_claims)}ê°œ"
            )

        # ULTRATHINK: ê²€ì¦ í†µê³„ ê³„ì‚°
        total_high_claims = len(high_confidence_claims)
        verified_count = len(verified_claims)
        unverified_count = len(unverified_claims)
        pending_count = len(pending_claims)

        logger.info(f"[ULTRATHINK] ê²€ì¦ ì™„ë£Œ í†µê³„:")
        logger.info(f"  - HIGH confidence claims: {total_high_claims}ê°œ")
        logger.info(f"  - ê²€ì¦ í†µê³¼: {verified_count}ê°œ")
        logger.info(f"  - ê²€ì¦ ì‹¤íŒ¨: {unverified_count}ê°œ")
        logger.info(f"  - ê²€ì¦ ë³´ë¥˜ (MEDIUM/LOW): {pending_count}ê°œ")

        return VerificationResult(
            verified=not requires_retry,
            grounding_score=avg_score,
            confidence_level=confidence_level,
            claims=claims,
            unverified_claims=unverified_claims,
            requires_retry=requires_retry,
            retry_reason=retry_reason,
            referenced_contexts=referenced_contexts,
            claim_evidence_map=claim_evidence_map,
            # ULTRATHINK MODE: í™•ì‹ ë„ë³„ ë¶„ë¥˜ ê²°ê³¼
            verified_claims=verified_claims,
            pending_claims=pending_claims,
            claim_grounding_scores=claim_grounding_scores,
            # Phase 9 ê°œì„ : ì„±ëŠ¥ ì£¼ì¥ ì •ë³´
            performance_claims=performance_info,
        )

    def _get_claim_verifier(self, context: Dict[str, Any]):
        """
        ClaimVerifier ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (Lazy initialization)

        Args:
            context: ê²€ì¦ ì»¨í…ìŠ¤íŠ¸ (project_id, project_path, claim_verifier í¬í•¨ ê°€ëŠ¥)

        Returns:
            ClaimVerifier ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: project_id ë˜ëŠ” project_pathê°€ ì—†ì„ ë•Œ
        """
        # CRITICAL FIX: contextì—ì„œ memory_managerì˜ ClaimVerifier ìš°ì„  ì‚¬ìš©
        if "claim_verifier" in context and context["claim_verifier"] is not None:
            verifier = context["claim_verifier"]
            print(f"[DEBUG] auto_verifier: memory_managerì˜ ClaimVerifier ì‚¬ìš© (Evidence Graph ê³µìœ )")
            print(f"[DEBUG]   - ClaimVerifier Evidence Graph ê°ì²´ ID: {id(verifier.evidence_graph)}")
            print(f"[DEBUG]   - Evidence Graph íŒŒì¼ ê²½ë¡œ: {verifier.evidence_graph._get_graph_path()}")
            return verifier

        project_id = context.get("project_id")
        project_path = context.get("project_path")

        if not project_id:
            raise ValueError(
                "project_id is required for claim verification.\n"
                "Please provide project_id in the context parameter."
            )

        if not project_path:
            raise ValueError(
                "project_path is required for claim verification.\n"
                "Please provide project_path in the context parameter."
            )

        # CACHE REMOVED: í•­ìƒ ìƒˆë¡œìš´ ClaimVerifier ìƒì„± (ì •í™•ì„± > ì„±ëŠ¥)
        # - íŒŒì¼ ìˆ˜ì • í›„ Evidence Graphê°€ ë³€ê²½ë˜ë¯€ë¡œ ìºì‹œ ì‚¬ìš© ë¶ˆê°€
        # - ë§¤ë²ˆ ìµœì‹  Evidence Graphë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ê²€ì¦ ìˆ˜í–‰ (~100ms ì¶”ê°€)
        from core.claim_verifier import ClaimVerifier

        verifier = ClaimVerifier(project_id=project_id, project_path=project_path)
        return verifier

    def _get_grounding_scorer(self, context: Dict[str, Any]):
        """
        GroundingScorer ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (Lazy initialization)

        Args:
            context: ê²€ì¦ ì»¨í…ìŠ¤íŠ¸ (project_id, project_path í¬í•¨)

        Returns:
            GroundingScorer ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: project_id ë˜ëŠ” project_pathê°€ ì—†ì„ ë•Œ
        """
        project_id = context.get("project_id")
        project_path = context.get("project_path")

        if not project_id:
            raise ValueError(
                "project_id is required for hallucination verification.\n"
                "Please provide project_id in the context parameter.\n"
                "Example: verify_response(response_text, context={'project_id': 'test', 'project_path': '/path/to/project'})"
            )

        if not project_path:
            raise ValueError(
                "project_path is required for hallucination verification.\n"
                "Evidence Graph needs to know where to store/load verification data.\n"
                "Please provide project_path in the context parameter.\n"
                "Example: verify_response(response_text, context={'project_id': 'test', 'project_path': '/path/to/project'})"
            )

        # CACHE REMOVED: í•­ìƒ ìƒˆë¡œìš´ GroundingScorer ìƒì„± (ì •í™•ì„± > ì„±ëŠ¥)
        # - íŒŒì¼ ìˆ˜ì • í›„ Evidence Graphê°€ ë³€ê²½ë˜ë¯€ë¡œ ìºì‹œ ì‚¬ìš© ë¶ˆê°€
        # - ClaimVerifierì˜ Evidence Graphë¥¼ ê³µìœ í•˜ì—¬ ìµœì‹  ìƒíƒœ ë°˜ì˜ (~50ms ì¶”ê°€)
        from core.grounding_scorer import GroundingScorer

        scorer = GroundingScorer(project_id=project_id, project_path=project_path)

        # CRITICAL: ClaimVerifierì˜ Evidence Graphë¥¼ ê³µìœ 
        # GroundingScorerê°€ ìì²´ Evidence Graphë¥¼ ìƒì„±í•˜ì§€ë§Œ,
        # ClaimVerifierê°€ íŒŒì¼ ë…¸ë“œë¥¼ ì¶”ê°€í•˜ë¯€ë¡œ ë™ì¼í•œ ê·¸ë˜í”„ë¥¼ ì°¸ì¡°í•´ì•¼ í•¨
        verifier = self._get_claim_verifier(context)
        scorer.evidence_graph = verifier.evidence_graph

        return scorer

    def _confidence_to_score(self, confidence_level: str) -> float:
        """
        í™•ì‹ ë„ ë ˆë²¨ì„ ì ìˆ˜ë¡œ ë³€í™˜

        Args:
            confidence_level: very_high, high, medium, low, none

        Returns:
            float: 0.0-1.0 ì ìˆ˜

        Note:
            Phase 9.7: hallucination_constants.pyì˜ CONFIDENCE_SCORES ì‚¬ìš©
            "none"ì€ 0.5 (neutral) - í™•ì‹ ë„ í‘œí˜„ì´ ì—†ëŠ” ê²ƒì€ "í‹€ë¦¼"ì´ ì•„ë‹Œ "ì¤‘ë¦½"
        """
        # Phase 9.7: ì¤‘ì•™ ìƒìˆ˜ ì‚¬ìš© (hallucination_constants.py)
        return CONFIDENCE_SCORES.get(confidence_level, 0.5)

    def _collect_evidence(self, claim: Claim, context: Dict[str, Any]) -> List[str]:
        """
        Claimì— ëŒ€í•œ ì¦ê±° ìˆ˜ì§‘

        Args:
            claim: ê²€ì¦í•  ì£¼ì¥
            context: ê²€ì¦ ì»¨í…ìŠ¤íŠ¸

        Returns:
            List[str]: ìˆ˜ì§‘ëœ ì¦ê±° ëª©ë¡
        """
        evidence = []

        # contextì—ì„œ ì¦ê±° ì¶”ì¶œ
        if "file_contents" in context:
            # íŒŒì¼ ë‚´ìš© ê²€ì¦
            for file_path, content in context["file_contents"].items():
                if claim.text in content:
                    evidence.append(f"íŒŒì¼ {file_path}ì—ì„œ ë°œê²¬")

        if "test_results" in context:
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦
            test_output = context["test_results"]
            if any(keyword in test_output for keyword in ["PASSED", "passed", "ì„±ê³µ", "ì™„ë£Œ"]):
                evidence.append("í…ŒìŠ¤íŠ¸ ê²°ê³¼ì—ì„œ í™•ì¸ë¨")

        if "command_output" in context:
            # ëª…ë ¹ ì¶œë ¥ ê²€ì¦
            output = context["command_output"]
            if claim.text in output:
                evidence.append("ëª…ë ¹ ì¶œë ¥ì—ì„œ í™•ì¸ë¨")

        return evidence

    def _populate_evidence_graph_from_context(
        self, context: Dict[str, Any], claim_verifier
    ) -> None:
        """
        BUG FIX: contextì—ì„œ Evidence Graphì— íŒŒì¼ ë…¸ë“œ ì¶”ê°€

        Args:
            context: ê²€ì¦ ì»¨í…ìŠ¤íŠ¸ (file_contents, test_results ë“± í¬í•¨)
            claim_verifier: ClaimVerifier ì¸ìŠ¤í„´ìŠ¤ (Evidence Graph í¬í•¨)

        Note:
            ì´ ë©”ì„œë“œëŠ” verify_response í˜¸ì¶œ ì‹œë§ˆë‹¤ ì‹¤í–‰ë˜ì–´
            contextì˜ file_contentsë¥¼ Evidence Graphì— ë…¸ë“œë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
            ì´ë¥¼ í†µí•´ Evidence Matchingì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
        """
        if not claim_verifier or not hasattr(claim_verifier, "evidence_graph"):
            logger.warning("[Evidence Graph] ClaimVerifierì— Evidence Graph ì—†ìŒ")
            return

        evidence_graph = claim_verifier.evidence_graph
        import hashlib
        from datetime import datetime

        # file_contentsë¥¼ Evidence Graphì— ì¶”ê°€
        if "file_contents" in context:
            for file_path, content in context["file_contents"].items():
                # content hash ìƒì„±
                content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]

                # íŒŒì¼ ë…¸ë“œ ì¶”ê°€
                success = evidence_graph.add_file_node(
                    file_path=file_path,
                    last_modified=datetime.now().isoformat(),
                    content_hash=content_hash,
                    metadata={"source": "context", "content_length": len(content)}
                )

                if success:
                    logger.info(f"[Evidence Graph] íŒŒì¼ ë…¸ë“œ ì¶”ê°€: {file_path} (hash: {content_hash})")
                else:
                    logger.debug(f"[Evidence Graph] íŒŒì¼ ë…¸ë“œ ì´ë¯¸ ì¡´ì¬: {file_path}")

        # test_resultsë¥¼ Evidence Graphì— ì¶”ê°€ (task nodeë¡œ)
        if "test_results" in context:
            task_id = f"test_result_{datetime.now().timestamp()}"
            evidence_graph.add_task_node(
                task_id=task_id,
                task_type="test_execution",
                description="í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê²°ê³¼",
                metadata={"test_output": context["test_results"]}
            )
            logger.info(f"[Evidence Graph] í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë…¸ë“œ ì¶”ê°€: {task_id}")

    def format_retry_message(self, result: VerificationResult) -> str:
        """
        ì¬ìˆ˜í–‰ ë©”ì‹œì§€ í¬ë§·

        Args:
            result: ê²€ì¦ ê²°ê³¼

        Returns:
            str: ì‚¬ìš©ìì—ê²Œ í‘œì‹œí•  ë©”ì‹œì§€
        """
        if not result.requires_retry:
            return ""

        msg = "âš ï¸ ê²€ì¦ ì¤‘ ê·¼ê±° ë¶€ì¡± ë°œê²¬. ì¬í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.\n\n"
        msg += f"ì‚¬ìœ : {result.retry_reason}\n"
        msg += f"í™•ì‹ ë„: {result.confidence_level}\n"
        msg += f"Grounding Score: {result.grounding_score:.2f}\n\n"

        # Issue #3: referenced_contexts í‘œì‹œ
        if result.referenced_contexts:
            msg += f"ì°¸ì¡°í•œ ë§¥ë½ ({len(result.referenced_contexts)}ê°œ):\n"
            for ctx in result.referenced_contexts:
                msg += f"  - {ctx}\n"
            msg += "\n"

        if result.unverified_claims:
            msg += "ê²€ì¦ ì‹¤íŒ¨í•œ ì£¼ì¥:\n"
            for i, claim in enumerate(result.unverified_claims, 1):
                msg += f"{i}. [{claim['claim_type']}] {claim['text']}\n"
                msg += f"   ì‚¬ìœ : {claim['reason']}\n"
            msg += "\n"

        # Phase 9 ê°œì„ : ì„±ëŠ¥ ì •ë³´ ë³„ë„ í‘œì‹œ (ì •ë³´ ì œê³µìš©)
        if result.performance_claims and result.performance_claims.get("total", 0) > 0:
            msg += "ğŸ“Š ì„±ëŠ¥ ì •ë³´ (ì˜ˆìƒê°’ - ê²€ì¦ ëŒ€ìƒ ì•„ë‹˜):\n"
            for claim in result.performance_claims.get("claims", []):
                msg += f"  - {claim['text']}\n"
            msg += "  * ì„±ëŠ¥ ì£¼ì¥ì€ ì •ë³´ ì œê³µìš©ì´ë©° êµ¬í˜„ ê²€ì¦ê³¼ ë¬´ê´€í•©ë‹ˆë‹¤.\n"

        return msg

    def format_verified_message(self, result: VerificationResult) -> str:
        """
        ê²€ì¦ ì™„ë£Œ ë©”ì‹œì§€ í¬ë§·

        Args:
            result: ê²€ì¦ ê²°ê³¼

        Returns:
            str: ì‚¬ìš©ìì—ê²Œ í‘œì‹œí•  ë©”ì‹œì§€
        """
        if result.requires_retry:
            return ""

        msg = f"âœ… êµ¬í˜„ ê²€ì¦: PASS (Grounding Score: {result.grounding_score:.2f})\n"

        # Issue #3: referenced_contexts í‘œì‹œ
        if result.referenced_contexts:
            msg += f"ì°¸ì¡°í•œ ë§¥ë½ ({len(result.referenced_contexts)}ê°œ):\n"
            for ctx in result.referenced_contexts:
                msg += f"  - {ctx}\n"
            msg += "\n"

        # Phase 9 ê°œì„ : ì„±ëŠ¥ ì •ë³´ ë³„ë„ í‘œì‹œ (ì •ë³´ ì œê³µìš©)
        if result.performance_claims and result.performance_claims.get("total", 0) > 0:
            msg += "ğŸ“Š ì„±ëŠ¥ ì •ë³´ (ì˜ˆìƒê°’ - ì‹¤ì¸¡ ë¯¸ì •):\n"
            for claim in result.performance_claims.get("claims", []):
                msg += f"  - {claim['text']}\n"
            msg += "  * ì„±ëŠ¥ ì£¼ì¥ì€ ì •ë³´ ì œê³µìš©ì´ë©° ê²€ì¦ ëŒ€ìƒì´ ì•„ë‹™ë‹ˆë‹¤.\n"
            msg += "  * ì‹¤ì œ ì„±ëŠ¥ì€ Phase 5 ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¸¡ì • ì˜ˆì •ì…ë‹ˆë‹¤.\n"

        return msg


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_auto_verifier = None


def get_auto_verifier() -> AutoVerifier:
    """AutoVerifier ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _auto_verifier
    if _auto_verifier is None:
        _auto_verifier = AutoVerifier()
    return _auto_verifier
