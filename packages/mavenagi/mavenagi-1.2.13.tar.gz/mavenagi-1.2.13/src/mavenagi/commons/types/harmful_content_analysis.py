# This file was auto-generated by Fern from our API Definition.

import typing

import pydantic
import typing_extensions
from ...core.pydantic_utilities import IS_PYDANTIC_V2, UniversalBaseModel
from ...core.serialization import FieldMetadata


class HarmfulContentAnalysis(UniversalBaseModel):
    category: str = pydantic.Field()
    """
    The category of harmful content analysis.
    """

    harmful_content_detected: typing_extensions.Annotated[bool, FieldMetadata(alias="harmfulContentDetected")] = (
        pydantic.Field()
    )
    """
    Whether the analysis detected harmful content for this category. Will be true if the severity is greater than or equal to the threshold.
    """

    severity: int = pydantic.Field()
    """
    The severity of the harmful content analysis. Will be 0 if the category was not detected.
    """

    threshold: int = pydantic.Field()
    """
    The threshold for the content safety analysis. If the severity is greater than or equal to the threshold, the analysis will be considered harmful.
    """

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow", frozen=True)  # type: ignore # Pydantic v2
    else:

        class Config:
            frozen = True
            smart_union = True
            extra = pydantic.Extra.allow
