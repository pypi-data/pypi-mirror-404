"""
Formula-based API for RustyStats GLM.

This module provides R-style formula support for fitting GLMs with DataFrames.
It uses the `formulaic` library for formula parsing and supports Polars DataFrames.

Example
-------
>>> import rustystats as rs
>>> import polars as pl
>>> 
>>> data = pl.read_parquet("insurance_data.parquet")
>>> model = rs.glm(
...     formula="ClaimNb ~ VehPower + VehAge + C(VehBrand)",
...     data=data,
...     family="poisson",
...     offset="Exposure"
... )
>>> result = model.fit()
>>> print(rs.summary(result))
"""

from __future__ import annotations

import weakref
from typing import Optional, Union, List, Dict, Any, TYPE_CHECKING
from dataclasses import dataclass
import warnings
import numpy as np

# Default dispersion parameter for Negative Binomial family
DEFAULT_NEGBINOMIAL_THETA = 1.0

# Canonical aliases for Negative Binomial family
NEGBINOMIAL_ALIASES = frozenset({
    "negbinomial", "negativebinomial", "negative_binomial", "neg-binomial", "nb"
})


def is_negbinomial_family(family: str) -> bool:
    """Check if the family string refers to a Negative Binomial distribution."""
    return family.lower() in NEGBINOMIAL_ALIASES


# Canonical default links for each family
_DEFAULT_LINKS = {
    "gaussian": "identity",
    "poisson": "log",
    "quasipoisson": "log",
    "negbinomial": "log",
    "negativebinomial": "log",
    "binomial": "logit",
    "quasibinomial": "logit",
    "gamma": "log",
    "inversegaussian": "inverse",
    "tweedie": "log",
}


def get_default_link(family: str) -> str:
    """
    Get the canonical default link function for a GLM family.
    
    Parameters
    ----------
    family : str
        Family name (e.g., "gaussian", "poisson", "binomial")
        
    Returns
    -------
    str
        Default link function name (e.g., "identity", "log", "logit")
        
    Raises
    ------
    ValueError
        If family is not recognized.
    """
    family_lower = family.lower()
    # Handle NegativeBinomial(theta=...) format from result strings
    if family_lower.startswith("negativebinomial"):
        return "log"
    link = _DEFAULT_LINKS.get(family_lower)
    if link is None:
        raise ValueError(
            f"Unknown family '{family}'. "
            f"Supported families: {sorted(_DEFAULT_LINKS.keys())}"
        )
    return link

# Lazy imports for optional dependencies
if TYPE_CHECKING:
    import polars as pl
    from rustystats.regularization_path import RegularizationPathInfo


def _get_column(data: "pl.DataFrame", column: str) -> np.ndarray:
    """Extract a column as numpy array from Polars DataFrame."""
    return data[column].to_numpy()


# Import from interactions module (the canonical implementation)
from rustystats.interactions import build_design_matrix, InteractionBuilder


def _get_constraint_indices(feature_names: List[str]) -> tuple:
    """
    Compute coefficient constraint indices from feature names.
    
    Returns
    -------
    nonneg_indices : list[int]
        Indices of coefficients that must be non-negative (β ≥ 0)
    nonpos_indices : list[int]
        Indices of coefficients that must be non-positive (β ≤ 0)
    """
    # ms()/ns() with + and pos() terms require non-negative coefficients
    nonneg_indices = [
        i for i, name in enumerate(feature_names)
        if name.startswith("pos(") or 
        (name.startswith("ms(") and ", +)" in name) or
        (name.startswith("ns(") and ", +)" in name)
    ]
    # ms()/ns() with - and neg() terms require non-positive coefficients
    nonpos_indices = [
        i for i, name in enumerate(feature_names)
        if name.startswith("neg(") or
        (name.startswith("ms(") and ", -)" in name) or
        (name.startswith("ns(") and ", -)" in name)
    ]
    return nonneg_indices, nonpos_indices


@dataclass
class SmoothTermResult:
    """Result for a single smooth term after fitting."""
    variable: str
    k: int
    edf: float
    lambda_: float
    gcv: float
    col_start: int
    col_end: int


def _fit_with_smooth_penalties(
    y: np.ndarray,
    X: np.ndarray,
    smooth_terms: List[Any],
    smooth_col_indices: List[tuple],
    family: str,
    link: str,
    var_power: float,
    theta: float,
    offset: Optional[np.ndarray],
    weights: Optional[np.ndarray],
    max_iter: int = 100,
    tol: float = 1e-6,
    n_lambda: int = 6,
    lambda_min: float = 1e-1,
    lambda_max: float = 1e3,
) -> tuple:
    """
    Fit GLM with penalized smooth terms using fast GCV optimization.
    
    Uses mgcv-style Brent's method for lambda optimization within IRLS,
    which is much faster than grid search for large datasets.
    
    Parameters
    ----------
    y : array
        Response variable
    X : array
        Full design matrix
    smooth_terms : list
        List of SplineTerm objects marked as smooth
    smooth_col_indices : list
        List of (start, end) column indices for each smooth term
    family, link, var_power, theta : model parameters
    offset, weights : optional arrays
    max_iter, tol : IRLS parameters
    n_lambda, lambda_min, lambda_max : GCV grid search parameters
    
    Returns
    -------
    result : GLMResult from Rust
    smooth_results : list of SmoothTermResult
    total_edf : float
    gcv : float
    """
    from rustystats._rustystats import fit_glm_py as _fit_glm_rust
    from rustystats._rustystats import fit_smooth_glm_fast_py as _fit_smooth_fast
    from rustystats._rustystats import fit_smooth_glm_monotonic_py as _fit_smooth_monotonic
    from rustystats.smooth import penalty_matrix, gcv_score, compute_edf
    
    n, p = X.shape
    n_terms = len(smooth_terms)
    
    if n_terms == 0:
        # No smooth terms - use standard fitting
        result = _fit_glm_rust(
            y, X, family, link, var_power, theta,
            offset, weights, 0.0, 0.0, max_iter, tol, None, None
        )
        return result, [], float(p), 0.0
    
    # For single smooth term, use fast Rust implementation
    if n_terms == 1:
        start, end = smooth_col_indices[0]
        
        # Split design matrix into parametric and smooth parts
        # Parametric = columns before smooth term + columns after smooth term
        x_before = X[:, :start]  # Columns before smooth term (including intercept)
        x_after = X[:, end:]     # Columns after smooth term (e.g., other predictors)
        if x_after.shape[1] > 0:
            x_parametric = np.column_stack([x_before, x_after])
        else:
            x_parametric = x_before
        smooth_basis = X[:, start:end]  # Smooth term columns
        
        # Check if this is a monotonic smooth term
        monotonicity = getattr(smooth_terms[0], '_smooth_monotonicity', None)
        
        # Use defaults for var_power and theta if not specified
        vp = var_power if var_power is not None else 1.5
        th = theta if theta is not None else 1.0
        
        if monotonicity:
            # Use monotonic solver with NNLS
            rust_result = _fit_smooth_monotonic(
                y, x_parametric, smooth_basis, family, monotonicity, link,
                offset, weights, max_iter, tol, lambda_min, lambda_max,
                vp, th
            )
        else:
            # Call fast Rust solver (unconstrained)
            rust_result = _fit_smooth_fast(
                y, x_parametric, smooth_basis, family, link,
                offset, weights, max_iter, tol, lambda_min, lambda_max,
                vp, th
            )
        
        # Reorder coefficients to match original design matrix order:
        # [parametric_before, smooth, parametric_after]
        coef = rust_result['coefficients']
        n_before = x_before.shape[1]
        n_smooth = smooth_basis.shape[1]
        n_after = x_after.shape[1]
        
        # Rust returns: [parametric (before+after), smooth]
        # We need: [before, smooth, after]
        coef_reordered = np.zeros(len(coef))
        coef_reordered[:n_before] = coef[:n_before]  # Before smooth
        coef_reordered[n_before:n_before+n_smooth] = coef[n_before+n_after:]  # Smooth
        if n_after > 0:
            coef_reordered[n_before+n_smooth:] = coef[n_before:n_before+n_after]  # After smooth
        rust_result['coefficients'] = coef_reordered
        
        # Also reorder covariance matrix
        cov = rust_result['covariance_unscaled']
        if cov.shape[0] == len(coef):
            # Build reordering indices
            idx_before = list(range(n_before))
            idx_smooth = list(range(n_before + n_after, n_before + n_after + n_smooth))
            idx_after = list(range(n_before, n_before + n_after))
            reorder_idx = idx_before + idx_smooth + idx_after
            cov_reordered = cov[np.ix_(reorder_idx, reorder_idx)]
            rust_result['covariance_unscaled'] = cov_reordered
        
        # Build result object compatible with existing code
        # Create a mock result object with the needed attributes
        class FastResult:
            def __init__(self, d, y, family_name, design_matrix=None, weights=None):
                self.coefficients = d['coefficients']
                self.fitted_values = d['fitted_values']
                self.linear_predictor = d['linear_predictor']
                self.deviance = d['deviance']
                self.iterations = d['iterations']
                self.converged = d['converged']
                self.covariance_unscaled = d['covariance_unscaled']
                self.cov_params_unscaled = d['covariance_unscaled']  # Alias for compatibility
                # Store design matrix and weights for score tests
                self.design_matrix = design_matrix
                self._weights = weights
                # Compute IRLS weights from fitted values (W = mu for Poisson, 1 for Gaussian)
                if family_name.lower() == 'poisson':
                    self.irls_weights = self.fitted_values
                elif family_name.lower() == 'binomial':
                    mu = np.clip(self.fitted_values, 1e-10, 1 - 1e-10)
                    self.irls_weights = mu * (1 - mu)
                elif family_name.lower() == 'gamma':
                    self.irls_weights = self.fitted_values ** 2
                else:  # Gaussian
                    self.irls_weights = np.ones(len(self.fitted_values))
                # Additional attributes needed for summary()
                self._y = y
                self._family_name = family_name
                self._n = len(y)
                self._p = len(d['coefficients'])
            
            @property
            def params(self):
                return self.coefficients
            
            @property
            def nobs(self):
                return self._n
            
            @property
            def df_resid(self):
                return self._n - self._p
            
            @property
            def df_model(self):
                return self._p - 1  # Exclude intercept
            
            @property
            def family(self):
                return self._family_name
            
            @property
            def is_regularized(self):
                return False
            
            @property
            def penalty_type(self):
                return "none"
            
            @property
            def alpha(self):
                return 0.0
            
            @property
            def l1_ratio(self):
                return 0.0
            
            def bse(self):
                return np.sqrt(np.diag(self.covariance_unscaled))
            
            def tvalues(self):
                return self.params / self.bse()
            
            def pvalues(self):
                # Two-tailed p-values using normal approximation
                # erfc is the complementary error function: erfc(x) = 1 - erf(x)
                # For standard normal: P(|Z| > |z|) = erfc(|z| / sqrt(2))
                from math import erfc, sqrt
                z = np.abs(self.tvalues())
                return np.array([erfc(abs(zi) / sqrt(2)) for zi in z])
            
            def conf_int(self, alpha=0.05):
                # Use standard normal quantile for confidence intervals
                # For 95% CI: z = 1.96
                z_multipliers = {0.05: 1.959964, 0.01: 2.575829, 0.10: 1.644854}
                z = z_multipliers.get(alpha, 1.959964)
                se = self.bse()
                return np.column_stack([self.params - z * se, self.params + z * se])
            
            def significance_codes(self):
                codes = []
                for p in self.pvalues():
                    if p < 0.001:
                        codes.append("***")
                    elif p < 0.01:
                        codes.append("**")
                    elif p < 0.05:
                        codes.append("*")
                    elif p < 0.1:
                        codes.append(".")
                    else:
                        codes.append("")
                return codes
            
            def llf(self):
                # Approximate log-likelihood from deviance
                return -0.5 * self.deviance
            
            def aic(self):
                return -2 * self.llf() + 2 * self._p
            
            def bic(self):
                return -2 * self.llf() + np.log(self._n) * self._p
            
            def pearson_chi2(self):
                # Approximate from fitted values
                residuals = self._y - self.fitted_values
                return np.sum(residuals**2 / np.maximum(self.fitted_values, 1e-10))
            
            def null_deviance(self):
                # Compute null deviance (intercept-only model)
                y_mean = np.mean(self._y)
                if self._family_name.lower() in ('poisson', 'gamma'):
                    # Poisson/Gamma deviance
                    y_safe = np.maximum(self._y, 1e-10)
                    return 2 * np.sum(self._y * np.log(y_safe / y_mean) - (self._y - y_mean))
                else:
                    # Gaussian
                    return np.sum((self._y - y_mean)**2)
            
            def scale(self):
                return 1.0
        
        result = FastResult(rust_result, y, family, design_matrix=X, weights=weights)
        
        # Build smooth term result
        smooth_results = [SmoothTermResult(
            variable=smooth_terms[0].var_name,
            k=smooth_terms[0].df,
            edf=rust_result['smooth_edfs'][0],
            lambda_=rust_result['lambdas'][0],
            gcv=rust_result['gcv'],
            col_start=start,
            col_end=end,
        )]
        
        # Update the smooth term with fitted values
        smooth_terms[0]._lambda = rust_result['lambdas'][0]
        smooth_terms[0]._edf = rust_result['smooth_edfs'][0]
        
        return result, smooth_results, rust_result['total_edf'], rust_result['gcv']
    
    # For multiple smooth terms, fall back to Python implementation for now
    # Build penalty matrices for each smooth term
    penalties = []
    for i, term in enumerate(smooth_terms):
        start, end = smooth_col_indices[i]
        n_cols = end - start
        penalty = penalty_matrix(n_cols, order=2)
        penalties.append(penalty)
    
    # For multiple smooth terms, use coordinate-wise optimization with coarse grid
    log_lambdas = np.linspace(np.log10(lambda_min), np.log10(lambda_max), n_lambda)
    lambda_grid = 10 ** log_lambdas
    lambdas = [1.0] * n_terms
    
    for _ in range(3):  # Reduced outer iterations (was 10)
        old_lambdas = lambdas.copy()
        
        for term_idx in range(n_terms):
            best_lambda = lambdas[term_idx]
            best_gcv = float('inf')
            
            for lam in lambda_grid:
                test_lambdas = lambdas.copy()
                test_lambdas[term_idx] = lam
                
                # Compute total penalty
                total_penalty = 0.0
                for i, (start, end) in enumerate(smooth_col_indices):
                    total_penalty += test_lambdas[i] * np.sum(np.diag(penalties[i]))
                
                alpha = total_penalty / max(p - 1, 1)
                
                try:
                    result = _fit_glm_rust(
                        y, X, family, link, var_power, theta,
                        offset, weights, alpha, 0.0, max_iter, tol, None, None
                    )
                    
                    # Compute total EDF
                    total_edf = smooth_col_indices[0][0]  # Parametric terms
                    for i, (start, end) in enumerate(smooth_col_indices):
                        total_edf += compute_edf(np.eye(end - start), penalties[i], test_lambdas[i])
                    
                    gcv = gcv_score(result.deviance, n, total_edf)
                    
                    if gcv < best_gcv:
                        best_gcv = gcv
                        best_lambda = lam
                except Exception as e:
                    warnings.warn(
                        f"Multi-smooth term {term_idx} lambda={lam:.4f} fit failed: {e}.",
                        RuntimeWarning,
                        stacklevel=2
                    )
                    continue
            
            lambdas[term_idx] = best_lambda
        
        # Check convergence
        max_change = max(abs(l1 - l2) / max(l2, 1e-10) for l1, l2 in zip(lambdas, old_lambdas))
        if max_change < 0.1:  # Relaxed convergence (was 0.01)
            break
    
    # Final fit with selected lambdas
    total_penalty = sum(
        lambdas[i] * np.sum(np.diag(penalties[i])) 
        for i in range(n_terms)
    )
    alpha = total_penalty / max(p - 1, 1)
    
    final_result = _fit_glm_rust(
        y, X, family, link, var_power, theta,
        offset, weights, alpha, 0.0, max_iter, tol, None, None
    )
    
    # Compute EDFs and build results
    smooth_results = []
    n_parametric = smooth_col_indices[0][0]
    total_edf = float(n_parametric)
    
    for i, term in enumerate(smooth_terms):
        start, end = smooth_col_indices[i]
        edf_term = compute_edf(np.eye(end - start), penalties[i], lambdas[i])
        total_edf += edf_term
        
        smooth_results.append(SmoothTermResult(
            variable=term.var_name,
            k=term.df,
            edf=edf_term,
            lambda_=lambdas[i],
            gcv=gcv_score(final_result.deviance, n, total_edf),
            col_start=start,
            col_end=end,
        ))
        
        term._lambda = lambdas[i]
        term._edf = edf_term
    
    final_gcv = gcv_score(final_result.deviance, n, total_edf)
    
    return final_result, smooth_results, total_edf, final_gcv


def _fit_glm_core(
    y: np.ndarray,
    X: np.ndarray,
    family: str,
    link: str,
    var_power: float,
    theta: float,
    offset: Optional[np.ndarray],
    weights: Optional[np.ndarray],
    alpha: float,
    l1_ratio: float,
    max_iter: int,
    tol: float,
    feature_names: List[str],
    builder: "InteractionBuilder",
) -> tuple:
    """
    Core GLM fitting logic shared by FormulaGLM and FormulaGLMDict.
    
    Handles smooth term fitting with GCV-based lambda selection and
    standard fitting with coefficient constraints.
    
    Returns
    -------
    result : GLMResult
        Fitted model result from Rust
    smooth_results : list or None
        Smooth term results if applicable
    total_edf : float or None
        Total effective degrees of freedom
    gcv : float or None
        GCV score for smooth models
    """
    from rustystats._rustystats import fit_glm_py as _fit_glm_rust
    
    # Check for smooth terms (s() terms with automatic lambda selection)
    smooth_terms, smooth_col_indices = builder.get_smooth_terms()
    
    if smooth_terms and alpha == 0.0:
        # Use penalized fitting with GCV-based lambda selection
        result, smooth_results, total_edf, gcv = _fit_with_smooth_penalties(
            y, X, smooth_terms, smooth_col_indices,
            family, link, var_power, theta,
            offset, weights, max_iter, tol,
        )
        return result, smooth_results, total_edf, gcv
    else:
        # Standard fitting (no smooth terms or regularization already applied)
        # Compute coefficient constraint indices
        nonneg_indices, nonpos_indices = _get_constraint_indices(feature_names)
        
        result = _fit_glm_rust(
            y, X, family, link, var_power, theta,
            offset, weights, alpha, l1_ratio, max_iter, tol,
            nonneg_indices if nonneg_indices else None,
            nonpos_indices if nonpos_indices else None,
        )
        return result, None, None, None


def _build_results(
    result,
    feature_names: List[str],
    formula: str,
    family: str,
    link: Optional[str],
    builder: "InteractionBuilder",
    X: np.ndarray,
    offset_spec,
    is_exposure_offset: bool,
    path_info,
    smooth_results,
    total_edf,
    gcv,
    store_design_matrix: bool = True,
    terms_dict: Optional[Dict[str, Dict[str, Any]]] = None,
    interactions_spec: Optional[List[Dict[str, Any]]] = None,
) -> "GLMModel":
    """Build GLMModel with all metadata."""
    # Clear builder caches to free memory (keep TE stats for prediction)
    if builder is not None:
        builder.clear_caches()
    
    return GLMModel(
        result=result,
        feature_names=feature_names,
        formula=formula,
        family=family,
        link=link,
        builder=builder,
        design_matrix=X if store_design_matrix else None,
        offset_spec=offset_spec,
        offset_is_exposure=is_exposure_offset,
        regularization_path_info=path_info,
        smooth_results=smooth_results,
        total_edf=total_edf,
        gcv=gcv,
        terms_dict=terms_dict,
        interactions_spec=interactions_spec,
    )


class FormulaGLM:
    """
    GLM model with formula-based specification.
    
    This class provides an R-like interface for fitting GLMs using
    formulas and DataFrames.
    
    Parameters
    ----------
    formula : str
        R-style formula specifying the model.
        Examples:
        - "y ~ x1 + x2": Linear model with intercept
        - "y ~ x1 + C(cat)": Include categorical variable
        - "y ~ 0 + x1 + x2": No intercept
        
    data : pl.DataFrame
        Polars DataFrame containing the data.
        
    family : str, default="gaussian"
        Distribution family: "gaussian", "poisson", "binomial", "gamma"
        
    link : str, optional
        Link function. If None, uses canonical link for family.
        
    offset : str or array-like, optional
        Offset term. Can be:
        - Column name (str): Will extract from data
        - Array: Use directly
        For Poisson family, typically log(exposure).
        
    weights : str or array-like, optional
        Prior weights. Can be column name or array.
        
    Attributes
    ----------
    formula : str
        The formula used
    data : DataFrame
        Original data
    family : str
        Distribution family
    feature_names : list[str]
        Names of features in the design matrix
        
    Examples
    --------
    >>> import rustystats as rs
    >>> import polars as pl
    >>> 
    >>> data = pl.DataFrame({
    ...     "claims": [0, 1, 2, 0, 1],
    ...     "age": [25, 35, 45, 55, 65],
    ...     "exposure": [1.0, 0.5, 1.0, 0.8, 1.0]
    ... })
    >>> 
    >>> model = rs.glm(
    ...     formula="claims ~ age",
    ...     data=data,
    ...     family="poisson",
    ...     offset="exposure"  # Will auto-apply log()
    ... )
    >>> result = model.fit()
    """
    
    def __init__(
        self,
        formula: str,
        data: "pl.DataFrame",
        family: str = "gaussian",
        link: Optional[str] = None,
        var_power: float = 1.5,
        theta: Optional[float] = None,
        offset: Optional[Union[str, np.ndarray]] = None,
        weights: Optional[Union[str, np.ndarray]] = None,
        seed: Optional[int] = None,
    ):
        self.formula = formula
        # Store weak reference to data to allow garbage collection
        self._data_ref = weakref.ref(data)
        self.family = family.lower()
        self.link = link
        self.var_power = var_power
        self.theta = theta  # None means auto-estimate for negbinomial
        self._offset_spec = offset
        self._weights_spec = weights
        self._seed = seed
        
        # Extract raw exposure for target encoding BEFORE building design matrix
        # For frequency models with log link, offset is typically log(exposure)
        # but target encoding needs raw exposure to compute claim rates
        raw_exposure = self._get_raw_exposure(offset)
        
        # Build design matrix (uses optimized backend for interactions)
        # Pass raw exposure so target encoding can use rate (y/exposure) instead of raw y
        # Pass seed for deterministic target encoding
        self._builder = InteractionBuilder(data)
        self.y, self.X, self.feature_names = self._builder.build_design_matrix(
            formula, exposure=raw_exposure, seed=seed
        )
        self.n_obs = len(self.y)
        self.n_params = self.X.shape[1]
        
        # Store validation results (computed lazily)
        self._validation_results = None
        
        # Process offset (applies log for Poisson/Gamma families)
        self.offset = self._process_offset(offset)
        
        # Process weights
        self.weights = self._process_weights(weights)
    
    @property
    def data(self):
        """Access the original DataFrame (may raise if garbage collected)."""
        d = self._data_ref()
        if d is None:
            raise RuntimeError(
                "Original DataFrame has been garbage collected. "
                "Keep a reference to the DataFrame if you need to access it after fitting."
            )
        return d
    
    def _uses_log_link(self) -> bool:
        """
        Check if model uses log link (explicit or canonical).
        
        Returns True if:
        - link is explicitly "log", OR
        - link is None (canonical) and family defaults to log link
        """
        if self.link == "log":
            return True
        if self.link is None and self.family in ("poisson", "quasipoisson", "negbinomial", "gamma"):
            return True
        return False
    
    def _process_offset(
        self, 
        offset: Optional[Union[str, np.ndarray]]
    ) -> Optional[np.ndarray]:
        """Process offset specification."""
        if offset is None:
            return None
            
        if isinstance(offset, str):
            # It's a column name
            offset_values = _get_column(self.data, offset)
            
            # For log-link models, auto-apply log to exposure
            if self._uses_log_link():
                # Check if values look like exposure (positive, not already logged)
                if np.all(offset_values > 0) and np.mean(offset_values) > 0.01:
                    offset_values = np.log(offset_values)
            
            return offset_values.astype(np.float64)
        else:
            return np.asarray(offset, dtype=np.float64)
    
    def _process_weights(
        self, 
        weights: Optional[Union[str, np.ndarray]]
    ) -> Optional[np.ndarray]:
        """Process weights specification."""
        if weights is None:
            return None
            
        if isinstance(weights, str):
            return _get_column(self.data, weights).astype(np.float64)
        else:
            return np.asarray(weights, dtype=np.float64)
    
    def _get_raw_exposure(
        self,
        offset: Optional[Union[str, np.ndarray]]
    ) -> Optional[np.ndarray]:
        """
        Get raw exposure values for target encoding.
        
        For frequency models (Poisson, NegBinomial, etc.), the offset is typically
        log(exposure). However, target encoding needs the raw exposure values
        to compute claim rates (claims/exposure) instead of raw claim counts.
        
        This method extracts the raw exposure BEFORE log transformation.
        """
        if offset is None:
            return None
        
        if isinstance(offset, str):
            # It's a column name - extract raw values
            return _get_column(self.data, offset).astype(np.float64)
        else:
            # It's an array - assume it's raw exposure values
            # (if user passed log(exposure), they'll get log-rate encoding which is also valid)
            return np.asarray(offset, dtype=np.float64)
    
    @property
    def df_model(self) -> int:
        """Degrees of freedom for model (number of parameters - 1)."""
        return self.n_params - 1
    
    @property
    def df_resid(self) -> int:
        """Degrees of freedom for residuals (n - p)."""
        return self.n_obs - self.n_params
    
    def validate(self, verbose: bool = True) -> dict:
        """
        Validate the design matrix before fitting.
        
        Checks for common issues that cause fitting failures:
        - Rank deficiency (linearly dependent columns)
        - High multicollinearity
        - Zero variance columns
        - NaN/Inf values
        
        Parameters
        ----------
        verbose : bool, default=True
            Print diagnostic messages with fix suggestions.
            
        Returns
        -------
        dict
            Validation results including 'valid' (bool) and 'suggestions' (list).
            
        Examples
        --------
        >>> model = rs.glm("y ~ ns(x, df=4) + C(cat)", data, family="poisson")
        >>> results = model.validate()
        >>> if not results['valid']:
        ...     print("Issues found:", results['suggestions'])
        """
        self._validation_results = self._builder.validate_design_matrix(
            self.X, self.feature_names, verbose=verbose
        )
        return self._validation_results
    
    def explore(
        self,
        categorical_factors: Optional[List[str]] = None,
        continuous_factors: Optional[List[str]] = None,
        n_bins: int = 10,
        rare_threshold_pct: float = 1.0,
        max_categorical_levels: int = 20,
        detect_interactions: bool = True,
        max_interaction_factors: int = 10,
    ):
        """
        Explore data before fitting the model.
        
        This provides pre-fit analysis including factor statistics and
        interaction detection based on the response variable.
        
        Parameters
        ----------
        categorical_factors : list of str, optional
            Names of categorical factors to analyze.
        continuous_factors : list of str, optional
            Names of continuous factors to analyze.
        n_bins : int, default=10
            Number of bins for continuous factors.
        rare_threshold_pct : float, default=1.0
            Threshold (%) below which categorical levels are grouped.
        max_categorical_levels : int, default=20
            Maximum categorical levels to show.
        detect_interactions : bool, default=True
            Whether to detect potential interactions.
        max_interaction_factors : int, default=10
            Maximum factors for interaction detection.
        
        Returns
        -------
        DataExploration
            Pre-fit exploration results with to_json() method.
        
        Examples
        --------
        >>> model = rs.glm("ClaimNb ~ Age + C(Region)", data, family="poisson")
        >>> 
        >>> # Explore before fitting
        >>> exploration = model.explore(
        ...     categorical_factors=["Region", "VehBrand"],
        ...     continuous_factors=["Age", "VehPower"],
        ... )
        >>> print(exploration.to_json())
        >>> 
        >>> # Then fit
        >>> result = model.fit()
        """
        from rustystats.diagnostics import explore_data
        
        # Parse formula to get response column name
        response = self.formula.split("~")[0].strip()
        
        # Get exposure column if set
        exposure_col = None
        if isinstance(self._offset_spec, str):
            exposure_col = self._offset_spec
        
        return explore_data(
            data=self.data,
            response=response,
            categorical_factors=categorical_factors,
            continuous_factors=continuous_factors,
            exposure=exposure_col,
            family=self.family,
            n_bins=n_bins,
            rare_threshold_pct=rare_threshold_pct,
            max_categorical_levels=max_categorical_levels,
            detect_interactions=detect_interactions,
            max_interaction_factors=max_interaction_factors,
        )
    
    def _fit_negbinomial_profile(
        self,
        X: np.ndarray,
        alpha: float,
        l1_ratio: float,
        max_iter: int,
        tol: float,
        theta_tol: float = 1e-5,
        max_theta_iter: int = 10,
    ) -> tuple:
        """
        Fit negative binomial GLM with moment-based theta estimation.
        
        Applies minimum ridge regularization (alpha >= 1e-6) for numerical
        stability when fitting negative binomial models.
        
        Parameters
        ----------
        X : np.ndarray
            Design matrix
        alpha : float
            User-specified regularization (will be at least 1e-6)
        l1_ratio : float
            Elastic net mixing parameter
        max_iter : int
            Maximum IRLS iterations per fit
        tol : float
            IRLS convergence tolerance
        theta_tol : float
            Convergence tolerance for theta estimation
        max_theta_iter : int
            Maximum iterations for theta estimation
            
        Returns
        -------
        tuple
            (GLMResults, family_string) where family_string includes theta
        """
        from rustystats._rustystats import fit_glm_py as _fit_glm_rust
        
        # Initial Poisson fit to get starting mu values
        poisson_result = _fit_glm_rust(
            self.y, X, "poisson", self.link, 1.5, 1.0,
            self.offset, self.weights, alpha, l1_ratio, max_iter, tol
        )
        mu = poisson_result.fittedvalues
        
        # Estimate initial theta from method of moments
        # Var(Y) = mu + mu^2/theta => theta = mu^2 / (Var(Y) - mu)
        y_arr = np.asarray(self.y)
        residuals = y_arr - mu
        var_estimate = np.mean(residuals**2)
        mean_mu = np.mean(mu)
        theta = max(0.01, min(1000.0, mean_mu**2 / max(var_estimate - mean_mu, 0.01)))
        
        # Profile likelihood iteration with minimum ridge for stability
        effective_alpha = max(alpha, 1e-6)
        
        coefficients = poisson_result.params
        result = poisson_result  # Fallback if all iterations fail
        
        for _ in range(max_theta_iter):
            result = _fit_glm_rust(
                self.y, X, "negbinomial", self.link, 1.5, theta,
                self.offset, self.weights, effective_alpha, l1_ratio, max_iter, tol
            )
            
            # If NaN, increase regularization and retry
            if np.any(np.isnan(result.params)):
                effective_alpha *= 10
                if effective_alpha > 1.0:
                    raise ValueError(
                        "Negative binomial fitting failed due to numerical instability. "
                        "Try simplifying the model or using Poisson instead."
                    )
                continue
            
            coefficients = result.params
            mu = result.fittedvalues
            
            # Moment-based theta update
            residuals = y_arr - mu
            excess_var = np.mean(residuals**2) - np.mean(mu)
            if excess_var > 0:
                new_theta = np.mean(mu)**2 / excess_var
                new_theta = max(0.01, min(1000.0, new_theta))
            else:
                new_theta = 1000.0  # No overdispersion
            
            if abs(new_theta - theta) < theta_tol:
                theta = new_theta
                break
            theta = new_theta
        
        # Final fit with converged theta
        final_result = _fit_glm_rust(
            self.y, X, "negbinomial", self.link, 1.5, theta,
            self.offset, self.weights, max(alpha, 1e-6), l1_ratio, max_iter, tol
        )
        
        # Fall back to iteration result if final has NaN
        if np.any(np.isnan(final_result.params)) and not np.any(np.isnan(coefficients)):
            final_result = result
        
        return final_result, f"NegativeBinomial(theta={theta:.4f})"
    
    def fit(
        self,
        alpha: float = 0.0,
        l1_ratio: float = 0.0,
        max_iter: int = 25,
        tol: float = 1e-8,
        # Cross-validation based regularization path parameters
        cv: Optional[int] = None,
        selection: str = "min",
        regularization: Optional[str] = None,
        n_alphas: int = 20,
        alpha_min_ratio: float = 0.0001,
        cv_seed: Optional[int] = None,
        include_unregularized: bool = True,
        verbose: bool = False,
        # Memory optimization
        store_design_matrix: bool = True,
    ):
        """
        Fit the GLM model, optionally with regularization.
        
        Parameters
        ----------
        alpha : float, default=0.0
            Regularization strength. Higher values = more shrinkage.
            - alpha=0: No regularization (standard GLM)
            - alpha>0: Regularized GLM
            Ignored if cv is specified.
            
        l1_ratio : float, default=0.0
            Elastic Net mixing parameter:
            - l1_ratio=0.0: Ridge (L2) penalty
            - l1_ratio=1.0: Lasso (L1) penalty - performs variable selection
            - 0 < l1_ratio < 1: Elastic Net
            Ignored if cv is specified with regularization type.
            
        max_iter : int, default=25
            Maximum IRLS iterations.
        tol : float, default=1e-8
            Convergence tolerance.
            
        cv : int, optional
            Number of cross-validation folds for regularization path.
            If specified, fits a path of alpha values and selects optimal via CV.
            Requires `regularization` to be set.
            
        selection : str, default="min"
            Selection method for CV-based regularization:
            - "min": Select alpha with minimum CV deviance
            - "1se": Select largest alpha within 1 SE of minimum (more conservative)
            
        regularization : str, optional
            Type of regularization for CV path: "ridge", "lasso", or "elastic_net".
            Required when cv is specified.
            
        n_alphas : int, default=20
            Number of alpha values to try in regularization path.
            
        alpha_min_ratio : float, default=0.001
            Smallest alpha as ratio of alpha_max.
            
        cv_seed : int, optional
            Random seed for CV fold creation.
            
        include_unregularized : bool, default=True
            Include unregularized model (alpha=0) in CV comparison.
            
        verbose : bool, default=False
            Print progress during CV fitting.
            
        store_design_matrix : bool, default=True
            Whether to store the design matrix in results. Required for VIF
            and other diagnostics. Set to False to reduce memory usage for
            large datasets where diagnostics are not needed.
            
        Returns
        -------
        GLMModel
            Fitted model results with feature names attached.
            When cv is used, includes additional attributes:
            - cv_deviance: CV deviance at selected alpha
            - cv_deviance_se: Standard error of CV deviance
            - regularization_path: Full path results
            
        Examples
        --------
        >>> # Standard GLM
        >>> result = model.fit()
        
        >>> # Ridge regularization with explicit alpha
        >>> result = model.fit(alpha=0.1, l1_ratio=0.0)
        
        >>> # Lasso for variable selection
        >>> result = model.fit(alpha=0.1, l1_ratio=1.0)
        
        >>> # CV-based regularization selection (recommended)
        >>> result = model.fit(cv=5, regularization="ridge", selection="1se")
        >>> print(f"Selected alpha: {result.alpha}")
        >>> print(f"CV deviance: {result.cv_deviance}")
        """
        from rustystats._rustystats import fit_glm_py as _fit_glm_rust, fit_negbinomial_py as _fit_negbinomial_rust
        
        # Check if we need auto theta estimation for negbinomial
        is_negbinomial = is_negbinomial_family(self.family)
        auto_theta = is_negbinomial and self.theta is None
        
        # Handle CV-based regularization path
        # If regularization is specified without cv, default to cv=5
        if regularization is not None and cv is None:
            cv = 5
        
        if cv is not None:
            if regularization is None:
                raise ValueError(
                    "When cv is specified, 'regularization' must be set to 'ridge', 'lasso', or 'elastic_net'"
                )
            
            from rustystats.regularization_path import fit_cv_regularization_path
            
            # For negbinomial with auto theta, first estimate theta then do CV
            if auto_theta:
                # Quick fit to estimate theta
                _, result_family = self._fit_negbinomial_profile(
                    self.X, 0.0, 0.0, max_iter, tol
                )
                # Extract theta from family string
                theta_start = result_family.find("theta=") + 6
                theta_end = result_family.find(")", theta_start)
                estimated_theta = float(result_family[theta_start:theta_end])
                self.theta = estimated_theta  # Store for CV fits
            
            # Determine l1_ratio from regularization type
            if regularization == "ridge":
                cv_l1_ratio = 0.0
            elif regularization == "lasso":
                cv_l1_ratio = 1.0
            elif regularization == "elastic_net":
                cv_l1_ratio = l1_ratio if l1_ratio > 0 else 0.5
            else:
                raise ValueError(f"Unknown regularization type: {regularization}")
            
            # Fit regularization path with CV
            path_info = fit_cv_regularization_path(
                glm_instance=self,
                cv=cv,
                selection=selection,
                regularization=regularization,
                n_alphas=n_alphas,
                alpha_min_ratio=alpha_min_ratio,
                l1_ratio=cv_l1_ratio,
                max_iter=max_iter,
                tol=tol,
                seed=cv_seed,
                include_unregularized=include_unregularized,
                verbose=verbose,
            )
            
            # Use selected alpha for final fit
            alpha = path_info.selected_alpha
            l1_ratio = path_info.selected_l1_ratio
            
            if verbose:
                print(f"\nRefitting on full data with alpha={alpha:.6f}")
        else:
            path_info = None
        
        # For negbinomial with auto theta, use Python-side profile likelihood
        # This allows regularization to be applied for numerical stability
        try:
            if auto_theta:
                result, result_family = self._fit_negbinomial_profile(
                    self.X, alpha, l1_ratio, max_iter, tol
                )
                self._smooth_results = None
                self._total_edf = None
                self._gcv = None
            else:
                # Use fixed theta (default for negbinomial if not auto)
                theta = self.theta if self.theta is not None else DEFAULT_NEGBINOMIAL_THETA
                
                # Use shared core fitting logic
                result, smooth_results, total_edf, gcv = _fit_glm_core(
                    self.y, self.X, self.family, self.link, self.var_power, theta,
                    self.offset, self.weights, alpha, l1_ratio, max_iter, tol,
                    self.feature_names, self._builder,
                )
                self._smooth_results = smooth_results
                self._total_edf = total_edf
                self._gcv = gcv
                
                # Include theta in family string for negbinomial so deviance calculations use correct theta
                if is_negbinomial:
                    result_family = f"NegativeBinomial(theta={theta:.4f})"
                else:
                    result_family = self.family
        except ValueError as e:
            if "singular" in str(e).lower() or "multicollinearity" in str(e).lower() or "nan" in str(e).lower():
                # Run validation to provide helpful diagnostics
                print("\n" + "=" * 60)
                print("MODEL FITTING FAILED - Running diagnostics...")
                print("=" * 60)
                validation = self.validate(verbose=True)
                raise ValueError(
                    f"GLM fitting failed due to design matrix issues. "
                    f"See diagnostics above for specific problems and fixes.\n"
                    f"You can also run model.validate() before fit() to check for issues.\n"
                    f"Original error: {e}"
                ) from None
            else:
                raise
        
        # Wrap result with formula metadata
        is_exposure_offset = self.family in ("poisson", "quasipoisson", "negbinomial", "gamma") and self.link in (None, "log")
        return _build_results(
            result, self.feature_names, self.formula, result_family, self.link,
            self._builder, self.X, self._offset_spec, is_exposure_offset, path_info,
            self._smooth_results, self._total_edf, self._gcv,
            store_design_matrix=store_design_matrix,
        )


class _DeserializedResult:
    """
    Minimal result object for deserialized models.
    
    This provides the interface needed by GLMModel for prediction
    without requiring the full Rust GLMResults object.
    
    Note: fittedvalues and linear_predictor are not stored as they're
    large arrays not needed for prediction on new data.
    """
    
    def __init__(
        self,
        params: np.ndarray,
        deviance: float,
        iterations: int,
        converged: bool,
        nobs: int,
        df_resid: int,
        df_model: int,
        alpha: float,
        l1_ratio: float,
        is_regularized: bool,
        penalty_type: str,
    ):
        self._params = params
        self._deviance = deviance
        self._iterations = iterations
        self._converged = converged
        self._nobs = nobs
        self._df_resid = df_resid
        self._df_model = df_model
        self._alpha = alpha
        self._l1_ratio = l1_ratio
        self._is_regularized = is_regularized
        self._penalty_type = penalty_type
    
    @property
    def params(self) -> np.ndarray:
        return self._params
    
    @property
    def fittedvalues(self) -> np.ndarray:
        raise AttributeError(
            "fittedvalues not available on deserialized models. "
            "Only coefficients are stored for prediction."
        )
    
    @property
    def linear_predictor(self) -> np.ndarray:
        raise AttributeError(
            "linear_predictor not available on deserialized models. "
            "Only coefficients are stored for prediction."
        )
    
    @property
    def deviance(self) -> float:
        return self._deviance
    
    @property
    def iterations(self) -> int:
        return self._iterations
    
    @property
    def converged(self) -> bool:
        return self._converged
    
    @property
    def nobs(self) -> int:
        return self._nobs
    
    @property
    def df_resid(self) -> int:
        return self._df_resid
    
    @property
    def df_model(self) -> int:
        return self._df_model
    
    @property
    def alpha(self) -> float:
        return self._alpha
    
    @property
    def l1_ratio(self) -> float:
        return self._l1_ratio
    
    @property
    def is_regularized(self) -> bool:
        return self._is_regularized
    
    @property
    def penalty_type(self) -> str:
        return self._penalty_type


class _DeserializedBuilder:
    """
    Minimal builder for deserialized models.
    
    This provides the transform_new_data interface needed for prediction
    using the saved encoding state.
    """
    
    def __init__(self, state: dict):
        self._parsed_formula = state["parsed_formula"]
        self._cat_encoding_cache = state["cat_encoding_cache"]
        self._fitted_splines = state["fitted_splines"]
        self._te_stats = state["te_stats"]
        self.dtype = state["dtype"]
    
    def _get_categorical_levels(self, name: str) -> List[str]:
        """Get cached categorical levels for a variable."""
        cache_key = f"{name}_True"
        if cache_key not in self._cat_encoding_cache:
            raise ValueError(f"Categorical variable '{name}' was not seen during training.")
        return self._cat_encoding_cache[cache_key].levels
    
    def _encode_categorical_new(
        self,
        new_data: "pl.DataFrame",
        var_name: str,
    ) -> np.ndarray:
        """Encode categorical variable using levels from training."""
        levels = self._get_categorical_levels(var_name)
        col = new_data[var_name].to_numpy()
        n = len(col)
        
        level_to_idx = {level: i for i, level in enumerate(levels)}
        n_dummies = len(levels) - 1
        encoding = np.zeros((n, n_dummies), dtype=self.dtype)
        
        for i, val in enumerate(col):
            val_str = str(val)
            if val_str in level_to_idx:
                idx = level_to_idx[val_str]
                if idx > 0:
                    encoding[i, idx - 1] = 1.0
        
        return encoding
    
    def _encode_target_new(
        self,
        new_data: "pl.DataFrame",
        te_term,
    ) -> np.ndarray:
        """Encode using target statistics from training."""
        if te_term.var_name not in self._te_stats:
            raise ValueError(
                f"Target encoding for '{te_term.var_name}' was not fitted during training."
            )
        
        stats = self._te_stats[te_term.var_name]
        prior = stats['prior']
        level_stats = stats['stats']
        prior_weight = stats['prior_weight']
        
        col = new_data[te_term.var_name].to_numpy()
        n = len(col)
        encoded = np.zeros(n, dtype=self.dtype)
        
        for i, val in enumerate(col):
            val_str = str(val)
            if val_str in level_stats:
                level_sum, level_count = level_stats[val_str]
                encoded[i] = (level_sum + prior * prior_weight) / (level_count + prior_weight)
            else:
                encoded[i] = prior
        
        return encoded
    
    def _parse_spline_factor(self, factor: str):
        """Parse a spline term from a factor name."""
        from rustystats.splines import SplineTerm
        
        factor_lower = factor.strip().lower()
        if factor_lower.startswith('bs(') or factor_lower.startswith('ns('):
            spline_type = 'bs' if factor_lower.startswith('bs(') else 'ns'
            content = factor[3:-1] if factor.endswith(')') else factor[3:]
            parts = [p.strip() for p in content.split(',')]
            var_name = parts[0]
            df = None
            k = None
            degree = 3
            monotonicity = None
            for part in parts[1:]:
                if '=' in part:
                    key, val = part.split('=', 1)
                    key = key.strip().lower()
                    val = val.strip()
                    if key == 'df':
                        df = int(val)
                    elif key == 'k':
                        k = int(val)
                    elif key == 'degree':
                        degree = int(val)
                    elif key == 'monotonicity':
                        monotonicity = val.strip("'\"").lower()
            
            is_smooth = False
            if df is None and k is None:
                effective_df = 10
                is_smooth = True
            elif k is not None:
                effective_df = k
                is_smooth = True
            else:
                effective_df = df
            
            term = SplineTerm(var_name=var_name, spline_type=spline_type, df=effective_df, 
                              degree=degree, monotonicity=monotonicity)
            if is_smooth:
                term._is_smooth = True
            return term
        
        return None
    
    def _build_identity_columns(self, identity, data: "pl.DataFrame"):
        """Evaluate an I() expression on data."""
        expr = identity.expression
        local_vars = {col: data[col].to_numpy().astype(self.dtype) for col in data.columns}
        result = eval(expr, {"__builtins__": {}}, local_vars)
        name = f"I({expr})"
        return np.asarray(result, dtype=self.dtype), name
    
    def _build_constraint_columns(self, constraint, data: "pl.DataFrame"):
        """Build columns for constraint terms."""
        col = data[constraint.var_name].to_numpy().astype(self.dtype)
        name = f"{constraint.constraint}({constraint.var_name})"
        return col, name
    
    def _build_categorical_level_indicators_new(self, cat_term, new_data: "pl.DataFrame"):
        """Build indicator columns for specific categorical levels."""
        col = new_data[cat_term.var_name].to_numpy()
        n = len(col)
        
        if cat_term.levels is None:
            return self._encode_categorical_new(new_data, cat_term.var_name), []
        
        n_levels = len(cat_term.levels)
        encoding = np.zeros((n, n_levels), dtype=self.dtype)
        names = []
        
        for j, level in enumerate(cat_term.levels):
            names.append(f"{cat_term.var_name}[{level}]")
            for i, val in enumerate(col):
                if str(val) == level:
                    encoding[i, j] = 1.0
        
        return encoding, names
    
    def _build_interaction_new(
        self,
        new_data: "pl.DataFrame",
        interaction,
        n: int,
    ) -> np.ndarray:
        """Build interaction columns for new data."""
        if interaction.is_pure_continuous:
            result = new_data[interaction.factors[0]].to_numpy().astype(self.dtype)
            for factor in interaction.factors[1:]:
                result = result * new_data[factor].to_numpy().astype(self.dtype)
            return result.reshape(-1, 1)
        
        elif interaction.is_pure_categorical:
            encodings = []
            for factor in interaction.factors:
                enc = self._encode_categorical_new(new_data, factor)
                encodings.append(enc)
            
            result = encodings[0]
            for enc in encodings[1:]:
                n_cols1, n_cols2 = result.shape[1], enc.shape[1]
                new_result = np.zeros((n, n_cols1 * n_cols2), dtype=self.dtype)
                for i in range(n_cols1):
                    for j in range(n_cols2):
                        new_result[:, i * n_cols2 + j] = result[:, i] * enc[:, j]
                result = new_result
            return result
        
        else:
            cat_factors = []
            cont_factors = []
            spline_factors = []
            
            for factor, is_cat in zip(interaction.factors, interaction.categorical_flags):
                if is_cat:
                    cat_factors.append(factor)
                else:
                    spline = self._parse_spline_factor(factor)
                    if spline is not None:
                        spline_factors.append((factor, spline))
                    else:
                        cont_factors.append(factor)
            
            if len(cat_factors) == 1:
                cat_enc = self._encode_categorical_new(new_data, cat_factors[0])
            else:
                cat_enc = self._encode_categorical_new(new_data, cat_factors[0])
                for factor in cat_factors[1:]:
                    enc = self._encode_categorical_new(new_data, factor)
                    n_cols1, n_cols2 = cat_enc.shape[1], enc.shape[1]
                    new_enc = np.zeros((n, n_cols1 * n_cols2), dtype=self.dtype)
                    for i in range(n_cols1):
                        for j in range(n_cols2):
                            new_enc[:, i * n_cols2 + j] = cat_enc[:, i] * enc[:, j]
                    cat_enc = new_enc
            
            if spline_factors:
                all_columns = []
                
                for spline_str, spline in spline_factors:
                    x = new_data[spline.var_name].to_numpy().astype(self.dtype)
                    fitted_spline = self._fitted_splines.get(spline.var_name, spline)
                    spline_basis, _ = fitted_spline.transform(x)
                    
                    for j in range(spline_basis.shape[1]):
                        for i in range(cat_enc.shape[1]):
                            col = cat_enc[:, i] * spline_basis[:, j]
                            all_columns.append(col)
                
                if cont_factors:
                    cont_product = new_data[cont_factors[0]].to_numpy().astype(self.dtype)
                    for factor in cont_factors[1:]:
                        cont_product = cont_product * new_data[factor].to_numpy().astype(self.dtype)
                    all_columns = [col * cont_product for col in all_columns]
                
                if all_columns:
                    return np.column_stack(all_columns)
                return np.zeros((n, 0), dtype=self.dtype)
            
            cont_product = new_data[cont_factors[0]].to_numpy().astype(self.dtype)
            for factor in cont_factors[1:]:
                cont_product = cont_product * new_data[factor].to_numpy().astype(self.dtype)
            
            result = cat_enc * cont_product.reshape(-1, 1)
            return result
    
    def transform_new_data(self, new_data: "pl.DataFrame") -> np.ndarray:
        """Transform new data using the encoding state from training."""
        if self._parsed_formula is None:
            raise ValueError("No formula has been fitted yet.")
        
        parsed = self._parsed_formula
        n_new = len(new_data)
        columns = []
        
        if parsed.has_intercept:
            columns.append(np.ones(n_new, dtype=self.dtype))
        
        for var in parsed.main_effects:
            if var in parsed.categorical_vars:
                enc = self._encode_categorical_new(new_data, var)
                columns.append(enc)
            else:
                col = new_data[var].to_numpy().astype(self.dtype)
                columns.append(col.reshape(-1, 1))
        
        for spline in parsed.spline_terms:
            x = new_data[spline.var_name].to_numpy().astype(self.dtype)
            fitted_spline = self._fitted_splines.get(spline.var_name, spline)
            spline_cols, _ = fitted_spline.transform(x)
            columns.append(spline_cols)
        
        for te_term in parsed.target_encoding_terms:
            te_col = self._encode_target_new(new_data, te_term)
            columns.append(te_col.reshape(-1, 1))
        
        for interaction in parsed.interactions:
            int_cols = self._build_interaction_new(new_data, interaction, n_new)
            if int_cols.ndim == 1:
                int_cols = int_cols.reshape(-1, 1)
            columns.append(int_cols)
        
        for identity in parsed.identity_terms:
            id_col, _ = self._build_identity_columns(identity, new_data)
            columns.append(id_col.reshape(-1, 1))
        
        for constraint in parsed.constraint_terms:
            con_col, _ = self._build_constraint_columns(constraint, new_data)
            columns.append(con_col.reshape(-1, 1))
        
        for cat_term in parsed.categorical_terms:
            cat_cols, _ = self._build_categorical_level_indicators_new(cat_term, new_data)
            columns.append(cat_cols)
        
        if columns:
            X = np.hstack([c if c.ndim == 2 else c.reshape(-1, 1) for c in columns])
        else:
            X = np.ones((n_new, 1), dtype=self.dtype)
        
        return X


class GLMModel:
    """
    Results from a formula-based GLM fit.
    
    This wraps the base GLMResults and adds formula-specific functionality
    like named coefficients and automatic summary formatting.
    
    Attributes
    ----------
    params : np.ndarray
        Fitted coefficients
    feature_names : list[str]
        Names corresponding to each coefficient
    formula : str
        The formula used to fit the model
    """
    
    def __init__(
        self,
        result,
        feature_names: List[str],
        formula: str,
        family: str,
        link: Optional[str],
        builder: Optional["InteractionBuilder"] = None,
        design_matrix: Optional[np.ndarray] = None,
        offset_spec: Optional[Union[str, np.ndarray]] = None,
        offset_is_exposure: bool = False,
        regularization_path_info: Optional["RegularizationPathInfo"] = None,
        smooth_results: Optional[List[SmoothTermResult]] = None,
        total_edf: Optional[float] = None,
        gcv: Optional[float] = None,
        terms_dict: Optional[Dict[str, Dict[str, Any]]] = None,
        interactions_spec: Optional[List[Dict[str, Any]]] = None,
    ):
        self._result = result
        self._smooth_results = smooth_results
        self._total_edf = total_edf
        self._gcv = gcv
        self.feature_names = feature_names
        self.formula = formula
        self.family = family
        self._regularization_path_info = regularization_path_info
        self.link = link or get_default_link(family)
        self._builder = builder
        self._design_matrix = design_matrix  # Store for VIF calculation
        self._offset_spec = offset_spec
        self._offset_is_exposure = offset_is_exposure
        self._terms_dict = terms_dict
        self._interactions_spec = interactions_spec
    
    @property
    def smooth_terms(self) -> Optional[List[SmoothTermResult]]:
        """Smooth term results with EDF, lambda, and GCV for each s() term."""
        return self._smooth_results
    
    @property
    def total_edf(self) -> Optional[float]:
        """Total effective degrees of freedom (parametric + smooth terms)."""
        return self._total_edf
    
    @property
    def gcv(self) -> Optional[float]:
        """Generalized Cross-Validation score for smoothness selection."""
        return self._gcv
    
    def has_smooth_terms(self) -> bool:
        """Check if model contains smooth terms with automatic smoothing."""
        return self._smooth_results is not None and len(self._smooth_results) > 0
    
    @property
    def terms_dict(self) -> Optional[Dict[str, Dict[str, Any]]]:
        """Original terms dictionary used to specify the model (dict API only)."""
        return self._terms_dict
    
    @property
    def interactions_spec(self) -> Optional[List[Dict[str, Any]]]:
        """Original interactions specification used to specify the model (dict API only)."""
        return self._interactions_spec
    
    # Delegate to underlying result
    @property
    def params(self) -> np.ndarray:
        """Fitted coefficients."""
        return self._result.params
    
    @property
    def fittedvalues(self) -> np.ndarray:
        """Fitted values (predicted means)."""
        return self._result.fittedvalues
    
    @property
    def linear_predictor(self) -> np.ndarray:
        """Linear predictor (eta = X @ beta)."""
        return self._result.linear_predictor
    
    @property
    def deviance(self) -> float:
        """Model deviance."""
        return self._result.deviance
    
    @property
    def converged(self) -> bool:
        """Whether IRLS converged."""
        return self._result.converged
    
    @property
    def iterations(self) -> int:
        """Number of IRLS iterations."""
        return self._result.iterations
    
    def bse(self) -> np.ndarray:
        """Standard errors of coefficients."""
        return self._result.bse()
    
    def tvalues(self) -> np.ndarray:
        """z/t statistics."""
        return self._result.tvalues()
    
    def pvalues(self) -> np.ndarray:
        """P-values for coefficients."""
        return self._result.pvalues()
    
    def conf_int(self, alpha: float = 0.05) -> np.ndarray:
        """Confidence intervals."""
        return self._result.conf_int(alpha)
    
    def significance_codes(self) -> List[str]:
        """Significance codes."""
        return self._result.significance_codes()
    
    # Robust standard errors (sandwich estimators)
    def bse_robust(self, cov_type: str = "HC1") -> np.ndarray:
        """Robust standard errors of coefficients (HC/sandwich estimator).
        
        Unlike model-based standard errors that assume correct variance
        specification, robust standard errors are valid under heteroscedasticity.
        
        Parameters
        ----------
        cov_type : str, optional
            Type of robust covariance. Options:
            - "HC0": No small-sample correction
            - "HC1": Degrees of freedom correction (default, recommended)
            - "HC2": Leverage-adjusted
            - "HC3": Jackknife-like (most conservative)
        
        Returns
        -------
        numpy.ndarray
            Array of robust standard errors, one for each coefficient.
        """
        return self._result.bse_robust(cov_type)
    
    def tvalues_robust(self, cov_type: str = "HC1") -> np.ndarray:
        """z/t statistics using robust standard errors.
        
        Parameters
        ----------
        cov_type : str, optional
            Type of robust covariance. Default "HC1".
        
        Returns
        -------
        numpy.ndarray
            Array of t/z statistics (coefficient / robust SE).
        """
        return self._result.tvalues_robust(cov_type)
    
    def pvalues_robust(self, cov_type: str = "HC1") -> np.ndarray:
        """P-values using robust standard errors.
        
        Parameters
        ----------
        cov_type : str, optional
            Type of robust covariance. Default "HC1".
        
        Returns
        -------
        numpy.ndarray
            Array of p-values.
        """
        return self._result.pvalues_robust(cov_type)
    
    def conf_int_robust(self, alpha: float = 0.05, cov_type: str = "HC1") -> np.ndarray:
        """Confidence intervals using robust standard errors.
        
        Parameters
        ----------
        alpha : float, optional
            Significance level. Default 0.05 gives 95% CI.
        cov_type : str, optional
            Type of robust covariance. Default "HC1".
        
        Returns
        -------
        numpy.ndarray
            2D array of shape (n_params, 2) with [lower, upper] bounds.
        """
        return self._result.conf_int_robust(alpha, cov_type)
    
    def cov_robust(self, cov_type: str = "HC1") -> np.ndarray:
        """Robust covariance matrix (HC/sandwich estimator).
        
        Parameters
        ----------
        cov_type : str, optional
            Type of robust covariance. Default "HC1".
        
        Returns
        -------
        numpy.ndarray
            Robust covariance matrix (p × p).
        """
        return self._result.cov_robust(cov_type)
    
    # Diagnostic methods (statsmodels-compatible)
    def resid_response(self) -> np.ndarray:
        """Response residuals: y - μ."""
        return self._result.resid_response()
    
    def resid_pearson(self) -> np.ndarray:
        """Pearson residuals: (y - μ) / √V(μ)."""
        return self._result.resid_pearson()
    
    def resid_deviance(self) -> np.ndarray:
        """Deviance residuals: sign(y - μ) × √d_i."""
        return self._result.resid_deviance()
    
    def resid_working(self) -> np.ndarray:
        """Working residuals: (y - μ) × g'(μ)."""
        return self._result.resid_working()
    
    def llf(self) -> float:
        """Log-likelihood of the fitted model."""
        return self._result.llf()
    
    def aic(self) -> float:
        """Akaike Information Criterion."""
        return self._result.aic()
    
    def bic(self) -> float:
        """Bayesian Information Criterion."""
        return self._result.bic()
    
    def null_deviance(self) -> float:
        """Deviance of intercept-only model."""
        return self._result.null_deviance()
    
    def pearson_chi2(self) -> float:
        """Pearson chi-squared statistic."""
        return self._result.pearson_chi2()
    
    def scale(self) -> float:
        """Estimated dispersion parameter (deviance-based)."""
        return self._result.scale()
    
    def scale_pearson(self) -> float:
        """Estimated dispersion parameter (Pearson-based)."""
        return self._result.scale_pearson()
    
    def get_design_matrix(self) -> np.ndarray:
        """Get the design matrix X used in fitting."""
        return np.asarray(self._result.design_matrix)
    
    def get_irls_weights(self) -> np.ndarray:
        """Get the IRLS working weights from final iteration."""
        return np.asarray(self._result.irls_weights)
    
    def get_bread_matrix(self) -> np.ndarray:
        """Get the (X'WX)^-1 matrix (unscaled covariance)."""
        return np.asarray(self._result.cov_params_unscaled)
    
    # Regularization properties
    @property
    def alpha(self) -> float:
        """Regularization strength (lambda)."""
        return self._result.alpha
    
    @property
    def l1_ratio(self):
        """L1 ratio for Elastic Net (1.0=Lasso, 0.0=Ridge)."""
        return self._result.l1_ratio
    
    @property
    def is_regularized(self) -> bool:
        """Whether this is a regularized model."""
        return self._result.is_regularized
    
    @property
    def penalty_type(self) -> str:
        """Type of penalty: 'none', 'ridge', 'lasso', or 'elasticnet'."""
        return self._result.penalty_type
    
    def n_nonzero(self) -> int:
        """Number of non-zero coefficients (excluding intercept)."""
        return self._result.n_nonzero()
    
    def selected_features(self) -> List[str]:
        """
        Get names of features with non-zero coefficients.
        
        Useful for Lasso/Elastic Net to see which variables were selected.
        """
        indices = self._result.selected_features()
        return [self.feature_names[i] for i in indices]
    
    # CV-based regularization path properties
    @property
    def cv_deviance(self) -> Optional[float]:
        """CV deviance at selected alpha (only available when fit with cv=)."""
        if self._regularization_path_info is None:
            return None
        return self._regularization_path_info.cv_deviance
    
    @property
    def cv_deviance_se(self) -> Optional[float]:
        """Standard error of CV deviance (only available when fit with cv=)."""
        if self._regularization_path_info is None:
            return None
        return self._regularization_path_info.cv_deviance_se
    
    @property
    def regularization_type(self) -> Optional[str]:
        """Type of regularization: 'ridge', 'lasso', 'elastic_net', or 'none'."""
        if self._regularization_path_info is None:
            # Fall back to penalty_type from underlying result
            return self.penalty_type
        return self._regularization_path_info.regularization_type
    
    @property
    def regularization_path(self) -> Optional[List[dict]]:
        """
        Full regularization path results (only available when fit with cv=).
        
        Returns list of dicts with keys: alpha, l1_ratio, cv_deviance_mean, 
        cv_deviance_se, n_nonzero, max_coef.
        """
        if self._regularization_path_info is None:
            return None
        return [
            {
                "alpha": r.alpha,
                "l1_ratio": r.l1_ratio,
                "cv_deviance_mean": r.cv_deviance_mean,
                "cv_deviance_se": r.cv_deviance_se,
                "n_nonzero": r.n_nonzero,
                "max_coef": r.max_coef,
            }
            for r in self._regularization_path_info.path
        ]
    
    @property
    def cv_selection_method(self) -> Optional[str]:
        """Selection method used: 'min' or '1se' (only available when fit with cv=)."""
        if self._regularization_path_info is None:
            return None
        return self._regularization_path_info.selection_method
    
    @property
    def n_cv_folds(self) -> Optional[int]:
        """Number of CV folds used (only available when fit with cv=)."""
        if self._regularization_path_info is None:
            return None
        return self._regularization_path_info.n_folds
    
    @property
    def nobs(self) -> int:
        """Number of observations."""
        return self._result.nobs
    
    @property
    def df_resid(self) -> int:
        """Residual degrees of freedom."""
        return self._result.df_resid
    
    @property
    def df_model(self) -> int:
        """Model degrees of freedom."""
        return self._result.df_model
    
    def compute_loss(
        self, 
        data: "pl.DataFrame",
        response: Optional[str] = None,
        exposure: Optional[str] = None,
    ) -> float:
        """
        Compute family-appropriate loss (mean deviance) on given data.
        
        This method re-predicts on the data to ensure consistent encoding,
        which is critical for TE() terms that use leave-one-out during fit
        but full encoding for prediction.
        
        Parameters
        ----------
        data : pl.DataFrame
            Data to compute loss on (can be train, test, or holdout).
        response : str, optional
            Response column name. Auto-detected from formula if not provided.
        exposure : str, optional
            Exposure column name for rate models.
            
        Returns
        -------
        float
            Mean deviance (family-appropriate loss metric).
            
        Examples
        --------
        >>> train_loss = result.compute_loss(train_data)
        >>> test_loss = result.compute_loss(test_data)
        >>> assert train_loss < test_loss  # Expected for non-overfitting models
        """
        from rustystats._rustystats import compute_loss_metrics_py as _rust_loss_metrics
        
        # Get response column from formula
        if response is None:
            formula_parts = self.formula.split('~')
            response = formula_parts[0].strip() if formula_parts else None
        
        if response is None or response not in data.columns:
            raise ValueError(f"Response column '{response}' not found in data")
        
        y = data[response].to_numpy().astype(np.float64)
        
        # Re-predict to get consistent encoding (critical for TE terms)
        mu = np.asarray(self.predict(data), dtype=np.float64)
        
        # Compute family-appropriate loss
        loss_metrics = _rust_loss_metrics(y, mu, self.family)
        return loss_metrics["family_loss"]
    
    def coef_table(self) -> "pl.DataFrame":
        """
        Return coefficients as a DataFrame with names.
        
        Returns
        -------
        pl.DataFrame
            DataFrame with columns: Feature, Estimate, Std.Error, z, Pr(>|z|), Signif
        """
        import polars as pl
        
        return pl.DataFrame({
            "Feature": self.feature_names,
            "Estimate": self.params,
            "Std.Error": self.bse(),
            "z": self.tvalues(),
            "Pr(>|z|)": self.pvalues(),
            "Signif": self.significance_codes(),
        })
    
    def relativities(self) -> "pl.DataFrame":
        """
        Return relativities (exp(coef)) for log-link models.
        
        Returns
        -------
        pl.DataFrame
            DataFrame with Feature, Relativity and confidence interval columns
        """
        import polars as pl
        
        if self.link not in ("log",):
            raise ValueError(
                f"Relativities only meaningful for log link, not '{self.link}'"
            )
        
        ci = self.conf_int()
        
        return pl.DataFrame({
            "Feature": self.feature_names,
            "Relativity": np.exp(self.params),
            "CI_Lower": np.exp(ci[:, 0]),
            "CI_Upper": np.exp(ci[:, 1]),
        })
    
    def summary(self) -> str:
        """
        Generate a formatted summary string.
        
        Returns
        -------
        str
            Formatted summary table
        """
        from rustystats.glm import summary
        return summary(self._result, feature_names=self.feature_names)
    
    def diagnostics(
        self,
        train_data: "pl.DataFrame",
        categorical_factors: Optional[List[str]] = None,
        continuous_factors: Optional[List[str]] = None,
        n_calibration_bins: int = 10,
        n_factor_bins: int = 10,
        rare_threshold_pct: float = 1.0,
        max_categorical_levels: int = 20,
        detect_interactions: bool = True,
        max_interaction_factors: int = 10,
        # Test data for overfitting detection (response/exposure auto-inferred)
        test_data: Optional["pl.DataFrame"] = None,
        # Control enhanced diagnostics
        compute_vif: bool = True,
        compute_coefficients: bool = True,
        compute_deviance_by_level: bool = True,
        compute_lift: bool = True,
        compute_partial_dep: bool = True,
        # Base predictions comparison
        base_predictions: Optional[str] = None,
        # Legacy parameter (deprecated)
        data: Optional["pl.DataFrame"] = None,
    ):
        """
        Compute comprehensive model diagnostics.
        
        Parameters
        ----------
        train_data : pl.DataFrame
            Training data used for fitting.
        categorical_factors : list of str, optional
            Names of categorical factors to analyze (both fitted and unfitted).
        continuous_factors : list of str, optional
            Names of continuous factors to analyze (both fitted and unfitted).
        n_calibration_bins : int, default=10
            Number of bins for calibration curve.
        n_factor_bins : int, default=10
            Number of quantile bins for continuous factors.
        rare_threshold_pct : float, default=1.0
            Threshold (%) below which categorical levels are grouped into "Other".
        max_categorical_levels : int, default=20
            Maximum number of categorical levels to show.
        detect_interactions : bool, default=True
            Whether to detect potential interactions.
        max_interaction_factors : int, default=10
            Maximum factors to consider for interaction detection.
        test_data : pl.DataFrame, optional
            Test/holdout data for overfitting detection. Response and exposure
            columns are automatically inferred from the model's formula.
        compute_vif : bool, default=True
            Compute VIF/multicollinearity scores for design matrix (train-only).
        compute_coefficients : bool, default=True
            Compute coefficient summary with interpretations (train-only).
        compute_deviance_by_level : bool, default=True
            Compute deviance breakdown by categorical factor levels.
        compute_lift : bool, default=True
            Compute full lift chart with all deciles.
        compute_partial_dep : bool, default=True
            Compute partial dependence plots for each variable.
        base_predictions : str, optional
            Column name in train_data containing predictions from another model
            (e.g., a base/benchmark model). When provided, computes:
            - A/E ratio, loss, Gini for base predictions
            - Model vs base decile analysis sorted by model/base ratio
            - Summary of which model performs better in each decile
        
        Returns
        -------
        ModelDiagnostics
            Complete diagnostics object with to_json() method.
            
            Fields for agentic workflows:
            - vif: VIF scores detecting multicollinearity (train-only)
            - coefficient_summary: Coefficient magnitudes and recommendations (train-only)
            - factor_deviance: Deviance by categorical level
            - lift_chart: Full lift chart showing discrimination by decile
            - partial_dependence: Marginal effect shapes for linear vs spline decisions
            - train_test: Comprehensive train vs test comparison with flags:
                - overfitting_risk: True if gini_gap > 0.03
                - calibration_drift: True if test A/E outside [0.95, 1.05]
                - unstable_factors: Factors where train/test A/E differ by > 0.1
        
        Examples
        --------
        >>> result = rs.glm("ClaimNb ~ Age + C(Region)", data, family="poisson", offset="Exposure").fit()
        >>> 
        >>> # Basic diagnostics
        >>> diagnostics = result.diagnostics(
        ...     train_data=train_data,
        ...     categorical_factors=["Region", "VehBrand"],
        ...     continuous_factors=["Age", "VehPower"]
        ... )
        >>> 
        >>> # With test data for overfitting detection
        >>> diagnostics = result.diagnostics(
        ...     train_data=train_data,
        ...     test_data=test_data,
        ...     categorical_factors=["Region"],
        ...     continuous_factors=["Age"],
        ... )
        >>> 
        >>> # Check overfitting flags
        >>> if diagnostics.train_test and diagnostics.train_test.overfitting_risk:
        ...     print("Warning: Overfitting detected!")
        >>> 
        >>> print(diagnostics.to_json())
        """
        from rustystats.diagnostics import compute_diagnostics
        
        # Support legacy 'data' parameter
        if train_data is None and data is not None:
            train_data = data
        
        # Get design matrix for VIF calculation
        design_matrix = None
        if compute_vif and self._design_matrix is not None:
            design_matrix = self._design_matrix
        
        return compute_diagnostics(
            result=self,
            train_data=train_data,
            categorical_factors=categorical_factors,
            continuous_factors=continuous_factors,
            n_calibration_bins=n_calibration_bins,
            n_factor_bins=n_factor_bins,
            rare_threshold_pct=rare_threshold_pct,
            max_categorical_levels=max_categorical_levels,
            detect_interactions=detect_interactions,
            max_interaction_factors=max_interaction_factors,
            test_data=test_data,
            design_matrix=design_matrix,
            compute_vif=compute_vif,
            compute_coefficients=compute_coefficients,
            compute_deviance_by_level=compute_deviance_by_level,
            compute_lift=compute_lift,
            compute_partial_dep=compute_partial_dep,
            base_predictions=base_predictions,
        )
    
    def diagnostics_json(
        self,
        train_data: "pl.DataFrame",
        categorical_factors: Optional[List[str]] = None,
        continuous_factors: Optional[List[str]] = None,
        n_calibration_bins: int = 10,
        n_factor_bins: int = 10,
        rare_threshold_pct: float = 1.0,
        max_categorical_levels: int = 20,
        detect_interactions: bool = True,
        max_interaction_factors: int = 10,
        test_data: Optional["pl.DataFrame"] = None,
        indent: Optional[int] = None,
        # Legacy parameter
        data: Optional["pl.DataFrame"] = None,
    ) -> str:
        """
        Compute diagnostics and return as JSON string.
        
        This is a convenience method that calls diagnostics() and converts
        the result to JSON. The output is optimized for LLM consumption.
        
        Parameters
        ----------
        train_data : pl.DataFrame
            Training data used for fitting.
        categorical_factors : list of str, optional
            Names of categorical factors to analyze.
        continuous_factors : list of str, optional
            Names of continuous factors to analyze.
        test_data : pl.DataFrame, optional
            Test data for overfitting detection.
        indent : int, optional
            JSON indentation. None for compact output.
        
        Returns
        -------
        str
            JSON string containing all diagnostics.
        """
        # Support legacy 'data' parameter
        if train_data is None and data is not None:
            train_data = data
        
        diag = self.diagnostics(
            train_data=train_data,
            categorical_factors=categorical_factors,
            continuous_factors=continuous_factors,
            n_calibration_bins=n_calibration_bins,
            n_factor_bins=n_factor_bins,
            rare_threshold_pct=rare_threshold_pct,
            max_categorical_levels=max_categorical_levels,
            detect_interactions=detect_interactions,
            max_interaction_factors=max_interaction_factors,
            test_data=test_data,
        )
        return diag.to_json(indent=indent)
    
    def predict(
        self,
        new_data: "pl.DataFrame",
        offset: Optional[Union[str, np.ndarray]] = None,
    ) -> np.ndarray:
        """
        Predict on new data using the fitted model.
        
        Parameters
        ----------
        new_data : pl.DataFrame
            New data to predict on. Must have the same columns as training data.
        offset : str or array-like, optional
            Offset for new data. If None and the model was fit with an offset
            column name, that column will be extracted from new_data.
            For Poisson/Gamma with log link, log() is auto-applied to exposure.
            
        Returns
        -------
        np.ndarray
            Predicted values (on the response scale, i.e., μ = E[Y]).
            
        Examples
        --------
        >>> model = rs.glm("ClaimNb ~ Age + C(Region)", data, family="poisson", offset="Exposure")
        >>> result = model.fit()
        >>> 
        >>> # Predict on new data
        >>> predictions = result.predict(new_data)
        >>> 
        >>> # Predict with custom offset
        >>> predictions = result.predict(new_data, offset=np.log(new_exposures))
        """
        if self._builder is None:
            raise ValueError(
                "Cannot predict: model was not fitted with formula API. "
                "Use fittedvalues for training data predictions."
            )
        
        # Build design matrix for new data using stored encoding state
        X_new = self._builder.transform_new_data(new_data)
        
        # Compute linear predictor: η = X @ β
        linear_pred = X_new @ self.params
        
        # Handle offset
        # If offset is provided as a string, extract column and apply log() for log-link models
        # If offset is provided as array, use directly (user handles transformation)
        # If offset is None but model was fit with offset, use the stored offset column
        offset_to_use = offset
        if offset_to_use is None and hasattr(self, '_offset_spec') and self._offset_spec is not None:
            # Auto-use the offset column from fitting
            offset_to_use = self._offset_spec
        
        if offset_to_use is not None:
            if isinstance(offset_to_use, str):
                offset_values = new_data[offset_to_use].to_numpy().astype(np.float64)
                # Apply log() for log-link models (same as fitting)
                if self._offset_is_exposure:
                    offset_values = np.log(offset_values)
            else:
                offset_values = np.asarray(offset_to_use, dtype=np.float64)
            linear_pred = linear_pred + offset_values
        
        # Apply inverse link function to get predictions on response scale
        return self._apply_inverse_link(linear_pred)
    
    def _apply_inverse_link(self, eta: np.ndarray) -> np.ndarray:
        """Apply inverse link function to linear predictor."""
        link = self.link
        if link == "identity":
            return eta
        elif link == "log":
            return np.exp(eta)
        elif link == "logit":
            return 1.0 / (1.0 + np.exp(-eta))
        elif link == "inverse":
            return 1.0 / eta
        else:
            # Default to identity
            return eta
    
    def to_bytes(self) -> bytes:
        """
        Serialize the fitted model to bytes for storage or transfer.
        
        The serialized model can be loaded with `GLMModel.from_bytes()`.
        All state needed for prediction is preserved, including:
        - Coefficients and feature names
        - Categorical encoding levels
        - Spline knot positions
        - Target encoding statistics
        
        Returns
        -------
        bytes
            Serialized model as bytes.
            
        Examples
        --------
        >>> result = rs.glm("y ~ x1 + C(cat)", data, family="poisson").fit()
        >>> model_bytes = result.to_bytes()
        >>> 
        >>> # Save to file
        >>> with open("model.bin", "wb") as f:
        ...     f.write(model_bytes)
        >>> 
        >>> # Load later
        >>> with open("model.bin", "rb") as f:
        ...     loaded = rs.GLMModel.from_bytes(f.read())
        >>> predictions = loaded.predict(new_data)
        """
        import pickle
        
        # Extract state from the Rust result object
        # NOTE: We intentionally exclude fittedvalues and linear_predictor
        # as they are large arrays not needed for prediction (can be ~5MB each)
        result_state = {
            "params": np.asarray(self._result.params),
            "deviance": self._result.deviance,
            "iterations": self._result.iterations,
            "converged": self._result.converged,
            "nobs": self._result.nobs,
            "df_resid": self._result.df_resid,
            "df_model": self._result.df_model,
            "alpha": self._result.alpha,
            "l1_ratio": self._result.l1_ratio,
            "is_regularized": self._result.is_regularized,
            "penalty_type": self._result.penalty_type,
        }
        
        # Extract builder state for prediction
        builder_state = None
        if self._builder is not None:
            builder_state = {
                "parsed_formula": self._builder._parsed_formula,
                "cat_encoding_cache": self._builder._cat_encoding_cache,
                "fitted_splines": self._builder._fitted_splines,
                "te_stats": getattr(self._builder, "_te_stats", {}),
                "dtype": self._builder.dtype,
            }
        
        state = {
            "version": 1,
            "result_state": result_state,
            "feature_names": self.feature_names,
            "formula": self.formula,
            "family": self.family,
            "link": self.link,
            "builder_state": builder_state,
            "offset_spec": self._offset_spec,
            "offset_is_exposure": self._offset_is_exposure,
            "smooth_results": self._smooth_results,
            "total_edf": self._total_edf,
            "gcv": self._gcv,
            "terms_dict": self._terms_dict,
            "interactions_spec": self._interactions_spec,
        }
        
        return pickle.dumps(state, protocol=pickle.HIGHEST_PROTOCOL)
    
    @classmethod
    def from_bytes(cls, data: bytes) -> "GLMModel":
        """
        Load a fitted model from bytes.
        
        Parameters
        ----------
        data : bytes
            Serialized model bytes from `to_bytes()`.
            
        Returns
        -------
        GLMModel
            Reconstructed fitted model ready for prediction.
            
        Examples
        --------
        >>> # Load from file
        >>> with open("model.bin", "rb") as f:
        ...     result = rs.GLMModel.from_bytes(f.read())
        >>> 
        >>> # Make predictions
        >>> predictions = result.predict(new_data)
        """
        import pickle
        
        state = pickle.loads(data)
        
        if state.get("version", 0) != 1:
            raise ValueError(
                f"Unsupported serialization version: {state.get('version')}. "
                "Model was saved with a different version of rustystats."
            )
        
        result_state = state["result_state"]
        
        # Create a minimal result object that supports prediction
        result = _DeserializedResult(
            params=result_state["params"],
            deviance=result_state["deviance"],
            iterations=result_state["iterations"],
            converged=result_state["converged"],
            nobs=result_state["nobs"],
            df_resid=result_state["df_resid"],
            df_model=result_state["df_model"],
            alpha=result_state["alpha"],
            l1_ratio=result_state["l1_ratio"],
            is_regularized=result_state["is_regularized"],
            penalty_type=result_state["penalty_type"],
        )
        
        # Reconstruct builder if it was saved
        builder = None
        if state["builder_state"] is not None:
            builder = _DeserializedBuilder(state["builder_state"])
        
        return cls(
            result=result,
            feature_names=state["feature_names"],
            formula=state["formula"],
            family=state["family"],
            link=state["link"],
            builder=builder,
            design_matrix=None,
            offset_spec=state["offset_spec"],
            offset_is_exposure=state["offset_is_exposure"],
            regularization_path_info=None,
            smooth_results=state["smooth_results"],
            total_edf=state["total_edf"],
            gcv=state["gcv"],
            terms_dict=state.get("terms_dict"),
            interactions_spec=state.get("interactions_spec"),
        )
    
    def __repr__(self) -> str:
        return (
            f"<GLMModel: {self.family} family, "
            f"{len(self.params)} parameters, "
            f"deviance={self.deviance:.2f}>"
        )


def glm(
    formula: str,
    data: "pl.DataFrame",
    family: str = "gaussian",
    link: Optional[str] = None,
    var_power: float = 1.5,
    theta: Optional[float] = None,
    offset: Optional[Union[str, np.ndarray]] = None,
    weights: Optional[Union[str, np.ndarray]] = None,
    seed: Optional[int] = None,
) -> FormulaGLM:
    """
    Create a GLM model from a formula and DataFrame.
    
    This is the main entry point for the formula-based API.
    
    Parameters
    ----------
    formula : str
        R-style formula specifying the model.
        
        Supported syntax:
        - Main effects: ``x1``, ``x2``, ``C(cat)`` (categorical)
        - Two-way interactions: ``x1:x2`` (interaction only), ``x1*x2`` (main effects + interaction)
        - Categorical interactions: ``C(cat1)*C(cat2)``, ``C(cat):x``
        - Higher-order: ``x1:x2:x3``
        - Splines: ``bs(x, df=5)``, ``ns(x, df=4)``
        - Intercept: included by default, use ``0 +`` or ``- 1`` to remove
        
    data : pl.DataFrame
        Polars DataFrame containing the variables.
        
    family : str, default="gaussian"
        Distribution family: "gaussian", "poisson", "binomial", "gamma", "tweedie",
        "quasipoisson", "quasibinomial", or "negbinomial"
        
    link : str, optional
        Link function. If None, uses canonical link.
        
    var_power : float, default=1.5
        Variance power for Tweedie family (ignored for others).
        
    theta : float, optional
        Dispersion parameter for Negative Binomial family (ignored for others).
        If None (default), theta is automatically estimated using profile likelihood.
        
    offset : str or array-like, optional
        Offset term. If string, treated as column name.
        For Poisson, log() is auto-applied to exposure columns.
        
    weights : str or array-like, optional
        Prior weights. If string, treated as column name.
        
    seed : int, optional
        Random seed for deterministic target encoding (TE). If None,
        TE uses random permutations which may produce different results
        on each run. Set to a fixed value for reproducibility.
        
    Returns
    -------
    FormulaGLM
        Model object. Call .fit() to fit the model.
        
    Examples
    --------
    >>> import rustystats as rs
    >>> import polars as pl
    >>> 
    >>> # Load data
    >>> data = pl.read_parquet("insurance.parquet")
    >>> 
    >>> # Fit Poisson model for claim frequency
    >>> model = rs.glm(
    ...     formula="ClaimNb ~ VehPower + VehAge + C(VehBrand) + C(Area)",
    ...     data=data,
    ...     family="poisson",
    ...     offset="Exposure"
    ... )
    >>> result = model.fit()
    >>> 
    >>> # Model with interactions
    >>> model = rs.glm(
    ...     formula="ClaimNb ~ VehPower*VehAge + C(Area):DrivAge",
    ...     data=data,
    ...     family="poisson",
    ...     offset="Exposure"
    ... )
    >>> result = model.fit()
    >>> print(result.summary())
    """
    return FormulaGLM(
        formula=formula,
        data=data,
        family=family,
        link=link,
        var_power=var_power,
        theta=theta,
        offset=offset,
        weights=weights,
        seed=seed,
    )


# =============================================================================
# Dict-based API
# =============================================================================

from typing import Dict, Any, Set
from rustystats.interactions import (
    ParsedFormula, InteractionTerm, TargetEncodingTermSpec, 
    IdentityTermSpec, CategoricalTermSpec, ConstraintTermSpec
)
from rustystats.splines import SplineTerm


def _parse_term_spec(
    var_name: str,
    spec: Dict[str, Any],
    categorical_vars: Set[str],
    main_effects: List[str],
    spline_terms: List[SplineTerm],
    target_encoding_terms: List[TargetEncodingTermSpec],
    identity_terms: List[IdentityTermSpec],
    categorical_terms: List[CategoricalTermSpec],
    constraint_terms: List[ConstraintTermSpec],
    frequency_encoding_terms: Optional[List] = None,
) -> None:
    """Parse a single term specification and add to appropriate lists."""
    # Valid keys for each term type
    VALID_KEYS = {
        "linear": {"type", "monotonicity"},
        "categorical": {"type", "levels"},
        "bs": {"type", "df", "k", "degree", "monotonicity"},
        "ns": {"type", "df", "k"},
        "target_encoding": {"type", "prior_weight", "n_permutations", "interaction", "variable"},
        "frequency_encoding": {"type", "variable"},
        "expression": {"type", "expr", "monotonicity"},
    }
    
    term_type = spec.get("type", "linear")
    
    # Validate keys
    valid_keys = VALID_KEYS.get(term_type, set())
    unknown_keys = set(spec.keys()) - valid_keys
    if unknown_keys:
        # Check for common typos
        typo_suggestions = {
            "monoticity": "monotonicity",
            "montonicity": "monotonicity",
            "increaing": "increasing",
            "decreaing": "decreasing",
        }
        suggestions = []
        for key in unknown_keys:
            if key in typo_suggestions:
                suggestions.append(f"'{key}' (did you mean '{typo_suggestions[key]}'?)")
            else:
                suggestions.append(f"'{key}'")
        raise ValueError(
            f"Unknown key(s) in term spec for '{var_name}': {', '.join(suggestions)}. "
            f"Valid keys for type='{term_type}' are: {sorted(valid_keys)}"
        )
    
    monotonicity = spec.get("monotonicity")  # "increasing" or "decreasing"
    
    if term_type == "linear":
        if monotonicity:
            # Constrained linear term
            constraint = "pos" if monotonicity == "increasing" else "neg"
            constraint_terms.append(ConstraintTermSpec(
                var_name=var_name,
                constraint=constraint,
            ))
        else:
            main_effects.append(var_name)
    
    elif term_type == "categorical":
        categorical_vars.add(var_name)
        levels = spec.get("levels")
        if levels:
            # Specific levels only
            categorical_terms.append(CategoricalTermSpec(
                var_name=var_name,
                levels=levels,
            ))
        else:
            main_effects.append(var_name)
    
    elif term_type == "bs":
        # Default to penalized smooth (k=10) if neither df nor k specified
        k = spec.get("k")
        df = spec.get("df")
        if df is None and k is None:
            df = 10  # Default: penalized smooth
            is_penalized = True
        elif k is not None:
            df = k
            is_penalized = True
        else:
            is_penalized = False
        degree = spec.get("degree", 3)
        term = SplineTerm(
            var_name=var_name,
            spline_type="bs",
            df=df,
            degree=degree,
            monotonicity=monotonicity,
        )
        if is_penalized:
            term._is_smooth = True
        if monotonicity:
            term._monotonic = True
        spline_terms.append(term)
    
    elif term_type == "ns":
        # Default to penalized smooth (k=10) if neither df nor k specified
        k = spec.get("k")
        df = spec.get("df")
        if df is None and k is None:
            df = 10  # Default: penalized smooth
            is_penalized = True
        elif k is not None:
            df = k
            is_penalized = True
        else:
            is_penalized = False
        if monotonicity:
            raise ValueError(
                f"Monotonicity constraints are not supported for natural splines (ns). "
                f"Use type='bs' with monotonicity parameter instead for monotonic effects."
            )
        term = SplineTerm(
            var_name=var_name,
            spline_type="ns",
            df=df,
        )
        if is_penalized:
            term._is_smooth = True
        spline_terms.append(term)
    
    elif term_type == "target_encoding":
        prior_weight = spec.get("prior_weight", 1.0)
        n_permutations = spec.get("n_permutations", 4)
        interaction_spec = spec.get("interaction")  # e.g., ["brand", "region"] for TE(brand:region)
        
        if interaction_spec:
            # TE interaction: TE(brand:region)
            if isinstance(interaction_spec, list) and len(interaction_spec) >= 2:
                combined_name = ":".join(interaction_spec)
                target_encoding_terms.append(TargetEncodingTermSpec(
                    var_name=combined_name,
                    prior_weight=prior_weight,
                    n_permutations=n_permutations,
                    interaction_vars=interaction_spec,
                ))
            else:
                raise ValueError(
                    f"'interaction' for target_encoding must be a list of at least 2 variable names, "
                    f"got: {interaction_spec}"
                )
        else:
            # Single variable TE - use 'variable' key if provided
            actual_var = spec.get("variable", var_name)
            existing_te_vars = {te.var_name for te in target_encoding_terms}
            if actual_var not in existing_te_vars:
                target_encoding_terms.append(TargetEncodingTermSpec(
                    var_name=actual_var,
                    prior_weight=prior_weight,
                    n_permutations=n_permutations,
                ))
    
    elif term_type == "frequency_encoding":
        from rustystats.interactions import FrequencyEncodingTermSpec as FETermSpec
        if frequency_encoding_terms is None:
            raise ValueError(
                f"frequency_encoding type not supported in this context. "
                f"Use formula string 'FE({var_name})' instead."
            )
        # Use 'variable' key if provided, otherwise use the dict key
        actual_var = spec.get("variable", var_name)
        frequency_encoding_terms.append(FETermSpec(var_name=actual_var))
    
    elif term_type == "expression":
        expr = spec.get("expr", var_name)
        if monotonicity:
            constraint = "pos" if monotonicity == "increasing" else "neg"
            constraint_terms.append(ConstraintTermSpec(
                var_name=f"I({expr})",
                constraint=constraint,
            ))
        else:
            identity_terms.append(IdentityTermSpec(expression=expr))
    
    else:
        raise ValueError(f"Unknown term type: {term_type}")


def _parse_interaction_spec(
    interaction: Dict[str, Any],
    interactions: List[InteractionTerm],
    categorical_vars: Set[str],
    main_effects: List[str],
    spline_terms: List[SplineTerm],
    target_encoding_terms: List[TargetEncodingTermSpec],
    identity_terms: List[IdentityTermSpec],
    categorical_terms: List[CategoricalTermSpec],
    constraint_terms: List[ConstraintTermSpec],
) -> None:
    """Parse an interaction specification."""
    include_main = interaction.get("include_main", False)
    
    # Extract variable specs (everything except include_main)
    var_specs = {k: v for k, v in interaction.items() if k != "include_main"}
    
    if len(var_specs) < 2:
        raise ValueError("Interaction must have at least 2 variables")
    
    # Determine which factors are categorical, splines, or TE
    cat_factors = set()
    spline_factors = []
    te_factor_names = {}  # Maps original name -> TE(name) format
    
    for var_name, spec in var_specs.items():
        term_type = spec.get("type", "linear")
        
        if term_type == "categorical":
            cat_factors.add(var_name)
            categorical_vars.add(var_name)
        elif term_type in ("bs", "ns", "s"):
            # For s() smooth terms, use k parameter; for bs/ns use df
            if term_type == "s":
                df = spec.get("k", 10)
            else:
                df = spec.get("df", 5 if term_type == "bs" else 4)
            degree = spec.get("degree", 3)
            monotonicity = spec.get("monotonicity")
            # Use unified bs with monotonicity parameter
            spline_type_out = "bs" if term_type == "s" else term_type
            spline = SplineTerm(
                var_name=var_name,
                spline_type=spline_type_out,
                df=df,
                degree=degree,
                monotonicity=monotonicity,
            )
            # Mark s() terms as smooth for penalized fitting
            if term_type == "s":
                spline._is_smooth = True
                if monotonicity:
                    spline._smooth_monotonicity = monotonicity
            spline_factors.append((var_name, spline))
        elif term_type == "target_encoding":
            prior_weight = spec.get("prior_weight", 1.0)
            te_factor_names[var_name] = f"TE({var_name})"
            # TE in interaction - add to TE terms so encoding is available (if not already present)
            existing_te_vars = {te.var_name for te in target_encoding_terms}
            if var_name not in existing_te_vars:
                target_encoding_terms.append(TargetEncodingTermSpec(
                    var_name=var_name,
                    prior_weight=prior_weight,
                ))
    
    # Build factors list, renaming TE factors to TE(name) format
    factors = [te_factor_names.get(k, k) for k in var_specs.keys()]
    
    # Build interaction term - categorical_flags is a bool for each factor
    categorical_flags = [f in cat_factors for f in factors]
    
    interaction_term = InteractionTerm(
        factors=factors,
        categorical_flags=categorical_flags,
    )
    interactions.append(interaction_term)
    
    # Add main effects if requested
    if include_main:
        for var_name, spec in var_specs.items():
            _parse_term_spec(
                var_name, spec, categorical_vars, main_effects,
                spline_terms, target_encoding_terms, identity_terms,
                categorical_terms, constraint_terms,
            )


def dict_to_parsed_formula(
    response: str,
    terms: Dict[str, Dict[str, Any]],
    interactions: Optional[List[Dict[str, Any]]] = None,
    intercept: bool = True,
) -> ParsedFormula:
    """
    Convert dict specification to ParsedFormula.
    
    Parameters
    ----------
    response : str
        Name of the response variable
    terms : dict
        Dictionary mapping variable names to term specifications
    interactions : list of dict, optional
        List of interaction specifications
    intercept : bool, default=True
        Whether to include an intercept
        
    Returns
    -------
    ParsedFormula
        Parsed formula object compatible with build_design_matrix
    """
    from rustystats.interactions import FrequencyEncodingTermSpec
    
    categorical_vars: Set[str] = set()
    main_effects: List[str] = []
    spline_terms_list: List[SplineTerm] = []
    target_encoding_terms_list: List[TargetEncodingTermSpec] = []
    frequency_encoding_terms_list: List[FrequencyEncodingTermSpec] = []
    identity_terms_list: List[IdentityTermSpec] = []
    categorical_terms_list: List[CategoricalTermSpec] = []
    constraint_terms_list: List[ConstraintTermSpec] = []
    interaction_terms_list: List[InteractionTerm] = []
    
    # Parse main terms
    for var_name, spec in terms.items():
        _parse_term_spec(
            var_name, spec, categorical_vars, main_effects,
            spline_terms_list, target_encoding_terms_list, identity_terms_list,
            categorical_terms_list, constraint_terms_list, frequency_encoding_terms_list,
        )
    
    # Parse interactions
    if interactions:
        for interaction in interactions:
            _parse_interaction_spec(
                interaction, interaction_terms_list, categorical_vars,
                main_effects, spline_terms_list, target_encoding_terms_list,
                identity_terms_list, categorical_terms_list, constraint_terms_list,
            )
    
    return ParsedFormula(
        response=response,
        main_effects=main_effects,
        interactions=interaction_terms_list,
        categorical_vars=categorical_vars,
        spline_terms=spline_terms_list,
        target_encoding_terms=target_encoding_terms_list,
        frequency_encoding_terms=frequency_encoding_terms_list,
        identity_terms=identity_terms_list,
        categorical_terms=categorical_terms_list,
        constraint_terms=constraint_terms_list,
        has_intercept=intercept,
    )


class FormulaGLMDict:
    """
    GLM model with dict-based specification.
    
    Alternative to formula strings for programmatic model building.
    """
    
    def __init__(
        self,
        response: str,
        terms: Dict[str, Dict[str, Any]],
        data: "pl.DataFrame",
        interactions: Optional[List[Dict[str, Any]]] = None,
        intercept: bool = True,
        family: str = "gaussian",
        link: Optional[str] = None,
        var_power: float = 1.5,
        theta: Optional[float] = None,
        offset: Optional[Union[str, np.ndarray]] = None,
        weights: Optional[Union[str, np.ndarray]] = None,
        seed: Optional[int] = None,
    ):
        self.response = response
        self.terms = terms
        self.interactions_spec = interactions
        self.intercept = intercept
        # Store weak reference to data to allow garbage collection
        self._data_ref = weakref.ref(data)
        self.family = family.lower()
        self.link = link
        self.var_power = var_power
        self.theta = theta
        self._offset_spec = offset
        self._weights_spec = weights
        self._seed = seed
        
        # Build formula string for compatibility (used in results/diagnostics)
        self.formula = self._build_formula_string()
        
        # Convert dict to ParsedFormula
        parsed = dict_to_parsed_formula(
            response=response,
            terms=terms,
            interactions=interactions,
            intercept=intercept,
        )
        
        # Extract raw exposure for target encoding
        raw_exposure = self._get_raw_exposure(offset)
        
        # Build design matrix using existing pipeline
        self._builder = InteractionBuilder(data)
        self.y, self.X, self.feature_names = self._builder.build_design_matrix_from_parsed(
            parsed, exposure=raw_exposure, seed=seed
        )
        self.n_obs = len(self.y)
        self.n_params = self.X.shape[1]
        
        # Process offset and weights
        self.offset = self._process_offset(offset)
        self.weights = self._process_weights(weights)
    
    def _build_formula_string(self) -> str:
        """Build a formula string representation for display purposes."""
        parts = [self.response, "~"]
        term_strs = []
        
        for var_name, spec in self.terms.items():
            term_type = spec.get("type", "linear")
            if term_type == "linear":
                term_strs.append(var_name)
            elif term_type == "categorical":
                term_strs.append(f"C({var_name})")
            elif term_type == "bs":
                df = spec.get("df", 5)
                term_strs.append(f"bs({var_name}, df={df})")
            elif term_type == "ns":
                df = spec.get("df", 4)
                term_strs.append(f"ns({var_name}, df={df})")
            elif term_type == "target_encoding":
                interaction = spec.get("interaction")
                if interaction:
                    term_strs.append(f"TE({':'.join(interaction)})")
                else:
                    term_strs.append(f"TE({var_name})")
            elif term_type == "frequency_encoding":
                term_strs.append(f"FE({var_name})")
            elif term_type == "expression":
                expr = spec.get("expr", var_name)
                term_strs.append(f"I({expr})")
        
        if not self.intercept:
            term_strs.insert(0, "0")
        
        parts.append(" + ".join(term_strs) if term_strs else "1")
        return " ".join(parts)
    
    @property
    def data(self):
        """Access the original DataFrame (may raise if garbage collected)."""
        d = self._data_ref()
        if d is None:
            raise RuntimeError(
                "Original DataFrame has been garbage collected. "
                "Keep a reference to the DataFrame if you need to access it after fitting."
            )
        return d
    
    def _get_raw_exposure(self, offset) -> Optional[np.ndarray]:
        """Extract raw exposure values for target encoding."""
        if offset is None:
            return None
        if isinstance(offset, str):
            return self.data[offset].to_numpy().astype(np.float64)
        return np.asarray(offset, dtype=np.float64)
    
    def _process_offset(self, offset) -> Optional[np.ndarray]:
        """Process offset, applying log for log-link families."""
        if offset is None:
            return None
        
        if isinstance(offset, str):
            offset_values = self.data[offset].to_numpy().astype(np.float64)
        else:
            offset_values = np.asarray(offset, dtype=np.float64)
        
        # Apply log for Poisson/Gamma families
        if self.family in ("poisson", "gamma", "quasipoisson", "tweedie", "negbinomial"):
            return np.log(offset_values)
        return offset_values
    
    def _process_weights(self, weights) -> Optional[np.ndarray]:
        """Process weights."""
        if weights is None:
            return None
        if isinstance(weights, str):
            return self.data[weights].to_numpy().astype(np.float64)
        return np.asarray(weights, dtype=np.float64)
    
    def fit(
        self,
        alpha: float = 0.0,
        l1_ratio: float = 0.0,
        max_iter: int = 25,
        tol: float = 1e-8,
        # Cross-validation based regularization path parameters
        cv: Optional[int] = None,
        selection: str = "min",
        regularization: Optional[str] = None,
        n_alphas: int = 20,
        alpha_min_ratio: float = 0.0001,
        cv_seed: Optional[int] = None,
        include_unregularized: bool = True,
        verbose: bool = False,
        # Memory optimization
        store_design_matrix: bool = True,
    ) -> GLMModel:
        """
        Fit the GLM model, optionally with regularization.
        
        Parameters
        ----------
        alpha : float, default=0.0
            Regularization strength. Higher values = more shrinkage.
            Ignored if regularization is specified (uses CV to find optimal).
            
        l1_ratio : float, default=0.0
            Elastic Net mixing parameter (0=Ridge, 1=Lasso).
            Ignored if regularization is specified with type.
            
        max_iter : int, default=25
            Maximum IRLS iterations.
        tol : float, default=1e-8
            Convergence tolerance.
            
        cv : int, optional
            Number of cross-validation folds. Defaults to 5 if regularization is set.
            
        selection : str, default="min"
            CV selection method: "min" or "1se".
            
        regularization : str, optional
            Type: "ridge", "lasso", or "elastic_net". Triggers CV-based alpha selection.
            
        n_alphas : int, default=20
            Number of alpha values in CV path.
            
        alpha_min_ratio : float, default=0.0001
            Smallest alpha as ratio of alpha_max.
            
        cv_seed : int, optional
            Random seed for CV folds.
            
        include_unregularized : bool, default=True
            Include alpha=0 in CV comparison.
            
        verbose : bool, default=False
            Print progress.
            
        Returns
        -------
        GLMModel
            Fitted model results.
        """
        is_negbinomial = is_negbinomial_family(self.family)
        
        # Handle CV-based regularization path
        # If regularization is specified without cv, default to cv=5
        if regularization is not None and cv is None:
            cv = 5
        
        if cv is not None:
            if regularization is None:
                raise ValueError(
                    "When cv is specified, 'regularization' must be set to 'ridge', 'lasso', or 'elastic_net'"
                )
            
            from rustystats.regularization_path import fit_cv_regularization_path
            
            # Determine l1_ratio from regularization type
            if regularization == "ridge":
                cv_l1_ratio = 0.0
            elif regularization == "lasso":
                cv_l1_ratio = 1.0
            elif regularization == "elastic_net":
                cv_l1_ratio = l1_ratio if l1_ratio > 0 else 0.5
            else:
                raise ValueError(f"Unknown regularization type: {regularization}")
            
            # Fit regularization path with CV
            path_info = fit_cv_regularization_path(
                glm_instance=self,
                cv=cv,
                selection=selection,
                regularization=regularization,
                n_alphas=n_alphas,
                alpha_min_ratio=alpha_min_ratio,
                l1_ratio=cv_l1_ratio,
                max_iter=max_iter,
                tol=tol,
                seed=cv_seed if cv_seed is not None else self._seed,
                include_unregularized=include_unregularized,
                verbose=verbose,
            )
            
            # Use selected alpha for final fit
            alpha = path_info.selected_alpha
            l1_ratio = path_info.selected_l1_ratio
            
            if verbose:
                print(f"\nRefitting on full data with alpha={alpha:.6f}")
        else:
            path_info = None
        
        theta = self.theta if self.theta is not None else DEFAULT_NEGBINOMIAL_THETA
        
        # Use shared core fitting logic
        result, smooth_results, total_edf, gcv = _fit_glm_core(
            self.y, self.X, self.family, self.link, self.var_power, theta,
            self.offset, self.weights, alpha, l1_ratio, max_iter, tol,
            self.feature_names, self._builder,
        )
        self._smooth_results = smooth_results
        self._total_edf = total_edf
        self._gcv = gcv
        
        result_family = f"NegativeBinomial(theta={theta:.4f})" if is_negbinomial else self.family
        
        # Wrap result with formula metadata
        is_exposure_offset = self.family in ("poisson", "quasipoisson", "negbinomial", "gamma") and self.link in (None, "log")
        return _build_results(
            result, self.feature_names, self.formula, result_family, self.link,
            self._builder, self.X, self._offset_spec, is_exposure_offset, path_info,
            self._smooth_results, self._total_edf, self._gcv,
            store_design_matrix=store_design_matrix,
            terms_dict=self.terms,
            interactions_spec=self.interactions_spec,
        )


def glm_dict(
    response: str,
    terms: Dict[str, Dict[str, Any]],
    data: "pl.DataFrame",
    interactions: Optional[List[Dict[str, Any]]] = None,
    intercept: bool = True,
    family: str = "gaussian",
    link: Optional[str] = None,
    var_power: float = 1.5,
    theta: Optional[float] = None,
    offset: Optional[Union[str, np.ndarray]] = None,
    weights: Optional[Union[str, np.ndarray]] = None,
    seed: Optional[int] = None,
) -> FormulaGLMDict:
    """
    Create a GLM model from a dict specification.
    
    This is an alternative to the formula-based API for programmatic model building.
    
    Parameters
    ----------
    response : str
        Name of the response variable column.
    terms : dict
        Dictionary mapping variable names to term specifications.
        Each specification is a dict with 'type' and optional parameters:
        
        - ``{"type": "linear"}`` - continuous variable
        - ``{"type": "categorical"}`` - dummy encoding
        - ``{"type": "categorical", "levels": ["A", "B"]}`` - specific levels
        - ``{"type": "bs", "df": 5}`` - B-spline
        - ``{"type": "bs", "df": 5, "degree": 2}`` - quadratic B-spline
        - ``{"type": "ns", "df": 4}`` - natural spline
        - ``{"type": "bs", "df": 4, "monotonicity": "increasing"}`` - monotonic
        - ``{"type": "target_encoding"}`` - target encoding
        - ``{"type": "expression", "expr": "x**2"}`` - expression
        - ``{"type": "linear", "monotonicity": "increasing"}`` - constrained
        
    data : pl.DataFrame
        Polars DataFrame containing the data.
    interactions : list of dict, optional
        List of interaction specifications. Each is a dict with variable
        names as keys and their specs as values, plus 'include_main'.
    intercept : bool, default=True
        Whether to include an intercept.
    family : str, default="gaussian"
        Distribution family.
    link : str, optional
        Link function. If None, uses canonical link.
    var_power : float, default=1.5
        Variance power for Tweedie family.
    theta : float, optional
        Dispersion for Negative Binomial.
    offset : str or array-like, optional
        Offset term.
    weights : str or array-like, optional
        Prior weights.
    seed : int, optional
        Random seed for deterministic target encoding.
        
    Returns
    -------
    FormulaGLMDict
        Model object. Call .fit() to fit the model.
        
    Examples
    --------
    >>> result = rs.glm_dict(
    ...     response="ClaimCount",
    ...     terms={
    ...         "VehAge": {"type": "linear"},
    ...         "DrivAge": {"type": "bs", "df": 5},
    ...         "Region": {"type": "categorical"},
    ...         "Brand": {"type": "target_encoding"},
    ...     },
    ...     interactions=[
    ...         {"VehAge": {"type": "linear"}, "Region": {"type": "categorical"}, "include_main": True},
    ...     ],
    ...     data=data,
    ...     family="poisson",
    ...     offset="Exposure",
    ... ).fit()
    """
    return FormulaGLMDict(
        response=response,
        terms=terms,
        data=data,
        interactions=interactions,
        intercept=intercept,
        family=family,
        link=link,
        var_power=var_power,
        theta=theta,
        offset=offset,
        weights=weights,
        seed=seed,
    )
