Metadata-Version: 2.4
Name: isagellm-benchmark
Version: 0.4.0.4
Summary: Benchmark Suite & E2E Testing for sageLLM
Author-email: IntelliStream Team <shuhao_zhang@hust.edu.cn>
License: Private
Project-URL: Homepage, https://github.com/intellistream/sagellm-benchmark
Project-URL: Repository, https://github.com/intellistream/sagellm-benchmark
Project-URL: Issues, https://github.com/intellistream/sagellm-benchmark/issues
Keywords: llm,inference,benchmark,testing,domestic-hardware
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: ==3.11.*
Description-Content-Type: text/markdown
Requires-Dist: isagellm-protocol<0.5.0,>=0.4.0.0
Requires-Dist: isagellm-core<0.5.0,>=0.4.0.0
Requires-Dist: isagellm-backend<0.5.0,>=0.4.0.1
Requires-Dist: click>=8.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: datasets>=2.14.0
Requires-Dist: jsonschema>=4.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Provides-Extra: openai-client
Requires-Dist: openai>=1.0.0; extra == "openai-client"
Provides-Extra: vllm-client
Requires-Dist: vllm>=0.2.0; extra == "vllm-client"
Requires-Dist: openai>=1.0.0; extra == "vllm-client"
Provides-Extra: lmdeploy-client
Requires-Dist: lmdeploy>=0.2.0; extra == "lmdeploy-client"
Requires-Dist: httpx>=0.24.0; extra == "lmdeploy-client"
Provides-Extra: sagellm-client
Requires-Dist: isagellm-backend>=0.3.0.3; extra == "sagellm-client"
Provides-Extra: all-clients
Requires-Dist: openai>=1.0.0; extra == "all-clients"
Requires-Dist: vllm>=0.2.0; extra == "all-clients"
Requires-Dist: lmdeploy>=0.2.0; extra == "all-clients"
Requires-Dist: httpx>=0.24.0; extra == "all-clients"
Requires-Dist: isagellm-backend>=0.3.0.3; extra == "all-clients"

# sagellm-benchmark

## Protocol Compliance (Mandatory)

- MUST follow Protocol v0.1: https://github.com/intellistream/sagellm-docs/blob/main/docs/specs/protocol_v0.1.md
- Any globally shared definitions (fields, error codes, metrics, IDs, schemas) MUST be added to Protocol first.

[![CI](https://github.com/intellistream/sagellm-benchmark/actions/workflows/ci.yml/badge.svg)](https://github.com/intellistream/sagellm-benchmark/actions/workflows/ci.yml)
[![codecov](https://codecov.io/gh/intellistream/sagellm-benchmark/branch/main/graph/badge.svg)](https://codecov.io/gh/intellistream/sagellm-benchmark)
[![PyPI version](https://badge.fury.io/py/isagellm-benchmark.svg)](https://badge.fury.io/py/isagellm-benchmark)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: Private](https://img.shields.io/badge/License-Private-red.svg)](LICENSE)

Benchmark suite for sageLLM inference engine performance and validation.

New here? See [QUICKSTART.md](QUICKSTART.md) for a 5-minute guide.

## Features

- End-to-end workload execution (short, long, stress)
- Standardized JSON metrics and reports
- One-command benchmark runner
- Extensible backend support

## Installation

```bash
pip install isagellm-benchmark
```

## Quick Start

```bash
# Run all workloads and generate reports
./run_benchmark.sh

# Specify a custom output directory
./run_benchmark.sh ./my_results
```

CLI examples:

```bash
# Run the full suite with the CPU backend
sagellm-benchmark run --workload year1 --backend cpu

# Run with a CPU model
sagellm-benchmark run --workload year1 --backend cpu --model gpt2

# Run a single workload
sagellm-benchmark run --workload short --backend cpu

# Generate reports
sagellm-benchmark report --input ./benchmark_results/benchmark_summary.json --format markdown
```

## Workloads

- **Short**: 128 prompt â†’ 128 output (5 requests)
- **Long**: 200 prompt â†’ 200 output (3 requests)
- **Stress**: 256 prompt â†’ 256 output (10 concurrent requests)

## Outputs

After running the benchmark, results are written to a folder like:

```
benchmark_results/
â”œâ”€â”€ benchmark_summary.json
â”œâ”€â”€ short_input_metrics.json
â”œâ”€â”€ long_input_metrics.json
â”œâ”€â”€ stress_test_metrics.json
â””â”€â”€ REPORT.md
```

Metrics include latency, throughput, memory, and error rates. See
[docs/USAGE.md](docs/USAGE.md) for details.

## Backends

- **cpu**: CPU inference via HuggingFace Transformers (requires `--model`)
- **planned**: lmdeploy, vllm

## Documentation

- [QUICKSTART.md](QUICKSTART.md) - 5 åˆ†é’Ÿå¿«é€Ÿå¼€å§‹
- [docs/USAGE.md](docs/USAGE.md) - è¯¦ç»†ä½¿ç”¨æŒ‡å—
- [docs/CLIENTS_GUIDE.md](docs/CLIENTS_GUIDE.md) - å®¢æˆ·ç«¯é€‰æ‹©æŒ‡å—
- [docs/DEPLOYMENT_ARCHITECTURE.md](docs/DEPLOYMENT_ARCHITECTURE.md) - éƒ¨ç½²æ¶æ„è¯´æ˜ï¼ˆHTTP API vs ç›´è¿ï¼‰

## ğŸ”„ è´¡çŒ®æŒ‡å—

è¯·éµå¾ªä»¥ä¸‹å·¥ä½œæµç¨‹ï¼š

1. **åˆ›å»º Issue** - æè¿°é—®é¢˜/éœ€æ±‚
   ```bash
   gh issue create --title "[Bug] æè¿°" --label "bug,sagellm-benchmark"
   ```

2. **å¼€å‘ä¿®å¤** - åœ¨æœ¬åœ° `fix/#123-xxx` åˆ†æ”¯è§£å†³
   ```bash
   git checkout -b fix/#123-xxx origin/main-dev
   # å¼€å‘ã€æµ‹è¯•...
   pytest -v
   ruff format . && ruff check . --fix
   ```

3. **å‘èµ· PR** - æäº¤åˆ° `main-dev` åˆ†æ”¯
   ```bash
   gh pr create --base main-dev --title "Fix: æè¿°" --body "Closes #123"
   ```

4. **åˆå¹¶** - å®¡æ‰¹ååˆå¹¶åˆ° `main-dev`

æ›´å¤šè¯¦æƒ…è§ [.github/copilot-instructions.md](.github/copilot-instructions.md)

## License

Private - IntelliStream Research Project
