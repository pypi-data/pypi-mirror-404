"""Markdownify fetcher - local HTML to Markdown conversion."""

from __future__ import annotations

import re

import httpx

from middleware.web.fetchers.base import BaseFetcher
from middleware.web.types import ContentChunk, FetchLimits, FetchResult

try:
    from markdownify import markdownify as md

    HAS_MARKDOWNIFY = True
except ImportError:
    HAS_MARKDOWNIFY = False

try:
    from bs4 import BeautifulSoup

    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False


class MarkdownifyFetcher(BaseFetcher):
    """
    Fetcher using markdownify for HTML to Markdown conversion.

    Falls back to BeautifulSoup text extraction if markdownify unavailable.
    """

    def __init__(
        self,
        limits: FetchLimits | None = None,
        timeout: int = 10,
        user_agent: str = "Mozilla/5.0 (compatible; LeonBot/1.0)",
    ):
        super().__init__(limits, timeout)
        self.user_agent = user_agent
        self.has_markdownify = HAS_MARKDOWNIFY
        self.has_bs4 = HAS_BS4

    async def fetch(self, url: str) -> FetchResult:
        """Fetch URL content and convert to Markdown."""
        result = FetchResult(url=url)

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(url, headers={"User-Agent": self.user_agent})
                response.raise_for_status()

            content_type = response.headers.get("Content-Type", "")

            if "text/html" in content_type:
                content = self._process_html(response.text, result)
            else:
                content = response.text

            if len(content) > self.limits.max_chars:
                content = content[: self.limits.max_chars]
                result.truncated = True
                result.truncation_reason = f"max_chars={self.limits.max_chars}"

            result.total_chars = len(content)

            chunk_data = self._split_into_chunks(content)
            result.chunks = [
                ContentChunk(position=pos, content=cont, heading=head)
                for pos, cont, head in chunk_data
            ]
            result.total_chunks = len(result.chunks)

            result.content = content

        except httpx.TimeoutException:
            result.error = f"Timeout fetching URL: {url}"
        except httpx.HTTPStatusError as e:
            result.error = f"HTTP error {e.response.status_code}: {url}"
        except httpx.RequestError as e:
            result.error = f"Error fetching URL: {e}"

        return result

    def _process_html(self, html: str, result: FetchResult) -> str:
        """Process HTML content to Markdown or plain text."""
        if self.has_markdownify:
            return self._markdownify_html(html, result)
        elif self.has_bs4:
            return self._bs4_extract(html, result)
        else:
            return self._basic_extract(html, result)

    def _markdownify_html(self, html: str, result: FetchResult) -> str:
        """Convert HTML to Markdown using markdownify."""
        if self.has_bs4:
            soup = BeautifulSoup(html, "html.parser")

            title_tag = soup.find("title")
            if title_tag:
                result.title = title_tag.get_text(strip=True)

            og_desc = soup.find("meta", property="og:description")
            if og_desc and og_desc.get("content"):
                result.description = og_desc["content"]

            for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
                tag.decompose()

            main_content = (
                soup.find("main")
                or soup.find("article")
                or soup.find("div", class_="content")
                or soup.find("body")
            )

            if main_content:
                html = str(main_content)

        content = md(
            html,
            heading_style="ATX",
            bullets="-",
            strip=["script", "style", "nav", "footer", "header", "aside"],
        )

        content = re.sub(r"\n{3,}", "\n\n", content)
        content = content.strip()

        return content

    def _bs4_extract(self, html: str, result: FetchResult) -> str:
        """Extract text using BeautifulSoup."""
        soup = BeautifulSoup(html, "html.parser")

        title_tag = soup.find("title")
        if title_tag:
            result.title = title_tag.get_text(strip=True)

        og_desc = soup.find("meta", property="og:description")
        if og_desc and og_desc.get("content"):
            result.description = og_desc["content"]

        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()

        main_content = (
            soup.find("main")
            or soup.find("article")
            or soup.find("div", class_="content")
            or soup.find("body")
        )

        if main_content:
            text = main_content.get_text(separator="\n\n", strip=True)
        else:
            text = soup.get_text(separator="\n\n", strip=True)

        text = re.sub(r"\n{3,}", "\n\n", text)

        return text

    def _basic_extract(self, html: str, result: FetchResult) -> str:
        """Basic HTML extraction without external libraries."""
        title_match = re.search(r"<title[^>]*>(.*?)</title>", html, re.IGNORECASE | re.DOTALL)
        if title_match:
            result.title = title_match.group(1).strip()

        text = re.sub(r"<script[^>]*>.*?</script>", "", html, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r"<style[^>]*>.*?</style>", "", text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r"<[^>]+>", " ", text)
        text = re.sub(r"\s+", " ", text).strip()

        return text
