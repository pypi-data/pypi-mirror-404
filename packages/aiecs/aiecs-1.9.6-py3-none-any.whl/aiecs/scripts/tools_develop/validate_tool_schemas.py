#!/usr/bin/env python3
"""
å·¥å…· Schema è´¨é‡éªŒè¯å™¨

ç”¨äºå·¥å…·å¼€å‘å’Œç»´æŠ¤ï¼ŒéªŒè¯è‡ªåŠ¨ç”Ÿæˆçš„ Schema è´¨é‡ã€‚
å¸®åŠ©å¼€å‘è€…è¯†åˆ«éœ€è¦æ”¹è¿›çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæå‡ Schema æè¿°è´¨é‡ã€‚

ä½¿ç”¨æ–¹æ³•:
    # éªŒè¯æ‰€æœ‰å·¥å…·
    aiecs tools validate-schemas

    # éªŒè¯ç‰¹å®šå·¥å…·
    aiecs tools validate-schemas pandas

    # æ˜¾ç¤ºè¯¦ç»†çš„æ”¹è¿›å»ºè®®
    aiecs tools validate-schemas pandas --verbose

    # æ˜¾ç¤ºç¤ºä¾‹ Schema
    aiecs tools validate-schemas pandas --show-examples
"""

from aiecs.tools.schema_generator import generate_schema_from_method
from aiecs.tools import discover_tools, TOOL_CLASSES
import sys
from typing import Dict, List, Any, Type, Optional, Callable
from pydantic import BaseModel

# ç¡®ä¿å¯ä»¥å¯¼å…¥ aiecs
import os

sys.path.insert(
    0,
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))),
)


class SchemaQualityMetrics:
    """Schema è´¨é‡æŒ‡æ ‡"""

    def __init__(self):
        self.total_methods = 0
        self.schemas_generated = 0
        self.schemas_failed = 0
        self.total_fields = 0
        self.fields_with_meaningful_descriptions = 0
        self.fields_with_types = 0
        self.quality_issues = []

    def add_method(self, has_schema: bool):
        """æ·»åŠ æ–¹æ³•ç»Ÿè®¡"""
        self.total_methods += 1
        if has_schema:
            self.schemas_generated += 1
        else:
            self.schemas_failed += 1

    def add_field(self, has_type: bool, has_meaningful_desc: bool):
        """æ·»åŠ å­—æ®µç»Ÿè®¡"""
        self.total_fields += 1
        if has_type:
            self.fields_with_types += 1
        if has_meaningful_desc:
            self.fields_with_meaningful_descriptions += 1

    def add_issue(self, issue: str):
        """æ·»åŠ è´¨é‡é—®é¢˜"""
        self.quality_issues.append(issue)

    def get_scores(self) -> Dict[str, float]:
        """è®¡ç®—è´¨é‡åˆ†æ•°"""
        generation_rate = (self.schemas_generated / self.total_methods * 100) if self.total_methods > 0 else 0
        description_rate = (self.fields_with_meaningful_descriptions / self.total_fields * 100) if self.total_fields > 0 else 0
        type_coverage = (self.fields_with_types / self.total_fields * 100) if self.total_fields > 0 else 0

        return {
            "generation_rate": generation_rate,
            "description_quality": description_rate,
            "type_coverage": type_coverage,
            "overall_score": (generation_rate + description_rate + type_coverage) / 3,
        }


def validate_schema_quality(schema: Type[BaseModel], method: Callable[..., Any], method_name: str) -> List[str]:
    """
    éªŒè¯å•ä¸ª Schema çš„è´¨é‡

    Returns:
        è´¨é‡é—®é¢˜åˆ—è¡¨ï¼ˆæ”¹è¿›å»ºè®®ï¼‰
    """
    issues = []

    # 1. æ£€æŸ¥ Schema æè¿°
    if not schema.__doc__ or schema.__doc__.strip() == f"Execute {method_name} operation":
        issues.append("ğŸ’¡ åœ¨æ–¹æ³•æ–‡æ¡£å­—ç¬¦ä¸²çš„ç¬¬ä¸€è¡Œæ·»åŠ æœ‰æ„ä¹‰çš„æè¿°")

    # 2. æ£€æŸ¥å­—æ®µ
    if not schema.model_fields:
        return issues

    for field_name, field_info in schema.model_fields.items():
        # æ£€æŸ¥å­—æ®µæè¿°
        description = field_info.description
        if not description or description == f"Parameter {field_name}":
            issues.append(f"ğŸ’¡ åœ¨æ–‡æ¡£å­—ç¬¦ä¸²çš„ Args éƒ¨åˆ†ä¸ºå‚æ•° '{field_name}' æ·»åŠ æè¿°")

    return issues


def find_manual_schema(tool_class: Type, method_name: str) -> Optional[Type[BaseModel]]:
    """
    æŸ¥æ‰¾æ‰‹åŠ¨å®šä¹‰çš„ Schemaï¼ˆä¸ langchain_adapter é€»è¾‘ä¸€è‡´ï¼‰

    Args:
        tool_class: å·¥å…·ç±»
        method_name: æ–¹æ³•å

    Returns:
        æ‰¾åˆ°çš„ Schema ç±»ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
    """
    schemas = {}

    # 1. æ£€æŸ¥ç±»çº§åˆ«çš„ schemas
    for attr_name in dir(tool_class):
        attr = getattr(tool_class, attr_name)
        if isinstance(attr, type) and issubclass(attr, BaseModel) and attr.__name__.endswith("Schema"):
            # æ ‡å‡†åŒ–ï¼šç§»é™¤ 'Schema' åç¼€ï¼Œè½¬å°å†™ï¼Œç§»é™¤ä¸‹åˆ’çº¿
            schema_base_name = attr.__name__.replace("Schema", "")
            normalized_name = schema_base_name.replace("_", "").lower()
            schemas[normalized_name] = attr

    # 2. æ£€æŸ¥æ¨¡å—çº§åˆ«çš„ schemas
    import inspect

    tool_module = inspect.getmodule(tool_class)
    if tool_module:
        for attr_name in dir(tool_module):
            if attr_name.startswith("_"):
                continue
            attr = getattr(tool_module, attr_name)
            if isinstance(attr, type) and issubclass(attr, BaseModel) and attr.__name__.endswith("Schema"):
                schema_base_name = attr.__name__.replace("Schema", "")
                normalized_name = schema_base_name.replace("_", "").lower()
                if normalized_name not in schemas:
                    schemas[normalized_name] = attr

    # æ ‡å‡†åŒ–æ–¹æ³•åï¼šç§»é™¤ä¸‹åˆ’çº¿å¹¶è½¬å°å†™
    normalized_method_name = method_name.replace("_", "").lower()

    # æŸ¥æ‰¾åŒ¹é…çš„ schema
    return schemas.get(normalized_method_name)


def analyze_tool_schemas(tool_name: str, tool_class: Type) -> Dict[str, Any]:
    """åˆ†æå·¥å…·çš„ Schema ç”Ÿæˆæƒ…å†µï¼ˆæ”¯æŒæ‰‹åŠ¨å®šä¹‰å’Œè‡ªåŠ¨ç”Ÿæˆï¼‰"""

    metrics = SchemaQualityMetrics()
    methods_info = []

    for method_name in dir(tool_class):
        # è·³è¿‡ç§æœ‰æ–¹æ³•å’Œç‰¹æ®Šæ–¹æ³•
        if method_name.startswith("_"):
            continue

        # è·³è¿‡åŸºç±»æ–¹æ³•
        if method_name in ["run", "run_async", "run_batch", "close", "get_schema_coverage"]:
            continue

        method = getattr(tool_class, method_name)

        # è·³è¿‡éæ–¹æ³•å±æ€§
        if not callable(method) or isinstance(method, type):
            continue

        # é¦–å…ˆå°è¯•æŸ¥æ‰¾æ‰‹åŠ¨å®šä¹‰çš„ Schema
        manual_schema = find_manual_schema(tool_class, method_name)

        schema: Optional[Type[BaseModel]]
        if manual_schema:
            schema = manual_schema
            schema_type = "manual"
        else:
            # å¦‚æœæ²¡æœ‰æ‰‹åŠ¨ Schemaï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆ
            schema = generate_schema_from_method(method, method_name)
            schema_type = "auto"

        method_info: Dict[str, Any] = {
            "name": method_name,
            "schema": schema,
            "schema_type": schema_type,
            "issues": [],
        }

        if schema:
            metrics.add_method(True)

            # éªŒè¯è´¨é‡
            issues = validate_schema_quality(schema, method, method_name)
            method_info["issues"] = issues

            # ç»Ÿè®¡å­—æ®µ
            for field_name, field_info in schema.model_fields.items():
                has_type = field_info.annotation is not None
                has_meaningful_desc = bool(field_info.description and field_info.description != f"Parameter {field_name}")
                metrics.add_field(has_type, has_meaningful_desc)

            # è®°å½•é—®é¢˜
            for issue in issues:
                metrics.add_issue(f"{tool_name}.{method_name}: {issue}")
        else:
            metrics.add_method(False)
            method_info["issues"] = ["âš ï¸  æ— æ³•ç”Ÿæˆ Schemaï¼ˆå¯èƒ½æ˜¯æ— å‚æ•°æ–¹æ³•ï¼‰"]

        methods_info.append(method_info)

    return {"metrics": metrics, "methods": methods_info}


def print_tool_report(
    tool_name: str,
    result: Dict,
    verbose: bool = False,
    show_examples: bool = False,
):
    """æ‰“å°å·¥å…·æŠ¥å‘Š"""

    metrics = result["metrics"]
    methods = result["methods"]
    scores = metrics.get_scores()

    # ç»Ÿè®¡æ‰‹åŠ¨å’Œè‡ªåŠ¨ schema
    manual_schemas = [m for m in methods if m.get("schema_type") == "manual"]
    auto_schemas = [m for m in methods if m.get("schema_type") == "auto"]

    # çŠ¶æ€å›¾æ ‡
    overall = scores["overall_score"]
    if overall >= 90:
        status = "âœ…"
        grade = "A (ä¼˜ç§€)"
    elif overall >= 80:
        status = "âš ï¸"
        grade = "B (è‰¯å¥½)"
    elif overall >= 70:
        status = "âš ï¸"
        grade = "C (ä¸­ç­‰)"
    else:
        status = "âŒ"
        grade = "D (éœ€æ”¹è¿›)"

    print(f"\n{status} {tool_name}")
    print(f"  æ–¹æ³•æ•°: {metrics.total_methods}")
    print(f"  æˆåŠŸç”Ÿæˆ Schema: {metrics.schemas_generated} ({scores['generation_rate']:.1f}%)")
    print(f"    - æ‰‹åŠ¨å®šä¹‰: {len(manual_schemas)} ä¸ª")
    print(f"    - è‡ªåŠ¨ç”Ÿæˆ: {len(auto_schemas)} ä¸ª")
    print(f"  æè¿°è´¨é‡: {scores['description_quality']:.1f}%")
    print(f"  ç»¼åˆè¯„åˆ†: {scores['overall_score']:.1f}% ({grade})")

    # æ˜¾ç¤ºéœ€è¦æ”¹è¿›çš„æ–¹æ³•
    methods_with_issues = [m for m in methods if m["issues"] and m["schema"]]

    if methods_with_issues and (verbose or scores["description_quality"] < 80):
        print(f"\n  éœ€è¦æ”¹è¿›çš„æ–¹æ³• ({len(methods_with_issues)} ä¸ª):")

        for method_info in methods_with_issues[: 5 if not verbose else None]:
            print(f"\n    {method_info['name']}:")
            for issue in method_info["issues"]:
                print(f"      {issue}")

        if not verbose and len(methods_with_issues) > 5:
            print(f"\n    ... è¿˜æœ‰ {len(methods_with_issues) - 5} ä¸ªæ–¹æ³•éœ€è¦æ”¹è¿›")
            print("    ä½¿ç”¨ --verbose æŸ¥çœ‹å…¨éƒ¨")

    # æ˜¾ç¤ºç¤ºä¾‹ Schema
    if show_examples:
        methods_with_schema = [m for m in methods if m["schema"]]
        if methods_with_schema:
            print("\n  ç¤ºä¾‹ Schema:")
            for method_info in methods_with_schema[:2]:
                schema = method_info["schema"]
                schema_type_label = "ğŸ”§ æ‰‹åŠ¨å®šä¹‰" if method_info.get("schema_type") == "manual" else "ğŸ¤– è‡ªåŠ¨ç”Ÿæˆ"
                print(f"\n    {method_info['name']} â†’ {schema.__name__} [{schema_type_label}]")
                print(f"      æè¿°: {schema.__doc__}")
                print("      å­—æ®µ:")
                for field_name, field_info in list(schema.model_fields.items())[:3]:
                    required = "å¿…éœ€" if field_info.is_required() else "å¯é€‰"
                    print(f"        - {field_name}: {field_info.description} [{required}]")


def validate_schemas(
    tool_names: Optional[List[str]] = None,
    verbose: bool = False,
    show_examples: bool = False,
    export_coverage: Optional[str] = None,
    min_coverage: float = 0.0,
) -> Dict[str, Any]:
    """
    éªŒè¯å·¥å…·çš„ Schema è´¨é‡

    Args:
        tool_names: è¦éªŒè¯çš„å·¥å…·åç§°åˆ—è¡¨ï¼ŒNone è¡¨ç¤ºéªŒè¯æ‰€æœ‰å·¥å…·
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        show_examples: æ˜¯å¦æ˜¾ç¤ºç¤ºä¾‹ Schema
        export_coverage: å¯¼å‡ºè¦†ç›–ç‡æŠ¥å‘Šçš„æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ .json, .html, .txtï¼‰
        min_coverage: æœ€å°è¦†ç›–ç‡é˜ˆå€¼ï¼ˆ0-100ï¼‰ï¼Œä½äºæ­¤å€¼çš„å·¥å…·ä¼šè¢«æ ‡è®°

    Returns:
        åŒ…å«æ‰€æœ‰å·¥å…·åˆ†æç»“æœçš„å­—å…¸
    """
    print("=" * 100)
    print("å·¥å…· Schema è´¨é‡éªŒè¯å™¨")
    print("=" * 100)

    discover_tools()

    # ç¡®å®šè¦éªŒè¯çš„å·¥å…·
    if tool_names:
        tools_to_check = {}
        for name in tool_names:
            if name in TOOL_CLASSES:
                tools_to_check[name] = TOOL_CLASSES[name]
            else:
                print(f"\nâŒ å·¥å…· '{name}' ä¸å­˜åœ¨")

        if not tools_to_check:
            print("\næ²¡æœ‰æ‰¾åˆ°è¦éªŒè¯çš„å·¥å…·")
            return {}
    else:
        tools_to_check = TOOL_CLASSES

    # éªŒè¯æ¯ä¸ªå·¥å…·
    all_results = {}
    for tool_name in sorted(tools_to_check.keys()):
        tool_class = tools_to_check[tool_name]
        result = analyze_tool_schemas(tool_name, tool_class)
        all_results[tool_name] = result

        print_tool_report(tool_name, result, verbose, show_examples)

    # æ€»ä½“ç»Ÿè®¡
    if len(all_results) > 1:
        total_methods = sum(r["metrics"].total_methods for r in all_results.values())
        total_generated = sum(r["metrics"].schemas_generated for r in all_results.values())
        total_fields = sum(r["metrics"].total_fields for r in all_results.values())
        total_meaningful = sum(r["metrics"].fields_with_meaningful_descriptions for r in all_results.values())
        total_with_types = sum(r["metrics"].fields_with_types for r in all_results.values())

        overall_generation = (total_generated / total_methods * 100) if total_methods > 0 else 0
        overall_description = (total_meaningful / total_fields * 100) if total_fields > 0 else 0
        overall_type_coverage = (total_with_types / total_fields * 100) if total_fields > 0 else 0
        overall_score = (overall_generation + overall_description + overall_type_coverage) / 3

        print("\n" + "=" * 100)
        print("æ€»ä½“ç»Ÿè®¡:")
        print(f"  å·¥å…·æ•°: {len(all_results)}")
        print(f"  æ–¹æ³•æ•°: {total_methods}")
        print(f"  Schema ç”Ÿæˆç‡: {total_generated}/{total_methods} ({overall_generation:.1f}%)")
        print(f"  æè¿°è´¨é‡: {overall_description:.1f}%")
        print(f"  ç±»å‹è¦†ç›–ç‡: {overall_type_coverage:.1f}%")
        print(f"  ç»¼åˆè¯„åˆ†: {overall_score:.1f}%")
        print("=" * 100)
        
        # Coverage summary by tool
        print("\nè¦†ç›–ç‡æ‘˜è¦:")
        tools_by_coverage = []
        for tool_name, result in all_results.items():
            metrics = result["metrics"]
            scores = metrics.get_scores()
            coverage = scores["generation_rate"]
            tools_by_coverage.append((tool_name, coverage, scores))
        
        # Sort by coverage (lowest first)
        tools_by_coverage.sort(key=lambda x: x[1])
        
        # Show tools below 90%
        low_coverage_tools = [t for t in tools_by_coverage if t[1] < 90]
        if low_coverage_tools:
            print(f"\n  éœ€è¦æ”¹è¿›çš„å·¥å…· ({len(low_coverage_tools)} ä¸ªï¼Œè¦†ç›–ç‡ < 90%):")
            for tool_name, coverage, scores in low_coverage_tools[:10]:
                print(f"    - {tool_name}: {coverage:.1f}% (ç”Ÿæˆç‡: {scores['generation_rate']:.1f}%, "
                      f"æè¿°: {scores['description_quality']:.1f}%, ç±»å‹: {scores['type_coverage']:.1f}%)")
            if len(low_coverage_tools) > 10:
                print(f"    ... è¿˜æœ‰ {len(low_coverage_tools) - 10} ä¸ªå·¥å…·éœ€è¦æ”¹è¿›")
        
        # Show tools at 90%+
        high_coverage_tools = [t for t in tools_by_coverage if t[1] >= 90]
        if high_coverage_tools:
            print(f"\n  âœ… è¾¾æ ‡å·¥å…· ({len(high_coverage_tools)} ä¸ªï¼Œè¦†ç›–ç‡ â‰¥ 90%):")
            for tool_name, coverage, scores in high_coverage_tools[:5]:
                print(f"    - {tool_name}: {coverage:.1f}%")
            if len(high_coverage_tools) > 5:
                print(f"    ... è¿˜æœ‰ {len(high_coverage_tools) - 5} ä¸ªå·¥å…·å·²è¾¾æ ‡")

    print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    print("  1. åœ¨æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ç¬¬ä¸€è¡Œæ·»åŠ ç®€çŸ­æè¿°")
    print("  2. åœ¨ Args éƒ¨åˆ†ä¸ºæ¯ä¸ªå‚æ•°æ·»åŠ è¯¦ç»†æè¿°")
    print("  3. ä½¿ç”¨ Google æˆ– NumPy é£æ ¼çš„æ–‡æ¡£å­—ç¬¦ä¸²")
    print("\nç¤ºä¾‹:")
    print("  def filter(self, records: List[Dict], condition: str) -> List[Dict]:")
    print('      """')
    print("      Filter DataFrame based on a condition.")
    print("      ")
    print("      Args:")
    print("          records: List of records to filter")
    print("          condition: Filter condition (pandas query syntax)")
    print('      """')
    
    # Export coverage report if requested
    if export_coverage:
        from aiecs.scripts.tools_develop.schema_coverage import generate_coverage_report
        report_format = "json" if export_coverage.endswith(".json") else "html" if export_coverage.endswith(".html") else "text"
        generate_coverage_report(
            tool_names=tool_names,
            format=report_format,
            output=export_coverage,
            min_coverage=min_coverage,
        )
    
    return all_results


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(
        description="éªŒè¯å·¥å…· Schema çš„ç”Ÿæˆè´¨é‡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # éªŒè¯æ‰€æœ‰å·¥å…·
  aiecs tools validate-schemas

  # éªŒè¯ç‰¹å®šå·¥å…·
  aiecs tools validate-schemas pandas

  # æ˜¾ç¤ºè¯¦ç»†çš„æ”¹è¿›å»ºè®®
  aiecs tools validate-schemas pandas --verbose

  # æ˜¾ç¤ºç¤ºä¾‹ Schema
  aiecs tools validate-schemas pandas --show-examples
        """,
    )

    parser.add_argument("tools", nargs="*", help="è¦éªŒè¯çš„å·¥å…·åç§°ï¼ˆä¸æŒ‡å®šåˆ™éªŒè¯æ‰€æœ‰å·¥å…·ï¼‰")

    parser.add_argument("-v", "--verbose", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†çš„æ”¹è¿›å»ºè®®")

    parser.add_argument("-e", "--show-examples", action="store_true", help="æ˜¾ç¤ºç¤ºä¾‹ Schema")
    
    parser.add_argument(
        "--export-coverage",
        type=str,
        help="å¯¼å‡ºè¦†ç›–ç‡æŠ¥å‘Šåˆ°æ–‡ä»¶ï¼ˆæ”¯æŒ .json, .html, .txt æ ¼å¼ï¼‰"
    )
    
    parser.add_argument(
        "--min-coverage",
        type=float,
        default=0.0,
        help="æœ€å°è¦†ç›–ç‡é˜ˆå€¼ï¼ˆ0-100ï¼‰ï¼Œç”¨äºå¯¼å‡ºæŠ¥å‘Šæ—¶è¿‡æ»¤å·¥å…·"
    )

    args = parser.parse_args()

    tool_names = args.tools if args.tools else None
    validate_schemas(
        tool_names,
        args.verbose,
        args.show_examples,
        args.export_coverage,
        args.min_coverage,
    )


if __name__ == "__main__":
    main()
