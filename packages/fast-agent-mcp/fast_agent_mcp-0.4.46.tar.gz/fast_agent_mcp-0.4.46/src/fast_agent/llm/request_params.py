"""
Request parameters definitions for LLM interactions.
"""

from typing import TYPE_CHECKING, Any

from mcp import SamplingMessage
from mcp.types import CreateMessageRequestParams
from pydantic import Field

from fast_agent.constants import DEFAULT_MAX_ITERATIONS, DEFAULT_STREAMING_TIMEOUT

if TYPE_CHECKING:
    from fast_agent.mcp.tool_execution_handler import ToolExecutionHandler
else:
    ToolExecutionHandler = Any


class RequestParams(CreateMessageRequestParams):
    """
    Parameters to configure the FastAgentLLM 'generate' requests.
    """

    messages: list[SamplingMessage] = Field(exclude=True, default=[])
    """
    Ignored. 'messages' are removed from CreateMessageRequestParams 
    to avoid confusion with the 'message' parameter on 'generate' method.
    """

    maxTokens: int = 2048
    """The maximum number of tokens to sample, as requested by the server."""

    model: str | None = None
    """
    The model to use for the LLM generation. This can only be set during Agent creation.
    If specified, this overrides the 'modelPreferences' selection criteria.
    """

    use_history: bool = True
    """
    Agent/LLM maintains conversation history. Does not include applied Prompts
    """

    max_iterations: int = DEFAULT_MAX_ITERATIONS
    """
    The maximum number of tool calls allowed in a conversation turn
    """

    parallel_tool_calls: bool = True
    """
    Whether to allow simultaneous tool calls
    """
    response_format: Any | None = None
    """
    Override response format for structured calls. Prefer sending pydantic model - only use in exceptional circumstances
    """

    template_vars: dict[str, Any] = Field(default_factory=dict)
    """
    Optional dictionary of template variables for dynamic templates. Currently only works for TensorZero inference backend
    """

    mcp_metadata: dict[str, Any] | None = None
    """
    Metadata to pass through to MCP tool calls via the _meta field.
    """

    tool_execution_handler: ToolExecutionHandler | None = Field(default=None, repr=False)
    """
    Internal per-request tool execution handler (not sent to LLM providers).
    """

    emit_loop_progress: bool = False
    """
    Emit monotonic progress updates for the internal tool loop when supported.
    """

    streaming_timeout: float | None = DEFAULT_STREAMING_TIMEOUT
    """
    Maximum time in seconds to wait for streaming completion. Set to None to disable.
    """
