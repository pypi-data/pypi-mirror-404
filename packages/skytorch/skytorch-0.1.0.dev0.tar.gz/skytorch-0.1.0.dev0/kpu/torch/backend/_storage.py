"""
KPU Storage Manager - Tracks remote storage allocations.

This module manages storage allocations for KPU tensors, tracking storage IDs
and their metadata. Storage IDs are used as proxy data pointers in the
allocator, avoiding actual memory allocation on the client side.
"""

from __future__ import annotations

import weakref
from collections import defaultdict
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Optional
from weakref import WeakValueDictionary

import torch

from kpu.torch.client.tensor import get_storage_id, get_tensor_id

if TYPE_CHECKING:
    from kpu.client.compute import Compute


@dataclass
class StorageInfo:
    """Information about a storage allocation."""

    nbytes: int
    device_index: int
    _compute_ref: weakref.ref[Compute] = field(repr=False)

    @property
    def compute(self) -> Optional[Compute]:
        """Get the associated Compute, or None if garbage collected."""
        return self._compute_ref()


class StorageManager:
    """
    Manages storage allocations for KPU tensors.

    Storage IDs are simple integers that serve as references to remote storage.
    The data pointer in PyTorch tensors is set to the storage ID cast to void*,
    allowing efficient tracking without actual memory allocation.

    IMPORTANT: Storage registration is lazy - storage IDs are generated by the
    C++ allocator using an atomic counter, but registration with StorageManager
    (including Compute association) is deferred until first tensor use. This
    keeps the allocator GIL-free for PyTorch 2.10 compatibility.
    """

    def __init__(self):
        self._storages: dict[int, StorageInfo] = {}
        self._tensor_to_storage: WeakValueDictionary[
            int, torch.UntypedStorage
        ] = WeakValueDictionary()
        self._storage_to_tensors: dict[int, set[int]] = defaultdict(set)

    def register_storage(
        self,
        storage_id: int,
        nbytes: int,
        device_index: int,
    ) -> None:
        """
        Register storage lazily if not already registered.

        This is called at first tensor use to register storage that was
        allocated by the C++ allocator. The allocator generates storage IDs
        without calling Python (GIL-free), so registration is deferred here.

        Args:
            storage_id: Storage ID from the C++ allocator
            nbytes: Size of the storage in bytes
            device_index: Device index for the storage
        """
        if storage_id in self._storages:
            return  # Already registered

        from kpu.torch.backend._device import device_manager

        compute = device_manager.get_compute(device_index)
        if compute is None:
            raise RuntimeError(
                f"No Compute registered for device index {device_index}. "
                "Ensure you have called compute.device() to register the device."
            )

        info = StorageInfo(nbytes, device_index, _compute_ref=weakref.ref(compute))
        self._storages[storage_id] = info

    def free_storage(self, storage_id: int) -> None:
        """
        Free a storage allocation.

        Handles both registered and unregistered storage IDs.
        If the storage was never used (lazy allocation), this is a no-op.

        Args:
            storage_id: ID of the storage to free
        """
        # Handle both registered and unregistered storage IDs
        # If not registered, the tensor was never used (lazy allocation)
        if storage_id in self._storages:
            del self._storages[storage_id]

    def resize_storage(self, storage_id: int, new_nbytes: int) -> None:
        """
        Resize a storage allocation.

        Args:
            storage_id: ID of the storage to resize
            new_nbytes: New size in bytes
        """
        if storage_id in self._storages:
            self._storages[storage_id].nbytes = new_nbytes

    def get_storage(self, storage_id: int) -> Optional[StorageInfo]:
        """
        Get storage info by ID.

        Args:
            storage_id: ID of the storage

        Returns:
            StorageInfo or None if not found
        """
        return self._storages.get(storage_id)

    def register_tensor(self, tensor: torch.Tensor) -> int:
        """
        Register a tensor with its associated storage.

        Args:
            tensor: The KPU tensor to register

        Returns:
            The tensor ID
        """
        tensor_id = get_tensor_id(tensor)
        storage_id = get_storage_id(tensor)
        self._tensor_to_storage[tensor_id] = tensor.untyped_storage()
        self._storage_to_tensors[storage_id].add(tensor_id)
        return tensor_id

    def tensor_ref(self, tensor: torch.Tensor) -> Optional[int]:
        """
        Get a registered tensor ID for this tensor or its storage.

        Args:
            tensor: The tensor to look up

        Returns:
            The tensor's ID if registered, otherwise the first registered
            tensor ID sharing the same storage, otherwise None
        """
        tensor_id = get_tensor_id(tensor)
        if tensor_id in self._tensor_to_storage:
            return tensor_id

        storage_id = get_storage_id(tensor)
        tensor_ids = self._storage_to_tensors.get(storage_id)
        if tensor_ids:
            return next(iter(tensor_ids))

        return None


# Global storage manager singleton
storage_manager = StorageManager()
