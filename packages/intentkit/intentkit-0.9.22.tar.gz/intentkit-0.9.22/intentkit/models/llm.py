import csv
import json
import logging
from datetime import UTC, datetime
from decimal import ROUND_HALF_UP, Decimal
from enum import Enum
from pathlib import Path
from typing import Annotated, Any

from langchain.chat_models.base import BaseChatModel
from pydantic import BaseModel, ConfigDict, Field
from sqlalchemy import Boolean, DateTime, Integer, Numeric, String, func, select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Mapped, mapped_column

from intentkit.config.base import Base
from intentkit.config.config import config
from intentkit.config.db import get_session
from intentkit.config.redis import get_redis
from intentkit.models.app_setting import AppSetting
from intentkit.utils.error import IntentKitAPIError

logger = logging.getLogger(__name__)

_credit_per_usdc = None
FOURPLACES = Decimal("0.0001")


def _parse_bool(value: str | None) -> bool:
    if value is None:
        return False
    return value.strip().lower() in {"true", "1", "yes"}


def _parse_optional_int(value: str | None) -> int | None:
    if value is None:
        return None
    value = value.strip()
    return int(value) if value else None


def _load_default_llm_models() -> dict[str, "LLMModelInfo"]:
    """Load default LLM models from a CSV file."""

    path = Path(__file__).with_name("llm.csv")
    if not path.exists():
        logger.warning("Default LLM CSV not found at %s", path)
        return {}

    defaults: dict[str, LLMModelInfo] = {}
    with path.open(newline="", encoding="utf-8") as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            try:
                timestamp = datetime.now(UTC)
                model = LLMModelInfo(
                    id=row["id"],
                    name=row["name"],
                    provider=LLMProvider(row["provider"]),
                    enabled=_parse_bool(row.get("enabled")),
                    input_price=Decimal(row["input_price"]),
                    output_price=Decimal(row["output_price"]),
                    price_level=_parse_optional_int(row.get("price_level")),
                    context_length=int(row["context_length"]),
                    output_length=int(row["output_length"]),
                    intelligence=int(row["intelligence"]),
                    speed=int(row["speed"]),
                    supports_image_input=_parse_bool(row.get("supports_image_input")),
                    supports_skill_calls=_parse_bool(row.get("supports_skill_calls")),
                    supports_structured_output=_parse_bool(
                        row.get("supports_structured_output")
                    ),
                    has_reasoning=_parse_bool(row.get("has_reasoning")),
                    supports_search=_parse_bool(row.get("supports_search")),
                    supports_temperature=_parse_bool(row.get("supports_temperature")),
                    supports_frequency_penalty=_parse_bool(
                        row.get("supports_frequency_penalty")
                    ),
                    supports_presence_penalty=_parse_bool(
                        row.get("supports_presence_penalty")
                    ),
                    api_base=row.get("api_base", "").strip() or None,
                    timeout=int(row.get("timeout", "") or 180),
                    created_at=timestamp,
                    updated_at=timestamp,
                )
                if not model.enabled:
                    continue

                # Check if provider is configured
                is_configured = True
                if model.provider == LLMProvider.OPENAI:
                    is_configured = bool(config.openai_api_key)
                elif model.provider == LLMProvider.GOOGLE:
                    is_configured = bool(config.google_api_key)
                elif model.provider == LLMProvider.DEEPSEEK:
                    is_configured = bool(config.deepseek_api_key)
                elif model.provider == LLMProvider.XAI:
                    is_configured = bool(config.xai_api_key)
                elif model.provider == LLMProvider.OPENROUTER:
                    is_configured = bool(config.openrouter_api_key)
                elif model.provider == LLMProvider.ETERNAL:
                    is_configured = bool(config.eternal_api_key)
                elif model.provider == LLMProvider.REIGENT:
                    is_configured = bool(config.reigent_api_key)
                elif model.provider == LLMProvider.VENICE:
                    is_configured = bool(config.venice_api_key)

                if not is_configured:
                    continue
            except Exception as exc:
                logger.error(
                    "Failed to load default LLM model %s: %s", row.get("id"), exc
                )
                continue
            defaults[model.id] = model

    return defaults


class LLMProvider(str, Enum):
    OPENAI = "openai"
    GOOGLE = "google"
    DEEPSEEK = "deepseek"
    XAI = "xai"
    OPENROUTER = "openrouter"
    ETERNAL = "eternal"
    REIGENT = "reigent"
    VENICE = "venice"
    OLLAMA = "ollama"

    def display_name(self) -> str:
        """Return user-friendly display name for the provider."""
        display_names = {
            self.OPENAI: "OpenAI",
            self.GOOGLE: "Google",
            self.DEEPSEEK: "DeepSeek",
            self.XAI: "xAI",
            self.OPENROUTER: "OpenRouter",
            self.ETERNAL: "Eternal",
            self.REIGENT: "Reigent",
            self.VENICE: "Venice",
            self.OLLAMA: "Ollama",
        }
        return display_names.get(self, self.value)


class LLMModelInfoTable(Base):
    """Database table model for LLM model information."""

    __tablename__ = "llm_models"

    id: Mapped[str] = mapped_column(String, primary_key=True)
    name: Mapped[str] = mapped_column(String, nullable=False)
    provider: Mapped[str] = mapped_column(String, nullable=False)  # Stored as enum
    enabled: Mapped[bool] = mapped_column(Boolean, nullable=False, default=True)
    input_price: Mapped[Decimal] = mapped_column(
        Numeric(22, 4), nullable=False
    )  # Price per 1M input tokens in USD
    output_price: Mapped[Decimal] = mapped_column(
        Numeric(22, 4), nullable=False
    )  # Price per 1M output tokens in USD
    price_level: Mapped[int | None] = mapped_column(
        Integer, nullable=True
    )  # Price level rating
    context_length: Mapped[int] = mapped_column(
        Integer, nullable=False
    )  # Context length
    output_length: Mapped[int] = mapped_column(Integer, nullable=False)  # Output length
    intelligence: Mapped[int] = mapped_column(
        Integer, nullable=False
    )  # Intelligence rating
    speed: Mapped[int] = mapped_column(Integer, nullable=False)  # Speed rating
    supports_image_input: Mapped[bool] = mapped_column(
        Boolean, nullable=False, default=False
    )
    supports_skill_calls: Mapped[bool] = mapped_column(
        Boolean, nullable=False, default=False
    )
    supports_structured_output: Mapped[bool] = mapped_column(
        Boolean, nullable=False, default=False
    )
    has_reasoning: Mapped[bool] = mapped_column(Boolean, nullable=False, default=False)
    supports_search: Mapped[bool] = mapped_column(
        Boolean, nullable=False, default=False
    )
    supports_temperature: Mapped[bool] = mapped_column(
        Boolean, nullable=False, default=True
    )
    supports_frequency_penalty: Mapped[bool] = mapped_column(
        Boolean, nullable=False, default=True
    )
    supports_presence_penalty: Mapped[bool] = mapped_column(
        Boolean, nullable=False, default=True
    )
    api_base: Mapped[str | None] = mapped_column(String, nullable=True)
    timeout: Mapped[int] = mapped_column(
        Integer, nullable=False, default=180
    )  # Timeout seconds
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        nullable=False,
        server_default=func.now(),
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        nullable=False,
        server_default=func.now(),
        onupdate=lambda: datetime.now(UTC),
    )


class LLMModelInfo(BaseModel):
    """Information about an LLM model."""

    model_config = ConfigDict(
        from_attributes=True,
        use_enum_values=True,
        json_encoders={datetime: lambda v: v.isoformat(timespec="milliseconds")},
    )

    id: str
    name: str
    provider: LLMProvider
    enabled: bool = Field(default=True)
    input_price: Decimal  # Price per 1M input tokens in USD
    output_price: Decimal  # Price per 1M output tokens in USD
    price_level: int | None = Field(
        default=None, ge=1, le=5
    )  # Price level rating from 1-5
    context_length: int  # Maximum context length in tokens
    output_length: int  # Maximum output length in tokens
    intelligence: int = Field(ge=1, le=5)  # Intelligence rating from 1-5
    speed: int = Field(ge=1, le=5)  # Speed rating from 1-5
    supports_image_input: bool = False  # Whether the model supports image inputs
    supports_skill_calls: bool = False  # Whether the model supports skill/tool calls
    supports_structured_output: bool = (
        False  # Whether the model supports structured output
    )
    has_reasoning: bool = False  # Whether the model has strong reasoning capabilities
    supports_search: bool = (
        False  # Whether the model supports native search functionality
    )
    supports_temperature: bool = (
        True  # Whether the model supports temperature parameter
    )
    supports_frequency_penalty: bool = (
        True  # Whether the model supports frequency_penalty parameter
    )
    supports_presence_penalty: bool = (
        True  # Whether the model supports presence_penalty parameter
    )
    api_base: str | None = None  # Custom API base URL if not using provider's default
    timeout: int = 180  # Default timeout in seconds
    created_at: Annotated[
        datetime,
        Field(
            description="Timestamp when this data was created",
            default=datetime.now(UTC),
        ),
    ]
    updated_at: Annotated[
        datetime,
        Field(
            description="Timestamp when this data was updated",
            default=datetime.now(UTC),
        ),
    ]

    @staticmethod
    async def get(model_id: str) -> "LLMModelInfo":
        """Get a model by ID with Redis caching.

        The model info is cached in Redis for 3 minutes.

        Args:
            model_id: ID of the model to retrieve

        Returns:
            LLMModelInfo: The model info if found, None otherwise
        """
        # Redis cache key for model info
        cache_key = f"intentkit:llm_model:{model_id}"
        cache_ttl = 180  # 3 minutes in seconds

        # Try to get from Redis cache first
        redis = get_redis()
        cached_data = await redis.get(cache_key)

        if cached_data:
            # If found in cache, deserialize and return
            try:
                return LLMModelInfo.model_validate_json(cached_data)
            except (json.JSONDecodeError, TypeError):
                # If cache is corrupted, invalidate it
                await redis.delete(cache_key)

        # If not in cache or cache is invalid, get from database
        async with get_session() as session:
            # Query the database for the model
            stmt = select(LLMModelInfoTable).where(LLMModelInfoTable.id == model_id)
            model = await session.scalar(stmt)

            # If model exists in database, convert to LLMModelInfo model and cache it
            if model:
                # Convert provider string to enum
                model_info = LLMModelInfo.model_validate(model)

                # Cache the model in Redis
                await redis.set(
                    cache_key,
                    model_info.model_dump_json(),
                    ex=cache_ttl,
                )

                return model_info

        # If not found in database, check AVAILABLE_MODELS
        if model_id in AVAILABLE_MODELS:
            model_info = AVAILABLE_MODELS[model_id]

            # Cache the model in Redis
            await redis.set(cache_key, model_info.model_dump_json(), ex=cache_ttl)

            return model_info

        # Not found anywhere
        raise IntentKitAPIError(
            400,
            "ModelNotFound",
            f"Model {model_id} not found, maybe deprecated, please change it in the agent configuration.",
        )

    @classmethod
    async def get_all(cls, session: AsyncSession | None = None) -> list["LLMModelInfo"]:
        """Return all models merged from defaults and database overrides."""

        if session is None:
            async with get_session() as db:
                return await cls.get_all(session=db)

        models: dict[str, LLMModelInfo] = {
            model_id: model.model_copy(deep=True)
            for model_id, model in AVAILABLE_MODELS.items()
        }

        result = await session.execute(select(LLMModelInfoTable))
        for row in result.scalars():
            model_info = cls.model_validate(row)
            models[model_info.id] = model_info

        return list(models.values())

    async def calculate_cost(self, input_tokens: int, output_tokens: int) -> Decimal:
        global _credit_per_usdc
        if not _credit_per_usdc:
            _credit_per_usdc = (await AppSetting.payment()).credit_per_usdc
        """Calculate the cost for a given number of tokens."""
        input_cost = (
            _credit_per_usdc
            * Decimal(input_tokens)
            * self.input_price
            / Decimal(1000000)
        ).quantize(FOURPLACES, rounding=ROUND_HALF_UP)
        output_cost = (
            _credit_per_usdc
            * Decimal(output_tokens)
            * self.output_price
            / Decimal(1000000)
        ).quantize(FOURPLACES, rounding=ROUND_HALF_UP)
        return (input_cost + output_cost).quantize(FOURPLACES, rounding=ROUND_HALF_UP)


# Default models loaded from CSV
AVAILABLE_MODELS = _load_default_llm_models()


class LLMModel(BaseModel):
    """Base model for LLM configuration."""

    model_name: str
    temperature: float = 0.7
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    info: LLMModelInfo

    async def model_info(self) -> LLMModelInfo:
        """Get the model information with caching.

        First tries to get from cache, then database, then default models loaded from CSV.
        Raises ValueError if model is not found anywhere.
        """
        model_info = await LLMModelInfo.get(self.model_name)
        return model_info

    # This will be implemented by subclasses to return the appropriate LLM instance
    async def create_instance(self, params: dict[str, Any] = {}) -> BaseChatModel:
        """Create and return the LLM instance based on the configuration."""
        raise NotImplementedError("Subclasses must implement create_instance")

    async def get_token_limit(self) -> int:
        """Get the token limit for this model."""
        info = await self.model_info()
        return info.context_length

    async def calculate_cost(self, input_tokens: int, output_tokens: int) -> Decimal:
        """Calculate the cost for a given number of tokens."""
        info = await self.model_info()
        return await info.calculate_cost(input_tokens, output_tokens)


class OpenAILLM(LLMModel):
    """OpenAI LLM configuration."""

    async def create_instance(self, params: dict[str, Any] = {}) -> BaseChatModel:
        """Create and return a ChatOpenAI instance."""
        from langchain_openai import ChatOpenAI

        info = await self.model_info()

        kwargs = {
            "model_name": self.model_name,
            "openai_api_key": config.openai_api_key,
            "timeout": info.timeout,
            "use_responses_api": True,
        }

        # Add optional parameters based on model support
        if info.supports_temperature:
            kwargs["temperature"] = self.temperature

        if info.supports_frequency_penalty:
            kwargs["frequency_penalty"] = self.frequency_penalty

        if info.supports_presence_penalty:
            kwargs["presence_penalty"] = self.presence_penalty

        if info.api_base:
            kwargs["openai_api_base"] = info.api_base

        if self.model_name == "gpt-5-mini" or self.model_name == "gpt-5-nano":
            kwargs["reasoning_effort"] = "minimal"
        elif self.model_name == "gpt-5.1-codex":
            kwargs["reasoning_effort"] = "high"

        # Update kwargs with params to allow overriding
        kwargs.update(params)

        logger.debug(f"Creating ChatOpenAI instance with kwargs: {kwargs}")

        return ChatOpenAI(**kwargs)


class DeepseekLLM(LLMModel):
    """Deepseek LLM configuration."""

    async def create_instance(self, params: dict[str, Any] = {}) -> BaseChatModel:
        """Create and return a ChatDeepseek instance."""

        from langchain_deepseek import ChatDeepSeek

        info = await self.model_info()

        kwargs = {
            "model": self.model_name,
            "api_key": config.deepseek_api_key,
            "timeout": info.timeout,
            "max_retries": 3,
        }

        # Add optional parameters based on model support
        if info.supports_temperature:
            kwargs["temperature"] = self.temperature

        if info.supports_frequency_penalty:
            kwargs["frequency_penalty"] = self.frequency_penalty

        if info.supports_presence_penalty:
            kwargs["presence_penalty"] = self.presence_penalty

        if info.api_base:
            kwargs["api_base"] = info.api_base

        # Update kwargs with params to allow overriding
        kwargs.update(params)

        return ChatDeepSeek(**kwargs)


class XAILLM(LLMModel):
    """XAI (Grok) LLM configuration."""

    async def create_instance(self, params: dict[str, Any] = {}) -> BaseChatModel:
        """Create and return a ChatXAI instance."""

        from langchain_xai import ChatXAI

        info = await self.model_info()

        kwargs = {
            "model_name": self.model_name,
            "xai_api_key": config.xai_api_key,
            "timeout": info.timeout,
        }

        # Add optional parameters based on model support
        if info.supports_temperature:
            kwargs["temperature"] = self.temperature

        if info.supports_frequency_penalty:
            kwargs["frequency_penalty"] = self.frequency_penalty

        if info.supports_presence_penalty:
            kwargs["presence_penalty"] = self.presence_penalty

        # Update kwargs with params to allow overriding
        kwargs.update(params)

        return ChatXAI(**kwargs)


class OpenRouterLLM(LLMModel):
    """OpenRouter LLM configuration."""

    async def create_instance(self, params: dict[str, Any] = {}) -> BaseChatModel:
        """Create and return a ChatOpenAI instance configured for OpenRouter."""
        from langchain_openai import ChatOpenAI

        info = await self.model_info()

        kwargs = {
            "model": self.model_name,
            "api_key": config.openrouter_api_key,
            "base_url": "https://openrouter.ai/api/v1",
            "timeout": info.timeout,
            "max_completion_tokens": 999,
        }

        # Add optional parameters based on model support
        if info.supports_temperature:
            kwargs["temperature"] = self.temperature

        if info.supports_frequency_penalty:
            kwargs["frequency_penalty"] = self.frequency_penalty

        if info.supports_presence_penalty:
            kwargs["presence_penalty"] = self.presence_penalty

        # Update kwargs with params to allow overriding
        kwargs.update(params)

        return ChatOpenAI(**kwargs)


class EternalLLM(LLMModel):
    """Eternal AI LLM configuration."""

    async def create_instance(self, params: dict[str, Any] = {}) -> BaseChatModel:
        """Create and return a ChatOpenAI instance configured for Eternal AI."""
        from langchain_openai import ChatOpenAI

        info = await self.model_info()

        # Override model name for Eternal AI
        actual_model = "unsloth/Llama-3.3-70B-Instruct-bnb-4bit"

        kwargs = {
            "model_name": actual_model,
            "openai_api_key": config.eternal_api_key,
            "openai_api_base": info.api_base,
            "timeout": info.timeout,
        }

        # Add optional parameters based on model support
        if info.supports_temperature:
            kwargs["temperature"] = self.temperature

        if info.supports_frequency_penalty:
            kwargs["frequency_penalty"] = self.frequency_penalty

        if info.supports_presence_penalty:
            kwargs["presence_penalty"] = self.presence_penalty

        # Update kwargs with params to allow overriding
        kwargs.update(params)

        return ChatOpenAI(**kwargs)


class ReigentLLM(LLMModel):
    """Reigent LLM configuration."""

    async def create_instance(self, params: dict[str, Any] = {}) -> BaseChatModel:
        """Create and return a ChatOpenAI instance configured for Reigent."""
        from langchain_openai import ChatOpenAI

        info = await self.model_info()

        kwargs = {
            "openai_api_key": config.reigent_api_key,
            "openai_api_base": info.api_base,
            "timeout": info.timeout,
            "model_kwargs": {
                # Override any specific parameters required for Reigent API
                # The Reigent API requires 'tools' instead of 'functions' and might have some specific formatting requirements
            },
        }

        # Update kwargs with params to allow overriding
        kwargs.update(params)

        return ChatOpenAI(**kwargs)


class VeniceLLM(LLMModel):
    """Venice LLM configuration."""

    async def create_instance(self, params: dict[str, Any] = {}) -> BaseChatModel:
        """Create and return a ChatOpenAI instance configured for Venice."""
        from langchain_openai import ChatOpenAI

        info = await self.model_info()

        kwargs = {
            "openai_api_key": config.venice_api_key,
            "openai_api_base": info.api_base,
            "timeout": info.timeout,
        }

        # Update kwargs with params to allow overriding
        kwargs.update(params)

        return ChatOpenAI(**kwargs)


class GoogleLLM(LLMModel):
    """Google LLM configuration."""

    async def create_instance(self, params: dict[str, Any] = {}) -> BaseChatModel:
        """Create and return a ChatGoogleGenerativeAI instance."""
        from langchain_google_genai import ChatGoogleGenerativeAI

        info = await self.model_info()

        kwargs = {
            "model": self.model_name,
            "google_api_key": config.google_api_key,
            "timeout": info.timeout,
        }

        # Add optional parameters based on model support
        if info.supports_temperature:
            kwargs["temperature"] = self.temperature

        # Update kwargs with params to allow overriding
        kwargs.update(params)

        return ChatGoogleGenerativeAI(**kwargs)


# Factory function to create the appropriate LLM model based on the model name
class OllamaLLM(LLMModel):
    """Ollama LLM configuration."""

    async def create_instance(self, params: dict[str, Any] = {}) -> BaseChatModel:
        """Create and return a ChatOllama instance."""
        from langchain_ollama import ChatOllama

        info = await self.model_info()

        kwargs = {
            "model": self.model_name,
            "base_url": info.api_base or "http://localhost:11434",
            "temperature": self.temperature,
            # Ollama specific parameters
            "keep_alive": -1,  # Keep the model loaded indefinitely
        }

        if info.supports_frequency_penalty:
            kwargs["frequency_penalty"] = self.frequency_penalty

        if info.supports_presence_penalty:
            kwargs["presence_penalty"] = self.presence_penalty

        # Update kwargs with params to allow overriding
        kwargs.update(params)

        return ChatOllama(**kwargs)


# Factory function to create the appropriate LLM model based on the model name
async def create_llm_model(
    model_name: str,
    temperature: float = 0.7,
    frequency_penalty: float = 0.0,
    presence_penalty: float = 0.0,
) -> LLMModel:
    """
    Create an LLM model instance based on the model name.

    Args:
        model_name: The name of the model to use
        temperature: The temperature parameter for the model
        frequency_penalty: The frequency penalty parameter for the model
        presence_penalty: The presence penalty parameter for the model

    Returns:
        An instance of a subclass of LLMModel
    """
    info = await LLMModelInfo.get(model_name)

    base_params = {
        "model_name": model_name,
        "temperature": temperature,
        "frequency_penalty": frequency_penalty,
        "presence_penalty": presence_penalty,
        "info": info,
    }

    provider = info.provider

    if provider == LLMProvider.GOOGLE:
        return GoogleLLM(**base_params)
    elif provider == LLMProvider.DEEPSEEK:
        return DeepseekLLM(**base_params)
    elif provider == LLMProvider.XAI:
        return XAILLM(**base_params)
    elif provider == LLMProvider.ETERNAL:
        return EternalLLM(**base_params)
    elif provider == LLMProvider.REIGENT:
        return ReigentLLM(**base_params)
    elif provider == LLMProvider.VENICE:
        return VeniceLLM(**base_params)
    elif provider == LLMProvider.OPENROUTER:
        return OpenRouterLLM(**base_params)
    elif provider == LLMProvider.OLLAMA:
        return OllamaLLM(**base_params)
    else:
        # Default to OpenAI
        return OpenAILLM(**base_params)
