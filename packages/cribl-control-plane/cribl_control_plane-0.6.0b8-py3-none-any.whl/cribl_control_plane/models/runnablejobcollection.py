"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .collector import Collector, CollectorTypedDict
from .jobtypeoptionsrunnablejobcollection import JobTypeOptionsRunnableJobCollection
from .logleveloptionsrunnablejobcollectionschedulerun import (
    LogLevelOptionsRunnableJobCollectionScheduleRun,
)
from .metricsstore import MetricsStore, MetricsStoreTypedDict
from .scheduletyperunnablejobcollection import (
    ScheduleTypeRunnableJobCollection,
    ScheduleTypeRunnableJobCollectionTypedDict,
)
from .typecollectionwithbreakerrulesetsconstraint import (
    TypeCollectionWithBreakerRulesetsConstraint,
    TypeCollectionWithBreakerRulesetsConstraintTypedDict,
)
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
from enum import Enum
import pydantic
from pydantic import field_serializer, model_serializer
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class RunnableJobCollectionMode(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job."""

    LIST = "list"
    PREVIEW = "preview"
    RUN = "run"


class TimeRange(str, Enum, metaclass=utils.OpenEnumMeta):
    ABSOLUTE = "absolute"
    RELATIVE = "relative"


class WhereToCapture(int, Enum, metaclass=utils.OpenEnumMeta):
    # 1. Before pre-processing Pipeline
    ZERO = 0
    # 2. Before the Routes
    ONE = 1
    # 3. Before post-processing Pipeline
    TWO = 2
    # 4. Before the Destination
    THREE = 3


class CaptureSettingsTypedDict(TypedDict):
    duration: NotRequired[float]
    r"""Amount of time to keep capture open, in seconds"""
    max_events: NotRequired[float]
    r"""Maximum number of events to capture"""
    level: NotRequired[WhereToCapture]


class CaptureSettings(BaseModel):
    duration: Optional[float] = None
    r"""Amount of time to keep capture open, in seconds"""

    max_events: Annotated[Optional[float], pydantic.Field(alias="maxEvents")] = None
    r"""Maximum number of events to capture"""

    level: Optional[WhereToCapture] = None

    @field_serializer("level")
    def serialize_level(self, value):
        if isinstance(value, str):
            try:
                return models.WhereToCapture(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["duration", "maxEvents", "level"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunnableJobCollectionRunTypedDict(TypedDict):
    mode: RunnableJobCollectionMode
    r"""Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job."""
    reschedule_dropped_tasks: NotRequired[bool]
    r"""Reschedule tasks that failed with non-fatal errors"""
    max_task_reschedule: NotRequired[float]
    r"""Maximum number of times a task can be rescheduled"""
    log_level: NotRequired[LogLevelOptionsRunnableJobCollectionScheduleRun]
    r"""Level at which to set task logging"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time."""
    time_range_type: NotRequired[TimeRange]
    earliest: NotRequired[float]
    r"""Earliest time to collect data for the selected timezone"""
    latest: NotRequired[float]
    r"""Latest time to collect data for the selected timezone"""
    timestamp_timezone: NotRequired[str]
    r"""Timezone to use for Earliest and Latest times"""
    time_warning: NotRequired[MetricsStoreTypedDict]
    expression: NotRequired[str]
    r"""A filter for tokens in the provided collect path and/or the events being collected"""
    min_task_size: NotRequired[str]
    r"""Limits the bundle size for small tasks. For example,


    if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task.
    """
    max_task_size: NotRequired[str]
    r"""Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB,


    you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks.
    """
    discover_to_routes: NotRequired[bool]
    r"""Send discover results to Routes"""
    capture: NotRequired[CaptureSettingsTypedDict]


class RunnableJobCollectionRun(BaseModel):
    mode: RunnableJobCollectionMode
    r"""Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job."""

    reschedule_dropped_tasks: Annotated[
        Optional[bool], pydantic.Field(alias="rescheduleDroppedTasks")
    ] = None
    r"""Reschedule tasks that failed with non-fatal errors"""

    max_task_reschedule: Annotated[
        Optional[float], pydantic.Field(alias="maxTaskReschedule")
    ] = None
    r"""Maximum number of times a task can be rescheduled"""

    log_level: Annotated[
        Optional[LogLevelOptionsRunnableJobCollectionScheduleRun],
        pydantic.Field(alias="logLevel"),
    ] = None
    r"""Level at which to set task logging"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = None
    r"""Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time."""

    time_range_type: Annotated[
        Optional[TimeRange], pydantic.Field(alias="timeRangeType")
    ] = None

    earliest: Optional[float] = None
    r"""Earliest time to collect data for the selected timezone"""

    latest: Optional[float] = None
    r"""Latest time to collect data for the selected timezone"""

    timestamp_timezone: Annotated[
        Optional[str], pydantic.Field(alias="timestampTimezone")
    ] = None
    r"""Timezone to use for Earliest and Latest times"""

    time_warning: Annotated[
        Optional[MetricsStore], pydantic.Field(alias="timeWarning")
    ] = None

    expression: Optional[str] = None
    r"""A filter for tokens in the provided collect path and/or the events being collected"""

    min_task_size: Annotated[Optional[str], pydantic.Field(alias="minTaskSize")] = None
    r"""Limits the bundle size for small tasks. For example,


    if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task.
    """

    max_task_size: Annotated[Optional[str], pydantic.Field(alias="maxTaskSize")] = None
    r"""Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB,


    you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks.
    """

    discover_to_routes: Annotated[
        Optional[bool], pydantic.Field(alias="discoverToRoutes")
    ] = None
    r"""Send discover results to Routes"""

    capture: Optional[CaptureSettings] = None

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.LogLevelOptionsRunnableJobCollectionScheduleRun(value)
            except ValueError:
                return value
        return value

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.RunnableJobCollectionMode(value)
            except ValueError:
                return value
        return value

    @field_serializer("time_range_type")
    def serialize_time_range_type(self, value):
        if isinstance(value, str):
            try:
                return models.TimeRange(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "rescheduleDroppedTasks",
                "maxTaskReschedule",
                "logLevel",
                "jobTimeout",
                "timeRangeType",
                "earliest",
                "latest",
                "timestampTimezone",
                "timeWarning",
                "expression",
                "minTaskSize",
                "maxTaskSize",
                "discoverToRoutes",
                "capture",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class RunnableJobCollectionTypedDict(TypedDict):
    collector: CollectorTypedDict
    r"""Collector configuration"""
    run: RunnableJobCollectionRunTypedDict
    id: NotRequired[str]
    r"""Unique ID for this Job"""
    description: NotRequired[str]
    type: NotRequired[JobTypeOptionsRunnableJobCollection]
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    remove_fields: NotRequired[List[str]]
    r"""List of fields to remove from Discover results. Wildcards (for example, aws*) are allowed. This is useful when discovery returns sensitive fields that should not be exposed in the Jobs user interface."""
    resume_on_boot: NotRequired[bool]
    r"""Resume the ad hoc job if a failure condition causes Stream to restart during job execution"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    schedule: NotRequired[ScheduleTypeRunnableJobCollectionTypedDict]
    r"""Configuration for a scheduled job"""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    worker_affinity: NotRequired[bool]
    r"""If enabled, tasks are created and run by the same Worker Node"""
    input: NotRequired[TypeCollectionWithBreakerRulesetsConstraintTypedDict]


class RunnableJobCollection(BaseModel):
    collector: Collector
    r"""Collector configuration"""

    run: RunnableJobCollectionRun

    id: Optional[str] = None
    r"""Unique ID for this Job"""

    description: Optional[str] = None

    type: Optional[JobTypeOptionsRunnableJobCollection] = None

    ttl: Optional[str] = None
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = None
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    remove_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="removeFields")
    ] = None
    r"""List of fields to remove from Discover results. Wildcards (for example, aws*) are allowed. This is useful when discovery returns sensitive fields that should not be exposed in the Jobs user interface."""

    resume_on_boot: Annotated[Optional[bool], pydantic.Field(alias="resumeOnBoot")] = (
        None
    )
    r"""Resume the ad hoc job if a failure condition causes Stream to restart during job execution"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    schedule: Optional[ScheduleTypeRunnableJobCollection] = None
    r"""Configuration for a scheduled job"""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    worker_affinity: Annotated[
        Optional[bool], pydantic.Field(alias="workerAffinity")
    ] = None
    r"""If enabled, tasks are created and run by the same Worker Node"""

    input: Optional[TypeCollectionWithBreakerRulesetsConstraint] = None

    @field_serializer("type")
    def serialize_type(self, value):
        if isinstance(value, str):
            try:
                return models.JobTypeOptionsRunnableJobCollection(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "id",
                "description",
                "type",
                "ttl",
                "ignoreGroupJobsLimit",
                "removeFields",
                "resumeOnBoot",
                "environment",
                "schedule",
                "streamtags",
                "workerAffinity",
                "input",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
