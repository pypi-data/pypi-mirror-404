"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
from enum import Enum
import pydantic
from pydantic import field_serializer, model_serializer
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class PipelineFunctionAggregateMetricsID(str, Enum):
    r"""Function ID"""

    AGGREGATE_METRICS = "aggregate_metrics"


class PipelineFunctionAggregateMetricsMetricType(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""The output metric type"""

    AUTOMATIC = "automatic"
    COUNTER = "counter"
    DISTRIBUTION = "distribution"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    SUMMARY = "summary"
    TIMER = "timer"


class AggregationTypedDict(TypedDict):
    metric_type: PipelineFunctionAggregateMetricsMetricType
    r"""The output metric type"""
    agg: str
    r"""Aggregate function to perform on events. Example: sum(bytes).where(action=='REJECT').as(TotalBytes)"""


class Aggregation(BaseModel):
    metric_type: Annotated[
        PipelineFunctionAggregateMetricsMetricType, pydantic.Field(alias="metricType")
    ]
    r"""The output metric type"""

    agg: str
    r"""Aggregate function to perform on events. Example: sum(bytes).where(action=='REJECT').as(TotalBytes)"""

    @field_serializer("metric_type")
    def serialize_metric_type(self, value):
        if isinstance(value, str):
            try:
                return models.PipelineFunctionAggregateMetricsMetricType(value)
            except ValueError:
                return value
        return value


class PipelineFunctionAggregateMetricsAddTypedDict(TypedDict):
    value: str
    r"""JavaScript expression to compute the value (can be constant)"""
    name: NotRequired[str]


class PipelineFunctionAggregateMetricsAdd(BaseModel):
    value: str
    r"""JavaScript expression to compute the value (can be constant)"""

    name: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["name"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class PipelineFunctionAggregateMetricsConfTypedDict(TypedDict):
    time_window: str
    r"""The time span of the tumbling window for aggregating events. Must be a valid time string (such as 10s)."""
    aggregations: List[AggregationTypedDict]
    r"""Combination of Aggregation function and output metric type"""
    passthrough: NotRequired[bool]
    r"""Pass through the original events along with the aggregation events"""
    preserve_group_bys: NotRequired[bool]
    r"""Preserve the structure of the original aggregation event's groupby fields"""
    sufficient_stats_only: NotRequired[bool]
    r"""Output only statistics that are sufficient for the supplied aggregations"""
    prefix: NotRequired[str]
    r"""A prefix that is prepended to all of the fields output by this Aggregations Function"""
    groupbys: NotRequired[List[str]]
    r"""Optional: One or more dimensions to group aggregates by. Supports wildcard expressions. Wrap dimension names in quotes if using literal identifiers, such as 'service.name'. Warning: Using wildcard '*' causes all dimensions in the event to be included, which can result in high cardinality and increased memory usage. Exclude dimensions that can result in high cardinality before using wildcards. Example: !_time, !_numericValue, *"""
    flush_event_limit: NotRequired[float]
    r"""The maximum number of events to include in any given aggregation event"""
    flush_mem_limit: NotRequired[str]
    r"""The memory usage limit to impose upon aggregations. Defaults to 80% of the process memory; value configured above default limit is ignored. Accepts numerals with units like KB and MB (example: 128MB)."""
    cumulative: NotRequired[bool]
    r"""Enable to retain aggregations for cumulative aggregations when flushing out an aggregation table event. When disabled (the default), aggregations are reset to 0 on flush."""
    should_treat_dots_as_literals: NotRequired[bool]
    r"""Treat dots in dimension names as literals. This is useful for top-level dimensions that contain dots, such as 'service.name'."""
    add: NotRequired[List[PipelineFunctionAggregateMetricsAddTypedDict]]
    r"""Set of key-value pairs to evaluate and add/set"""
    flush_on_input_close: NotRequired[bool]
    r"""Flush aggregations when an input stream is closed. If disabled, Time Window Settings control flush behavior."""


class PipelineFunctionAggregateMetricsConf(BaseModel):
    time_window: Annotated[str, pydantic.Field(alias="timeWindow")]
    r"""The time span of the tumbling window for aggregating events. Must be a valid time string (such as 10s)."""

    aggregations: List[Aggregation]
    r"""Combination of Aggregation function and output metric type"""

    passthrough: Optional[bool] = None
    r"""Pass through the original events along with the aggregation events"""

    preserve_group_bys: Annotated[
        Optional[bool], pydantic.Field(alias="preserveGroupBys")
    ] = None
    r"""Preserve the structure of the original aggregation event's groupby fields"""

    sufficient_stats_only: Annotated[
        Optional[bool], pydantic.Field(alias="sufficientStatsOnly")
    ] = None
    r"""Output only statistics that are sufficient for the supplied aggregations"""

    prefix: Optional[str] = None
    r"""A prefix that is prepended to all of the fields output by this Aggregations Function"""

    groupbys: Optional[List[str]] = None
    r"""Optional: One or more dimensions to group aggregates by. Supports wildcard expressions. Wrap dimension names in quotes if using literal identifiers, such as 'service.name'. Warning: Using wildcard '*' causes all dimensions in the event to be included, which can result in high cardinality and increased memory usage. Exclude dimensions that can result in high cardinality before using wildcards. Example: !_time, !_numericValue, *"""

    flush_event_limit: Annotated[
        Optional[float], pydantic.Field(alias="flushEventLimit")
    ] = None
    r"""The maximum number of events to include in any given aggregation event"""

    flush_mem_limit: Annotated[Optional[str], pydantic.Field(alias="flushMemLimit")] = (
        None
    )
    r"""The memory usage limit to impose upon aggregations. Defaults to 80% of the process memory; value configured above default limit is ignored. Accepts numerals with units like KB and MB (example: 128MB)."""

    cumulative: Optional[bool] = None
    r"""Enable to retain aggregations for cumulative aggregations when flushing out an aggregation table event. When disabled (the default), aggregations are reset to 0 on flush."""

    should_treat_dots_as_literals: Annotated[
        Optional[bool], pydantic.Field(alias="shouldTreatDotsAsLiterals")
    ] = None
    r"""Treat dots in dimension names as literals. This is useful for top-level dimensions that contain dots, such as 'service.name'."""

    add: Optional[List[PipelineFunctionAggregateMetricsAdd]] = None
    r"""Set of key-value pairs to evaluate and add/set"""

    flush_on_input_close: Annotated[
        Optional[bool], pydantic.Field(alias="flushOnInputClose")
    ] = None
    r"""Flush aggregations when an input stream is closed. If disabled, Time Window Settings control flush behavior."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "passthrough",
                "preserveGroupBys",
                "sufficientStatsOnly",
                "prefix",
                "groupbys",
                "flushEventLimit",
                "flushMemLimit",
                "cumulative",
                "shouldTreatDotsAsLiterals",
                "add",
                "flushOnInputClose",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class PipelineFunctionAggregateMetricsTypedDict(TypedDict):
    id: PipelineFunctionAggregateMetricsID
    r"""Function ID"""
    conf: PipelineFunctionAggregateMetricsConfTypedDict
    filter_: NotRequired[str]
    r"""Filter that selects data to be fed through this Function"""
    description: NotRequired[str]
    r"""Simple description of this step"""
    disabled: NotRequired[bool]
    r"""If true, data will not be pushed through this function"""
    final: NotRequired[bool]
    r"""If enabled, stops the results of this Function from being passed to the downstream Functions"""
    group_id: NotRequired[str]
    r"""Group ID"""


class PipelineFunctionAggregateMetrics(BaseModel):
    id: PipelineFunctionAggregateMetricsID
    r"""Function ID"""

    conf: PipelineFunctionAggregateMetricsConf

    filter_: Annotated[Optional[str], pydantic.Field(alias="filter")] = None
    r"""Filter that selects data to be fed through this Function"""

    description: Optional[str] = None
    r"""Simple description of this step"""

    disabled: Optional[bool] = None
    r"""If true, data will not be pushed through this function"""

    final: Optional[bool] = None
    r"""If enabled, stops the results of this Function from being passed to the downstream Functions"""

    group_id: Annotated[Optional[str], pydantic.Field(alias="groupId")] = None
    r"""Group ID"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(["filter", "description", "disabled", "final", "groupId"])
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
