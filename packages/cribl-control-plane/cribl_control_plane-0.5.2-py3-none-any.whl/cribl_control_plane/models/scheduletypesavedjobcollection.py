"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
from enum import Enum
import pydantic
from pydantic import field_serializer, model_serializer
from typing import Any, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class ScheduleTypeSavedJobCollectionType(str, Enum, metaclass=utils.OpenEnumMeta):
    COLLECTION = "collection"


class ScheduleTypeSavedJobCollectionLogLevel(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Level at which to set task logging"""

    ERROR = "error"
    WARN = "warn"
    INFO = "info"
    DEBUG = "debug"
    SILLY = "silly"


class TimeWarningTypedDict(TypedDict):
    pass


class TimeWarning(BaseModel):
    pass


class ScheduleTypeSavedJobCollectionRunSettingsTypedDict(TypedDict):
    mode: str
    r"""Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job."""
    type: NotRequired[ScheduleTypeSavedJobCollectionType]
    reschedule_dropped_tasks: NotRequired[bool]
    r"""Reschedule tasks that failed with non-fatal errors"""
    max_task_reschedule: NotRequired[float]
    r"""Maximum number of times a task can be rescheduled"""
    log_level: NotRequired[ScheduleTypeSavedJobCollectionLogLevel]
    r"""Level at which to set task logging"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time."""
    time_range_type: NotRequired[str]
    earliest: NotRequired[float]
    r"""Earliest time to collect data for the selected timezone"""
    latest: NotRequired[float]
    r"""Latest time to collect data for the selected timezone"""
    timestamp_timezone: NotRequired[Any]
    time_warning: NotRequired[TimeWarningTypedDict]
    expression: NotRequired[str]
    r"""A filter for tokens in the provided collect path and/or the events being collected"""
    min_task_size: NotRequired[str]
    r"""Limits the bundle size for small tasks. For example,


    if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task.
    """
    max_task_size: NotRequired[str]
    r"""Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB,


    you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks.
    """


class ScheduleTypeSavedJobCollectionRunSettings(BaseModel):
    mode: str
    r"""Job run mode. Preview will either return up to N matching results, or will run until capture time T is reached. Discovery will gather the list of files to turn into streaming tasks, without running the data collection job. Full Run will run the collection job."""

    type: Optional[ScheduleTypeSavedJobCollectionType] = None

    reschedule_dropped_tasks: Annotated[
        Optional[bool], pydantic.Field(alias="rescheduleDroppedTasks")
    ] = None
    r"""Reschedule tasks that failed with non-fatal errors"""

    max_task_reschedule: Annotated[
        Optional[float], pydantic.Field(alias="maxTaskReschedule")
    ] = None
    r"""Maximum number of times a task can be rescheduled"""

    log_level: Annotated[
        Optional[ScheduleTypeSavedJobCollectionLogLevel],
        pydantic.Field(alias="logLevel"),
    ] = None
    r"""Level at which to set task logging"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = None
    r"""Maximum time the job is allowed to run. Time unit defaults to seconds if not specified (examples: 30, 45s, 15m). Enter 0 for unlimited time."""

    time_range_type: Annotated[Optional[str], pydantic.Field(alias="timeRangeType")] = (
        None
    )

    earliest: Optional[float] = None
    r"""Earliest time to collect data for the selected timezone"""

    latest: Optional[float] = None
    r"""Latest time to collect data for the selected timezone"""

    timestamp_timezone: Annotated[
        Optional[Any], pydantic.Field(alias="timestampTimezone")
    ] = None

    time_warning: Annotated[
        Optional[TimeWarning], pydantic.Field(alias="timeWarning")
    ] = None

    expression: Optional[str] = None
    r"""A filter for tokens in the provided collect path and/or the events being collected"""

    min_task_size: Annotated[Optional[str], pydantic.Field(alias="minTaskSize")] = None
    r"""Limits the bundle size for small tasks. For example,


    if your lower bundle size is 1MB, you can bundle up to five 200KB files into one task.
    """

    max_task_size: Annotated[Optional[str], pydantic.Field(alias="maxTaskSize")] = None
    r"""Limits the bundle size for files above the lower task bundle size. For example, if your upper bundle size is 10MB,


    you can bundle up to five 2MB files into one task. Files greater than this size will be assigned to individual tasks.
    """

    @field_serializer("type")
    def serialize_type(self, value):
        if isinstance(value, str):
            try:
                return models.ScheduleTypeSavedJobCollectionType(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.ScheduleTypeSavedJobCollectionLogLevel(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "type",
                "rescheduleDroppedTasks",
                "maxTaskReschedule",
                "logLevel",
                "jobTimeout",
                "timeRangeType",
                "earliest",
                "latest",
                "timestampTimezone",
                "timeWarning",
                "expression",
                "minTaskSize",
                "maxTaskSize",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class ScheduleTypeSavedJobCollectionTypedDict(TypedDict):
    r"""Configuration for a scheduled job"""

    enabled: NotRequired[bool]
    r"""Enable to configure scheduling for this Collector"""
    skippable: NotRequired[bool]
    r"""Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits"""
    resume_missed: NotRequired[bool]
    r"""If Stream Leader (or single instance) restarts, run all missed jobs according to their original schedules"""
    cron_schedule: NotRequired[str]
    r"""A cron schedule on which to run this job"""
    max_concurrent_runs: NotRequired[float]
    r"""The maximum number of instances of this scheduled job that may be running at any time"""
    run: NotRequired[ScheduleTypeSavedJobCollectionRunSettingsTypedDict]


class ScheduleTypeSavedJobCollection(BaseModel):
    r"""Configuration for a scheduled job"""

    enabled: Optional[bool] = None
    r"""Enable to configure scheduling for this Collector"""

    skippable: Optional[bool] = None
    r"""Skippable jobs can be delayed, up to their next run time, if the system is hitting concurrency limits"""

    resume_missed: Annotated[Optional[bool], pydantic.Field(alias="resumeMissed")] = (
        None
    )
    r"""If Stream Leader (or single instance) restarts, run all missed jobs according to their original schedules"""

    cron_schedule: Annotated[Optional[str], pydantic.Field(alias="cronSchedule")] = None
    r"""A cron schedule on which to run this job"""

    max_concurrent_runs: Annotated[
        Optional[float], pydantic.Field(alias="maxConcurrentRuns")
    ] = None
    r"""The maximum number of instances of this scheduled job that may be running at any time"""

    run: Optional[ScheduleTypeSavedJobCollectionRunSettings] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "enabled",
                "skippable",
                "resumeMissed",
                "cronSchedule",
                "maxConcurrentRuns",
                "run",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
