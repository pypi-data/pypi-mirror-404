"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .datacompressionformatoptionspersistence import (
    DataCompressionFormatOptionsPersistence,
)
from .itemstypeconnectionsoptional import (
    ItemsTypeConnectionsOptional,
    ItemsTypeConnectionsOptionalTypedDict,
)
from .itemstypemetadata import ItemsTypeMetadata, ItemsTypeMetadataTypedDict
from .itemstyperules import ItemsTypeRules, ItemsTypeRulesTypedDict
from .pqtype import PqType, PqTypeTypedDict
from cribl_control_plane import models
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
from enum import Enum
import pydantic
from pydantic import field_serializer, model_serializer
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class InputKubeMetricsType(str, Enum):
    KUBE_METRICS = "kube_metrics"


class InputKubeMetricsPersistenceTypedDict(TypedDict):
    enable: NotRequired[bool]
    r"""Spool metrics on disk for Cribl Search"""
    time_window: NotRequired[str]
    r"""Time span for each file bucket"""
    max_data_size: NotRequired[str]
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""
    max_data_time: NotRequired[str]
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""
    compress: NotRequired[DataCompressionFormatOptionsPersistence]
    dest_path: NotRequired[str]
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>"""


class InputKubeMetricsPersistence(BaseModel):
    enable: Optional[bool] = None
    r"""Spool metrics on disk for Cribl Search"""

    time_window: Annotated[Optional[str], pydantic.Field(alias="timeWindow")] = None
    r"""Time span for each file bucket"""

    max_data_size: Annotated[Optional[str], pydantic.Field(alias="maxDataSize")] = None
    r"""Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted."""

    max_data_time: Annotated[Optional[str], pydantic.Field(alias="maxDataTime")] = None
    r"""Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted."""

    compress: Optional[DataCompressionFormatOptionsPersistence] = None

    dest_path: Annotated[Optional[str], pydantic.Field(alias="destPath")] = None
    r"""Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>"""

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.DataCompressionFormatOptionsPersistence(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "enable",
                "timeWindow",
                "maxDataSize",
                "maxDataTime",
                "compress",
                "destPath",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class InputKubeMetricsTypedDict(TypedDict):
    type: InputKubeMetricsType
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    interval: NotRequired[float]
    r"""Time, in seconds, between consecutive metrics collections. Default is 15 secs."""
    rules: NotRequired[List[ItemsTypeRulesTypedDict]]
    r"""Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true."""
    metadata: NotRequired[List[ItemsTypeMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    persistence: NotRequired[InputKubeMetricsPersistenceTypedDict]
    description: NotRequired[str]


class InputKubeMetrics(BaseModel):
    type: InputKubeMetricsType

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    interval: Optional[float] = None
    r"""Time, in seconds, between consecutive metrics collections. Default is 15 secs."""

    rules: Optional[List[ItemsTypeRules]] = None
    r"""Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true."""

    metadata: Optional[List[ItemsTypeMetadata]] = None
    r"""Fields to add to events from this input"""

    persistence: Optional[InputKubeMetricsPersistence] = None

    description: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "id",
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "interval",
                "rules",
                "metadata",
                "persistence",
                "description",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
