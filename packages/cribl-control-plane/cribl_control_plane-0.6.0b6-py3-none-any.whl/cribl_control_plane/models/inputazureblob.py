"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .authenticationmethodoptions import AuthenticationMethodOptions
from .certificatetypeazureblobauthtypeclientcert import (
    CertificateTypeAzureBlobAuthTypeClientCert,
    CertificateTypeAzureBlobAuthTypeClientCertTypedDict,
)
from .itemstypeconnectionsoptional import (
    ItemsTypeConnectionsOptional,
    ItemsTypeConnectionsOptionalTypedDict,
)
from .itemstypemetadata import ItemsTypeMetadata, ItemsTypeMetadataTypedDict
from .pqtype import PqType, PqTypeTypedDict
from cribl_control_plane import models
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
from enum import Enum
import pydantic
from pydantic import field_serializer, model_serializer
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class InputAzureBlobType(str, Enum):
    AZURE_BLOB = "azure_blob"


class InputAzureBlobTypedDict(TypedDict):
    type: InputAzureBlobType
    queue_name: str
    r"""The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    file_filter: NotRequired[str]
    r"""Regex matching file names to download and process. Defaults to: .*"""
    visibility_timeout: NotRequired[float]
    r"""The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request."""
    num_receivers: NotRequired[float]
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""
    max_messages: NotRequired[float]
    r"""The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32."""
    service_period_secs: NotRequired[float]
    r"""The duration (in seconds) which pollers should be validated and restarted if exited"""
    skip_on_error: NotRequired[bool]
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""
    metadata: NotRequired[List[ItemsTypeMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""
    auth_type: NotRequired[AuthenticationMethodOptions]
    description: NotRequired[str]
    connection_string: NotRequired[str]
    r"""Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING."""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    storage_account_name: NotRequired[str]
    r"""The name of your Azure storage account"""
    tenant_id: NotRequired[str]
    r"""The service principal's tenant ID"""
    client_id: NotRequired[str]
    r"""The service principal's client ID"""
    azure_cloud: NotRequired[str]
    r"""The Azure cloud to use. Defaults to Azure Public Cloud."""
    endpoint_suffix: NotRequired[str]
    r"""Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net."""
    client_text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    certificate: NotRequired[CertificateTypeAzureBlobAuthTypeClientCertTypedDict]
    template_queue_name: NotRequired[str]
    r"""Binds 'queueName' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'queueName' at runtime."""
    template_connection_string: NotRequired[str]
    r"""Binds 'connectionString' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'connectionString' at runtime."""
    template_tenant_id: NotRequired[str]
    r"""Binds 'tenantId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'tenantId' at runtime."""
    template_client_id: NotRequired[str]
    r"""Binds 'clientId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'clientId' at runtime."""


class InputAzureBlob(BaseModel):
    type: InputAzureBlobType

    queue_name: Annotated[str, pydantic.Field(alias="queueName")]
    r"""The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    file_filter: Annotated[Optional[str], pydantic.Field(alias="fileFilter")] = None
    r"""Regex matching file names to download and process. Defaults to: .*"""

    visibility_timeout: Annotated[
        Optional[float], pydantic.Field(alias="visibilityTimeout")
    ] = None
    r"""The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request."""

    num_receivers: Annotated[Optional[float], pydantic.Field(alias="numReceivers")] = (
        None
    )
    r"""How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead."""

    max_messages: Annotated[Optional[float], pydantic.Field(alias="maxMessages")] = None
    r"""The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32."""

    service_period_secs: Annotated[
        Optional[float], pydantic.Field(alias="servicePeriodSecs")
    ] = None
    r"""The duration (in seconds) which pollers should be validated and restarted if exited"""

    skip_on_error: Annotated[Optional[bool], pydantic.Field(alias="skipOnError")] = None
    r"""Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors."""

    metadata: Optional[List[ItemsTypeMetadata]] = None
    r"""Fields to add to events from this input"""

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = None
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = None
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = None
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified."""

    auth_type: Annotated[
        Optional[AuthenticationMethodOptions], pydantic.Field(alias="authType")
    ] = None

    description: Optional[str] = None

    connection_string: Annotated[
        Optional[str], pydantic.Field(alias="connectionString")
    ] = None
    r"""Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING."""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    storage_account_name: Annotated[
        Optional[str], pydantic.Field(alias="storageAccountName")
    ] = None
    r"""The name of your Azure storage account"""

    tenant_id: Annotated[Optional[str], pydantic.Field(alias="tenantId")] = None
    r"""The service principal's tenant ID"""

    client_id: Annotated[Optional[str], pydantic.Field(alias="clientId")] = None
    r"""The service principal's client ID"""

    azure_cloud: Annotated[Optional[str], pydantic.Field(alias="azureCloud")] = None
    r"""The Azure cloud to use. Defaults to Azure Public Cloud."""

    endpoint_suffix: Annotated[
        Optional[str], pydantic.Field(alias="endpointSuffix")
    ] = None
    r"""Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net."""

    client_text_secret: Annotated[
        Optional[str], pydantic.Field(alias="clientTextSecret")
    ] = None
    r"""Select or create a stored text secret"""

    certificate: Optional[CertificateTypeAzureBlobAuthTypeClientCert] = None

    template_queue_name: Annotated[
        Optional[str], pydantic.Field(alias="__template_queueName")
    ] = None
    r"""Binds 'queueName' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'queueName' at runtime."""

    template_connection_string: Annotated[
        Optional[str], pydantic.Field(alias="__template_connectionString")
    ] = None
    r"""Binds 'connectionString' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'connectionString' at runtime."""

    template_tenant_id: Annotated[
        Optional[str], pydantic.Field(alias="__template_tenantId")
    ] = None
    r"""Binds 'tenantId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'tenantId' at runtime."""

    template_client_id: Annotated[
        Optional[str], pydantic.Field(alias="__template_clientId")
    ] = None
    r"""Binds 'clientId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'clientId' at runtime."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "id",
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "fileFilter",
                "visibilityTimeout",
                "numReceivers",
                "maxMessages",
                "servicePeriodSecs",
                "skipOnError",
                "metadata",
                "breakerRulesets",
                "staleChannelFlushMs",
                "parquetChunkSizeMB",
                "parquetChunkDownloadTimeout",
                "authType",
                "description",
                "connectionString",
                "textSecret",
                "storageAccountName",
                "tenantId",
                "clientId",
                "azureCloud",
                "endpointSuffix",
                "clientTextSecret",
                "certificate",
                "__template_queueName",
                "__template_connectionString",
                "__template_tenantId",
                "__template_clientId",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
