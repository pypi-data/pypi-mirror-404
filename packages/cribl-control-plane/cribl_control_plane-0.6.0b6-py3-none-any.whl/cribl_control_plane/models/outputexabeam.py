"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .backpressurebehavioroptions1 import BackpressureBehaviorOptions1
from .diskspaceprotectionoptions import DiskSpaceProtectionOptions
from .objectacloptions1 import ObjectACLOptions1
from .retrysettingstype import RetrySettingsType, RetrySettingsTypeTypedDict
from .signatureversionoptions4 import SignatureVersionOptions4
from .storageclassoptions1 import StorageClassOptions1
from cribl_control_plane import models
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
from enum import Enum
import pydantic
from pydantic import field_serializer, model_serializer
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class OutputExabeamType(str, Enum):
    EXABEAM = "exabeam"


class OutputExabeamTypedDict(TypedDict):
    type: OutputExabeamType
    bucket: str
    r"""Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`."""
    region: str
    r"""Region where the bucket is located"""
    stage_path: str
    r"""Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage."""
    endpoint: str
    r"""Google Cloud Storage service endpoint"""
    collector_instance_id: str
    r"""ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888

    """
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    signature_version: NotRequired[SignatureVersionOptions4]
    r"""Signature version to use for signing Google Cloud Storage requests"""
    object_acl: NotRequired[ObjectACLOptions1]
    r"""Object ACL to assign to uploaded objects"""
    storage_class: NotRequired[StorageClassOptions1]
    r"""Storage class to select for uploaded objects"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    add_id_to_stage_path: NotRequired[bool]
    r"""Add the Output ID value to staging location"""
    remove_empty_dirs: NotRequired[bool]
    r"""Remove empty staging directories after moving files"""
    max_file_open_time_sec: NotRequired[float]
    r"""Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location."""
    max_file_idle_time_sec: NotRequired[float]
    r"""Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location."""
    max_open_files: NotRequired[float]
    r"""Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location."""
    on_backpressure: NotRequired[BackpressureBehaviorOptions1]
    r"""How to handle events when all receivers are exerting backpressure"""
    deadletter_enabled: NotRequired[bool]
    r"""If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors"""
    on_disk_full_backpressure: NotRequired[DiskSpaceProtectionOptions]
    r"""How to handle events when disk space is below the global 'Min free disk space' limit"""
    retry_settings: NotRequired[RetrySettingsTypeTypedDict]
    max_file_size_mb: NotRequired[float]
    r"""Maximum uncompressed output file size. Files of this size will be closed and moved to final output location."""
    encoded_configuration: NotRequired[str]
    r"""Enter an encoded string containing Exabeam configurations"""
    site_name: NotRequired[str]
    r"""Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants."""
    site_id: NotRequired[str]
    r"""Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name."""
    timezone_offset: NotRequired[str]
    aws_api_key: NotRequired[str]
    r"""HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`."""
    aws_secret_key: NotRequired[str]
    r"""HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`."""
    description: NotRequired[str]
    empty_dir_cleanup_sec: NotRequired[float]
    r"""How frequently, in seconds, to clean up empty directories"""
    directory_batch_size: NotRequired[float]
    r"""Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory."""
    deadletter_path: NotRequired[str]
    r"""Storage location for files that fail to reach their final destination after maximum retries are exceeded"""
    max_retry_num: NotRequired[float]
    r"""The maximum number of times a file will attempt to move to its final destination before being dead-lettered"""
    template_region: NotRequired[str]
    r"""Binds 'region' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'region' at runtime."""


class OutputExabeam(BaseModel):
    type: OutputExabeamType

    bucket: str
    r"""Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`."""

    region: str
    r"""Region where the bucket is located"""

    stage_path: Annotated[str, pydantic.Field(alias="stagePath")]
    r"""Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage."""

    endpoint: str
    r"""Google Cloud Storage service endpoint"""

    collector_instance_id: Annotated[str, pydantic.Field(alias="collectorInstanceId")]
    r"""ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888

    """

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    signature_version: Annotated[
        Optional[SignatureVersionOptions4], pydantic.Field(alias="signatureVersion")
    ] = None
    r"""Signature version to use for signing Google Cloud Storage requests"""

    object_acl: Annotated[
        Optional[ObjectACLOptions1], pydantic.Field(alias="objectACL")
    ] = None
    r"""Object ACL to assign to uploaded objects"""

    storage_class: Annotated[
        Optional[StorageClassOptions1], pydantic.Field(alias="storageClass")
    ] = None
    r"""Storage class to select for uploaded objects"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = None
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    add_id_to_stage_path: Annotated[
        Optional[bool], pydantic.Field(alias="addIdToStagePath")
    ] = None
    r"""Add the Output ID value to staging location"""

    remove_empty_dirs: Annotated[
        Optional[bool], pydantic.Field(alias="removeEmptyDirs")
    ] = None
    r"""Remove empty staging directories after moving files"""

    max_file_open_time_sec: Annotated[
        Optional[float], pydantic.Field(alias="maxFileOpenTimeSec")
    ] = None
    r"""Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location."""

    max_file_idle_time_sec: Annotated[
        Optional[float], pydantic.Field(alias="maxFileIdleTimeSec")
    ] = None
    r"""Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location."""

    max_open_files: Annotated[Optional[float], pydantic.Field(alias="maxOpenFiles")] = (
        None
    )
    r"""Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location."""

    on_backpressure: Annotated[
        Optional[BackpressureBehaviorOptions1], pydantic.Field(alias="onBackpressure")
    ] = None
    r"""How to handle events when all receivers are exerting backpressure"""

    deadletter_enabled: Annotated[
        Optional[bool], pydantic.Field(alias="deadletterEnabled")
    ] = None
    r"""If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors"""

    on_disk_full_backpressure: Annotated[
        Optional[DiskSpaceProtectionOptions],
        pydantic.Field(alias="onDiskFullBackpressure"),
    ] = None
    r"""How to handle events when disk space is below the global 'Min free disk space' limit"""

    retry_settings: Annotated[
        Optional[RetrySettingsType], pydantic.Field(alias="retrySettings")
    ] = None

    max_file_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="maxFileSizeMB")
    ] = None
    r"""Maximum uncompressed output file size. Files of this size will be closed and moved to final output location."""

    encoded_configuration: Annotated[
        Optional[str], pydantic.Field(alias="encodedConfiguration")
    ] = None
    r"""Enter an encoded string containing Exabeam configurations"""

    site_name: Annotated[Optional[str], pydantic.Field(alias="siteName")] = None
    r"""Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants."""

    site_id: Annotated[Optional[str], pydantic.Field(alias="siteId")] = None
    r"""Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name."""

    timezone_offset: Annotated[
        Optional[str], pydantic.Field(alias="timezoneOffset")
    ] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None
    r"""HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )
    r"""HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`."""

    description: Optional[str] = None

    empty_dir_cleanup_sec: Annotated[
        Optional[float], pydantic.Field(alias="emptyDirCleanupSec")
    ] = None
    r"""How frequently, in seconds, to clean up empty directories"""

    directory_batch_size: Annotated[
        Optional[float], pydantic.Field(alias="directoryBatchSize")
    ] = None
    r"""Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory."""

    deadletter_path: Annotated[
        Optional[str], pydantic.Field(alias="deadletterPath")
    ] = None
    r"""Storage location for files that fail to reach their final destination after maximum retries are exceeded"""

    max_retry_num: Annotated[Optional[float], pydantic.Field(alias="maxRetryNum")] = (
        None
    )
    r"""The maximum number of times a file will attempt to move to its final destination before being dead-lettered"""

    template_region: Annotated[
        Optional[str], pydantic.Field(alias="__template_region")
    ] = None
    r"""Binds 'region' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'region' at runtime."""

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions4(value)
            except ValueError:
                return value
        return value

    @field_serializer("object_acl")
    def serialize_object_acl(self, value):
        if isinstance(value, str):
            try:
                return models.ObjectACLOptions1(value)
            except ValueError:
                return value
        return value

    @field_serializer("storage_class")
    def serialize_storage_class(self, value):
        if isinstance(value, str):
            try:
                return models.StorageClassOptions1(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.BackpressureBehaviorOptions1(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_disk_full_backpressure")
    def serialize_on_disk_full_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.DiskSpaceProtectionOptions(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "id",
                "pipeline",
                "systemFields",
                "environment",
                "streamtags",
                "signatureVersion",
                "objectACL",
                "storageClass",
                "reuseConnections",
                "rejectUnauthorized",
                "addIdToStagePath",
                "removeEmptyDirs",
                "maxFileOpenTimeSec",
                "maxFileIdleTimeSec",
                "maxOpenFiles",
                "onBackpressure",
                "deadletterEnabled",
                "onDiskFullBackpressure",
                "retrySettings",
                "maxFileSizeMB",
                "encodedConfiguration",
                "siteName",
                "siteId",
                "timezoneOffset",
                "awsApiKey",
                "awsSecretKey",
                "description",
                "emptyDirCleanupSec",
                "directoryBatchSize",
                "deadletterPath",
                "maxRetryNum",
                "__template_region",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
