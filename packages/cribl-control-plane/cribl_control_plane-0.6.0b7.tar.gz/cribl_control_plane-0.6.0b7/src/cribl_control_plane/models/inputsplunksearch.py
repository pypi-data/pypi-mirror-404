"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .itemstypeconnectionsoptional import (
    ItemsTypeConnectionsOptional,
    ItemsTypeConnectionsOptionalTypedDict,
)
from .itemstypemetadata import ItemsTypeMetadata, ItemsTypeMetadataTypedDict
from .outputmodeoptionssplunkcollectorconf import OutputModeOptionsSplunkCollectorConf
from .pqtype import PqType, PqTypeTypedDict
from .retryrulestype import RetryRulesType, RetryRulesTypeTypedDict
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
from enum import Enum
import pydantic
from pydantic import field_serializer, model_serializer
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class InputSplunkSearchType(str, Enum):
    SPLUNK_SEARCH = "splunk_search"


class EndpointParamTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings."""


class EndpointParam(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings."""


class EndpointHeaderTypedDict(TypedDict):
    name: str
    value: str
    r"""JavaScript expression to compute the header's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings."""


class EndpointHeader(BaseModel):
    name: str

    value: str
    r"""JavaScript expression to compute the header's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings."""


class InputSplunkSearchLogLevel(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Collector runtime log level (verbosity)"""

    ERROR = "error"
    WARN = "warn"
    INFO = "info"
    DEBUG = "debug"


class InputSplunkSearchAuthenticationType(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Splunk Search authentication type"""

    # None
    NONE = "none"
    # Basic
    BASIC = "basic"
    # Basic (credentials secret)
    CREDENTIALS_SECRET = "credentialsSecret"
    # Token
    TOKEN = "token"
    # Token (text secret)
    TEXT_SECRET = "textSecret"


class InputSplunkSearchTypedDict(TypedDict):
    type: InputSplunkSearchType
    search_head: str
    r"""Search head base URL. Can be an expression. Default is https://localhost:8089."""
    search: str
    r"""Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'"""
    cron_schedule: str
    r"""A cron schedule on which to run this job"""
    endpoint: str
    r"""REST API used to create a search"""
    output_mode: OutputModeOptionsSplunkCollectorConf
    r"""Format of the returned output"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    earliest: NotRequired[str]
    r"""The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'"""
    latest: NotRequired[str]
    r"""The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'"""
    endpoint_params: NotRequired[List[EndpointParamTypedDict]]
    r"""Optional request parameters to send to the endpoint"""
    endpoint_headers: NotRequired[List[EndpointHeaderTypedDict]]
    r"""Optional request headers to send to the endpoint"""
    log_level: NotRequired[InputSplunkSearchLogLevel]
    r"""Collector runtime log level (verbosity)"""
    request_timeout: NotRequired[float]
    r"""HTTP request inactivity timeout. Use 0 for no timeout."""
    use_round_robin_dns: NotRequired[bool]
    r"""When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)"""
    encoding: NotRequired[str]
    r"""Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters."""
    keep_alive_time: NotRequired[float]
    r"""How often workers should check in with the scheduler to keep job subscription alive"""
    job_timeout: NotRequired[str]
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""
    max_missed_keep_alives: NotRequired[float]
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    metadata: NotRequired[List[ItemsTypeMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    retry_rules: NotRequired[RetryRulesTypeTypedDict]
    breaker_rulesets: NotRequired[List[str]]
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""
    stale_channel_flush_ms: NotRequired[float]
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""
    auth_type: NotRequired[InputSplunkSearchAuthenticationType]
    r"""Splunk Search authentication type"""
    description: NotRequired[str]
    username: NotRequired[str]
    password: NotRequired[str]
    token: NotRequired[str]
    r"""Bearer token to include in the authorization header"""
    credentials_secret: NotRequired[str]
    r"""Select or create a secret that references your credentials"""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""


class InputSplunkSearch(BaseModel):
    type: InputSplunkSearchType

    search_head: Annotated[str, pydantic.Field(alias="searchHead")]
    r"""Search head base URL. Can be an expression. Default is https://localhost:8089."""

    search: str
    r"""Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'"""

    cron_schedule: Annotated[str, pydantic.Field(alias="cronSchedule")]
    r"""A cron schedule on which to run this job"""

    endpoint: str
    r"""REST API used to create a search"""

    output_mode: Annotated[
        OutputModeOptionsSplunkCollectorConf, pydantic.Field(alias="outputMode")
    ]
    r"""Format of the returned output"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    earliest: Optional[str] = None
    r"""The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'"""

    latest: Optional[str] = None
    r"""The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'"""

    endpoint_params: Annotated[
        Optional[List[EndpointParam]], pydantic.Field(alias="endpointParams")
    ] = None
    r"""Optional request parameters to send to the endpoint"""

    endpoint_headers: Annotated[
        Optional[List[EndpointHeader]], pydantic.Field(alias="endpointHeaders")
    ] = None
    r"""Optional request headers to send to the endpoint"""

    log_level: Annotated[
        Optional[InputSplunkSearchLogLevel], pydantic.Field(alias="logLevel")
    ] = None
    r"""Collector runtime log level (verbosity)"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""HTTP request inactivity timeout. Use 0 for no timeout."""

    use_round_robin_dns: Annotated[
        Optional[bool], pydantic.Field(alias="useRoundRobinDns")
    ] = None
    r"""When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)"""

    encoding: Optional[str] = None
    r"""Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters."""

    keep_alive_time: Annotated[
        Optional[float], pydantic.Field(alias="keepAliveTime")
    ] = None
    r"""How often workers should check in with the scheduler to keep job subscription alive"""

    job_timeout: Annotated[Optional[str], pydantic.Field(alias="jobTimeout")] = None
    r"""Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time."""

    max_missed_keep_alives: Annotated[
        Optional[float], pydantic.Field(alias="maxMissedKeepAlives")
    ] = None
    r"""The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked."""

    ttl: Optional[str] = None
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = None
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    metadata: Optional[List[ItemsTypeMetadata]] = None
    r"""Fields to add to events from this input"""

    retry_rules: Annotated[
        Optional[RetryRulesType], pydantic.Field(alias="retryRules")
    ] = None

    breaker_rulesets: Annotated[
        Optional[List[str]], pydantic.Field(alias="breakerRulesets")
    ] = None
    r"""A list of event-breaking rulesets that will be applied, in order, to the input data stream"""

    stale_channel_flush_ms: Annotated[
        Optional[float], pydantic.Field(alias="staleChannelFlushMs")
    ] = None
    r"""How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines"""

    auth_type: Annotated[
        Optional[InputSplunkSearchAuthenticationType], pydantic.Field(alias="authType")
    ] = None
    r"""Splunk Search authentication type"""

    description: Optional[str] = None

    username: Optional[str] = None

    password: Optional[str] = None

    token: Optional[str] = None
    r"""Bearer token to include in the authorization header"""

    credentials_secret: Annotated[
        Optional[str], pydantic.Field(alias="credentialsSecret")
    ] = None
    r"""Select or create a secret that references your credentials"""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    @field_serializer("output_mode")
    def serialize_output_mode(self, value):
        if isinstance(value, str):
            try:
                return models.OutputModeOptionsSplunkCollectorConf(value)
            except ValueError:
                return value
        return value

    @field_serializer("log_level")
    def serialize_log_level(self, value):
        if isinstance(value, str):
            try:
                return models.InputSplunkSearchLogLevel(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.InputSplunkSearchAuthenticationType(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "id",
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "earliest",
                "latest",
                "endpointParams",
                "endpointHeaders",
                "logLevel",
                "requestTimeout",
                "useRoundRobinDns",
                "rejectUnauthorized",
                "encoding",
                "keepAliveTime",
                "jobTimeout",
                "maxMissedKeepAlives",
                "ttl",
                "ignoreGroupJobsLimit",
                "metadata",
                "retryRules",
                "breakerRulesets",
                "staleChannelFlushMs",
                "authType",
                "description",
                "username",
                "password",
                "token",
                "credentialsSecret",
                "textSecret",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
