"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .authenticationmethodoptionss3collectorconf import (
    AuthenticationMethodOptionsS3CollectorConf,
)
from .itemstypeconnectionsoptional import (
    ItemsTypeConnectionsOptional,
    ItemsTypeConnectionsOptionalTypedDict,
)
from .itemstypemetadata import ItemsTypeMetadata, ItemsTypeMetadataTypedDict
from .kafkaschemaregistryauthenticationtype import (
    KafkaSchemaRegistryAuthenticationType,
    KafkaSchemaRegistryAuthenticationTypeTypedDict,
)
from .pqtype import PqType, PqTypeTypedDict
from .signatureversionoptions import SignatureVersionOptions
from .tlssettingsclientsidetypekafkaschemaregistry import (
    TLSSettingsClientSideTypeKafkaSchemaRegistry,
    TLSSettingsClientSideTypeKafkaSchemaRegistryTypedDict,
)
from cribl_control_plane import models
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
from enum import Enum
import pydantic
from pydantic import field_serializer, model_serializer
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class InputMskType(str, Enum):
    MSK = "msk"


class InputMskTypedDict(TypedDict):
    type: InputMskType
    brokers: List[str]
    r"""Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092)."""
    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""
    aws_authentication_method: AuthenticationMethodOptionsS3CollectorConf
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    region: str
    r"""Region where the MSK cluster is located"""
    id: NotRequired[str]
    r"""Unique ID for this input"""
    disabled: NotRequired[bool]
    pipeline: NotRequired[str]
    r"""Pipeline to process data from this Source before sending it through the Routes"""
    send_to_routes: NotRequired[bool]
    r"""Select whether to send data to Routes, or directly to Destinations."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    pq_enabled: NotRequired[bool]
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    connections: NotRequired[List[ItemsTypeConnectionsOptionalTypedDict]]
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""
    pq: NotRequired[PqTypeTypedDict]
    group_id: NotRequired[str]
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""
    from_beginning: NotRequired[bool]
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""
    session_timeout: NotRequired[float]
    r"""
    Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """
    rebalance_timeout: NotRequired[float]
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """
    heartbeat_interval: NotRequired[float]
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """
    metadata: NotRequired[List[ItemsTypeMetadataTypedDict]]
    r"""Fields to add to events from this input"""
    kafka_schema_registry: NotRequired[KafkaSchemaRegistryAuthenticationTypeTypedDict]
    connection_timeout: NotRequired[float]
    r"""Maximum time to wait for a connection to complete successfully"""
    request_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to a request"""
    max_retries: NotRequired[float]
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""
    max_back_off: NotRequired[float]
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""
    initial_backoff: NotRequired[float]
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""
    backoff_rate: NotRequired[float]
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""
    authentication_timeout: NotRequired[float]
    r"""Maximum time to wait for Kafka to respond to an authentication request"""
    reauthentication_threshold: NotRequired[float]
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint."""
    signature_version: NotRequired[SignatureVersionOptions]
    r"""Signature version to use for signing MSK cluster requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access MSK"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    tls: NotRequired[TLSSettingsClientSideTypeKafkaSchemaRegistryTypedDict]
    auto_commit_interval: NotRequired[float]
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    auto_commit_threshold: NotRequired[float]
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""
    max_bytes_per_partition: NotRequired[float]
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""
    max_bytes: NotRequired[float]
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""
    max_socket_errors: NotRequired[float]
    r"""Maximum number of network errors before the consumer re-creates a socket"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    template_aws_secret_key: NotRequired[str]
    r"""Binds 'awsSecretKey' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'awsSecretKey' at runtime."""
    template_region: NotRequired[str]
    r"""Binds 'region' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'region' at runtime."""
    template_assume_role_arn: NotRequired[str]
    r"""Binds 'assumeRoleArn' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'assumeRoleArn' at runtime."""
    template_assume_role_external_id: NotRequired[str]
    r"""Binds 'assumeRoleExternalId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'assumeRoleExternalId' at runtime."""
    template_aws_api_key: NotRequired[str]
    r"""Binds 'awsApiKey' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'awsApiKey' at runtime."""


class InputMsk(BaseModel):
    type: InputMskType

    brokers: List[str]
    r"""Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092)."""

    topics: List[str]
    r"""Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only."""

    aws_authentication_method: Annotated[
        AuthenticationMethodOptionsS3CollectorConf,
        pydantic.Field(alias="awsAuthenticationMethod"),
    ]
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    region: str
    r"""Region where the MSK cluster is located"""

    id: Optional[str] = None
    r"""Unique ID for this input"""

    disabled: Optional[bool] = None

    pipeline: Optional[str] = None
    r"""Pipeline to process data from this Source before sending it through the Routes"""

    send_to_routes: Annotated[Optional[bool], pydantic.Field(alias="sendToRoutes")] = (
        None
    )
    r"""Select whether to send data to Routes, or directly to Destinations."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    pq_enabled: Annotated[Optional[bool], pydantic.Field(alias="pqEnabled")] = None
    r"""Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers)."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    connections: Optional[List[ItemsTypeConnectionsOptional]] = None
    r"""Direct connections to Destinations, and optionally via a Pipeline or a Pack"""

    pq: Optional[PqType] = None

    group_id: Annotated[Optional[str], pydantic.Field(alias="groupId")] = None
    r"""The consumer group to which this instance belongs. Defaults to 'Cribl'."""

    from_beginning: Annotated[Optional[bool], pydantic.Field(alias="fromBeginning")] = (
        None
    )
    r"""Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message"""

    session_timeout: Annotated[
        Optional[float], pydantic.Field(alias="sessionTimeout")
    ] = None
    r"""
    Timeout used to detect client failures when using Kafka's group-management facilities.
    If the client sends no heartbeats to the broker before the timeout expires,
    the broker will remove the client from the group and initiate a rebalance.
    Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
    """

    rebalance_timeout: Annotated[
        Optional[float], pydantic.Field(alias="rebalanceTimeout")
    ] = None
    r"""Maximum allowed time for each worker to join the group after a rebalance begins.
    If the timeout is exceeded, the coordinator broker will remove the worker from the group.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
    """

    heartbeat_interval: Annotated[
        Optional[float], pydantic.Field(alias="heartbeatInterval")
    ] = None
    r"""Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
    Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
    See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
    """

    metadata: Optional[List[ItemsTypeMetadata]] = None
    r"""Fields to add to events from this input"""

    kafka_schema_registry: Annotated[
        Optional[KafkaSchemaRegistryAuthenticationType],
        pydantic.Field(alias="kafkaSchemaRegistry"),
    ] = None

    connection_timeout: Annotated[
        Optional[float], pydantic.Field(alias="connectionTimeout")
    ] = None
    r"""Maximum time to wait for a connection to complete successfully"""

    request_timeout: Annotated[
        Optional[float], pydantic.Field(alias="requestTimeout")
    ] = None
    r"""Maximum time to wait for Kafka to respond to a request"""

    max_retries: Annotated[Optional[float], pydantic.Field(alias="maxRetries")] = None
    r"""If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data"""

    max_back_off: Annotated[Optional[float], pydantic.Field(alias="maxBackOff")] = None
    r"""The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds)."""

    initial_backoff: Annotated[
        Optional[float], pydantic.Field(alias="initialBackoff")
    ] = None
    r"""Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes)."""

    backoff_rate: Annotated[Optional[float], pydantic.Field(alias="backoffRate")] = None
    r"""Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details."""

    authentication_timeout: Annotated[
        Optional[float], pydantic.Field(alias="authenticationTimeout")
    ] = None
    r"""Maximum time to wait for Kafka to respond to an authentication request"""

    reauthentication_threshold: Annotated[
        Optional[float], pydantic.Field(alias="reauthenticationThreshold")
    ] = None
    r"""Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint."""

    signature_version: Annotated[
        Optional[SignatureVersionOptions], pydantic.Field(alias="signatureVersion")
    ] = None
    r"""Signature version to use for signing MSK cluster requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = None
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = None
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = None
    r"""Use Assume Role credentials to access MSK"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = None
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    tls: Optional[TLSSettingsClientSideTypeKafkaSchemaRegistry] = None

    auto_commit_interval: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitInterval")
    ] = None
    r"""How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    auto_commit_threshold: Annotated[
        Optional[float], pydantic.Field(alias="autoCommitThreshold")
    ] = None
    r"""How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch."""

    max_bytes_per_partition: Annotated[
        Optional[float], pydantic.Field(alias="maxBytesPerPartition")
    ] = None
    r"""Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB)."""

    max_bytes: Annotated[Optional[float], pydantic.Field(alias="maxBytes")] = None
    r"""Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB)."""

    max_socket_errors: Annotated[
        Optional[float], pydantic.Field(alias="maxSocketErrors")
    ] = None
    r"""Maximum number of network errors before the consumer re-creates a socket"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    template_aws_secret_key: Annotated[
        Optional[str], pydantic.Field(alias="__template_awsSecretKey")
    ] = None
    r"""Binds 'awsSecretKey' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'awsSecretKey' at runtime."""

    template_region: Annotated[
        Optional[str], pydantic.Field(alias="__template_region")
    ] = None
    r"""Binds 'region' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'region' at runtime."""

    template_assume_role_arn: Annotated[
        Optional[str], pydantic.Field(alias="__template_assumeRoleArn")
    ] = None
    r"""Binds 'assumeRoleArn' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'assumeRoleArn' at runtime."""

    template_assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="__template_assumeRoleExternalId")
    ] = None
    r"""Binds 'assumeRoleExternalId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'assumeRoleExternalId' at runtime."""

    template_aws_api_key: Annotated[
        Optional[str], pydantic.Field(alias="__template_awsApiKey")
    ] = None
    r"""Binds 'awsApiKey' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'awsApiKey' at runtime."""

    @field_serializer("aws_authentication_method")
    def serialize_aws_authentication_method(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptionsS3CollectorConf(value)
            except ValueError:
                return value
        return value

    @field_serializer("signature_version")
    def serialize_signature_version(self, value):
        if isinstance(value, str):
            try:
                return models.SignatureVersionOptions(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "id",
                "disabled",
                "pipeline",
                "sendToRoutes",
                "environment",
                "pqEnabled",
                "streamtags",
                "connections",
                "pq",
                "groupId",
                "fromBeginning",
                "sessionTimeout",
                "rebalanceTimeout",
                "heartbeatInterval",
                "metadata",
                "kafkaSchemaRegistry",
                "connectionTimeout",
                "requestTimeout",
                "maxRetries",
                "maxBackOff",
                "initialBackoff",
                "backoffRate",
                "authenticationTimeout",
                "reauthenticationThreshold",
                "awsSecretKey",
                "endpoint",
                "signatureVersion",
                "reuseConnections",
                "rejectUnauthorized",
                "enableAssumeRole",
                "assumeRoleArn",
                "assumeRoleExternalId",
                "durationSeconds",
                "tls",
                "autoCommitInterval",
                "autoCommitThreshold",
                "maxBytesPerPartition",
                "maxBytes",
                "maxSocketErrors",
                "description",
                "awsApiKey",
                "awsSecret",
                "__template_awsSecretKey",
                "__template_region",
                "__template_assumeRoleArn",
                "__template_assumeRoleExternalId",
                "__template_awsApiKey",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
