"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .certificatetypeazureblobauthtypeclientcert import (
    CertificateTypeAzureBlobAuthTypeClientCert,
    CertificateTypeAzureBlobAuthTypeClientCertTypedDict,
)
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
from cribl_control_plane.utils import get_discriminator
from enum import Enum
import pydantic
from pydantic import Discriminator, Tag, field_serializer, model_serializer
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class AzureBlobAuthTypeClientCertAuthenticationMethod(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Enter authentication data directly, or select a secret referencing your auth data"""

    MANUAL = "manual"
    SECRET = "secret"
    CLIENT_SECRET = "clientSecret"
    CLIENT_CERT = "clientCert"


class AzureBlobAuthTypeClientCertExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""A JavaScript expression that accesses a corresponding <token> through the value variable and evaluates the token to populate event fields. Example: {date: new Date(+value*1000)}"""


class AzureBlobAuthTypeClientCertExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""A JavaScript expression that accesses a corresponding <token> through the value variable and evaluates the token to populate event fields. Example: {date: new Date(+value*1000)}"""


class AzureBlobAuthTypeClientCertTypedDict(TypedDict):
    storage_account_name: str
    r"""The name of your Azure storage account"""
    tenant_id: str
    r"""The service principal's tenant ID"""
    client_id: str
    r"""The service principal's client ID"""
    certificate: CertificateTypeAzureBlobAuthTypeClientCertTypedDict
    container_name: str
    r"""Container to collect from. This value can be a constant, or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: myBucket-${C.vars.myVar}"""
    auth_type: NotRequired[AzureBlobAuthTypeClientCertAuthenticationMethod]
    r"""Enter authentication data directly, or select a secret referencing your auth data"""
    azure_cloud: NotRequired[str]
    r"""The Azure cloud to use. Defaults to Azure Public Cloud."""
    endpoint_suffix: NotRequired[str]
    r"""The endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net."""
    output_name: NotRequired[str]
    r"""An optional predefined Destination that will be used to auto-populate Collector settings"""
    path: NotRequired[str]
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""
    extractors: NotRequired[List[AzureBlobAuthTypeClientCertExtractorTypedDict]]
    r"""Extractors allow use of template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)} will enrich discovery results with a human-readable \"date\" field."""
    recurse: NotRequired[bool]
    r"""Recurse through subdirectories"""
    include_metadata: NotRequired[bool]
    r"""Include Azure Blob metadata in collected events. In each event, metadata will be located at: __collectible.metadata."""
    include_tags: NotRequired[bool]
    r"""Include Azure Blob tags in collected events. In each event, tags will be located at: __collectible.tags. Disable this feature when using a Shared Access Signature Connection String, to prevent errors."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""


class AzureBlobAuthTypeClientCert(BaseModel):
    storage_account_name: Annotated[str, pydantic.Field(alias="storageAccountName")]
    r"""The name of your Azure storage account"""

    tenant_id: Annotated[str, pydantic.Field(alias="tenantId")]
    r"""The service principal's tenant ID"""

    client_id: Annotated[str, pydantic.Field(alias="clientId")]
    r"""The service principal's client ID"""

    certificate: CertificateTypeAzureBlobAuthTypeClientCert

    container_name: Annotated[str, pydantic.Field(alias="containerName")]
    r"""Container to collect from. This value can be a constant, or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: myBucket-${C.vars.myVar}"""

    auth_type: Annotated[
        Optional[AzureBlobAuthTypeClientCertAuthenticationMethod],
        pydantic.Field(alias="authType"),
    ] = None
    r"""Enter authentication data directly, or select a secret referencing your auth data"""

    azure_cloud: Annotated[Optional[str], pydantic.Field(alias="azureCloud")] = None
    r"""The Azure cloud to use. Defaults to Azure Public Cloud."""

    endpoint_suffix: Annotated[
        Optional[str], pydantic.Field(alias="endpointSuffix")
    ] = None
    r"""The endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""An optional predefined Destination that will be used to auto-populate Collector settings"""

    path: Optional[str] = None
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""

    extractors: Optional[List[AzureBlobAuthTypeClientCertExtractor]] = None
    r"""Extractors allow use of template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)} will enrich discovery results with a human-readable \"date\" field."""

    recurse: Optional[bool] = None
    r"""Recurse through subdirectories"""

    include_metadata: Annotated[
        Optional[bool], pydantic.Field(alias="includeMetadata")
    ] = None
    r"""Include Azure Blob metadata in collected events. In each event, metadata will be located at: __collectible.metadata."""

    include_tags: Annotated[Optional[bool], pydantic.Field(alias="includeTags")] = None
    r"""Include Azure Blob tags in collected events. In each event, tags will be located at: __collectible.tags. Disable this feature when using a Shared Access Signature Connection String, to prevent errors."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        None
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = None
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = None
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AzureBlobAuthTypeClientCertAuthenticationMethod(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "authType",
                "azureCloud",
                "endpointSuffix",
                "outputName",
                "path",
                "extractors",
                "recurse",
                "includeMetadata",
                "includeTags",
                "maxBatchSize",
                "parquetChunkSizeMB",
                "parquetChunkDownloadTimeout",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class AzureBlobAuthTypeClientSecretAuthenticationMethod(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Enter authentication data directly, or select a secret referencing your auth data"""

    MANUAL = "manual"
    SECRET = "secret"
    CLIENT_SECRET = "clientSecret"
    CLIENT_CERT = "clientCert"


class AzureBlobAuthTypeClientSecretExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""A JavaScript expression that accesses a corresponding <token> through the value variable and evaluates the token to populate event fields. Example: {date: new Date(+value*1000)}"""


class AzureBlobAuthTypeClientSecretExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""A JavaScript expression that accesses a corresponding <token> through the value variable and evaluates the token to populate event fields. Example: {date: new Date(+value*1000)}"""


class AzureBlobAuthTypeClientSecretTypedDict(TypedDict):
    storage_account_name: str
    r"""The name of your Azure storage account"""
    tenant_id: str
    r"""The service principal's tenant ID"""
    client_id: str
    r"""The service principal's client ID"""
    client_text_secret: str
    r"""Text secret containing the client secret"""
    container_name: str
    r"""Container to collect from. This value can be a constant, or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: myBucket-${C.vars.myVar}"""
    auth_type: NotRequired[AzureBlobAuthTypeClientSecretAuthenticationMethod]
    r"""Enter authentication data directly, or select a secret referencing your auth data"""
    endpoint_suffix: NotRequired[str]
    r"""The endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net."""
    azure_cloud: NotRequired[str]
    r"""The Azure cloud to use. Defaults to Azure Public Cloud."""
    output_name: NotRequired[str]
    r"""An optional predefined Destination that will be used to auto-populate Collector settings"""
    path: NotRequired[str]
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""
    extractors: NotRequired[List[AzureBlobAuthTypeClientSecretExtractorTypedDict]]
    r"""Extractors allow use of template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)} will enrich discovery results with a human-readable \"date\" field."""
    recurse: NotRequired[bool]
    r"""Recurse through subdirectories"""
    include_metadata: NotRequired[bool]
    r"""Include Azure Blob metadata in collected events. In each event, metadata will be located at: __collectible.metadata."""
    include_tags: NotRequired[bool]
    r"""Include Azure Blob tags in collected events. In each event, tags will be located at: __collectible.tags. Disable this feature when using a Shared Access Signature Connection String, to prevent errors."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""


class AzureBlobAuthTypeClientSecret(BaseModel):
    storage_account_name: Annotated[str, pydantic.Field(alias="storageAccountName")]
    r"""The name of your Azure storage account"""

    tenant_id: Annotated[str, pydantic.Field(alias="tenantId")]
    r"""The service principal's tenant ID"""

    client_id: Annotated[str, pydantic.Field(alias="clientId")]
    r"""The service principal's client ID"""

    client_text_secret: Annotated[str, pydantic.Field(alias="clientTextSecret")]
    r"""Text secret containing the client secret"""

    container_name: Annotated[str, pydantic.Field(alias="containerName")]
    r"""Container to collect from. This value can be a constant, or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: myBucket-${C.vars.myVar}"""

    auth_type: Annotated[
        Optional[AzureBlobAuthTypeClientSecretAuthenticationMethod],
        pydantic.Field(alias="authType"),
    ] = None
    r"""Enter authentication data directly, or select a secret referencing your auth data"""

    endpoint_suffix: Annotated[
        Optional[str], pydantic.Field(alias="endpointSuffix")
    ] = None
    r"""The endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net."""

    azure_cloud: Annotated[Optional[str], pydantic.Field(alias="azureCloud")] = None
    r"""The Azure cloud to use. Defaults to Azure Public Cloud."""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""An optional predefined Destination that will be used to auto-populate Collector settings"""

    path: Optional[str] = None
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""

    extractors: Optional[List[AzureBlobAuthTypeClientSecretExtractor]] = None
    r"""Extractors allow use of template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)} will enrich discovery results with a human-readable \"date\" field."""

    recurse: Optional[bool] = None
    r"""Recurse through subdirectories"""

    include_metadata: Annotated[
        Optional[bool], pydantic.Field(alias="includeMetadata")
    ] = None
    r"""Include Azure Blob metadata in collected events. In each event, metadata will be located at: __collectible.metadata."""

    include_tags: Annotated[Optional[bool], pydantic.Field(alias="includeTags")] = None
    r"""Include Azure Blob tags in collected events. In each event, tags will be located at: __collectible.tags. Disable this feature when using a Shared Access Signature Connection String, to prevent errors."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        None
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = None
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = None
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AzureBlobAuthTypeClientSecretAuthenticationMethod(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "authType",
                "endpointSuffix",
                "azureCloud",
                "outputName",
                "path",
                "extractors",
                "recurse",
                "includeMetadata",
                "includeTags",
                "maxBatchSize",
                "parquetChunkSizeMB",
                "parquetChunkDownloadTimeout",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class AzureBlobAuthTypeSecretAuthenticationMethod(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Enter authentication data directly, or select a secret referencing your auth data"""

    MANUAL = "manual"
    SECRET = "secret"
    CLIENT_SECRET = "clientSecret"
    CLIENT_CERT = "clientCert"


class AzureBlobAuthTypeSecretExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""A JavaScript expression that accesses a corresponding <token> through the value variable and evaluates the token to populate event fields. Example: {date: new Date(+value*1000)}"""


class AzureBlobAuthTypeSecretExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""A JavaScript expression that accesses a corresponding <token> through the value variable and evaluates the token to populate event fields. Example: {date: new Date(+value*1000)}"""


class AzureBlobAuthTypeSecretTypedDict(TypedDict):
    text_secret: str
    r"""Text secret"""
    container_name: str
    r"""Container to collect from. This value can be a constant, or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: myBucket-${C.vars.myVar}"""
    auth_type: NotRequired[AzureBlobAuthTypeSecretAuthenticationMethod]
    r"""Enter authentication data directly, or select a secret referencing your auth data"""
    output_name: NotRequired[str]
    r"""An optional predefined Destination that will be used to auto-populate Collector settings"""
    path: NotRequired[str]
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""
    extractors: NotRequired[List[AzureBlobAuthTypeSecretExtractorTypedDict]]
    r"""Extractors allow use of template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)} will enrich discovery results with a human-readable \"date\" field."""
    recurse: NotRequired[bool]
    r"""Recurse through subdirectories"""
    include_metadata: NotRequired[bool]
    r"""Include Azure Blob metadata in collected events. In each event, metadata will be located at: __collectible.metadata."""
    include_tags: NotRequired[bool]
    r"""Include Azure Blob tags in collected events. In each event, tags will be located at: __collectible.tags. Disable this feature when using a Shared Access Signature Connection String, to prevent errors."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""


class AzureBlobAuthTypeSecret(BaseModel):
    text_secret: Annotated[str, pydantic.Field(alias="textSecret")]
    r"""Text secret"""

    container_name: Annotated[str, pydantic.Field(alias="containerName")]
    r"""Container to collect from. This value can be a constant, or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: myBucket-${C.vars.myVar}"""

    auth_type: Annotated[
        Optional[AzureBlobAuthTypeSecretAuthenticationMethod],
        pydantic.Field(alias="authType"),
    ] = None
    r"""Enter authentication data directly, or select a secret referencing your auth data"""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""An optional predefined Destination that will be used to auto-populate Collector settings"""

    path: Optional[str] = None
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""

    extractors: Optional[List[AzureBlobAuthTypeSecretExtractor]] = None
    r"""Extractors allow use of template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)} will enrich discovery results with a human-readable \"date\" field."""

    recurse: Optional[bool] = None
    r"""Recurse through subdirectories"""

    include_metadata: Annotated[
        Optional[bool], pydantic.Field(alias="includeMetadata")
    ] = None
    r"""Include Azure Blob metadata in collected events. In each event, metadata will be located at: __collectible.metadata."""

    include_tags: Annotated[Optional[bool], pydantic.Field(alias="includeTags")] = None
    r"""Include Azure Blob tags in collected events. In each event, tags will be located at: __collectible.tags. Disable this feature when using a Shared Access Signature Connection String, to prevent errors."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        None
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = None
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = None
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AzureBlobAuthTypeSecretAuthenticationMethod(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "authType",
                "outputName",
                "path",
                "extractors",
                "recurse",
                "includeMetadata",
                "includeTags",
                "maxBatchSize",
                "parquetChunkSizeMB",
                "parquetChunkDownloadTimeout",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


class AzureBlobAuthTypeManualAuthenticationMethod(
    str, Enum, metaclass=utils.OpenEnumMeta
):
    r"""Enter authentication data directly, or select a secret referencing your auth data"""

    MANUAL = "manual"
    SECRET = "secret"
    CLIENT_SECRET = "clientSecret"
    CLIENT_CERT = "clientCert"


class AzureBlobAuthTypeManualExtractorTypedDict(TypedDict):
    key: str
    r"""A token from the template path, such as epoch"""
    expression: str
    r"""A JavaScript expression that accesses a corresponding <token> through the value variable and evaluates the token to populate event fields. Example: {date: new Date(+value*1000)}"""


class AzureBlobAuthTypeManualExtractor(BaseModel):
    key: str
    r"""A token from the template path, such as epoch"""

    expression: str
    r"""A JavaScript expression that accesses a corresponding <token> through the value variable and evaluates the token to populate event fields. Example: {date: new Date(+value*1000)}"""


class AzureBlobAuthTypeManualTypedDict(TypedDict):
    connection_string: str
    r"""Enter your Azure storage account Connection String. If left blank, Cribl Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING."""
    container_name: str
    r"""Container to collect from. This value can be a constant, or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: myBucket-${C.vars.myVar}"""
    auth_type: NotRequired[AzureBlobAuthTypeManualAuthenticationMethod]
    r"""Enter authentication data directly, or select a secret referencing your auth data"""
    output_name: NotRequired[str]
    r"""An optional predefined Destination that will be used to auto-populate Collector settings"""
    path: NotRequired[str]
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""
    extractors: NotRequired[List[AzureBlobAuthTypeManualExtractorTypedDict]]
    r"""Extractors allow use of template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)} will enrich discovery results with a human-readable \"date\" field."""
    recurse: NotRequired[bool]
    r"""Recurse through subdirectories"""
    include_metadata: NotRequired[bool]
    r"""Include Azure Blob metadata in collected events. In each event, metadata will be located at: __collectible.metadata."""
    include_tags: NotRequired[bool]
    r"""Include Azure Blob tags in collected events. In each event, tags will be located at: __collectible.tags. Disable this feature when using a Shared Access Signature Connection String, to prevent errors."""
    max_batch_size: NotRequired[float]
    r"""Maximum number of metadata objects to batch before recording as results"""
    parquet_chunk_size_mb: NotRequired[float]
    r"""Maximum file size for each Parquet chunk"""
    parquet_chunk_download_timeout: NotRequired[float]
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""


class AzureBlobAuthTypeManual(BaseModel):
    connection_string: Annotated[str, pydantic.Field(alias="connectionString")]
    r"""Enter your Azure storage account Connection String. If left blank, Cribl Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING."""

    container_name: Annotated[str, pydantic.Field(alias="containerName")]
    r"""Container to collect from. This value can be a constant, or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: myBucket-${C.vars.myVar}"""

    auth_type: Annotated[
        Optional[AzureBlobAuthTypeManualAuthenticationMethod],
        pydantic.Field(alias="authType"),
    ] = None
    r"""Enter authentication data directly, or select a secret referencing your auth data"""

    output_name: Annotated[Optional[str], pydantic.Field(alias="outputName")] = None
    r"""An optional predefined Destination that will be used to auto-populate Collector settings"""

    path: Optional[str] = None
    r"""The directory from which to collect data. Templating is supported, such as myDir/${datacenter}/${host}/${app}/. Time-based tokens are supported, such as myOtherDir/${_time:%Y}/${_time:%m}/${_time:%d}/."""

    extractors: Optional[List[AzureBlobAuthTypeManualExtractor]] = None
    r"""Extractors allow use of template tokens as context for expressions that enrich discovery results. For example, given a template /path/${epoch}, an extractor under key \"epoch\" with an expression {date: new Date(+value*1000)} will enrich discovery results with a human-readable \"date\" field."""

    recurse: Optional[bool] = None
    r"""Recurse through subdirectories"""

    include_metadata: Annotated[
        Optional[bool], pydantic.Field(alias="includeMetadata")
    ] = None
    r"""Include Azure Blob metadata in collected events. In each event, metadata will be located at: __collectible.metadata."""

    include_tags: Annotated[Optional[bool], pydantic.Field(alias="includeTags")] = None
    r"""Include Azure Blob tags in collected events. In each event, tags will be located at: __collectible.tags. Disable this feature when using a Shared Access Signature Connection String, to prevent errors."""

    max_batch_size: Annotated[Optional[float], pydantic.Field(alias="maxBatchSize")] = (
        None
    )
    r"""Maximum number of metadata objects to batch before recording as results"""

    parquet_chunk_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkSizeMB")
    ] = None
    r"""Maximum file size for each Parquet chunk"""

    parquet_chunk_download_timeout: Annotated[
        Optional[float], pydantic.Field(alias="parquetChunkDownloadTimeout")
    ] = None
    r"""The maximum time allowed for downloading a Parquet chunk. Processing will abort if a chunk cannot be downloaded within the time specified."""

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AzureBlobAuthTypeManualAuthenticationMethod(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "authType",
                "outputName",
                "path",
                "extractors",
                "recurse",
                "includeMetadata",
                "includeTags",
                "maxBatchSize",
                "parquetChunkSizeMB",
                "parquetChunkDownloadTimeout",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m


AzureBlobCollectorConfTypedDict = TypeAliasType(
    "AzureBlobCollectorConfTypedDict",
    Union[
        AzureBlobAuthTypeManualTypedDict,
        AzureBlobAuthTypeSecretTypedDict,
        AzureBlobAuthTypeClientSecretTypedDict,
        AzureBlobAuthTypeClientCertTypedDict,
    ],
)


AzureBlobCollectorConf = Annotated[
    Union[
        Annotated[AzureBlobAuthTypeManual, Tag("manual")],
        Annotated[AzureBlobAuthTypeSecret, Tag("secret")],
        Annotated[AzureBlobAuthTypeClientSecret, Tag("clientSecret")],
        Annotated[AzureBlobAuthTypeClientCert, Tag("clientCert")],
    ],
    Discriminator(lambda m: get_discriminator(m, "auth_type", "authType")),
]
