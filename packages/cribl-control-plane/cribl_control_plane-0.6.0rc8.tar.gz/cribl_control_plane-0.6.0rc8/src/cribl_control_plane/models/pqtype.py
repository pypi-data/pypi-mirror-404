"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .compressionoptionspq import CompressionOptionsPq
from .modeoptionspq import ModeOptionsPq
from cribl_control_plane import models
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
import pydantic
from pydantic import field_serializer, model_serializer
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class PqTypePqControlsTypedDict(TypedDict):
    pass


class PqTypePqControls(BaseModel):
    pass


class PqTypeTypedDict(TypedDict):
    mode: NotRequired[ModeOptionsPq]
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""
    max_buffer_size: NotRequired[float]
    r"""The maximum number of events to hold in memory before writing the events to disk"""
    commit_frequency: NotRequired[float]
    r"""The number of events to send downstream before committing that Stream has read them"""
    max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""
    max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""
    compress: NotRequired[CompressionOptionsPq]
    r"""Codec to use to compress the persisted data"""
    pq_controls: NotRequired[PqTypePqControlsTypedDict]


class PqType(BaseModel):
    mode: Optional[ModeOptionsPq] = None
    r"""With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine."""

    max_buffer_size: Annotated[
        Optional[float], pydantic.Field(alias="maxBufferSize")
    ] = None
    r"""The maximum number of events to hold in memory before writing the events to disk"""

    commit_frequency: Annotated[
        Optional[float], pydantic.Field(alias="commitFrequency")
    ] = None
    r"""The number of events to send downstream before committing that Stream has read them"""

    max_file_size: Annotated[Optional[str], pydantic.Field(alias="maxFileSize")] = None
    r"""The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc."""

    max_size: Annotated[Optional[str], pydantic.Field(alias="maxSize")] = None
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    path: Optional[str] = None
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>"""

    compress: Optional[CompressionOptionsPq] = None
    r"""Codec to use to compress the persisted data"""

    pq_controls: Annotated[
        Optional[PqTypePqControls], pydantic.Field(alias="pqControls")
    ] = None

    @field_serializer("mode")
    def serialize_mode(self, value):
        if isinstance(value, str):
            try:
                return models.ModeOptionsPq(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionOptionsPq(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "mode",
                "maxBufferSize",
                "commitFrequency",
                "maxFileSize",
                "maxSize",
                "path",
                "compress",
                "pqControls",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
