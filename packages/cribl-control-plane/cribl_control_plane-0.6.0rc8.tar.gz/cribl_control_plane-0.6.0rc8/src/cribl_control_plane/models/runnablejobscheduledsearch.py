"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .jobtypeoptionsrunnablejobcollection import JobTypeOptionsRunnableJobCollection
from .scheduletyperunnablejobcollection import (
    ScheduleTypeRunnableJobCollection,
    ScheduleTypeRunnableJobCollectionTypedDict,
)
from cribl_control_plane import models
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
import pydantic
from pydantic import field_serializer, model_serializer
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class RunnableJobScheduledSearchTypedDict(TypedDict):
    type: JobTypeOptionsRunnableJobCollection
    saved_query_id: str
    r"""Identifies which search query to run"""
    id: NotRequired[str]
    r"""Unique ID for this Job"""
    description: NotRequired[str]
    ttl: NotRequired[str]
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""
    ignore_group_jobs_limit: NotRequired[bool]
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""
    remove_fields: NotRequired[List[str]]
    r"""List of fields to remove from Discover results. Wildcards (for example, aws*) are allowed. This is useful when discovery returns sensitive fields that should not be exposed in the Jobs user interface."""
    resume_on_boot: NotRequired[bool]
    r"""Resume the ad hoc job if a failure condition causes Stream to restart during job execution"""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    schedule: NotRequired[ScheduleTypeRunnableJobCollectionTypedDict]
    r"""Configuration for a scheduled job"""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""


class RunnableJobScheduledSearch(BaseModel):
    type: JobTypeOptionsRunnableJobCollection

    saved_query_id: Annotated[str, pydantic.Field(alias="savedQueryId")]
    r"""Identifies which search query to run"""

    id: Optional[str] = None
    r"""Unique ID for this Job"""

    description: Optional[str] = None

    ttl: Optional[str] = None
    r"""Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector."""

    ignore_group_jobs_limit: Annotated[
        Optional[bool], pydantic.Field(alias="ignoreGroupJobsLimit")
    ] = None
    r"""When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live."""

    remove_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="removeFields")
    ] = None
    r"""List of fields to remove from Discover results. Wildcards (for example, aws*) are allowed. This is useful when discovery returns sensitive fields that should not be exposed in the Jobs user interface."""

    resume_on_boot: Annotated[Optional[bool], pydantic.Field(alias="resumeOnBoot")] = (
        None
    )
    r"""Resume the ad hoc job if a failure condition causes Stream to restart during job execution"""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    schedule: Optional[ScheduleTypeRunnableJobCollection] = None
    r"""Configuration for a scheduled job"""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    @field_serializer("type")
    def serialize_type(self, value):
        if isinstance(value, str):
            try:
                return models.JobTypeOptionsRunnableJobCollection(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "id",
                "description",
                "ttl",
                "ignoreGroupJobsLimit",
                "removeFields",
                "resumeOnBoot",
                "environment",
                "schedule",
                "streamtags",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
