"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .authenticationmethodoptions import AuthenticationMethodOptions
from .backpressurebehavioroptions1 import BackpressureBehaviorOptions1
from .certificatetypeazureblobauthtypeclientcert import (
    CertificateTypeAzureBlobAuthTypeClientCert,
    CertificateTypeAzureBlobAuthTypeClientCertTypedDict,
)
from .compressionleveloptions import CompressionLevelOptions
from .compressionoptions2 import CompressionOptions2
from .dataformatoptions import DataFormatOptions
from .datapageversionoptions import DataPageVersionOptions
from .diskspaceprotectionoptions import DiskSpaceProtectionOptions
from .itemstypekeyvaluemetadata import (
    ItemsTypeKeyValueMetadata,
    ItemsTypeKeyValueMetadataTypedDict,
)
from .parquetversionoptions import ParquetVersionOptions
from .retrysettingstype import RetrySettingsType, RetrySettingsTypeTypedDict
from cribl_control_plane import models, utils
from cribl_control_plane.types import BaseModel, UNSET_SENTINEL
from enum import Enum
import pydantic
from pydantic import field_serializer, model_serializer
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class OutputAzureBlobType(str, Enum):
    AZURE_BLOB = "azure_blob"


class BlobAccessTier(str, Enum, metaclass=utils.OpenEnumMeta):
    # Default account access tier
    INFERRED = "Inferred"
    # Hot tier
    HOT = "Hot"
    # Cool tier
    COOL = "Cool"
    # Cold tier
    COLD = "Cold"
    # Archive tier
    ARCHIVE = "Archive"


class OutputAzureBlobTypedDict(TypedDict):
    type: OutputAzureBlobType
    container_name: str
    r"""The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env[\"CRIBL_WORKER_ID\"]}`."""
    stage_path: str
    r"""Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage."""
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    create_container: NotRequired[bool]
    r"""Create the configured container in Azure Blob Storage if it does not already exist"""
    dest_path: NotRequired[str]
    r"""Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env[\"CRIBL_WORKER_ID\"]}`."""
    add_id_to_stage_path: NotRequired[bool]
    r"""Add the Output ID value to staging location"""
    max_concurrent_file_parts: NotRequired[float]
    r"""Maximum number of parts to upload in parallel per file"""
    remove_empty_dirs: NotRequired[bool]
    r"""Remove empty staging directories after moving files"""
    partition_expr: NotRequired[str]
    r"""JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory."""
    format_: NotRequired[DataFormatOptions]
    r"""Format of the output data"""
    base_file_name: NotRequired[str]
    r"""JavaScript expression to define the output filename prefix (can be constant)"""
    file_name_suffix: NotRequired[str]
    r"""JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`)."""
    max_file_size_mb: NotRequired[float]
    r"""Maximum uncompressed output file size. Files of this size will be closed and moved to final output location."""
    max_file_open_time_sec: NotRequired[float]
    r"""Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location."""
    max_file_idle_time_sec: NotRequired[float]
    r"""Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location."""
    max_open_files: NotRequired[float]
    r"""Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location."""
    header_line: NotRequired[str]
    r"""If set, this line will be written to the beginning of each output file"""
    write_high_water_mark: NotRequired[float]
    r"""Buffer size used to write to a file"""
    on_backpressure: NotRequired[BackpressureBehaviorOptions1]
    r"""How to handle events when all receivers are exerting backpressure"""
    deadletter_enabled: NotRequired[bool]
    r"""If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors"""
    on_disk_full_backpressure: NotRequired[DiskSpaceProtectionOptions]
    r"""How to handle events when disk space is below the global 'Min free disk space' limit"""
    force_close_on_shutdown: NotRequired[bool]
    r"""Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss."""
    retry_settings: NotRequired[RetrySettingsTypeTypedDict]
    auth_type: NotRequired[AuthenticationMethodOptions]
    storage_class: NotRequired[BlobAccessTier]
    description: NotRequired[str]
    compress: NotRequired[CompressionOptions2]
    r"""Data compression format to apply to HTTP content before it is delivered"""
    compression_level: NotRequired[CompressionLevelOptions]
    r"""Compression level to apply before moving files to final destination"""
    automatic_schema: NotRequired[bool]
    r"""Automatically calculate the schema based on the events of each Parquet file generated"""
    parquet_schema: NotRequired[str]
    r"""To add a new schema, navigate to Processing > Knowledge > Parquet Schemas"""
    parquet_version: NotRequired[ParquetVersionOptions]
    r"""Determines which data types are supported and how they are represented"""
    parquet_data_page_version: NotRequired[DataPageVersionOptions]
    r"""Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it."""
    parquet_row_group_length: NotRequired[float]
    r"""The number of rows that every group will contain. The final group can contain a smaller number of rows."""
    parquet_page_size: NotRequired[str]
    r"""Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression."""
    should_log_invalid_rows: NotRequired[bool]
    r"""Log up to 3 rows that @{product} skips due to data mismatch"""
    key_value_metadata: NotRequired[List[ItemsTypeKeyValueMetadataTypedDict]]
    r"""The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: \"key\":\"OCSF Event Class\", \"value\":\"9001\" """
    enable_statistics: NotRequired[bool]
    r"""Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics."""
    enable_write_page_index: NotRequired[bool]
    r"""One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping."""
    enable_page_checksum: NotRequired[bool]
    r"""Parquet tools can use the checksum of a Parquet page to verify data integrity"""
    empty_dir_cleanup_sec: NotRequired[float]
    r"""How frequently, in seconds, to clean up empty directories"""
    directory_batch_size: NotRequired[float]
    r"""Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory."""
    deadletter_path: NotRequired[str]
    r"""Storage location for files that fail to reach their final destination after maximum retries are exceeded"""
    max_retry_num: NotRequired[float]
    r"""The maximum number of times a file will attempt to move to its final destination before being dead-lettered"""
    connection_string: NotRequired[str]
    r"""Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING."""
    text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    storage_account_name: NotRequired[str]
    r"""The name of your Azure storage account"""
    tenant_id: NotRequired[str]
    r"""The service principal's tenant ID"""
    client_id: NotRequired[str]
    r"""The service principal's client ID"""
    azure_cloud: NotRequired[str]
    r"""The Azure cloud to use. Defaults to Azure Public Cloud."""
    endpoint_suffix: NotRequired[str]
    r"""Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net."""
    client_text_secret: NotRequired[str]
    r"""Select or create a stored text secret"""
    certificate: NotRequired[CertificateTypeAzureBlobAuthTypeClientCertTypedDict]
    template_container_name: NotRequired[str]
    r"""Binds 'containerName' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'containerName' at runtime."""
    template_format: NotRequired[str]
    r"""Binds 'format' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'format' at runtime."""
    template_connection_string: NotRequired[str]
    r"""Binds 'connectionString' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'connectionString' at runtime."""
    template_tenant_id: NotRequired[str]
    r"""Binds 'tenantId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'tenantId' at runtime."""
    template_client_id: NotRequired[str]
    r"""Binds 'clientId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'clientId' at runtime."""


class OutputAzureBlob(BaseModel):
    type: OutputAzureBlobType

    container_name: Annotated[str, pydantic.Field(alias="containerName")]
    r"""The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env[\"CRIBL_WORKER_ID\"]}`."""

    stage_path: Annotated[str, pydantic.Field(alias="stagePath")]
    r"""Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage."""

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    create_container: Annotated[
        Optional[bool], pydantic.Field(alias="createContainer")
    ] = None
    r"""Create the configured container in Azure Blob Storage if it does not already exist"""

    dest_path: Annotated[Optional[str], pydantic.Field(alias="destPath")] = None
    r"""Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env[\"CRIBL_WORKER_ID\"]}`."""

    add_id_to_stage_path: Annotated[
        Optional[bool], pydantic.Field(alias="addIdToStagePath")
    ] = None
    r"""Add the Output ID value to staging location"""

    max_concurrent_file_parts: Annotated[
        Optional[float], pydantic.Field(alias="maxConcurrentFileParts")
    ] = None
    r"""Maximum number of parts to upload in parallel per file"""

    remove_empty_dirs: Annotated[
        Optional[bool], pydantic.Field(alias="removeEmptyDirs")
    ] = None
    r"""Remove empty staging directories after moving files"""

    partition_expr: Annotated[Optional[str], pydantic.Field(alias="partitionExpr")] = (
        None
    )
    r"""JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory."""

    format_: Annotated[Optional[DataFormatOptions], pydantic.Field(alias="format")] = (
        None
    )
    r"""Format of the output data"""

    base_file_name: Annotated[Optional[str], pydantic.Field(alias="baseFileName")] = (
        None
    )
    r"""JavaScript expression to define the output filename prefix (can be constant)"""

    file_name_suffix: Annotated[
        Optional[str], pydantic.Field(alias="fileNameSuffix")
    ] = None
    r"""JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`)."""

    max_file_size_mb: Annotated[
        Optional[float], pydantic.Field(alias="maxFileSizeMB")
    ] = None
    r"""Maximum uncompressed output file size. Files of this size will be closed and moved to final output location."""

    max_file_open_time_sec: Annotated[
        Optional[float], pydantic.Field(alias="maxFileOpenTimeSec")
    ] = None
    r"""Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location."""

    max_file_idle_time_sec: Annotated[
        Optional[float], pydantic.Field(alias="maxFileIdleTimeSec")
    ] = None
    r"""Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location."""

    max_open_files: Annotated[Optional[float], pydantic.Field(alias="maxOpenFiles")] = (
        None
    )
    r"""Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location."""

    header_line: Annotated[Optional[str], pydantic.Field(alias="headerLine")] = None
    r"""If set, this line will be written to the beginning of each output file"""

    write_high_water_mark: Annotated[
        Optional[float], pydantic.Field(alias="writeHighWaterMark")
    ] = None
    r"""Buffer size used to write to a file"""

    on_backpressure: Annotated[
        Optional[BackpressureBehaviorOptions1], pydantic.Field(alias="onBackpressure")
    ] = None
    r"""How to handle events when all receivers are exerting backpressure"""

    deadletter_enabled: Annotated[
        Optional[bool], pydantic.Field(alias="deadletterEnabled")
    ] = None
    r"""If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors"""

    on_disk_full_backpressure: Annotated[
        Optional[DiskSpaceProtectionOptions],
        pydantic.Field(alias="onDiskFullBackpressure"),
    ] = None
    r"""How to handle events when disk space is below the global 'Min free disk space' limit"""

    force_close_on_shutdown: Annotated[
        Optional[bool], pydantic.Field(alias="forceCloseOnShutdown")
    ] = None
    r"""Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss."""

    retry_settings: Annotated[
        Optional[RetrySettingsType], pydantic.Field(alias="retrySettings")
    ] = None

    auth_type: Annotated[
        Optional[AuthenticationMethodOptions], pydantic.Field(alias="authType")
    ] = None

    storage_class: Annotated[
        Optional[BlobAccessTier], pydantic.Field(alias="storageClass")
    ] = None

    description: Optional[str] = None

    compress: Optional[CompressionOptions2] = None
    r"""Data compression format to apply to HTTP content before it is delivered"""

    compression_level: Annotated[
        Optional[CompressionLevelOptions], pydantic.Field(alias="compressionLevel")
    ] = None
    r"""Compression level to apply before moving files to final destination"""

    automatic_schema: Annotated[
        Optional[bool], pydantic.Field(alias="automaticSchema")
    ] = None
    r"""Automatically calculate the schema based on the events of each Parquet file generated"""

    parquet_schema: Annotated[Optional[str], pydantic.Field(alias="parquetSchema")] = (
        None
    )
    r"""To add a new schema, navigate to Processing > Knowledge > Parquet Schemas"""

    parquet_version: Annotated[
        Optional[ParquetVersionOptions], pydantic.Field(alias="parquetVersion")
    ] = None
    r"""Determines which data types are supported and how they are represented"""

    parquet_data_page_version: Annotated[
        Optional[DataPageVersionOptions], pydantic.Field(alias="parquetDataPageVersion")
    ] = None
    r"""Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it."""

    parquet_row_group_length: Annotated[
        Optional[float], pydantic.Field(alias="parquetRowGroupLength")
    ] = None
    r"""The number of rows that every group will contain. The final group can contain a smaller number of rows."""

    parquet_page_size: Annotated[
        Optional[str], pydantic.Field(alias="parquetPageSize")
    ] = None
    r"""Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression."""

    should_log_invalid_rows: Annotated[
        Optional[bool], pydantic.Field(alias="shouldLogInvalidRows")
    ] = None
    r"""Log up to 3 rows that @{product} skips due to data mismatch"""

    key_value_metadata: Annotated[
        Optional[List[ItemsTypeKeyValueMetadata]],
        pydantic.Field(alias="keyValueMetadata"),
    ] = None
    r"""The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: \"key\":\"OCSF Event Class\", \"value\":\"9001\" """

    enable_statistics: Annotated[
        Optional[bool], pydantic.Field(alias="enableStatistics")
    ] = None
    r"""Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics."""

    enable_write_page_index: Annotated[
        Optional[bool], pydantic.Field(alias="enableWritePageIndex")
    ] = None
    r"""One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping."""

    enable_page_checksum: Annotated[
        Optional[bool], pydantic.Field(alias="enablePageChecksum")
    ] = None
    r"""Parquet tools can use the checksum of a Parquet page to verify data integrity"""

    empty_dir_cleanup_sec: Annotated[
        Optional[float], pydantic.Field(alias="emptyDirCleanupSec")
    ] = None
    r"""How frequently, in seconds, to clean up empty directories"""

    directory_batch_size: Annotated[
        Optional[float], pydantic.Field(alias="directoryBatchSize")
    ] = None
    r"""Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory."""

    deadletter_path: Annotated[
        Optional[str], pydantic.Field(alias="deadletterPath")
    ] = None
    r"""Storage location for files that fail to reach their final destination after maximum retries are exceeded"""

    max_retry_num: Annotated[Optional[float], pydantic.Field(alias="maxRetryNum")] = (
        None
    )
    r"""The maximum number of times a file will attempt to move to its final destination before being dead-lettered"""

    connection_string: Annotated[
        Optional[str], pydantic.Field(alias="connectionString")
    ] = None
    r"""Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING."""

    text_secret: Annotated[Optional[str], pydantic.Field(alias="textSecret")] = None
    r"""Select or create a stored text secret"""

    storage_account_name: Annotated[
        Optional[str], pydantic.Field(alias="storageAccountName")
    ] = None
    r"""The name of your Azure storage account"""

    tenant_id: Annotated[Optional[str], pydantic.Field(alias="tenantId")] = None
    r"""The service principal's tenant ID"""

    client_id: Annotated[Optional[str], pydantic.Field(alias="clientId")] = None
    r"""The service principal's client ID"""

    azure_cloud: Annotated[Optional[str], pydantic.Field(alias="azureCloud")] = None
    r"""The Azure cloud to use. Defaults to Azure Public Cloud."""

    endpoint_suffix: Annotated[
        Optional[str], pydantic.Field(alias="endpointSuffix")
    ] = None
    r"""Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net."""

    client_text_secret: Annotated[
        Optional[str], pydantic.Field(alias="clientTextSecret")
    ] = None
    r"""Select or create a stored text secret"""

    certificate: Optional[CertificateTypeAzureBlobAuthTypeClientCert] = None

    template_container_name: Annotated[
        Optional[str], pydantic.Field(alias="__template_containerName")
    ] = None
    r"""Binds 'containerName' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'containerName' at runtime."""

    template_format: Annotated[
        Optional[str], pydantic.Field(alias="__template_format")
    ] = None
    r"""Binds 'format' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'format' at runtime."""

    template_connection_string: Annotated[
        Optional[str], pydantic.Field(alias="__template_connectionString")
    ] = None
    r"""Binds 'connectionString' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'connectionString' at runtime."""

    template_tenant_id: Annotated[
        Optional[str], pydantic.Field(alias="__template_tenantId")
    ] = None
    r"""Binds 'tenantId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'tenantId' at runtime."""

    template_client_id: Annotated[
        Optional[str], pydantic.Field(alias="__template_clientId")
    ] = None
    r"""Binds 'clientId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'clientId' at runtime."""

    @field_serializer("format_")
    def serialize_format_(self, value):
        if isinstance(value, str):
            try:
                return models.DataFormatOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_backpressure")
    def serialize_on_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.BackpressureBehaviorOptions1(value)
            except ValueError:
                return value
        return value

    @field_serializer("on_disk_full_backpressure")
    def serialize_on_disk_full_backpressure(self, value):
        if isinstance(value, str):
            try:
                return models.DiskSpaceProtectionOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("auth_type")
    def serialize_auth_type(self, value):
        if isinstance(value, str):
            try:
                return models.AuthenticationMethodOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("storage_class")
    def serialize_storage_class(self, value):
        if isinstance(value, str):
            try:
                return models.BlobAccessTier(value)
            except ValueError:
                return value
        return value

    @field_serializer("compress")
    def serialize_compress(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionOptions2(value)
            except ValueError:
                return value
        return value

    @field_serializer("compression_level")
    def serialize_compression_level(self, value):
        if isinstance(value, str):
            try:
                return models.CompressionLevelOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("parquet_version")
    def serialize_parquet_version(self, value):
        if isinstance(value, str):
            try:
                return models.ParquetVersionOptions(value)
            except ValueError:
                return value
        return value

    @field_serializer("parquet_data_page_version")
    def serialize_parquet_data_page_version(self, value):
        if isinstance(value, str):
            try:
                return models.DataPageVersionOptions(value)
            except ValueError:
                return value
        return value

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = set(
            [
                "id",
                "pipeline",
                "systemFields",
                "environment",
                "streamtags",
                "createContainer",
                "destPath",
                "addIdToStagePath",
                "maxConcurrentFileParts",
                "removeEmptyDirs",
                "partitionExpr",
                "format",
                "baseFileName",
                "fileNameSuffix",
                "maxFileSizeMB",
                "maxFileOpenTimeSec",
                "maxFileIdleTimeSec",
                "maxOpenFiles",
                "headerLine",
                "writeHighWaterMark",
                "onBackpressure",
                "deadletterEnabled",
                "onDiskFullBackpressure",
                "forceCloseOnShutdown",
                "retrySettings",
                "authType",
                "storageClass",
                "description",
                "compress",
                "compressionLevel",
                "automaticSchema",
                "parquetSchema",
                "parquetVersion",
                "parquetDataPageVersion",
                "parquetRowGroupLength",
                "parquetPageSize",
                "shouldLogInvalidRows",
                "keyValueMetadata",
                "enableStatistics",
                "enableWritePageIndex",
                "enablePageChecksum",
                "emptyDirCleanupSec",
                "directoryBatchSize",
                "deadletterPath",
                "maxRetryNum",
                "connectionString",
                "textSecret",
                "storageAccountName",
                "tenantId",
                "clientId",
                "azureCloud",
                "endpointSuffix",
                "clientTextSecret",
                "certificate",
                "__template_containerName",
                "__template_format",
                "__template_connectionString",
                "__template_tenantId",
                "__template_clientId",
            ]
        )
        serialized = handler(self)
        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val != UNSET_SENTINEL:
                if val is not None or k not in optional_fields:
                    m[k] = val

        return m
