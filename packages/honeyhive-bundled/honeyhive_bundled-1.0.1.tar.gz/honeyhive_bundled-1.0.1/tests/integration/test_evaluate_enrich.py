"""Integration tests for evaluate() + enrich_span() pattern.

⚠️  SKIPPED: Pending v1 evaluation API migration
This test suite is skipped because the evaluate() function no longer exists in v1.
The v1 evaluation API uses a different pattern and these tests need to be migrated.

This module tests the end-to-end functionality of the evaluate() pattern
with enrich_span() calls, validating that tracer discovery works correctly
via baggage propagation after the v1.0 selective propagation fix.
"""

# pylint: disable=unused-argument,import-outside-toplevel,unused-variable,unused-import
# Justification: Dynamic test imports; unused: Test scaffolding
# Justification:
# - Unused datapoint arg in test fixture

import os
from typing import Any, Dict

import pytest

# Skip entire module - v0 evaluate() function no longer exists in v1
pytestmark = pytest.mark.skip(
    reason="Skipped pending v1 evaluation API migration - evaluate() function no longer exists in v1"
)

# Import handling: evaluate() doesn't exist in v1, but we keep the import
# for reference. The module is skipped so tests won't run anyway.
try:
    from honeyhive import HoneyHiveTracer, enrich_span, evaluate
except ImportError:
    # evaluate() doesn't exist in v1 - this is expected
    # Module is skipped via pytestmark above
    HoneyHiveTracer = None  # type: ignore
    enrich_span = None  # type: ignore
    evaluate = None  # type: ignore


@pytest.mark.integration
@pytest.mark.skipif(
    not os.environ.get("HH_API_KEY"), reason="Requires HH_API_KEY environment variable"
)
class TestEvaluateEnrichIntegration:
    """Test evaluate() with enrich_span() pattern (v1.0 baggage fix validation)."""

    def test_evaluate_with_enrich_span_tracer_discovery(self) -> None:
        """Test that enrich_span() works within evaluate() via tracer discovery.

        This test validates the v1.0 fix for selective baggage propagation.
        The tracer should be discovered via honeyhive_tracer_id in baggage.
        """
        # Track calls
        calls: list = []

        def user_function(datapoint: Dict[str, Any]) -> Dict[str, Any]:
            """User function with enrich_span call."""
            calls.append("function_called")

            # This should work via tracer discovery (v1.0 fix)
            enrich_span(
                metadata={"input": datapoint["inputs"]},
                metrics={"call_count": len(calls)},
            )
            calls.append("enrich_called")

            return {"output": "test_result", "status": "success"}

        # Run evaluation with small dataset
        result = evaluate(
            function=user_function,
            dataset=[{"inputs": {"text": "test1"}}, {"inputs": {"text": "test2"}}],
            api_key=os.environ["HH_API_KEY"],
            project="test-evaluate-enrich-integration",
            name="v1.0-baggage-fix-validation",
        )

        # Verify evaluation completed
        assert result is not None
        assert hasattr(result, "status")

        # Verify both datapoints were processed
        assert len(calls) >= 4  # 2 function calls + 2 enrich calls

    def test_evaluate_with_explicit_tracer_enrich(self) -> None:
        """Test evaluate() with explicit tracer and instance method enrichment.

        This is the recommended pattern for v1.0+ (instance methods).
        """
        tracer = HoneyHiveTracer(
            api_key=os.environ["HH_API_KEY"],
            project="test-evaluate-explicit-tracer",
            session_name="v1.0-instance-method-pattern",
        )

        def user_function_with_tracer(datapoint: Dict[str, Any]) -> Dict[str, Any]:
            """User function with explicit tracer (recommended pattern)."""
            # Instance method pattern (PRIMARY in v1.0)
            tracer.enrich_span(
                metadata={"input": datapoint["inputs"]},
                metrics={"datapoint_processed": 1},
            )

            return {"output": "processed", "status": "success"}

        # Run evaluation
        result = evaluate(
            function=user_function_with_tracer,
            dataset=[{"inputs": {"text": "test"}}],
            api_key=os.environ["HH_API_KEY"],
            project="test-evaluate-explicit-tracer",
            name="explicit-tracer-pattern",
        )

        assert result is not None
        assert hasattr(result, "status")

    def test_evaluate_enrich_span_with_evaluation_context(self) -> None:
        """Test that evaluation context (run_id, datapoint_id) propagates correctly.

        Validates that the v1.0 selective baggage fix propagates
        evaluation context keys (run_id, dataset_id, datapoint_id).
        """
        captured_metadata: list = []

        def user_function(datapoint: Dict[str, Any]) -> Dict[str, Any]:
            """Capture metadata to verify context propagation."""
            # Enrich with metadata
            enrich_span(
                metadata={
                    "datapoint_input": datapoint["inputs"],
                    "test_marker": "context_propagation_test",
                }
            )

            captured_metadata.append(datapoint["inputs"])
            return {"output": "ok"}

        # Run evaluation
        result = evaluate(
            function=user_function,
            dataset=[
                {"inputs": {"idx": 1}},
                {"inputs": {"idx": 2}},
                {"inputs": {"idx": 3}},
            ],
            api_key=os.environ["HH_API_KEY"],
            project="test-evaluation-context-propagation",
            name="context-propagation-validation",
        )

        # Verify all datapoints processed
        assert len(captured_metadata) == 3
        assert result is not None

    def test_evaluate_child_spans_have_evaluation_metadata(self) -> None:
        """Test that child spans created during evaluate() have evaluation metadata.

        This test validates the baggage propagation fix that ensures run_id,
        dataset_id, and datapoint_id propagate to all child spans.
        """
        import time

        from honeyhive import HoneyHive, trace

        # Track span creation
        span_names = []

        @trace(event_type="tool", event_name="child_operation")
        def child_operation(text: str) -> str:
            """Child function that creates a span."""
            span_names.append("child_operation")
            return text.upper()

        def user_function(datapoint: Dict[str, Any]) -> Dict[str, Any]:
            """Function that creates child spans."""
            inputs = datapoint.get("inputs", {})
            text = inputs.get("text", "")

            # Create child span
            result = child_operation(text)

            return {"output": result, "status": "success"}

        # Run evaluation
        result = evaluate(
            function=user_function,
            dataset=[
                {"inputs": {"text": "test1"}},
                {"inputs": {"text": "test2"}},
            ],
            api_key=os.environ["HH_API_KEY"],
            project="test-evaluation-metadata-propagation",
            name="child-span-metadata-test",
        )

        # Verify evaluation completed
        assert result is not None
        assert hasattr(result, "status")
        assert result.status == "completed"

        # Verify child spans were created
        assert len(span_names) == 2  # One child span per datapoint

        # Give backend time to process spans
        time.sleep(3)

        # Verify evaluation metadata was set
        assert hasattr(result, "run_id")
        run_id = result.run_id

        # Validate that run was created with correct structure
        # The backend validation happens during evaluate() execution
        # If child spans have evaluation metadata, the run linking will work correctly

        # NOTE: Full backend validation would require:
        # 1. Fetching the run via API
        # 2. Fetching associated events/sessions
        # 3. Validating run_id, dataset_id, datapoint_id in event metadata
        #
        # This is tested implicitly by the evaluate() success and the verbose
        # logs showing the attributes are set on spans before export.

    def test_evaluate_enrich_span_error_handling(self) -> None:
        """Test that enrich_span gracefully handles errors in evaluate().

        Validates that enrichment failures don't crash evaluation.
        """
        processed_count = 0

        def user_function_with_error(datapoint: Dict[str, Any]) -> Dict[str, Any]:
            """Function that attempts enrichment."""
            nonlocal processed_count
            processed_count += 1

            # This might fail but shouldn't crash
            try:
                enrich_span(metadata={"count": processed_count})
            except Exception:
                pass  # Graceful degradation

            return {"output": f"processed_{processed_count}"}

        # Run evaluation
        result = evaluate(
            function=user_function_with_error,
            dataset=[{"inputs": {"test": i}} for i in range(5)],
            api_key=os.environ["HH_API_KEY"],
            project="test-enrich-error-handling",
            name="error-handling-validation",
        )

        # Should complete despite any enrichment issues
        assert processed_count == 5
        assert result is not None
