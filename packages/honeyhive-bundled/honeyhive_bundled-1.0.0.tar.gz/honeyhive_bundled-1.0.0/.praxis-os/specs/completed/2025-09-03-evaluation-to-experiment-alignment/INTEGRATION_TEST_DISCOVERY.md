# Integration Test Discovery from HoneyHive Documentation

**Source**: HoneyHive documentation site (docs.honeyhive.ai)  
**Extracted**: 2025-10-02  
**Purpose**: Comprehensive test case extraction for experiment/evaluation integration tests

---

## ðŸ“‹ Table of Contents

1. [Core Experiment Functionality](#core-experiment-functionality)
2. [Dataset Management](#dataset-management)
3. [Evaluator Framework](#evaluator-framework)
4. [Server-Side Integration](#server-side-integration)
5. [External Logs & Historical Data](#external-logs--historical-data)
6. [Multi-Step Pipelines](#multi-step-pipelines)
7. [Comparison & Analysis](#comparison--analysis)
8. [Tracing Integration](#tracing-integration)
9. [Priority Matrix](#priority-matrix)

---

## 1. Core Experiment Functionality

### From `/evaluation/quickstart.md`

#### âœ… **IMPLEMENTED** (Basic Flow)
- [x] Run experiment with local dataset (list of dicts)
- [x] Function receives `inputs` and `ground_truths` from datapoint
- [x] Client-side evaluators execute on each datapoint
- [x] Results visible in dashboard
- [x] Session metadata includes run_id, dataset_id, datapoint_id

#### ðŸ”¨ **TO IMPLEMENT**

**Test: `test_multi_threaded_execution`** ðŸ”´ **HIGH PRIORITY**
- **Feature**: "Concurrent execution with ThreadPoolExecutor and max_workers"
- **Test Case**: Execute `evaluate()` with `max_workers=4` on large dataset
- **Validation**:
  - âœ… Multiple threads execute concurrently
  - âœ… Each tracer instance is isolated (no cross-contamination)
  - âœ… Session IDs are unique per datapoint
  - âœ… Metrics collected from all threads
  - âœ… No race conditions or thread safety issues
  - âœ… All datapoints processed successfully
  - âœ… Execution time < sequential time (performance gain)
  - âœ… Thread pool cleanup happens correctly

**Test: `test_evaluate_basic_workflow`**
- **Feature**: "Run experiments using local datasets defined directly in your code"
- **Test Case**: Execute `evaluate()` with inline dataset (list of dicts)
- **Validation**:
  - âœ… Function executes for each datapoint
  - âœ… `inputs` and `ground_truths` correctly passed
  - âœ… Outputs captured and stored
  - âœ… Run created in platform with correct name
  - âœ… Session count matches dataset size

**Test: `test_evaluator_parameter_order`**
- **Feature**: "Evaluators receive (outputs, inputs, ground_truths)"
- **Test Case**: Verify parameter order is strictly enforced
- **Validation**:
  - âœ… First param is function output
  - âœ… Second param is inputs dict
  - âœ… Third param is ground_truths dict
  - âœ… Error if params passed in wrong order

**Test: `test_server_url_configuration`**
- **Feature**: "server_url for self-hosted/dedicated deployments"
- **Test Case**: Pass custom `server_url` to `evaluate()`
- **Validation**:
  - âœ… API calls route to custom URL
  - âœ… Works with both `hh_api_key` and `api_key` params
  - âœ… Error handling for invalid URLs

---

## 2. Dataset Management

### From `/evaluation/managed_datasets.md`

#### âœ… **IMPLEMENTED**
- [x] Pass `dataset_id` to use HoneyHive managed dataset
- [x] Fetch datapoints from HoneyHive platform

#### ðŸ”¨ **TO IMPLEMENT**

**Test: `test_managed_dataset_evaluation`**
- **Feature**: "Run experiments using datasets managed through HoneyHive platform"
- **Setup**: Upload JSONL dataset via SDK, get `dataset_id`
- **Test Case**: Execute `evaluate()` with `dataset_id` param
- **Validation**:
  - âœ… SDK uploads a dataset with datapoints to the platform
  - âœ… SDK fetches datapoints from platform
  - âœ… Dataset structure includes `inputs` and `ground_truths`
  - âœ… Function receives correct fields
  - âœ… Run links to dataset via `dataset_id`
  - âœ… Datapoint IDs correctly associated

**Test: `test_dataset_format_support`**
- **Feature**: "Supports JSON, JSONL, and CSV formats"
- **Test Cases**: Upload datasets in different formats
- **Validation**:
  - âœ… JSONL format works
  - âœ… JSON format works
  - âœ… CSV format works
  - âœ… All formats produce same datapoint structure

**Test: `test_dataset_versioning`**
- **Feature**: "Centralized and versioned datasets for team collaboration"
- **Test Case**: Run experiment on specific dataset version
- **Validation**:
  - âœ… Can specify dataset version (if supported)
  - âœ… Different versions produce different results
  - âœ… Version info visible in run metadata

---

## 3. Evaluator Framework

### From `/evaluators/client_side.md` and `/evaluation/quickstart.md`

#### âœ… **IMPLEMENTED**
- [x] `@evaluator()` decorator
- [x] Sync and async evaluators
- [x] Multiple evaluators per experiment
- [x] Return numeric or dict of metrics

#### ðŸ”¨ **TO IMPLEMENT**

**Test: `test_evaluator_return_types`**
- **Feature**: "Evaluators can return single value or dict of metrics"
- **Test Cases**:
  ```python
  @evaluator()
  def single_value(outputs, inputs, ground_truths):
      return 0.85
  
  @evaluator()
  def multiple_metrics(outputs, inputs, ground_truths):
      return {"accuracy": 0.85, "precision": 0.90}
  ```
- **Validation**:
  - âœ… Single value stored as metric
  - âœ… Dict values stored as separate metrics
  - âœ… Metric names in dashboard match dict keys

**Test: `test_evaluator_error_handling`**
- **Feature**: "Graceful handling of evaluator failures"
- **Test Case**: Evaluator that raises exception
- **Validation**:
  - âœ… Experiment continues despite evaluator failure
  - âœ… Error logged but doesn't crash
  - âœ… Failed metric shows as None or error state
  - âœ… Other evaluators still execute

**Test: `test_evaluator_with_optional_ground_truth`**
- **Feature**: "ground_truths is optional parameter"
- **Test Case**: Evaluator without ground_truth param
- **Validation**:
  - âœ… Works when ground_truth not in dataset
  - âœ… Works when evaluator signature excludes ground_truth
  - âœ… No error when ground_truth is None

**Test: `test_async_evaluator_execution`**
- **Feature**: "Support for async evaluators (@aevaluator)"
- **Test Case**: Mix of sync and async evaluators
- **Validation**:
  - âœ… Async evaluators execute correctly
  - âœ… All evaluators complete regardless of sync/async
  - âœ… Metrics from both types stored
  - âœ… No blocking issues

---

## 4. Server-Side Integration

### From `/evaluation/server_side_evaluators.md`

#### âœ… **IMPLEMENTED**
- [x] Server-side evaluators auto-execute (no client config)
- [x] Metrics appear in dashboard without passing to `evaluators=[]`

#### ðŸ”¨ **TO IMPLEMENT**

**Test: `test_server_side_evaluator_execution`** âœ… **DONE** (from previous session)
- **Feature**: "Server-side evaluators execute automatically"
- **Setup**: Create Python evaluator in HoneyHive platform
- **Test Case**: Run `evaluate()` WITHOUT passing evaluators
- **Validation**:
  - âœ… Server-side evaluator runs automatically
  - âœ… Metrics appear in run results
  - âœ… Event type filtering works (e.g., "model" events only)
  - âœ… Access to `event["outputs"]["content"]` path

**Test: `test_mixed_client_server_evaluators`** âœ… **PARTIALLY DONE**
- **Feature**: "Client-side and server-side evaluators work together"
- **Test Case**: Pass client evaluators while server evaluators exist
- **Validation**:
  - âœ… Both types execute
  - âœ… All metrics stored
  - âœ… No conflicts or overwrites
  - âœ… Metric sources identifiable

**Test: `test_server_evaluator_event_filtering`**
- **Feature**: "Server evaluators filter by event type"
- **Setup**: Create evaluator targeting "model" events
- **Test Case**: Multi-step pipeline with various event types
- **Validation**:
  - âœ… Evaluator only runs on matching event types
  - âœ… Skips non-matching events
  - âœ… Event attributes accessible in evaluator

---

## 5. External Logs & Historical Data

### From `/evaluation/external_logs.md`

#### ðŸ”¨ **TO IMPLEMENT**

**Test: `test_external_log_evaluation`**
- **Feature**: "Upload and evaluate existing logs from external sources"
- **Test Case**: Pass-through function with pre-existing outputs
  ```python
  def pass_through_logged_data(inputs, ground_truths):
      return ground_truths["highlights"]  # Use logged output
  ```
- **Validation**:
  - âœ… Function can return pre-logged outputs
  - âœ… Evaluators run on historical data
  - âœ… No need to re-generate outputs
  - âœ… Metrics computed on existing logs

**Test: `test_csv_pandas_dataset_loading`**
- **Feature**: "Load logs from CSV/DataFrame"
- **Test Case**: `df.to_dict('records')` â†’ `evaluate()`
- **Validation**:
  - âœ… CSV loads correctly
  - âœ… DataFrame conversion works
  - âœ… Dataset structure matches expected format
  - âœ… All rows processed

**Test: `test_benchmark_historical_prompts`**
- **Feature**: "Benchmark different versions using past data"
- **Test Case**: Same dataset, different evaluators/prompts
- **Validation**:
  - âœ… Can compare old vs new prompts
  - âœ… Metrics show differences
  - âœ… No re-execution of LLM needed

---

## 6. Multi-Step Pipelines

### From `/evaluation/multi_step_evals.md`

#### ðŸ”¨ **TO IMPLEMENT**

**Test: `test_multi_step_rag_pipeline`**
- **Feature**: "Evaluate multi-step RAG (retrieval + generation)"
- **Test Case**: Pipeline with `@trace` decorators
  ```python
  @trace
  def get_relevant_docs(query): ...
  
  @trace
  def generate_response(docs, query): ...
  
  def rag_pipeline(inputs, ground_truths):
      docs = get_relevant_docs(inputs["query"])
      return generate_response(docs, inputs["query"])
  ```
- **Validation**:
  - âœ… Both steps traced as spans
  - âœ… Parent-child relationship maintained
  - âœ… Span-level metrics via `enrich_span()`
  - âœ… Session-level metrics via `enrich_session()`

**Test: `test_span_level_metrics`**
- **Feature**: "Log metrics for specific pipeline steps"
- **Test Case**: Retrieval evaluator on retrieval span
  ```python
  @trace
  def get_relevant_docs(query):
      # ... retrieval logic
      enrich_span(metrics={"retrieval_relevance": 0.85})
  ```
- **Validation**:
  - âœ… Metric attached to correct span
  - âœ… Visible in trace viewer
  - âœ… Separate from session metrics
  - âœ… Aggregated in run results

**Test: `test_session_level_metrics`**
- **Feature**: "Log pipeline-wide metrics"
- **Test Case**: Overall pipeline metrics
  ```python
  def rag_pipeline(inputs, ground_truths):
      # ... pipeline logic
      enrich_session(metrics={
          "num_retrieved_docs": 3,
          "query_length": 10
      })
  ```
- **Validation**:
  - âœ… Metrics attached to session
  - âœ… Visible in session view
  - âœ… Aggregated across all sessions
  - âœ… Separate from span metrics

**Test: `test_vector_search_evaluation`**
- **Feature**: "Evaluate retrieval quality in RAG"
- **Test Case**: Cosine similarity between query and retrieved docs
- **Validation**:
  - âœ… Retrieval relevance metric computed
  - âœ… Low scores indicate poor retrieval
  - âœ… High scores indicate relevant docs
  - âœ… Correlates with final response quality

**Test: `test_response_consistency_evaluation`**
- **Feature**: "Measure semantic similarity to ground truth"
- **Test Case**: Embedding similarity evaluator
- **Validation**:
  - âœ… Consistency metric computed
  - âœ… Detects hallucinations (low retrieval, high consistency)
  - âœ… Detects poor responses (low both)
  - âœ… Identifies good responses (high both)

---

## 7. Comparison & Analysis

### From `/evaluation/comparing_evals.md`

#### âœ… **IMPLEMENTED**
- [x] Basic comparison of two runs
- [x] Common datapoints identification
- [x] Metric improvements/regressions

#### ðŸ”¨ **TO IMPLEMENT**

**Test: `test_step_level_comparison`** âœ… **PARTIALLY DONE**
- **Feature**: "Compare individual steps across experiments"
- **Test Case**: Two runs with multi-step pipelines
- **Validation**:
  - âœ… Compare retrieval step across runs
  - âœ… Compare generation step across runs
  - âœ… Identify which step improved/regressed
  - âœ… Step-level metric deltas

**Test: `test_aggregated_metrics_comparison`**
- **Feature**: "View aggregated metrics (server-side, client-side, composite)"
- **Test Case**: Compare runs with different evaluators
- **Validation**:
  - âœ… Server-side metrics aggregated
  - âœ… Client-side metrics aggregated
  - âœ… Composite metrics calculated
  - âœ… All metrics visible in comparison view

**Test: `test_improved_regressed_filtering`**
- **Feature**: "Filter for events that improved or regressed"
- **Test Case**: Comparison with mixed results
- **Validation**:
  - âœ… Filter shows only improved events
  - âœ… Filter shows only regressed events
  - âœ… Filter shows unchanged events
  - âœ… Metric thresholds configurable

**Test: `test_output_diff_viewer`**
- **Feature**: "View side-by-side output differences"
- **Test Case**: Two runs with different outputs
- **Validation**:
  - âœ… Diff view shows changes
  - âœ… Highlights added/removed content
  - âœ… Side-by-side comparison
  - âœ… Per-datapoint diff available

**Test: `test_metric_distribution_analysis`**
- **Feature**: "Analyze distribution of various metrics"
- **Test Case**: Comparison with metric histograms
- **Validation**:
  - âœ… Histogram shows metric distribution
  - âœ… Compare distributions across runs
  - âœ… Identify outliers
  - âœ… Statistical summary (mean, median, std)

**Test: `test_comparison_best_practices`**
- **Feature**: Best practices from docs
- **Test Cases**:
  1. Same dataset for both runs âœ…
  2. Meaningful run names âœ…
  3. Consistent evaluation criteria âœ…
  4. Multiple metrics for comprehensive view
  5. Representative dataset size
- **Validation**: Each best practice enforced/encouraged

**Test: `test_event_level_comparison`**
- **Feature**: "Detailed per-datapoint comparison with matching"
- **Test Case**: Use `/runs/compare/events` endpoint
- **Validation**:
  - âœ… Events matched by `datapoint_id`
  - âœ… Per-metric improved/degraded/same lists
  - âœ… Event presence information
  - âœ… Paired events (event_1, event_2) returned
  - âœ… Common datapoints count correct

---

## 8. Tracing Integration

### From `/tracing/client-side-evals.md` and multi-step guide

#### ðŸ”¨ **TO IMPLEMENT**

**Test: `test_trace_decorator_integration`**
- **Feature**: "Use @trace decorator in experiment functions"
- **Test Case**: Function with nested @trace calls
- **Validation**:
  - âœ… All spans created
  - âœ… Hierarchy preserved
  - âœ… Experiment context maintained
  - âœ… Run ID propagated to all spans

**Test: `test_enrich_span_in_experiment`**
- **Feature**: "Log span-level metrics during experiment"
- **Test Case**: Call `enrich_span()` within traced function
- **Validation**:
  - âœ… Metrics attached to correct span
  - âœ… Visible in span details
  - âœ… Included in run aggregation
  - âœ… No conflicts with session metrics

**Test: `test_enrich_session_in_experiment`**
- **Feature**: "Log session-level metrics during experiment"
- **Test Case**: Call `enrich_session()` in experiment function
- **Validation**:
  - âœ… Metrics attached to session
  - âœ… Visible in session view
  - âœ… Aggregated in run results
  - âœ… Separate from evaluator metrics

**Test: `test_distributed_tracing_in_experiment`**
- **Feature**: "Maintain trace context across services"
- **Test Case**: Experiment function calls external service
- **Validation**:
  - âœ… Trace context propagated
  - âœ… External service spans linked
  - âœ… Full trace visible in platform
  - âœ… Run ID maintained

---

## 9. Priority Matrix

### ðŸ”´ **HIGH PRIORITY** (Core Functionality)

These are essential for basic experiment workflow:

1. âœ… `test_evaluate_basic_workflow` - **DONE**
2. âœ… `test_managed_dataset_evaluation` - **DONE** (HoneyHive dataset support)
3. âœ… `test_server_side_evaluator_execution` - **DONE**
4. âœ… `test_mixed_client_server_evaluators` - **PARTIALLY DONE**
5. âœ… `test_evaluator_parameter_order` - **DONE** (validated in integration test)
6. âœ… `test_comparison_workflow` - **DONE**
7. ðŸ”¨ `test_event_level_comparison` - **TO IMPLEMENT**
8. ðŸ”¨ `test_multi_threaded_execution` - **TO IMPLEMENT** (CRITICAL for performance)

### ðŸŸ¡ **MEDIUM PRIORITY** (Enhanced Features)

Important for advanced use cases:

8. `test_multi_step_rag_pipeline`
9. `test_span_level_metrics`
10. `test_session_level_metrics`
11. `test_evaluator_return_types`
12. `test_evaluator_error_handling`
13. `test_server_url_configuration`
14. `test_dataset_format_support`

### ðŸŸ¢ **LOW PRIORITY** (Nice to Have)

Useful but not critical:

15. `test_external_log_evaluation`
16. `test_csv_pandas_dataset_loading`
17. `test_benchmark_historical_prompts`
18. `test_dataset_versioning`
19. `test_async_evaluator_execution`
20. `test_evaluator_with_optional_ground_truth`
21. `test_output_diff_viewer`
22. `test_metric_distribution_analysis`

---

## ðŸ“Š Coverage Summary

| Category | Total Tests | Implemented | To Implement | Priority |
|----------|------------|-------------|--------------|----------|
| **Core Functionality** | 8 | 6 | 2 | ðŸ”´ HIGH |
| **Dataset Management** | 4 | 1 | 3 | ðŸŸ¡ MEDIUM |
| **Evaluator Framework** | 6 | 2 | 4 | ðŸŸ¡ MEDIUM |
| **Server-Side** | 3 | 2 | 1 | ðŸ”´ HIGH |
| **External Logs** | 3 | 0 | 3 | ðŸŸ¢ LOW |
| **Multi-Step** | 5 | 0 | 5 | ðŸŸ¡ MEDIUM |
| **Comparison** | 6 | 2 | 4 | ðŸ”´ HIGH |
| **Tracing** | 4 | 0 | 4 | ðŸŸ¡ MEDIUM |
| **TOTAL** | **39** | **13** | **26** | - |

---

## ðŸŽ¯ Recommended Implementation Order

### Phase 1: Complete High-Priority Coverage
1. `test_event_level_comparison` - Event-level comparison endpoint
2. `test_multi_threaded_execution` - Concurrent execution with thread safety validation

### Phase 2: Multi-Step & Tracing (Critical for Real Pipelines)
3. `test_multi_step_rag_pipeline`
4. `test_span_level_metrics`
5. `test_session_level_metrics`
6. `test_trace_decorator_integration`

### Phase 3: Evaluator Robustness
7. `test_evaluator_return_types`
8. `test_evaluator_error_handling`
9. `test_async_evaluator_execution`
10. `test_evaluator_with_optional_ground_truth`

### Phase 4: Dataset Flexibility
11. `test_dataset_format_support`
12. `test_server_url_configuration`
13. `test_external_log_evaluation`

### Phase 5: Advanced Analysis
14. `test_step_level_comparison`
15. `test_aggregated_metrics_comparison`
16. `test_improved_regressed_filtering`
17. Remaining low-priority tests as needed

---

## ðŸ“ Test Template

For each test to implement, use this structure:

```python
def test_feature_name(
    self,
    real_api_key: str,
    real_project: str,
    integration_client: HoneyHive,
) -> None:
    """
    Test [feature description from docs].
    
    Documentation Reference: /evaluation/[page].md
    
    This test validates:
    1. [Validation point 1]
    2. [Validation point 2]
    3. [Validation point 3]
    """
    
    # Setup
    # ...
    
    # Execute
    # ...
    
    # Validate
    # ...
    
    # Cleanup (if needed)
    # ...
```

---

## ðŸ”— Related Documentation

- **Agent OS Testing Framework**: `.praxis-os/standards/ai-assistant/code-generation/tests/v3/FRAMEWORK-LAUNCHER.md`
- **Integration Testing Standards**: `.praxis-os/standards/testing/integration-testing-standards.md`
- **Backend Validation**: `.praxis-os/specs/2025-09-03-evaluation-to-experiment-alignment/BACKEND_VALIDATION_ANALYSIS.md`
- **Endpoint Coverage**: `.praxis-os/specs/2025-09-03-evaluation-to-experiment-alignment/ENDPOINT_COVERAGE_MATRIX.md`
- **HoneyHive Docs Access**: `.praxis-os/standards/documentation/honeyhive-docs-access.md`

---

**Last Updated**: 2025-10-02  
**Status**: 13/39 tests implemented (33% coverage)  
**Next Actions**:
1. Implement `test_event_level_comparison` from Phase 1
2. Implement `test_multi_threaded_execution` from Phase 1 (CRITICAL)

