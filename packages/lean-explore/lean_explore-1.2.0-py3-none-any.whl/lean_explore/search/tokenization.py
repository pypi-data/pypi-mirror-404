"""Text tokenization utilities for search indexing and querying.

This module provides tokenization strategies for Lean declaration names,
supporting both spaced tokenization (splits on dots, underscores, camelCase)
and raw tokenization (preserves structure for exact matching).
"""

import re


def tokenize_spaced(text: str) -> list[str]:
    """Tokenize text with spacing on dots, underscores, and camelCase.

    Args:
        text: Input text to tokenize.

    Returns:
        List of lowercase word tokens.
    """
    if not text:
        return []
    # Replace dots and underscores with spaces
    text = text.replace(".", " ").replace("_", " ")
    # Split camelCase: insert space before uppercase letters
    text = re.sub(r"([a-z])([A-Z])", r"\1 \2", text)
    return re.findall(r"\w+", text.lower())


def tokenize_raw(text: str) -> list[str]:
    """Tokenize text as single token (preserves dots).

    Args:
        text: Input text to tokenize.

    Returns:
        List with the full text as a single lowercase token.
    """
    if not text:
        return []
    return [text.lower()]


def tokenize_words(text: str) -> list[str]:
    """Simple word tokenization for natural language text.

    Splits on whitespace and punctuation, returns lowercase tokens.

    Args:
        text: Input text to tokenize.

    Returns:
        List of lowercase word tokens.
    """
    if not text:
        return []
    return [w.lower() for w in re.findall(r"\w+", text)]


def is_autogenerated(name: str) -> bool:
    """Check if a declaration name is auto-generated by Lean.

    Auto-generated declarations include:
    - .mk constructors (e.g., Nat.mk)

    Args:
        name: Fully qualified declaration name.

    Returns:
        True if the declaration is auto-generated.
    """
    return name.endswith(".mk")
