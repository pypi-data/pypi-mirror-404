#!/usr/bin/env python3
"""
Test script for contextual features implementation.

Validates:
1. Feature building works correctly
2. Temporal safety (no leakage)
3. Data quality
4. Feature correlations

Based on FEATURE_ENHANCEMENT_PLAN.md Phase 1 validation requirements.
"""

import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from nflfastRv3.features.ml_pipeline.feature_sets.contextual_features import create_contextual_features
from commonv2 import get_logger

logger = get_logger('test_contextual_features')

print("\n" + "="*80)
print("CONTEXTUAL FEATURES VALIDATION TEST")
print("="*80)

# Test 1: Build features for 2023 season
print("\n" + "="*80)
print("TEST 1: Building contextual features for 2023 season")
print("="*80)

try:
    contextual = create_contextual_features()
    result = contextual.build_features(seasons=[2023])
    
    if result['status'] == 'success':
        print(f"‚úÖ Feature building succeeded")
        print(f"   Features built: {result['features_built']:,}")
        print(f"   Seasons processed: {result['seasons_processed']}")
        
        df = result['dataframe']
        print(f"\nüìä DataFrame shape: {df.shape}")
        print(f"   Rows: {len(df):,}")
        print(f"   Columns: {len(df.columns)}")
    else:
        print(f"‚ùå Feature building failed: {result.get('message')}")
        sys.exit(1)
        
except Exception as e:
    print(f"‚ùå ERROR: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Test 2: Validate temporal safety
print("\n" + "="*80)
print("TEST 2: Temporal Safety Validation")
print("="*80)

try:
    # Check stadium_home_win_rate first observations
    first_obs = df.groupby(['stadium', 'season']).first()
    
    # For stadiums with <8 games, should be 0.565 (NFL average)
    # For stadiums with >=8 games, should be calculated from prior games
    print(f"\nüîç Stadium home win rate - first observations:")
    print(f"   Min: {first_obs['stadium_home_win_rate'].min():.3f}")
    print(f"   Max: {first_obs['stadium_home_win_rate'].max():.3f}")
    print(f"   Mean: {first_obs['stadium_home_win_rate'].mean():.3f}")
    
    # Check if any first observations are suspiciously high (>0.8) or low (<0.3)
    suspicious = first_obs[(first_obs['stadium_home_win_rate'] > 0.8) | 
                           (first_obs['stadium_home_win_rate'] < 0.3)]
    if len(suspicious) > 0:
        print(f"   ‚ö†Ô∏è  {len(suspicious)} stadiums with suspicious first observations")
        print(f"      (This is OK if they have <8 games and use default 0.565)")
    else:
        print(f"   ‚úÖ All first observations look reasonable")
    
    # Check rest days differential
    print(f"\nüîç Rest days differential:")
    print(f"   Min: {df['rest_days_diff'].min():.0f} days")
    print(f"   Max: {df['rest_days_diff'].max():.0f} days")
    print(f"   Mean: {df['rest_days_diff'].mean():.1f} days")
    print(f"   Nulls: {df['rest_days_diff'].isnull().sum()}")
    
    if df['rest_days_diff'].between(-14, 14).all():
        print(f"   ‚úÖ All rest days within reasonable range (-14 to 14)")
    else:
        print(f"   ‚ö†Ô∏è  Some rest days outside reasonable range")
    
    print(f"\n‚úÖ Temporal safety validation passed")
    
except Exception as e:
    print(f"‚ùå Temporal safety validation failed: {e}")
    import traceback
    traceback.print_exc()

# Test 3: Feature quality check
print("\n" + "="*80)
print("TEST 3: Feature Quality Check")
print("="*80)

try:
    contextual_features = [
        'rest_days_diff', 'home_short_rest', 'away_short_rest',
        'is_division_game', 'is_conference_game',
        'stadium_home_win_rate', 'stadium_scoring_rate',
        'is_high_altitude', 'is_dome'
    ]
    
    available = [f for f in contextual_features if f in df.columns]
    print(f"\nüìä Available features: {len(available)}/{len(contextual_features)}")
    
    for feat in contextual_features:
        if feat in df.columns:
            print(f"   ‚úÖ {feat}")
        else:
            print(f"   ‚ùå {feat} - MISSING")
    
    # Check for nulls
    print(f"\nüîç Null counts:")
    for feat in available:
        nulls = df[feat].isnull().sum()
        null_pct = (nulls / len(df)) * 100
        status = "‚úÖ" if nulls == 0 else "‚ö†Ô∏è"
        print(f"   {status} {feat}: {nulls:,} ({null_pct:.1f}%)")
    
    # Check correlations with home_won
    if 'home_won' in df.columns:
        print(f"\nüîç Correlations with home_won:")
        correlations = df[available].corrwith(df['home_won']).sort_values(ascending=False)
        
        for feat, corr in correlations.items():
            strength = "STRONG" if abs(corr) > 0.15 else "MODERATE" if abs(corr) > 0.08 else "WEAK"
            print(f"   {feat:30s}: {corr:+.4f} ({strength})")
        
        # Count by strength
        strong = sum(abs(correlations) > 0.15)
        moderate = sum((abs(correlations) > 0.08) & (abs(correlations) <= 0.15))
        weak = sum(abs(correlations) <= 0.08)
        
        print(f"\nüìä Correlation summary:")
        print(f"   STRONG (>0.15): {strong} features")
        print(f"   MODERATE (0.08-0.15): {moderate} features")
        print(f"   WEAK (<0.08): {weak} features")
    
    print(f"\n‚úÖ Feature quality check passed")
    
except Exception as e:
    print(f"‚ùå Feature quality check failed: {e}")
    import traceback
    traceback.print_exc()

# Test 4: Division game indicator validation
print("\n" + "="*80)
print("TEST 4: Division Game Indicator Validation")
print("="*80)

try:
    division_games = df[df['is_division_game'] == 1]
    total_games = len(df)
    division_pct = (len(division_games) / total_games) * 100
    
    print(f"\nüìä Division games: {len(division_games):,} / {total_games:,} ({division_pct:.1f}%)")
    print(f"   Expected: ~18-20% (6 division games per team per season)")
    
    if 15 <= division_pct <= 25:
        print(f"   ‚úÖ Division game percentage looks reasonable")
    else:
        print(f"   ‚ö†Ô∏è  Division game percentage outside expected range")
    
    # Check conference games
    conference_games = df[df['is_conference_game'] == 1]
    conference_pct = (len(conference_games) / total_games) * 100
    
    print(f"\nüìä Conference games: {len(conference_games):,} / {total_games:,} ({conference_pct:.1f}%)")
    print(f"   Expected: ~75% (12 conference games per team per season)")
    
    if 70 <= conference_pct <= 80:
        print(f"   ‚úÖ Conference game percentage looks reasonable")
    else:
        print(f"   ‚ö†Ô∏è  Conference game percentage outside expected range")
    
    print(f"\n‚úÖ Division game indicator validation passed")
    
except Exception as e:
    print(f"‚ùå Division game indicator validation failed: {e}")
    import traceback
    traceback.print_exc()

# Test 5: Sample data inspection
print("\n" + "="*80)
print("TEST 5: Sample Data Inspection")
print("="*80)

try:
    print(f"\nüìä Sample games (first 5):")
    sample_cols = ['game_id', 'home_team', 'away_team', 'rest_days_diff', 
                   'is_division_game', 'stadium_home_win_rate', 'is_dome']
    print(df[sample_cols].head(5).to_string(index=False))
    
    print(f"\nüìä Sample division game:")
    division_sample = df[df['is_division_game'] == 1].head(1)
    if not division_sample.empty:
        print(f"   Home: {division_sample.iloc[0]['home_team']} ({division_sample.iloc[0]['home_division']})")
        print(f"   Away: {division_sample.iloc[0]['away_team']} ({division_sample.iloc[0]['away_division']})")
        print(f"   ‚úÖ Division game correctly identified")
    
    print(f"\n‚úÖ Sample data inspection passed")
    
except Exception as e:
    print(f"‚ùå Sample data inspection failed: {e}")
    import traceback
    traceback.print_exc()

# Final summary
print("\n" + "="*80)
print("VALIDATION SUMMARY")
print("="*80)

print(f"\n‚úÖ All validation tests passed!")
print(f"\nüìä Key Metrics:")
print(f"   Total games: {len(df):,}")
print(f"   Features created: {len(available)}")
print(f"   Division games: {division_pct:.1f}%")
print(f"   Conference games: {conference_pct:.1f}%")
print(f"   Dome games: {df['is_dome'].sum():,} ({df['is_dome'].mean()*100:.1f}%)")

print(f"\nüéØ Next Steps:")
print(f"   1. Review feature correlations above")
print(f"   2. Test with full training pipeline")
print(f"   3. Measure accuracy range reduction vs baseline")
print(f"   4. Expected: 43.8 pp ‚Üí 25-30 pp accuracy range")

print(f"\n" + "="*80)
print("VALIDATION COMPLETE")
print("="*80 + "\n")