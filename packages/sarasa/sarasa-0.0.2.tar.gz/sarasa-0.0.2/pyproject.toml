[build-system]
requires = ["hatchling >= 1.26", "uv-dynamic-versioning"]
build-backend = "hatchling.build"

[project]
name = "sarasa"
dynamic = ["version"]
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "datasets>=4.5.0",
    "loguru>=0.7.3",
    "numpy>=2.4.1",
    "rich>=14.2.0",
    "tensorboard>=2.20.0",
    "tokenizers>=0.22.2",
    "tyro>=1.0.5",
]

[tool.hatch.version]
source = "uv-dynamic-versioning"

[tool.uv-dynamic-versioning]
enable = true
vcs = "git"
format-jinja = """
    {%- if distance == 0 -%}
        {{ base }}
    {%- else -%}
        {{ base }}.dev{{ distance }}
    {%- endif -%}
"""

[dependency-groups]
dev = [
    "pytest>=9.0.2",
    "ruff>=0.14.14",
]

[project.optional-dependencies]
cpu = [
  "torch>=2.10.0",
]
cu128 = [
  "torch>=2.10.0",
]
cu130 = [
  "torch>=2.10.0",
]
flash_attn = [
  "flash-attn-cute",
]

[tool.uv]
conflicts = [
  [
    { extra = "cpu" },
    { extra = "cu128" },
    { extra = "cu130" },
  ],
]

[tool.uv.sources]
torch = [
  { index = "pytorch-cpu", extra = "cpu" },
  { index = "pytorch-cu128", extra = "cu128" },
  { index = "pytorch-cu130", extra = "cu130" },
]
flash-attn-cute = { git = "https://github.com/Dao-AILab/flash-attention/", subdirectory = "flash_attn/cute" }

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu130"
url = "https://download.pytorch.org/whl/cu130"
explicit = true

[[tool.uv.index]]
name = "testpypi"
url = "https://test.pypi.org/simple/"
publish-url = "https://test.pypi.org/legacy/"
explicit = true