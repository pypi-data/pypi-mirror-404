"""Conversation compaction utilities."""

from pydantic_ai.messages import ModelMessage
from pydantic_ai.usage import RequestUsage

from shotgun.agents.models import AgentDeps
from shotgun.logging_config import get_logger
from shotgun.posthog_telemetry import track_event

from .token_estimation import estimate_tokens_from_messages

logger = get_logger(__name__)


async def apply_persistent_compaction(
    messages: list[ModelMessage], deps: AgentDeps, force: bool = False
) -> list[ModelMessage]:
    """Apply compaction to message history for persistent storage.

    This ensures that compacted history is actually used as the conversation baseline,
    preventing cascading compaction issues across both CLI and TUI usage patterns.

    Compaction happens in two phases:
    1. Deterministic pre-compaction: Remove file content (no LLM needed)
    2. LLM-based compaction: Summarize conversation if still over threshold

    Args:
        messages: Full message history from agent run
        deps: Agent dependencies containing model config
        force: If True, force compaction even if below token threshold

    Returns:
        Compacted message history that should be stored as conversation state
    """
    from .file_content_deduplication import deduplicate_file_content
    from .history_processors import token_limit_compactor

    try:
        # STEP 1: Deterministic pre-compaction (no LLM cost)
        # Remove file content from tool returns - files are still accessible
        # via retrieve_code (codebase) or read_file (.shotgun/ folder)
        messages, tokens_saved = deduplicate_file_content(
            messages,
            retention_window=3,  # Keep last 3 messages' file content intact
        )

        if tokens_saved > 0:
            logger.info(
                f"Pre-compaction: removed ~{tokens_saved:,} tokens of file content"
            )
            track_event(
                "file_content_deduplication",
                {
                    "tokens_saved_estimate": tokens_saved,
                    "retention_window": 3,
                    "model_name": deps.llm_model.name_str,
                },
            )

        # STEP 2: Count tokens after pre-compaction
        estimated_tokens = await estimate_tokens_from_messages(messages, deps.llm_model)

        # Create minimal usage info for compaction check
        usage = RequestUsage(
            input_tokens=estimated_tokens,
            output_tokens=0,
        )

        # Create a minimal context object for compaction
        class MockContext:
            def __init__(self, deps: AgentDeps, usage: RequestUsage | None):
                self.deps = deps
                self.usage = usage

        ctx = MockContext(deps, usage)
        compacted_messages = await token_limit_compactor(ctx, messages, force=force)

        # Log the result for monitoring
        original_size = len(messages)
        compacted_size = len(compacted_messages)

        if compacted_size < original_size:
            reduction_pct = ((original_size - compacted_size) / original_size) * 100
            logger.debug(
                f"Persistent compaction applied: {original_size} â†’ {compacted_size} messages "
                f"({reduction_pct:.1f}% reduction)"
            )

            # Track persistent compaction event with simple metrics (fast, no token counting)
            track_event(
                "persistent_compaction_applied",
                {
                    # Basic compaction metrics
                    "messages_before": original_size,
                    "messages_after": compacted_size,
                    "reduction_percentage": round(reduction_pct, 2),
                    "agent_mode": deps.agent_mode.value
                    if hasattr(deps, "agent_mode") and deps.agent_mode
                    else "unknown",
                    # Model and provider info (no computation needed)
                    "model_name": deps.llm_model.name_str,
                    "provider": deps.llm_model.provider.value,
                    "key_provider": deps.llm_model.key_provider.value,
                },
            )
        else:
            logger.debug(
                f"No persistent compaction needed: {original_size} messages unchanged"
            )

        return compacted_messages

    except Exception as e:
        # If compaction fails, return original messages
        # This ensures the system remains functional even if compaction has issues
        logger.warning(f"Persistent compaction failed, using original history: {e}")
        return messages


def should_apply_persistent_compaction(deps: AgentDeps) -> bool:
    """Check if persistent compaction should be applied.

    Args:
        deps: Agent dependencies

    Returns:
        True if persistent compaction should be applied
    """
    # For now, always apply persistent compaction
    # Future: Add configuration option in deps or environment variable
    return True
