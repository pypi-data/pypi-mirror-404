"""
Integration tests for NodePK migration utilities.

These tests validate the migration logic for discovering nodes, detecting orphans,
and performing bulk insertions. All tests run against live IRIS database.

Test Strategy:
- Tests written BEFORE implementation (TDD)
- All tests should FAIL initially (migration utilities not implemented)
- After implementation, all tests should PASS
"""

import pytest
import os
import time
from dotenv import load_dotenv

# NOTE: iris_module is not needed - tests use iris_connection fixture from conftest.py
from scripts.migrations.migrate_to_nodepk import (
    discover_nodes,
    bulk_insert_nodes,
    detect_orphans,
    validate_migration,
    execute_migration,
    execute_sql_migration
)


# NOTE: iris_connection fixture is provided by tests/conftest.py
# Do not define a local fixture here to avoid shadowing


@pytest.fixture
def iris_connection_with_sample_data(iris_connection):
    """Create sample data across all tables for testing migration."""
    cursor = iris_connection.cursor()

    # Clean up first - IRIS uppercases VARCHAR so we need both cases
    test_prefixes = ['TEST:', 'SAMPLE:', 'SAMPLE', 'TEST']  # Include uppercase versions
    for prefix in test_prefixes:
        try:
            # Delete from dependent tables first (FK constraints)
            cursor.execute("DELETE FROM rdf_edges WHERE s LIKE ? OR o_id LIKE ?", [f"{prefix}%", f"{prefix}%"])
            cursor.execute("DELETE FROM rdf_props WHERE s LIKE ?", [f"{prefix}%"])
            cursor.execute("DELETE FROM rdf_labels WHERE s LIKE ?", [f"{prefix}%"])
            cursor.execute("DELETE FROM nodes WHERE node_id LIKE ?", [f"{prefix}%"])
            # Try kg_NodeEmbeddings if it exists
            try:
                cursor.execute("DELETE FROM kg_NodeEmbeddings WHERE id LIKE ?", [f"{prefix}%"])
            except:
                pass
            iris_connection.commit()
        except:
            iris_connection.rollback()

    # Create sample data - nodes must be created first due to FK constraints
    # Create all nodes that will be referenced
    sample_nodes = ['SAMPLE:node1', 'SAMPLE:node2', 'SAMPLE:node3', 'SAMPLE:node4',
                    'SAMPLE:node5', 'SAMPLE:node6', 'SAMPLE:node7']
    for node_id in sample_nodes:
        cursor.execute("INSERT INTO nodes (node_id) VALUES (?)", [node_id])

    # Labels
    cursor.execute("INSERT INTO rdf_labels (s, label) VALUES (?, ?)", ['SAMPLE:node1', 'type1'])
    cursor.execute("INSERT INTO rdf_labels (s, label) VALUES (?, ?)", ['SAMPLE:node2', 'type2'])
    cursor.execute("INSERT INTO rdf_labels (s, label) VALUES (?, ?)", ['SAMPLE:node3', 'type1'])

    # Properties
    cursor.execute("INSERT INTO rdf_props (s, key, val) VALUES (?, ?, ?)", ['SAMPLE:node1', 'prop1', 'val1'])
    cursor.execute("INSERT INTO rdf_props (s, key, val) VALUES (?, ?, ?)", ['SAMPLE:node2', 'prop2', 'val2'])
    cursor.execute("INSERT INTO rdf_props (s, key, val) VALUES (?, ?, ?)", ['SAMPLE:node4', 'prop3', 'val3'])

    # Edges (introduces more nodes) - edge_id auto-generated by IDENTITY column
    cursor.execute("INSERT INTO rdf_edges (s, p, o_id) VALUES (?, ?, ?)",
                   ['SAMPLE:node1', 'rel1', 'SAMPLE:node5'])
    cursor.execute("INSERT INTO rdf_edges (s, p, o_id) VALUES (?, ?, ?)",
                   ['SAMPLE:node5', 'rel2', 'SAMPLE:node6'])
    cursor.execute("INSERT INTO rdf_edges (s, p, o_id) VALUES (?, ?, ?)",
                   ['SAMPLE:node2', 'rel3', 'SAMPLE:node3'])

    # Embeddings - skip if kg_NodeEmbeddings doesn't exist
    try:
        dummy_vector = '[' + ','.join(['0.1'] * 768) + ']'
        cursor.execute("INSERT INTO kg_NodeEmbeddings (id, emb) VALUES (?, TO_VECTOR(?))", ['SAMPLE:node1', dummy_vector])
        cursor.execute("INSERT INTO kg_NodeEmbeddings (id, emb) VALUES (?, TO_VECTOR(?))", ['SAMPLE:node7', dummy_vector])
    except:
        pass  # kg_NodeEmbeddings may not exist

    iris_connection.commit()
    yield iris_connection

    # Cleanup after test
    for prefix in test_prefixes:
        try:
            cursor.execute("DELETE FROM kg_NodeEmbeddings WHERE id LIKE ?", [f"{prefix}%"])
            cursor.execute("DELETE FROM rdf_edges WHERE s LIKE ? OR o_id LIKE ?", [f"{prefix}%", f"{prefix}%"])
            cursor.execute("DELETE FROM rdf_props WHERE s LIKE ?", [f"{prefix}%"])
            cursor.execute("DELETE FROM rdf_labels WHERE s LIKE ?", [f"{prefix}%"])
            cursor.execute("DELETE FROM nodes WHERE node_id LIKE ?", [f"{prefix}%"])
            iris_connection.commit()
        except:
            iris_connection.rollback()


@pytest.mark.requires_database
@pytest.mark.integration
class TestBulkNodeInsertion:
    """Contract 7: Bulk Node Insertion tests."""

    def test_bulk_insert_discovers_all_nodes(self, iris_connection_with_sample_data):
        """
        GIVEN: sample data across rdf_labels, rdf_props, rdf_edges, kg_NodeEmbeddings
        WHEN: running node discovery
        THEN: all unique node IDs are discovered

        Expected: FAIL initially (discover_nodes() not implemented)
        """
        # Discover nodes
        discovered_nodes = discover_nodes(iris_connection_with_sample_data)

        # Expected unique nodes from sample data:
        # From labels: node1, node2, node3
        # From props: node1, node2, node4
        # From edges (s): node1, node5, node2
        # From edges (o_id): node5, node6, node3
        # From embeddings: node1, node7 (if kg_NodeEmbeddings exists)
        # Unique total: node1, node2, node3, node4, node5, node6 (+ node7 if embeddings table exists)

        # IRIS uppercases VARCHAR values by default
        expected_nodes = {
            'SAMPLE:NODE1', 'SAMPLE:NODE2', 'SAMPLE:NODE3', 'SAMPLE:NODE4',
            'SAMPLE:NODE5', 'SAMPLE:NODE6'
        }

        # Filter discovered nodes to only those starting with SAMPLE:
        # (database may contain other data from production use)
        discovered_set = {n for n in discovered_nodes if n.startswith('SAMPLE:')}

        # Check base nodes are all present
        assert expected_nodes.issubset(discovered_set), \
            f"Missing expected nodes. Expected {expected_nodes}, got {discovered_set}"

        # node7 is optional (depends on kg_NodeEmbeddings table)
        if 'SAMPLE:NODE7' in discovered_set:
            expected_nodes.add('SAMPLE:NODE7')

        assert discovered_set == expected_nodes, \
            f"Unexpected SAMPLE nodes found. Expected {expected_nodes}, got {discovered_set}"

    def test_bulk_insert_handles_duplicates(self, iris_connection_with_sample_data):
        """
        GIVEN: nodes table exists with sample nodes already inserted
        WHEN: bulk inserting discovered nodes (which includes duplicates)
        THEN: duplicates are ignored and each node appears only once

        Expected: FAIL initially (bulk_insert_nodes() not implemented)
        """
        # Nodes table already exists from fixture with 7 nodes inserted
        # Try to insert them again - should handle duplicates gracefully
        discovered_nodes = discover_nodes(iris_connection_with_sample_data)

        # Filter to only SAMPLE:* nodes (database may contain other data)
        sample_nodes = [n for n in discovered_nodes if n.startswith('SAMPLE:')]

        # Insert again (should handle duplicates)
        inserted_count = bulk_insert_nodes(iris_connection_with_sample_data, sample_nodes)

        # Verify each node appears only once (no duplicates created)
        cursor = iris_connection_with_sample_data.cursor()
        cursor.execute("SELECT COUNT(DISTINCT node_id), COUNT(*) FROM nodes WHERE node_id LIKE 'SAMPLE:%'")
        unique_count, total_count = cursor.fetchone()

        assert unique_count == total_count, "Each node should appear only once (no duplicates)"
        # inserted_count should be 0 since all nodes already exist
        assert inserted_count == 0, f"Should insert 0 nodes (all duplicates), got {inserted_count}"

    def test_bulk_insert_performance(self, iris_connection):
        """
        GIVEN: nodes table exists
        WHEN: inserting 10,000 nodes
        THEN: insertion rate is ≥1000 nodes/second

        Expected: FAIL initially (bulk_insert_nodes() not implemented)
        """
        # Create nodes table
        cursor = iris_connection.cursor()
        try:
            cursor.execute("CREATE TABLE IF NOT EXISTS nodes(node_id VARCHAR(256) PRIMARY KEY NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)")
            iris_connection.commit()
        except:
            iris_connection.rollback()

        # Generate 10K test nodes
        test_nodes = [f'TEST:perf_node_{i}' for i in range(10000)]

        # Measure insertion time
        start_time = time.time()
        inserted_count = bulk_insert_nodes(iris_connection, test_nodes)
        end_time = time.time()

        # Calculate rate
        duration = end_time - start_time
        rate = inserted_count / duration if duration > 0 else 0

        # Cleanup
        cursor.execute("DELETE FROM nodes WHERE node_id LIKE 'TEST:perf_node_%'")
        iris_connection.commit()

        assert inserted_count == 10000, f"Should insert all 10K nodes, got {inserted_count}"
        assert rate >= 1000, f"Insertion rate {rate:.2f} nodes/sec should be ≥1000 nodes/sec"


@pytest.mark.requires_database
@pytest.mark.integration
class TestNodeDiscovery:
    """Test node discovery from existing data."""

    def test_discover_nodes_from_labels(self, iris_connection):
        """Test discovering nodes from rdf_labels table."""
        cursor = iris_connection.cursor()

        # Create nodes first (FK constraint requires this)
        cursor.execute("INSERT INTO nodes (node_id) VALUES (?)", ['TEST:label_node1'])
        cursor.execute("INSERT INTO nodes (node_id) VALUES (?)", ['TEST:label_node2'])
        # Create test data
        cursor.execute("INSERT INTO rdf_labels (s, label) VALUES (?, ?)", ['TEST:label_node1', 'label1'])
        cursor.execute("INSERT INTO rdf_labels (s, label) VALUES (?, ?)", ['TEST:label_node2', 'label2'])
        iris_connection.commit()

        # Discover nodes
        nodes = discover_nodes(iris_connection)

        # Cleanup
        cursor.execute("DELETE FROM rdf_labels WHERE s LIKE 'TEST:label_node%'")
        cursor.execute("DELETE FROM nodes WHERE node_id LIKE 'TEST:label_node%'")
        iris_connection.commit()

        # IRIS uppercases VARCHAR values
        assert 'TEST:LABEL_NODE1' in nodes or 'TEST:label_node1' in nodes
        assert 'TEST:LABEL_NODE2' in nodes or 'TEST:label_node2' in nodes

    def test_discover_nodes_from_props(self, iris_connection):
        """Test discovering nodes from rdf_props table."""
        cursor = iris_connection.cursor()

        # Create nodes first (FK constraint requires this)
        cursor.execute("INSERT INTO nodes (node_id) VALUES (?)", ['TEST:prop_node1'])
        cursor.execute("INSERT INTO nodes (node_id) VALUES (?)", ['TEST:prop_node2'])
        # Create test data
        cursor.execute("INSERT INTO rdf_props (s, key, val) VALUES (?, ?, ?)", ['TEST:prop_node1', 'k1', 'v1'])
        cursor.execute("INSERT INTO rdf_props (s, key, val) VALUES (?, ?, ?)", ['TEST:prop_node2', 'k2', 'v2'])
        iris_connection.commit()

        # Discover nodes
        nodes = discover_nodes(iris_connection)

        # Cleanup
        cursor.execute("DELETE FROM rdf_props WHERE s LIKE 'TEST:prop_node%'")
        cursor.execute("DELETE FROM nodes WHERE node_id LIKE 'TEST:prop_node%'")
        iris_connection.commit()

        # IRIS uppercases VARCHAR values
        assert 'TEST:PROP_NODE1' in nodes or 'TEST:prop_node1' in nodes
        assert 'TEST:PROP_NODE2' in nodes or 'TEST:prop_node2' in nodes

    def test_discover_nodes_from_edges_source(self, iris_connection):
        """Test discovering source nodes from rdf_edges."""
        cursor = iris_connection.cursor()

        # Create nodes first (FK constraint requires this)
        cursor.execute("INSERT INTO nodes (node_id) VALUES (?)", ['TEST:edge_source1'])
        cursor.execute("INSERT INTO nodes (node_id) VALUES (?)", ['TEST:edge_dest1'])
        # Create test data - edge_id auto-generated by IDENTITY column
        cursor.execute("INSERT INTO rdf_edges (s, p, o_id) VALUES (?, ?, ?)",
                      ['TEST:edge_source1', 'rel', 'TEST:edge_dest1'])
        iris_connection.commit()

        # Discover nodes
        nodes = discover_nodes(iris_connection)

        # Cleanup
        cursor.execute("DELETE FROM rdf_edges WHERE s LIKE 'TEST:edge_%' OR o_id LIKE 'TEST:edge_%'")
        cursor.execute("DELETE FROM nodes WHERE node_id LIKE 'TEST:edge_%'")
        iris_connection.commit()

        # IRIS uppercases VARCHAR values
        assert 'TEST:EDGE_SOURCE1' in nodes or 'TEST:edge_source1' in nodes

    def test_discover_nodes_from_edges_dest(self, iris_connection):
        """Test discovering destination nodes from rdf_edges."""
        cursor = iris_connection.cursor()

        # Create nodes first (FK constraint requires this)
        cursor.execute("INSERT INTO nodes (node_id) VALUES (?)", ['TEST:edge_source2'])
        cursor.execute("INSERT INTO nodes (node_id) VALUES (?)", ['TEST:edge_dest2'])
        # Create test data - edge_id auto-generated by IDENTITY column
        cursor.execute("INSERT INTO rdf_edges (s, p, o_id) VALUES (?, ?, ?)",
                      ['TEST:edge_source2', 'rel', 'TEST:edge_dest2'])
        iris_connection.commit()

        # Discover nodes
        nodes = discover_nodes(iris_connection)

        # Cleanup
        cursor.execute("DELETE FROM rdf_edges WHERE s LIKE 'TEST:edge_%' OR o_id LIKE 'TEST:edge_%'")
        cursor.execute("DELETE FROM nodes WHERE node_id LIKE 'TEST:edge_%'")
        iris_connection.commit()

        # IRIS uppercases VARCHAR values
        assert 'TEST:EDGE_DEST2' in nodes or 'TEST:edge_dest2' in nodes

    def test_discover_nodes_from_embeddings(self, iris_connection):
        """Test discovering nodes from kg_NodeEmbeddings."""
        cursor = iris_connection.cursor()

        # Create node first (FK constraint requires this)
        cursor.execute("INSERT INTO nodes (node_id) VALUES (?)", ['TEST:emb_node1'])
        # Create test data
        dummy_vector = '[' + ','.join(['0.1'] * 768) + ']'
        cursor.execute("INSERT INTO kg_NodeEmbeddings (id, emb) VALUES (?, TO_VECTOR(?))",
                      ['TEST:emb_node1', dummy_vector])
        iris_connection.commit()

        # Discover nodes
        nodes = discover_nodes(iris_connection)

        # Cleanup
        cursor.execute("DELETE FROM kg_NodeEmbeddings WHERE id LIKE 'TEST:emb_node%'")
        cursor.execute("DELETE FROM nodes WHERE node_id LIKE 'TEST:emb_node%'")
        iris_connection.commit()

        # IRIS uppercases VARCHAR values
        assert 'TEST:EMB_NODE1' in nodes or 'TEST:emb_node1' in nodes


@pytest.mark.requires_database
@pytest.mark.integration
class TestMigrationValidation:
    """Test suite for T024: validate_migration() function"""

    def test_validation_passes_with_valid_data(self, iris_connection_with_sample_data):
        """Test that validation passes when database has valid data with FK constraints"""
        from scripts.migrations.migrate_to_nodepk import validate_migration

        cursor = iris_connection_with_sample_data.cursor()

        # Clean up any orphaned embeddings from other tests first
        # (GraphQL tests may leave orphaned PROTEIN:* embeddings)
        try:
            cursor.execute("""
                DELETE FROM kg_NodeEmbeddings
                WHERE id NOT IN (SELECT node_id FROM nodes)
            """)
            iris_connection_with_sample_data.commit()
        except:
            iris_connection_with_sample_data.rollback()

        report = validate_migration(iris_connection_with_sample_data)

        # Validation should pass
        assert report['ready_for_migration'] is True, f"Validation should pass with valid data. Issues: {report.get('issues', [])}, Orphans: {report.get('orphans', {})}"
        assert len(report['issues']) == 0, f"Should have no issues, got: {report['issues']}"

        # Check node count
        assert report['node_count'] > 0, "Should discover at least some nodes"

        # Check no orphans (after cleanup)
        assert len(report['orphans']) == 0, f"Should have no orphans with FK constraints enforced, found: {report['orphans']}"

        # Check table breakdown exists
        assert 'rdf_labels' in report['table_breakdown']
        assert 'rdf_props' in report['table_breakdown']
        assert 'rdf_edges_source' in report['table_breakdown']
        assert 'rdf_edges_dest' in report['table_breakdown']

    def test_validation_report_structure(self, iris_connection_with_sample_data):
        """Test that validation report has all required fields"""
        from scripts.migrations.migrate_to_nodepk import validate_migration

        report = validate_migration(iris_connection_with_sample_data)

        # Check all required fields exist
        required_fields = [
            'discovered_nodes', 'node_count', 'orphans',
            'ready_for_migration', 'issues', 'table_breakdown'
        ]
        for field in required_fields:
            assert field in report, f"Report missing required field: {field}"

        # Check types
        assert isinstance(report['discovered_nodes'], list)
        assert isinstance(report['node_count'], int)
        assert isinstance(report['orphans'], dict)
        assert isinstance(report['ready_for_migration'], bool)
        assert isinstance(report['issues'], list)
        assert isinstance(report['table_breakdown'], dict)


@pytest.mark.requires_database
@pytest.mark.integration
class TestOrphanDetection:
    """Test orphan detection functionality."""

    @pytest.mark.skip(reason="Cannot create orphans with FK constraints in place - orphan detection is for pre-migration validation")
    def test_detect_orphaned_edges(self, iris_connection):
        """
        GIVEN: edges exist but nodes table is populated without some referenced nodes
        WHEN: running orphan detection
        THEN: orphaned references are detected

        NOTE: This scenario is impossible with FK constraints enabled.
        Orphan detection is meant for pre-migration validation where FK constraints
        don't exist yet. With FK constraints in place (as they are now), orphans
        cannot be created through normal database operations.

        Expected: FAIL initially (detect_orphans() not implemented)
        """
        pass  # Test skipped - see reason above

    def test_no_orphans_detected_valid_data(self, iris_connection):
        """
        GIVEN: all references are valid (nodes exist)
        WHEN: running orphan detection
        THEN: no orphans detected for TEST:* prefixed data

        Expected: FAIL initially (detect_orphans() not implemented)
        """
        cursor = iris_connection.cursor()

        # Clean up any orphaned embeddings from other tests first
        # (GraphQL tests may leave orphaned PROTEIN:* embeddings)
        try:
            cursor.execute("""
                DELETE FROM kg_NodeEmbeddings
                WHERE id NOT IN (SELECT node_id FROM nodes)
            """)
            iris_connection.commit()
        except:
            iris_connection.rollback()

        # Clean up any leftover TEST:* data
        try:
            cursor.execute("DELETE FROM rdf_edges WHERE s LIKE 'TEST:%' OR o_id LIKE 'TEST:%'")
            cursor.execute("DELETE FROM nodes WHERE node_id LIKE 'TEST:%'")
            iris_connection.commit()
        except:
            iris_connection.rollback()

        # Create nodes table and populate
        try:
            cursor.execute("CREATE TABLE IF NOT EXISTS nodes(node_id VARCHAR(256) PRIMARY KEY NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)")
            cursor.execute("INSERT INTO nodes (node_id) VALUES (?)", ['TEST:node_a'])
            cursor.execute("INSERT INTO nodes (node_id) VALUES (?)", ['TEST:node_b'])
            iris_connection.commit()
        except:
            iris_connection.rollback()

        # Create valid edge - edge_id auto-generated by IDENTITY column
        cursor.execute("INSERT INTO rdf_edges (s, p, o_id) VALUES (?, ?, ?)",
                      ['TEST:node_a', 'rel', 'TEST:node_b'])
        iris_connection.commit()

        # Detect orphans
        orphans = detect_orphans(iris_connection)

        # Cleanup
        cursor.execute("DELETE FROM rdf_edges WHERE s LIKE 'TEST:%' OR o_id LIKE 'TEST:%'")
        cursor.execute("DELETE FROM nodes WHERE node_id LIKE 'TEST:%'")
        iris_connection.commit()

        # Filter orphans to only TEST:* prefixed data (other data may exist from other tests)
        test_orphans = {}
        for table, orphan_list in orphans.items():
            test_prefix_orphans = [o for o in orphan_list if o.startswith('TEST:')]
            if test_prefix_orphans:
                test_orphans[table] = test_prefix_orphans

        # Should have no TEST:* orphans
        total_test_orphans = sum(len(orphan_list) for orphan_list in test_orphans.values())
        assert total_test_orphans == 0, f"Expected no TEST:* orphans, found: {test_orphans}"


@pytest.mark.requires_database
@pytest.mark.integration
class TestMigrationWorkflow:
    """Test full migration workflow."""

    def test_migration_validate_only_mode(self, iris_connection_with_sample_data):
        """
        GIVEN: existing graph data with nodes table and FK constraints
        WHEN: running migration validation
        THEN: report is generated reflecting current state

        Note: This test validates the report structure. Since FK constraints
        require nodes to exist before dependent data, the fixture creates
        nodes first. The validation mode should correctly report the current
        state without making changes.
        """
        # Get node count before validation
        cursor = iris_connection_with_sample_data.cursor()
        cursor.execute("SELECT COUNT(*) FROM nodes WHERE node_id LIKE 'SAMPLE:%'")
        node_count_before = cursor.fetchone()[0]

        # Run validation
        report = validate_migration(iris_connection_with_sample_data)

        # Check report structure (discovered_nodes is a list, node_count is an int)
        assert 'discovered_nodes' in report
        assert 'node_count' in report
        assert 'orphans' in report
        assert 'ready_for_migration' in report
        assert 'issues' in report
        assert 'table_breakdown' in report

        # Check that discovered_nodes is a list
        assert isinstance(report['discovered_nodes'], list)
        assert isinstance(report['node_count'], int)

        # Filter to only SAMPLE:* nodes (database may contain other data)
        sample_nodes = [n for n in report['discovered_nodes'] if n.startswith('SAMPLE:')]

        # Verify SAMPLE node count matches what was created by fixture (7 nodes)
        assert len(sample_nodes) == 7, f"Expected 7 SAMPLE:* nodes, got {len(sample_nodes)}"

        # Verify no changes were made (count should be the same)
        cursor.execute("SELECT COUNT(*) FROM nodes WHERE node_id LIKE 'SAMPLE:%'")
        node_count_after = cursor.fetchone()[0]
        assert node_count_before == node_count_after, "Validation should not modify database"

    def test_migration_execute_mode(self, iris_connection_with_sample_data):
        """
        GIVEN: existing graph data without nodes table
        WHEN: running migration with --execute
        THEN: nodes table created, populated, and FK constraints added

        Expected: FAIL initially (execute_migration() not implemented)
        """
        # Execute migration
        success = execute_migration(iris_connection_with_sample_data)

        assert success, "Migration should succeed"

        cursor = iris_connection_with_sample_data.cursor()

        # Verify nodes table exists and is populated
        cursor.execute("SELECT COUNT(*) FROM nodes WHERE node_id LIKE 'SAMPLE:%'")
        node_count = cursor.fetchone()[0]
        assert node_count == 7, f"Expected 7 nodes, got {node_count}"

        # Verify FK constraints exist by trying to insert invalid edge
        try:
            cursor.execute("INSERT INTO rdf_edges (s, p, o_id) VALUES (?, ?, ?)",
                          ['INVALID:node', 'rel', 'SAMPLE:node1'])
            iris_connection_with_sample_data.commit()
            assert False, "FK constraint should prevent invalid edge insertion"
        except Exception as e:
            # Expected - FK constraint violation
            iris_connection_with_sample_data.rollback()
            assert 'foreign key' in str(e).lower() or 'constraint' in str(e).lower()

    def test_migration_idempotent(self, iris_connection_with_sample_data):
        """
        GIVEN: migration has already been executed
        WHEN: running migration again
        THEN: second run is no-op (idempotent)

        Expected: FAIL initially (execute_migration() not implemented)
        """
        # First execution
        success1 = execute_migration(iris_connection_with_sample_data)
        assert success1, "First migration should succeed"

        cursor = iris_connection_with_sample_data.cursor()
        cursor.execute("SELECT COUNT(*) FROM nodes WHERE node_id LIKE 'SAMPLE:%'")
        count_after_first = cursor.fetchone()[0]

        # Second execution
        success2 = execute_migration(iris_connection_with_sample_data)
        assert success2, "Second migration should succeed (no-op)"

        cursor.execute("SELECT COUNT(*) FROM nodes WHERE node_id LIKE 'SAMPLE:%'")
        count_after_second = cursor.fetchone()[0]

        assert count_after_first == count_after_second, \
            "Node count should not change on second migration"