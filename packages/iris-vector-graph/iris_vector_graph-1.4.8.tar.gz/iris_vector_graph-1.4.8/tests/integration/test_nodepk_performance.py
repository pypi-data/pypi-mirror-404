#!/usr/bin/env python3
"""
Performance validation tests for NodePK feature (T028).

These tests verify that the NodePK feature meets all performance requirements:
- Node lookup: <1ms
- Bulk node insertion: ≥1000 nodes/second
- FK constraint overhead: <10% degradation

Constitutional Compliance:
- Principle II: Live IRIS database testing (all tests use real IRIS)
- Principle III: Performance as a feature (gates prevent regression)
"""

import pytest
import time
from scripts.migrations.migrate_to_nodepk import (
    get_connection, bulk_insert_nodes, discover_nodes
)


@pytest.fixture
def iris_connection_for_performance():
    """
    Get clean IRIS connection for performance testing.
    Does NOT use sample data to ensure clean baseline.
    """
    conn = get_connection()
    cursor = conn.cursor()

    # Clean up any test data from previous runs
    try:
        cursor.execute("DELETE FROM rdf_edges WHERE s LIKE 'PERF:%'")
        cursor.execute("DELETE FROM nodes WHERE node_id LIKE 'PERF:%'")
        conn.commit()
    except:
        conn.rollback()

    yield conn

    # Cleanup after test
    try:
        cursor.execute("DELETE FROM rdf_edges WHERE s LIKE 'PERF:%'")
        cursor.execute("DELETE FROM nodes WHERE node_id LIKE 'PERF:%'")
        conn.commit()
    except:
        conn.rollback()

    conn.close()


@pytest.mark.performance
@pytest.mark.requires_database
class TestNodePKPerformance:
    """Performance validation tests for NodePK feature."""

    def test_node_lookup_under_1ms(self, iris_connection_for_performance):
        """
        Verify node lookup by primary key is <1ms even at scale.

        Performance gate: <1ms per lookup
        """
        cursor = iris_connection_for_performance.cursor()

        # Insert 1000 test nodes
        test_nodes = [f'PERF:lookup_test_{i}' for i in range(1000)]
        bulk_insert_nodes(iris_connection_for_performance, test_nodes)

        # Warm up (first query may be slower due to caching)
        cursor.execute("SELECT * FROM nodes WHERE node_id = ?", ['PERF:lookup_test_500'])
        cursor.fetchall()

        # Measure 100 lookups
        total_time = 0
        iterations = 100

        for i in range(iterations):
            node_id = f'PERF:lookup_test_{i % 1000}'
            start = time.perf_counter()
            cursor.execute("SELECT * FROM nodes WHERE node_id = ?", [node_id])
            result = cursor.fetchall()
            end = time.perf_counter()

            assert len(result) == 1, f"Should find exactly one node, got {len(result)}"
            total_time += (end - start)

        avg_time_ms = (total_time / iterations) * 1000

        print(f"\n  Average lookup time: {avg_time_ms:.3f}ms (target: <1ms)")
        print(f"  Total iterations: {iterations}")
        print(f"  Total time: {total_time * 1000:.2f}ms")

        assert avg_time_ms < 1.0, \
            f"Node lookup took {avg_time_ms:.3f}ms, should be <1ms"

    def test_bulk_insert_1000_per_second(self, iris_connection_for_performance):
        """
        Verify bulk node insertion achieves ≥1000 nodes/second.

        Performance gate: ≥1000 nodes/sec
        """
        # Generate 10K test nodes
        test_nodes = [f'PERF:bulk_insert_{i}' for i in range(10000)]

        # Measure insertion time
        start_time = time.perf_counter()
        inserted_count = bulk_insert_nodes(iris_connection_for_performance, test_nodes)
        end_time = time.perf_counter()

        # Calculate rate
        duration = end_time - start_time
        rate = inserted_count / duration if duration > 0 else 0

        print(f"\n  Inserted {inserted_count} nodes in {duration:.2f}s")
        print(f"  Insertion rate: {rate:.0f} nodes/sec (target: ≥1000 nodes/sec)")

        assert inserted_count == 10000, f"Should insert all 10K nodes, got {inserted_count}"
        assert rate >= 1000, \
            f"Insertion rate {rate:.0f} nodes/sec is below target (≥1000 nodes/sec)"

    @pytest.mark.xfail(reason="Performance varies by environment; baseline may not match Docker/CI")
    def test_edge_insert_degradation_under_10_percent(self, iris_connection_for_performance):
        """
        Verify FK constraint overhead is <10% for edge insertions.

        Performance gate: <10% degradation vs baseline

        Strategy:
        - Measure edge insertion rate with FK constraints in place
        - Compare against expected baseline (no FKs)
        - Since we can't easily disable FKs in test env, we use historical baseline
        """
        cursor = iris_connection_for_performance.cursor()

        # Create nodes for edges
        test_nodes = [f'PERF:edge_test_src_{i}' for i in range(1000)]
        test_nodes += [f'PERF:edge_test_dst_{i}' for i in range(1000)]
        bulk_insert_nodes(iris_connection_for_performance, test_nodes)

        # Measure edge insertion WITH FK constraints
        edge_count = 1000
        start_time = time.perf_counter()

        for i in range(edge_count):
            src = f'PERF:edge_test_src_{i}'
            dst = f'PERF:edge_test_dst_{i}'

            cursor.execute(
                "INSERT INTO rdf_edges (s, p, o_id) VALUES (?, ?, ?)",
                [src, 'perf:test', dst]
            )

        iris_connection_for_performance.commit()
        end_time = time.perf_counter()

        duration = end_time - start_time
        rate_with_fks = edge_count / duration if duration > 0 else 0

        # Historical baseline from tests without FK constraints
        # From T022 bulk insertion tests, we achieved ~3900 nodes/sec
        # Edge insertion without FKs is typically similar to node insertion
        baseline_rate = 3900  # nodes/sec from historical data

        # Calculate degradation
        degradation_pct = ((baseline_rate - rate_with_fks) / baseline_rate) * 100

        print(f"\n  Edge insertion WITH FK constraints:")
        print(f"    Inserted {edge_count} edges in {duration:.2f}s")
        print(f"    Rate: {rate_with_fks:.0f} edges/sec")
        print(f"  Historical baseline (no FKs): {baseline_rate:.0f} ops/sec")

        if degradation_pct < 0:
            print(f"  Improvement: {abs(degradation_pct):.1f}% faster (FK constraints improved performance!)")
        else:
            print(f"  Degradation: {degradation_pct:.1f}% (target: <10%)")

        # Verify performance gate
        # Note: Negative degradation means performance IMPROVED
        # We only care if degradation is positive and >10%
        assert degradation_pct < 10, \
            f"FK overhead {degradation_pct:.1f}% exceeds target (<10%)"

    def test_node_lookup_scales_with_data(self, iris_connection_for_performance):
        """
        Verify node lookup performance scales linearly (doesn't degrade significantly with dataset size).

        This test verifies that PRIMARY KEY index scaling is working correctly.
        We use <10ms as the gate (vs <1ms for warmed-up queries) because this test
        includes cache warmup overhead and measures at scale.
        """
        cursor = iris_connection_for_performance.cursor()

        # Insert 50K nodes
        print("\n  Inserting 50K nodes...")
        test_nodes = [f'PERF:scale_test_{i}' for i in range(50000)]
        bulk_insert_nodes(iris_connection_for_performance, test_nodes)

        # Verify total node count
        cursor.execute("SELECT COUNT(*) FROM nodes WHERE node_id LIKE 'PERF:scale_test_%'")
        count = cursor.fetchone()[0]
        print(f"  Inserted {count} nodes")

        # Warm up cache with a few queries
        for i in [0, 25000, 49999]:
            cursor.execute("SELECT * FROM nodes WHERE node_id = ?", [f'PERF:scale_test_{i}'])
            cursor.fetchall()

        # Measure lookups at different positions
        lookups = [
            ('PERF:scale_test_0', 'first'),
            ('PERF:scale_test_25000', 'middle'),
            ('PERF:scale_test_49999', 'last')
        ]

        for node_id, position in lookups:
            start = time.perf_counter()
            cursor.execute("SELECT * FROM nodes WHERE node_id = ?", [node_id])
            result = cursor.fetchall()
            end = time.perf_counter()

            lookup_time_ms = (end - start) * 1000

            assert len(result) == 1, f"Should find exactly one node at {position}"
            print(f"  Lookup at {position}: {lookup_time_ms:.3f}ms")

            # At 50K scale, lookups should still be fast (<10ms including driver overhead)
            # The core test_node_lookup_under_1ms validates <1ms for warmed queries
            assert lookup_time_ms < 10.0, \
                f"Lookup at {position} took {lookup_time_ms:.3f}ms, should scale well (<10ms)"
