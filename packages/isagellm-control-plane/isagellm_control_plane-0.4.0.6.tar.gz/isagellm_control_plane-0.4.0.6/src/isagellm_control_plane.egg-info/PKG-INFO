Metadata-Version: 2.4
Name: isagellm-control-plane
Version: 0.4.0.6
Summary: sageLLM Control Plane - Intelligent request routing, scheduling, and engine lifecycle management
Author: IntelliStream Team
License: Proprietary - IntelliStream
Project-URL: Homepage, https://github.com/IntelliStream/sagellm-control-plane
Project-URL: Repository, https://github.com/IntelliStream/sagellm-control-plane
Keywords: llm,inference,control-plane,scheduling,routing,autoscaling
Classifier: Development Status :: 3 - Alpha
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: ==3.11.*
Description-Content-Type: text/markdown
Requires-Dist: isagellm-protocol<0.5.0,>=0.4.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: httpx>=0.24.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: ruff>=0.8.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: isage-pypi-publisher>=0.2.0; extra == "dev"

# sageLLM Control Plane

## Protocol Compliance (Mandatory)

- MUST follow Protocol v0.1: https://github.com/intellistream/sagellm-docs/blob/main/docs/specs/protocol_v0.1.md
- Any globally shared definitions (fields, error codes, metrics, IDs, schemas) MUST be added to Protocol first.

[![CI Status](https://github.com/intellistream/sagellm-control-plane/actions/workflows/ci.yml/badge.svg)](https://github.com/intellistream/sagellm-control-plane/actions/workflows/ci.yml)
[![PyPI version](https://badge.fury.io/py/isagellm-control-plane.svg)](https://badge.fury.io/py/isagellm-control-plane)
[![Python Versions](https://img.shields.io/pypi/pyversions/isagellm-control-plane.svg)](https://pypi.org/project/isagellm-control-plane/)
[![License](https://img.shields.io/badge/License-Proprietary-red.svg)](LICENSE)
[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)

**Intelligent request routing, scheduling, and engine lifecycle management for sageLLM.**

## Features

- ğŸ¯ **Scheduling Policies** - FIFO, Priority, SLO-aware, Cost-optimized, Adaptive
- âš–ï¸ **Load Balancing** - Intelligent request routing across multiple engine instances
- ğŸ“ˆ **Autoscaling** - SLA-based autoscaling for Prefill/Decode instances
- ğŸ”„ **Engine Lifecycle** - Spawn, stop, health check, auto-restart
- ğŸ“Š **Observability** - Metrics collection, performance monitoring
- ğŸ§© **Parallelism** - TP, PP, DP, EP strategy optimization

## Installation

```bash
# Basic installation
pip install isagellm-control-plane

# With optional features
pip install isagellm-control-plane[gpu]      # GPU monitoring
pip install isagellm-control-plane[metrics]  # Prometheus metrics
pip install isagellm-control-plane[all]      # All features
```

**Requirements**: Python 3.10+

## ğŸš€ å¼€å‘è€…å¿«é€Ÿå¼€å§‹

```bash
git clone git@github.com:intellistream/sagellm-control-plane.git
cd sagellm-control-plane
./quickstart.sh   # ä¸€é”®å®‰è£…å¼€å‘ç¯å¢ƒï¼ˆå«ä¾èµ–ï¼‰

# æˆ–æ‰‹åŠ¨å®‰è£…
pip install -e ".[dev]"
```

è¿è¡Œæµ‹è¯•ï¼š
```bash
pytest tests/ -v
```

## Quick Start

### Running Modes

| Mode | Use Case | Backend |
|------|----------|----------|
| **CPU** | Development/CI | HuggingFace Transformers |
| **GPU** | Production | CUDA/Ascend |

### CPU Mode (Development)

```python
from sagellm_core.llm_engine import LLMEngine, LLMEngineConfig
from sagellm_control import LocalEngineClient
from sagellm_protocol import Request

# Create LLMEngine with TinyLlama (unified hardware-agnostic engine)
config = LLMEngineConfig(
    model_path="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    backend_type="cpu",  # or "cuda", "ascend", "auto"
    max_new_tokens=50,
)
engine = LLMEngine(config)
await engine.start()

# Create local client
client = LocalEngineClient(engine)

# Execute request
request = Request(
    request_id="req-001",
    trace_id="trace-001",
    model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    prompt="What is AI?",
    max_tokens=30,
)

response = await client.execute_request(request)
print(f"Response: {response.output_text}")
print(f"TTFT: {response.metrics.ttft_ms:.2f}ms")

await engine.stop()
```

See [examples/cpu_engine_demo.py](examples/cpu_engine_demo.py) for complete examples.

### Execution API

Complete inference execution interface:

```python
from sagellm_control import ControlPlaneManager
from sagellm_protocol import Request

# ä½¿ç”¨æœ¬åœ°æ‰§è¡Œå™¨ï¼ˆCPU æ¨¡å¼ï¼‰
cp = ControlPlaneManager(mode="local")

# 1. éæµå¼æ¨ç†
request = Request(
    request_id="req-001",
    trace_id="trace-001",
    model="test-model",
    prompt="Hello, how are you?",
    max_tokens=100,
    stream=False,
)
response = await cp.execute_request(request)
print(f"Output: {response.output_text}")
print(f"TTFT: {response.metrics.ttft_ms:.2f} ms")

# 2. æµå¼æ¨ç†
async for event in cp.stream_request(request):
    if event.event == "delta":
        print(event.chunk, end="", flush=True)

# 3. æ–‡æœ¬åµŒå…¥
embeddings = await cp.get_embeddings(
    texts=["Text 1", "Text 2", "Text 3"],
    model_id="embedding-model"
)
print(f"Generated {len(embeddings)} embeddings of dimension {len(embeddings[0])}")
```

See [examples/execution_layer_demo.py](examples/execution_layer_demo.py) for more examples.

## Architecture

```
sagellm_control/
â”œâ”€â”€ types.py           # Core data types (RequestMetadata, EngineInfo, etc.)
â”œâ”€â”€ strategies/        # Scheduling policies (FIFO, Priority, SLO, etc.)
â”œâ”€â”€ executors/         # Execution coordinators (HTTP, LocalAsync)
â”œâ”€â”€ router.py          # Request routing and load balancing
â”œâ”€â”€ autoscaler.py      # SLA-based autoscaling
â”œâ”€â”€ parallelism.py     # Parallelism strategy optimization
â”œâ”€â”€ manager.py         # Main ControlPlaneManager
â””â”€â”€ engine_lifecycle.py # Engine lifecycle management
```



## Documentation

- [TODO.md](docs/TODO.md) - Development roadmap
- [TEAM.md](docs/TEAM.md) - Team assignments

## ğŸ”„ è´¡çŒ®æŒ‡å—

è¯·éµå¾ªä»¥ä¸‹å·¥ä½œæµç¨‹ï¼š

1. **åˆ›å»º Issue** - æè¿°é—®é¢˜/éœ€æ±‚
   ```bash
   gh issue create --title "[Bug] æè¿°" --label "bug,sagellm-control-plane"
   ```

2. **å¼€å‘ä¿®å¤** - åœ¨æœ¬åœ° `fix/#123-xxx` åˆ†æ”¯è§£å†³
   ```bash
   git checkout -b fix/#123-xxx origin/main-dev
   # å¼€å‘ã€æµ‹è¯•...
   pytest -v
   ruff format . && ruff check . --fix
   ```

3. **å‘èµ· PR** - æäº¤åˆ° `main-dev` åˆ†æ”¯
   ```bash
   gh pr create --base main-dev --title "Fix: æè¿°" --body "Closes #123"
   ```

4. **åˆå¹¶** - å®¡æ‰¹ååˆå¹¶åˆ° `main-dev`

æ›´å¤šè¯¦æƒ…è§ [.github/copilot-instructions.md](.github/copilot-instructions.md)

### Related Repositories
- [sagellm](https://github.com/intellistream/sagellm) - Umbrella åŒ… + CLI
- [sagellm-protocol](https://github.com/intellistream/sagellm-protocol) - åè®®å®šä¹‰
- [sagellm-backend](https://github.com/intellistream/sagellm-backend) - åç«¯æŠ½è±¡
- [sagellm-gateway](https://github.com/intellistream/sagellm-gateway) - API ç½‘å…³

---

## License

Proprietary - IntelliStream
