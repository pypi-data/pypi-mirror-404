"""MLLM (å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹) å‘½ä»¤ç»„"""

import os
import sys

# å¼ºåˆ¶å¯ç”¨é¢œè‰²æ”¯æŒ
os.environ["FORCE_COLOR"] = "1"
if not os.environ.get("TERM"):
    os.environ["TERM"] = "xterm-256color"

from rich.console import Console
from rich import print
from rich.markdown import Markdown

console = Console(
    force_terminal=True,
    width=100,
    color_system="windows",
    legacy_windows=True,
    safe_box=True
)


def safe_print(*args, **kwargs):
    """å®‰å…¨çš„æ‰“å°å‡½æ•°ï¼Œç¡®ä¿åœ¨æ‰€æœ‰ç»ˆç«¯ä¸­æ­£ç¡®æ˜¾ç¤ºé¢œè‰²"""
    try:
        console.print(*args, **kwargs)
    except Exception:
        # é™çº§åˆ°æ™®é€šprintï¼Œå¤„ç†ç¼–ç é—®é¢˜
        import re
        import sys
        import builtins

        clean_args = []
        for arg in args:
            if isinstance(arg, str):
                # å»é™¤rich markup
                clean_arg = re.sub(r"\[/?[^\]]*\]", "", str(arg))
                # å¤„ç†emojiå’Œç‰¹æ®Šå­—ç¬¦
                try:
                    # å°è¯•ç¼–ç ä¸ºgbk (Windowsé»˜è®¤ç¼–ç )
                    clean_arg.encode('gbk')
                    clean_args.append(clean_arg)
                except UnicodeEncodeError:
                    # å¦‚æœåŒ…å«æ— æ³•ç¼–ç çš„å­—ç¬¦ï¼Œæ›¿æ¢emojiä¸ºæ–‡æœ¬æè¿°
                    clean_arg = re.sub(r'âŒ', '[é”™è¯¯]', clean_arg)
                    clean_arg = re.sub(r'âœ…', '[æˆåŠŸ]', clean_arg)
                    clean_arg = re.sub(r'ğŸ’¡', '[æç¤º]', clean_arg)
                    clean_arg = re.sub(r'ğŸš€', '[å¯åŠ¨]', clean_arg)
                    clean_arg = re.sub(r'ğŸ“¦', '[æ¨¡å‹]', clean_arg)
                    clean_arg = re.sub(r'ğŸŒ', '[æœåŠ¡å™¨]', clean_arg)
                    clean_arg = re.sub(r'ğŸ‘‹', '[å†è§]', clean_arg)
                    clean_arg = re.sub(r'ğŸ“', '[è®°å½•]', clean_arg)
                    clean_arg = re.sub(r'âš ï¸', '[è­¦å‘Š]', clean_arg)
                    clean_arg = re.sub(r'ğŸ”', '[æœç´¢]', clean_arg)
                    clean_arg = re.sub(r'ğŸ¤–', '[æœºå™¨äºº]', clean_arg)
                    clean_arg = re.sub(r'ğŸ“¡', '[ç½‘ç»œ]', clean_arg)
                    clean_arg = re.sub(r'ğŸ”Œ', '[è¿æ¥]', clean_arg)
                    clean_arg = re.sub(r'ğŸ“‹', '[é…ç½®]', clean_arg)
                    clean_arg = re.sub(r'ğŸ“', '[æ–‡ä»¶]', clean_arg)
                    clean_arg = re.sub(r'ğŸ”§', '[è®¾ç½®]', clean_arg)
                    clean_arg = re.sub(r'ğŸ¯', '[ç›®æ ‡]', clean_arg)
                    clean_arg = re.sub(r'ğŸ“Š', '[ç»Ÿè®¡]', clean_arg)
                    clean_arg = re.sub(r'ğŸ§ ', '[æ€è€ƒ]', clean_arg)
                    clean_arg = re.sub(r'ğŸ’­', '[æ¨ç†]', clean_arg)
                    clean_arg = re.sub(r'ğŸ”—', '[é€»è¾‘]', clean_arg)
                    # ç§»é™¤å…¶ä»–æ— æ³•æ˜¾ç¤ºçš„emoji
                    clean_arg = re.sub(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002600-\U000027BF\U0001F900-\U0001F9FF]', '', clean_arg)
                    clean_args.append(clean_arg)
            else:
                clean_args.append(str(arg))

        # ä½¿ç”¨å†…ç½®print
        try:
            builtins.print(*clean_args, **kwargs)
        except UnicodeEncodeError:
            # æœ€åçš„é™çº§ï¼šä½¿ç”¨é”™è¯¯æ›¿æ¢
            safe_args = [arg.encode('gbk', errors='replace').decode('gbk') if isinstance(arg, str) else arg for arg in clean_args]
            builtins.print(*safe_args, **kwargs)


def safe_print_stream(text, **kwargs):
    """å®‰å…¨çš„æµå¼æ‰“å°å‡½æ•°ï¼Œç”¨äºæµå¼è¾“å‡º

    é»˜è®¤ä½¿ç”¨åŸç”Ÿ print å®ç°çœŸæ­£çš„æµå¼è¾“å‡ºï¼Œé¿å… Rich console çš„æ ¼å¼åŒ–å¹²æ‰°ã€‚
    """
    import builtins

    flush = kwargs.pop('flush', True)  # æµå¼è¾“å‡ºé»˜è®¤ flush
    end = kwargs.pop('end', '')  # æµå¼è¾“å‡ºé»˜è®¤ä¸æ¢è¡Œ

    try:
        builtins.print(text, end=end, flush=flush, **kwargs)
    except UnicodeEncodeError:
        # ç¼–ç å¤±è´¥æ—¶ï¼Œå°è¯•ä½¿ç”¨ stdout buffer
        if hasattr(sys.stdout, 'buffer'):
            sys.stdout.buffer.write(text.encode('utf-8', errors='replace'))
            if flush:
                sys.stdout.buffer.flush()
        else:
            # æœ€åçš„é™çº§æ–¹æ¡ˆï¼šæ›¿æ¢æ— æ³•ç¼–ç çš„å­—ç¬¦
            safe_text = text.encode('gbk', errors='replace').decode('gbk')
            builtins.print(safe_text, end=end, flush=flush, **kwargs)


def safe_print_markdown(content, **kwargs):
    """å®‰å…¨çš„Markdownæ¸²æŸ“å‡½æ•°"""
    try:
        # ä½¿ç”¨Richçš„Markdownæ¸²æŸ“
        markdown = Markdown(content)
        console.print(markdown, **kwargs)
    except Exception:
        # é™çº§åˆ°æ™®é€šæ‰“å°
        safe_print(content, **kwargs)


class StreamingMarkdownRenderer:
    """æµå¼Markdownæ¸²æŸ“å™¨ - å®æ—¶è§£æå¹¶æ¸²æŸ“Markdown"""

    def __init__(self):
        self.buffer = ""
        self.last_rendered_length = 0
        self.in_code_block = False
        self.code_block_lang = ""

    def add_token(self, token):
        """æ·»åŠ æ–°tokenå¹¶å°è¯•æ¸²æŸ“"""
        self.buffer += token
        self._try_render_incremental()

    def _try_render_incremental(self):
        """å°è¯•å¢é‡æ¸²æŸ“Markdown"""
        # æ£€æµ‹ä»£ç å—
        if "```" in self.buffer[self.last_rendered_length:]:
            code_block_matches = self.buffer.count("```")
            self.in_code_block = (code_block_matches % 2) == 1

        # å¦‚æœåœ¨ä»£ç å—ä¸­ï¼Œç›´æ¥è¾“å‡ºåŸå§‹æ–‡æœ¬
        if self.in_code_block:
            new_content = self.buffer[self.last_rendered_length:]
            if new_content:
                safe_print_stream(new_content, end="", flush=True)
                self.last_rendered_length = len(self.buffer)
            return

        # å°è¯•æ‰¾åˆ°å¯ä»¥å®‰å…¨æ¸²æŸ“çš„è¾¹ç•Œï¼ˆå¥å­ã€æ®µè½ç­‰ï¼‰
        render_boundary = self._find_render_boundary()
        if render_boundary > self.last_rendered_length:
            content_to_render = self.buffer[self.last_rendered_length:render_boundary]
            self._render_content(content_to_render)
            self.last_rendered_length = render_boundary

    def _find_render_boundary(self):
        """æ‰¾åˆ°é€‚åˆæ¸²æŸ“çš„è¾¹ç•Œä½ç½®"""
        content = self.buffer

        # å¯»æ‰¾å¥å­ç»“æŸæ ‡è®°
        for i in range(len(content) - 1, self.last_rendered_length - 1, -1):
            char = content[i]
            # å¥å­ç»“æŸ
            if char in '.!?ã€‚ï¼ï¼Ÿ':
                # ç¡®ä¿åé¢æœ‰ç©ºæ ¼æˆ–æ¢è¡Œï¼Œé¿å…è¯¯åˆ¤å°æ•°ç‚¹ç­‰
                if i + 1 < len(content) and content[i + 1] in ' \n\t':
                    return i + 1
            # æ®µè½ç»“æŸ
            elif char == '\n' and (i + 1 >= len(content) or content[i + 1] == '\n'):
                return i + 1

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„è¾¹ç•Œï¼Œè¿”å›å½“å‰é•¿åº¦ï¼ˆä¸æ¸²æŸ“ï¼‰
        return self.last_rendered_length

    def _render_content(self, content):
        """æ¸²æŸ“å†…å®¹ç‰‡æ®µ"""
        if not content.strip():
            safe_print_stream(content, end="", flush=True)
            return

        # ç®€å•çš„è¡Œå†…Markdownæ¸²æŸ“
        try:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«Markdownå…ƒç´ 
            if any(marker in content for marker in ['**', '*', '`', '#', '-', '1.']):
                # ç®€å•çš„å®æ—¶æ¸²æŸ“ï¼Œåªå¤„ç†åŸºæœ¬å…ƒç´ 
                rendered = self._simple_markdown_render(content)
                safe_print_stream(rendered, end="", flush=True)
            else:
                # çº¯æ–‡æœ¬ç›´æ¥è¾“å‡º
                safe_print_stream(content, end="", flush=True)
        except Exception:
            # å‡ºé”™æ—¶é™çº§åˆ°åŸå§‹æ–‡æœ¬
            safe_print_stream(content, end="", flush=True)

    def _simple_markdown_render(self, content):
        """ç®€å•çš„Markdownæ¸²æŸ“ - åªå¤„ç†åŸºæœ¬æ ¼å¼"""
        import re

        # ç²—ä½“ **text**
        content = re.sub(r'\*\*([^\*]+)\*\*', r'[bold]\1[/bold]', content)
        # æ–œä½“ *text*
        content = re.sub(r'\*([^\*]+)\*', r'[italic]\1[/italic]', content)
        # è¡Œå†…ä»£ç  `code`
        content = re.sub(r'`([^`]+)`', r'[code]\1[/code]', content)

        return content

    def finalize(self):
        """å®Œæˆæ¸²æŸ“ï¼Œå¤„ç†å‰©ä½™å†…å®¹"""
        if self.last_rendered_length < len(self.buffer):
            remaining = self.buffer[self.last_rendered_length:]
            self._render_content(remaining)

        safe_print_stream("", end="\n")  # æ¢è¡Œ


def safe_print_stream_markdown(content, is_complete=False, **kwargs):
    """æµå¼Markdownæ¸²æŸ“å‡½æ•°ï¼Œç´¯ç§¯å†…å®¹åæ¸²æŸ“"""
    if is_complete:
        # å®Œæ•´å†…å®¹ï¼Œè¿›è¡ŒMarkdownæ¸²æŸ“
        try:
            markdown = Markdown(content)
            console.print(markdown, **kwargs)
        except Exception:
            safe_print_stream(content, **kwargs)
    else:
        # æµå¼è¾“å‡ºï¼Œç›´æ¥æ‰“å°åŸå§‹æ–‡æœ¬
        safe_print_stream(content, **kwargs)


def get_user_input(prompt_text="You"):
    """è·å–ç”¨æˆ·è¾“å…¥ï¼Œæ”¯æŒRichæ ¼å¼çš„æç¤º"""
    try:
        # ä½¿ç”¨console.inputæ¥æ”¯æŒRichæ ¼å¼
        return console.input(f"[bold yellow]{prompt_text}:[/bold yellow] ")
    except Exception:
        # é™çº§åˆ°æ™®é€šinput
        return input(f"{prompt_text}: ")


class AdvancedInput:
    """é«˜çº§è¾“å…¥å¤„ç†å™¨ï¼Œæ”¯æŒå¤šè¡Œè¾“å…¥ï¼ˆAlt+Enter æ¢è¡Œï¼‰"""

    def __init__(self):
        self._use_prompt_toolkit = False
        self._bindings = None
        self._init_prompt_toolkit()

    def _init_prompt_toolkit(self):
        """åˆå§‹åŒ– prompt_toolkit çš„é”®ç»‘å®š"""
        try:
            from prompt_toolkit.key_binding import KeyBindings
            from prompt_toolkit.keys import Keys

            # åˆ›å»ºå¿«æ·é”®ç»‘å®š
            self._bindings = KeyBindings()

            @self._bindings.add(Keys.Enter)
            def _(event):
                """Enter æäº¤è¾“å…¥"""
                event.current_buffer.validate_and_handle()

            # Alt+Enter (Escape + Enter) æ¢è¡Œ - æœ€å¯é çš„æ–¹å¼
            @self._bindings.add('escape', 'enter')
            def _(event):
                """Alt+Enter æ¢è¡Œ"""
                event.current_buffer.insert_text('\n')

            self._use_prompt_toolkit = True
        except ImportError:
            self._use_prompt_toolkit = False

    def _sync_prompt(self, prompt_text: str) -> str:
        """åŒæ­¥è°ƒç”¨ prompt_toolkitï¼ˆåœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        from prompt_toolkit import prompt as pt_prompt
        return pt_prompt(
            f"{prompt_text}: ",
            key_bindings=self._bindings,
            multiline=False,
        )

    def get_input(self, prompt_text="You") -> str:
        """è·å–ç”¨æˆ·è¾“å…¥ï¼Œæ”¯æŒå¤šè¡Œï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        if self._use_prompt_toolkit:
            try:
                return self._sync_prompt(prompt_text)
            except (KeyboardInterrupt, EOFError):
                raise
            except Exception:
                # å‡ºé”™æ—¶é™çº§åˆ°åŸºæœ¬è¾“å…¥
                self._use_prompt_toolkit = False

        # Fallback åˆ°åŸºæœ¬è¾“å…¥
        return get_user_input(prompt_text)

    async def get_input_async(self, prompt_text="You") -> str:
        """è·å–ç”¨æˆ·è¾“å…¥ï¼Œæ”¯æŒå¤šè¡Œï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼Œåœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        if self._use_prompt_toolkit:
            try:
                import asyncio
                # åœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œ prompt_toolkitï¼Œé¿å…ä¸ asyncio å†²çª
                return await asyncio.to_thread(self._sync_prompt, prompt_text)
            except (KeyboardInterrupt, EOFError):
                raise
            except Exception:
                # å‡ºé”™æ—¶é™çº§åˆ°åŸºæœ¬è¾“å…¥
                self._use_prompt_toolkit = False

        # Fallback åˆ°åŸºæœ¬è¾“å…¥
        return get_user_input(prompt_text)


class ChatCommands:
    """èŠå¤©å¿«æ·å‘½ä»¤å¤„ç†å™¨"""

    COMMANDS = {
        '/clear': 'æ¸…ç©ºå¯¹è¯å†å²',
        '/retry': 'é‡æ–°ç”Ÿæˆä¸Šä¸€æ¡å›å¤',
        '/save': 'ä¿å­˜å¯¹è¯åˆ°æ–‡ä»¶ (ç”¨æ³•: /save [æ–‡ä»¶å])',
        '/model': 'åˆ‡æ¢æ¨¡å‹ (ç”¨æ³•: /model [æ¨¡å‹å])',
        '/help': 'æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯',
    }

    @classmethod
    def is_command(cls, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯å‘½ä»¤"""
        return text.strip().startswith('/')

    @classmethod
    def parse(cls, text: str) -> tuple:
        """è§£æå‘½ä»¤ï¼Œè¿”å› (å‘½ä»¤å, å‚æ•°åˆ—è¡¨)"""
        parts = text.strip().split(maxsplit=1)
        cmd = parts[0].lower()
        args = parts[1] if len(parts) > 1 else ""
        return cmd, args

    @classmethod
    def show_help(cls):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        safe_print("\n[bold cyan]ğŸ“‹ å¯ç”¨å‘½ä»¤:[/bold cyan]")
        for cmd, desc in cls.COMMANDS.items():
            safe_print(f"  [green]{cmd:12}[/green] - {desc}")
        safe_print("")

    @classmethod
    def handle_clear(cls, messages: list, system_prompt: str = None) -> list:
        """æ¸…ç©ºå¯¹è¯å†å²"""
        new_messages = []
        if system_prompt:
            new_messages.append({"role": "system", "content": system_prompt})
        safe_print("[dim]ğŸ—‘ï¸  å¯¹è¯å†å²å·²æ¸…ç©º[/dim]\n")
        return new_messages

    @classmethod
    def handle_save(cls, messages: list, filename: str = None):
        """ä¿å­˜å¯¹è¯åˆ°æ–‡ä»¶"""
        import json
        from datetime import datetime

        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"chat_{timestamp}.json"

        if not filename.endswith('.json'):
            filename += '.json'

        # è¿‡æ»¤æ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œåªä¿å­˜ç”¨æˆ·å’ŒåŠ©æ‰‹çš„å¯¹è¯
        chat_history = [
            msg for msg in messages
            if msg.get('role') in ['user', 'assistant']
        ]

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'saved_at': datetime.now().isoformat(),
                    'messages': chat_history
                }, f, ensure_ascii=False, indent=2)
            safe_print(f"[green]ğŸ’¾ å¯¹è¯å·²ä¿å­˜åˆ°: {filename}[/green]\n")
        except Exception as e:
            safe_print(f"[red]âŒ ä¿å­˜å¤±è´¥: {e}[/red]\n")

    @classmethod
    def handle_retry(cls, messages: list) -> tuple:
        """å‡†å¤‡é‡è¯•ï¼šç§»é™¤æœ€åä¸€æ¡åŠ©æ‰‹å›å¤ï¼Œè¿”å›æ˜¯å¦éœ€è¦é‡è¯•"""
        if len(messages) < 2:
            safe_print("[yellow]âš ï¸  æ²¡æœ‰å¯ä»¥é‡è¯•çš„å›å¤[/yellow]\n")
            return messages, False

        # æ‰¾åˆ°æœ€åä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯å¹¶ç§»é™¤
        if messages[-1].get('role') == 'assistant':
            messages.pop()
            safe_print("[dim]ğŸ”„ æ­£åœ¨é‡æ–°ç”Ÿæˆ...[/dim]")
            return messages, True
        else:
            safe_print("[yellow]âš ï¸  æœ€åä¸€æ¡ä¸æ˜¯åŠ©æ‰‹å›å¤ï¼Œæ— æ³•é‡è¯•[/yellow]\n")
            return messages, False


class MllmGroup:
    """MLLMå‘½ä»¤ç»„ - ç»Ÿä¸€ç®¡ç†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç›¸å…³åŠŸèƒ½"""

    def __init__(self, cli_instance):
        self.cli = cli_instance

    def call_table(
        self,
        table_path: str,
        model: str = None,
        base_url: str = None,
        api_key: str = None,
        image_col: str = "image",
        system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒè¯†åˆ«ä¸“å®¶ã€‚",
        text_prompt: str = "è¯·æè¿°è¿™å¼ å›¾åƒã€‚",
        system_prompt_file: str = None,
        text_prompt_file: str = None,
        sheet_name: str = 0,
        max_num=None,
        output_file: str = "table_results.csv",
        temperature: float = 0.1,
        max_tokens: int = 2000,
        concurrency_limit: int = 10,
        max_qps: int = 50,
        retry_times: int = 3,
        skip_existing: bool = False,
        **kwargs,
    ):
        """å¯¹è¡¨æ ¼ä¸­çš„å›¾åƒåˆ—è¿›è¡Œæ‰¹é‡å¤§æ¨¡å‹è¯†åˆ«å’Œåˆ†æ

        Args:
            table_path: è¡¨æ ¼æ–‡ä»¶è·¯å¾„ (xlsx/csv)
            model: æ¨¡å‹åç§°
            base_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            image_col: å›¾ç‰‡åˆ—å
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            text_prompt: æ–‡æœ¬æç¤ºè¯
            system_prompt_file: ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆäº system_promptï¼‰
            text_prompt_file: æ–‡æœ¬æç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆäº text_promptï¼‰
            sheet_name: sheetåç§°
            max_num: æœ€å¤§å¤„ç†æ•°é‡
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            concurrency_limit: å¹¶å‘é™åˆ¶
            max_qps: æœ€å¤§QPS
            retry_times: é‡è¯•æ¬¡æ•°
            skip_existing: æ˜¯å¦è·³è¿‡å·²æœ‰ç»“æœçš„è¡Œï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        """
        import asyncio
        import pandas as pd
        import os
        from flexllm.mllm_client import MllmClient

        # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤å€¼
        mllm_config = self.cli.maque_config.get("mllm", {})
        model = model or mllm_config.get("model", "gemma3:latest")
        base_url = base_url or mllm_config.get("base_url", "http://localhost:11434/v1")
        api_key = api_key or mllm_config.get("api_key", "EMPTY")

        # ä»æ–‡ä»¶è¯»å– promptï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if system_prompt_file and os.path.exists(system_prompt_file):
            with open(system_prompt_file, 'r', encoding='utf-8') as f:
                system_prompt = f.read().strip()
            safe_print(f"[dim]ğŸ“„ ä»æ–‡ä»¶åŠ è½½ system_prompt: {system_prompt_file}[/dim]")

        if text_prompt_file and os.path.exists(text_prompt_file):
            with open(text_prompt_file, 'r', encoding='utf-8') as f:
                text_prompt = f.read().strip()
            safe_print(f"[dim]ğŸ“„ ä»æ–‡ä»¶åŠ è½½ text_prompt: {text_prompt_file}[/dim]")

        async def run_call_table():
            try:
                safe_print(f"\n[bold green]ğŸ“Š å¼€å§‹æ‰¹é‡å¤„ç†è¡¨æ ¼[/bold green]")
                safe_print(f"[cyan]ğŸ“ æ–‡ä»¶: {table_path}[/cyan]")
                safe_print(f"[dim]ğŸ”§ æ¨¡å‹: {model} | å¹¶å‘: {concurrency_limit} | QPS: {max_qps}[/dim]")

                # åˆå§‹åŒ–å®¢æˆ·ç«¯
                client = MllmClient(
                    model=model,
                    base_url=base_url,
                    api_key=api_key,
                    concurrency_limit=concurrency_limit,
                    max_qps=max_qps,
                    retry_times=retry_times,
                    **kwargs,
                )

                # åŠ è½½æ•°æ®
                if table_path.endswith(".xlsx"):
                    df = pd.read_excel(table_path, sheet_name=sheet_name)
                else:
                    df = pd.read_csv(table_path)

                total_rows = len(df)
                if max_num:
                    df = df.head(max_num)

                safe_print(f"[dim]ğŸ“ æ€»è¡Œæ•°: {total_rows}, å¤„ç†è¡Œæ•°: {len(df)}[/dim]")

                # æ£€æŸ¥å¹¶åˆ›å»ºç»“æœåˆ—
                result_col = "mllm_result"
                if result_col not in df.columns:
                    df[result_col] = None

                # æ–­ç‚¹ç»­ä¼ ï¼šè¿‡æ»¤å·²æœ‰ç»“æœçš„è¡Œ
                if skip_existing and os.path.exists(output_file):
                    existing_df = pd.read_csv(output_file) if output_file.endswith('.csv') else pd.read_excel(output_file)
                    if result_col in existing_df.columns:
                        # åˆå¹¶å·²æœ‰ç»“æœ
                        df[result_col] = existing_df[result_col] if len(existing_df) == len(df) else df[result_col]
                        safe_print(f"[yellow]â­ï¸  æ–­ç‚¹ç»­ä¼ : æ£€æµ‹åˆ°å·²æœ‰ç»“æœæ–‡ä»¶[/yellow]")

                # æ‰¾å‡ºéœ€è¦å¤„ç†çš„è¡Œ
                if skip_existing:
                    pending_mask = df[result_col].isna() | (df[result_col] == '') | (df[result_col] == 'None')
                    pending_indices = df[pending_mask].index.tolist()
                else:
                    pending_indices = df.index.tolist()

                if not pending_indices:
                    safe_print(f"[green]âœ… æ‰€æœ‰è¡Œå·²å¤„ç†å®Œæˆï¼Œæ— éœ€é‡æ–°å¤„ç†[/green]")
                    return

                safe_print(f"[cyan]ğŸ”„ å¾…å¤„ç†: {len(pending_indices)} è¡Œ[/cyan]")

                # æ„å»ºå¾…å¤„ç†çš„ messages
                messages_list = []
                for idx in pending_indices:
                    row = df.loc[idx]
                    messages = []
                    if system_prompt:
                        messages.append({"role": "system", "content": system_prompt})
                    messages.append({
                        "role": "user",
                        "content": [
                            {"type": "text", "text": text_prompt},
                            {"type": "image_url", "image_url": {"url": str(row[image_col])}},
                        ],
                    })
                    messages_list.append(messages)

                # è°ƒç”¨ MLLM
                results = await client.call_llm(
                    messages_list,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )

                # å¡«å……ç»“æœ
                for i, idx in enumerate(pending_indices):
                    df.at[idx, result_col] = results[i] if i < len(results) else None

                # ä¿å­˜ç»“æœ
                if output_file.endswith('.csv'):
                    df.to_csv(output_file, index=False, encoding='utf-8-sig')
                else:
                    df.to_excel(output_file, index=False)

                safe_print(f"\n[bold green]âœ… å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}[/bold green]")

                # ç»Ÿè®¡
                success_count = df[result_col].notna().sum()
                safe_print(f"[dim]ğŸ“Š æˆåŠŸ: {success_count}/{len(df)}[/dim]")

            except Exception as e:
                safe_print(f"[red]âŒ å¤„ç†å¤±è´¥: {e}[/red]")
                import traceback
                traceback.print_exc()

        return asyncio.run(run_call_table())

    def call_images(
        self,
        folder_path: str,
        model: str = None,
        base_url: str = None,
        api_key: str = None,
        system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒè¯†åˆ«ä¸“å®¶ã€‚",
        text_prompt: str = "è¯·æè¿°è¿™å¼ å›¾åƒã€‚",
        system_prompt_file: str = None,
        text_prompt_file: str = None,
        recursive: bool = True,
        max_num: int = None,
        extensions: str = None,
        output_file: str = "results.csv",
        temperature: float = 0.1,
        max_tokens: int = 2000,
        concurrency_limit: int = 10,
        max_qps: int = 50,
        retry_times: int = 3,
        skip_existing: bool = False,
        **kwargs,
    ):
        """å¯¹æ–‡ä»¶å¤¹ä¸­çš„å›¾åƒè¿›è¡Œæ‰¹é‡å¤§æ¨¡å‹è¯†åˆ«å’Œåˆ†æ

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            model: æ¨¡å‹åç§°
            base_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            text_prompt: æ–‡æœ¬æç¤ºè¯
            system_prompt_file: ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆäº system_promptï¼‰
            text_prompt_file: æ–‡æœ¬æç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆäº text_promptï¼‰
            recursive: æ˜¯å¦é€’å½’æ‰«æå­æ–‡ä»¶å¤¹
            max_num: æœ€å¤§å¤„ç†æ•°é‡
            extensions: æ”¯æŒçš„æ–‡ä»¶æ‰©å±•åï¼ˆé€—å·åˆ†éš”ï¼Œå¦‚ "jpg,png,webp"ï¼‰
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            concurrency_limit: å¹¶å‘é™åˆ¶
            max_qps: æœ€å¤§QPS
            retry_times: é‡è¯•æ¬¡æ•°
            skip_existing: æ˜¯å¦è·³è¿‡å·²å¤„ç†çš„å›¾ç‰‡ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        """
        import asyncio
        import pandas as pd
        import os
        from pathlib import Path
        from flexllm.mllm_client import MllmClient

        # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤å€¼
        mllm_config = self.cli.maque_config.get("mllm", {})
        model = model or mllm_config.get("model", "gemma3:latest")
        base_url = base_url or mllm_config.get("base_url", "http://localhost:11434/v1")
        api_key = api_key or mllm_config.get("api_key", "EMPTY")

        # ä»æ–‡ä»¶è¯»å– promptï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if system_prompt_file and os.path.exists(system_prompt_file):
            with open(system_prompt_file, 'r', encoding='utf-8') as f:
                system_prompt = f.read().strip()
            safe_print(f"[dim]ğŸ“„ ä»æ–‡ä»¶åŠ è½½ system_prompt: {system_prompt_file}[/dim]")

        if text_prompt_file and os.path.exists(text_prompt_file):
            with open(text_prompt_file, 'r', encoding='utf-8') as f:
                text_prompt = f.read().strip()
            safe_print(f"[dim]ğŸ“„ ä»æ–‡ä»¶åŠ è½½ text_prompt: {text_prompt_file}[/dim]")

        # è§£ææ‰©å±•å
        ext_set = None
        if extensions:
            ext_set = {f".{ext.strip().lower().lstrip('.')}" for ext in extensions.split(',')}

        async def run_call_images():
            try:
                safe_print(f"\n[bold green]ğŸ“ å¼€å§‹æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹å›¾ç‰‡[/bold green]")
                safe_print(f"[cyan]ğŸ“‚ è·¯å¾„: {folder_path}[/cyan]")
                safe_print(f"[dim]ğŸ”§ æ¨¡å‹: {model} | å¹¶å‘: {concurrency_limit} | QPS: {max_qps}[/dim]")

                # åˆå§‹åŒ–å®¢æˆ·ç«¯
                client = MllmClient(
                    model=model,
                    base_url=base_url,
                    api_key=api_key,
                    concurrency_limit=concurrency_limit,
                    max_qps=max_qps,
                    retry_times=retry_times,
                    **kwargs,
                )

                # æ‰«æå›¾ç‰‡æ–‡ä»¶
                image_files = client.folder.scan_folder_images(
                    folder_path=folder_path,
                    recursive=recursive,
                    max_num=max_num,
                    extensions=ext_set,
                )

                if not image_files:
                    safe_print(f"[yellow]âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶[/yellow]")
                    return

                # åˆ›å»ºç»“æœ DataFrame
                df = pd.DataFrame({'image_path': image_files})
                result_col = "mllm_result"
                df[result_col] = None

                # æ–­ç‚¹ç»­ä¼ ï¼šåŠ è½½å·²æœ‰ç»“æœ
                processed_paths = set()
                if skip_existing and os.path.exists(output_file):
                    try:
                        existing_df = pd.read_csv(output_file) if output_file.endswith('.csv') else pd.read_excel(output_file)
                        if 'image_path' in existing_df.columns and result_col in existing_df.columns:
                            # åˆ›å»ºè·¯å¾„åˆ°ç»“æœçš„æ˜ å°„
                            for _, row in existing_df.iterrows():
                                path = row['image_path']
                                result = row[result_col]
                                if pd.notna(result) and result != '' and result != 'None':
                                    processed_paths.add(path)
                                    # æ›´æ–° df ä¸­å¯¹åº”è¡Œçš„ç»“æœ
                                    mask = df['image_path'] == path
                                    if mask.any():
                                        df.loc[mask, result_col] = result
                            safe_print(f"[yellow]â­ï¸  æ–­ç‚¹ç»­ä¼ : å·²å¤„ç† {len(processed_paths)} ä¸ªæ–‡ä»¶[/yellow]")
                    except Exception as e:
                        safe_print(f"[yellow]âš ï¸  è¯»å–å·²æœ‰ç»“æœå¤±è´¥: {e}[/yellow]")

                # æ‰¾å‡ºéœ€è¦å¤„ç†çš„æ–‡ä»¶
                pending_indices = []
                for idx, row in df.iterrows():
                    if row['image_path'] not in processed_paths:
                        pending_indices.append(idx)

                if not pending_indices:
                    safe_print(f"[green]âœ… æ‰€æœ‰å›¾ç‰‡å·²å¤„ç†å®Œæˆï¼Œæ— éœ€é‡æ–°å¤„ç†[/green]")
                    return

                safe_print(f"[cyan]ğŸ”„ å¾…å¤„ç†: {len(pending_indices)} ä¸ªå›¾ç‰‡[/cyan]")

                # æ„å»º messages
                messages_list = []
                pending_files = []
                for idx in pending_indices:
                    image_path = df.loc[idx, 'image_path']
                    pending_files.append(image_path)
                    messages = []
                    if system_prompt:
                        messages.append({"role": "system", "content": system_prompt})
                    messages.append({
                        "role": "user",
                        "content": [
                            {"type": "text", "text": text_prompt},
                            {"type": "image_url", "image_url": {"url": f"file://{image_path}"}},
                        ],
                    })
                    messages_list.append(messages)

                # è°ƒç”¨ MLLM
                results = await client.call_llm(
                    messages_list,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )

                # å¡«å……ç»“æœ
                for i, idx in enumerate(pending_indices):
                    df.at[idx, result_col] = results[i] if i < len(results) else None

                # ä¿å­˜ç»“æœ
                if output_file.endswith('.csv'):
                    df.to_csv(output_file, index=False, encoding='utf-8-sig')
                else:
                    df.to_excel(output_file, index=False)

                safe_print(f"\n[bold green]âœ… å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}[/bold green]")

                # ç»Ÿè®¡
                success_count = df[result_col].notna().sum()
                safe_print(f"[dim]ğŸ“Š æˆåŠŸ: {success_count}/{len(df)}[/dim]")

            except Exception as e:
                safe_print(f"[red]âŒ å¤„ç†å¤±è´¥: {e}[/red]")
                import traceback
                traceback.print_exc()

        return asyncio.run(run_call_images())


    def eval(
        self,
        result_file: str,
        source: str = None,
        response_col: str = "content",
        label_col: str = None,
        extract: str = "direct",
        sep: str = None,
        mapping: str = None,
        output_dir: str = "record",
    ):
        """å¯¹ batch è¾“å‡ºçš„ .jsonl æ–‡ä»¶è¿›è¡Œè§£æ + æŒ‡æ ‡è®¡ç®—

        ä¸¤é˜¶æ®µè§£ææµç¨‹ï¼š
          é˜¶æ®µ1ï¼ˆè‡ªåŠ¨ï¼‰ï¼šå»é™¤ <think>...</think>ï¼Œæå– ```json...``` ä»£ç å—
          é˜¶æ®µ2ï¼ˆ--extract æŒ‡å®šï¼‰ï¼šç®¡é“å¼æå–ï¼Œæ”¯æŒ " | " åˆ†éš”çš„ç®—å­ç»„åˆ

        å¯ç”¨ç®—å­ï¼š
          direct          åŸæ ·è¿”å›
          tag:æ ‡ç­¾å      æå– XML æ ‡ç­¾å†…å®¹
          json_key:key    ä» JSON æå– key
          index:N         æŒ‰ --sep åˆ†å‰²å–ç¬¬ N é¡¹
          line:N          å–ç¬¬ N è¡Œï¼ˆæ”¯æŒè´Ÿæ•°ï¼Œ-1=æœ€åä¸€è¡Œï¼‰
          lines           æ‹†ä¸ºå¤šè¡Œï¼Œæ¯è¡Œç‹¬ç«‹èµ°åç»­ç®¡é“
          regex:pattern   æ­£åˆ™åŒ¹é…ï¼Œå–ç¬¬ä¸€ä¸ªæ•è·ç»„

        Args:
            result_file: batch è¾“å‡ºçš„ .jsonl æ–‡ä»¶è·¯å¾„
            source: åŸå§‹è¾“å…¥ .jsonlï¼ŒæŒ‰è¡Œå·å¯¹é½åˆå¹¶ï¼ˆå½“ result_file ä¸å« label æ—¶ä½¿ç”¨ï¼‰
            response_col: æ¨¡å‹å“åº”æ‰€åœ¨çš„å­—æ®µåï¼ˆæ”¯æŒç‚¹å·åµŒå¥—ï¼Œå¦‚ metadata.content_labelï¼‰
            label_col: æ ‡ç­¾å­—æ®µåï¼ˆä¸æŒ‡å®šæ—¶è‡ªåŠ¨æ£€æµ‹ï¼Œæ”¯æŒç‚¹å·åµŒå¥—ï¼‰
            extract: ç®¡é“å¼æå–è§„åˆ™ï¼Œç®—å­é—´ç”¨ " | " åˆ†éš”
            sep: é…åˆ index ç®—å­ä½¿ç”¨çš„åˆ†éš”ç¬¦
            mapping: å€¼æ˜ å°„ï¼Œæ ¼å¼ "k1:v1,k2:v2"ï¼Œå¯¹ pred å’Œ label åšæ˜ å°„
            output_dir: æŒ‡æ ‡æŠ¥å‘Šè¾“å‡ºç›®å½•

        Examples:
            maque mllm eval result.jsonl --label_col=label
            maque mllm eval result.jsonl --extract="tag:ä¸€çº§æ ‡ç­¾"
            maque mllm eval result.jsonl --extract="index:1" --sep="|"
            maque mllm eval result.jsonl --extract="lines | index:1" --sep="|"
            maque mllm eval result.jsonl --extract="line:-1 | index:1" --sep="|"
            maque mllm eval result.jsonl --extract="json_key:result | index:0" --sep=","
            maque mllm eval result.jsonl --extract="regex:risk_level=(\\w+)"
        """
        import pandas as pd
        from maque.io import jsonl_load
        from maque.nlp.parser import strip_think_tags, extract_code_snippets
        from maque.utils.helper_parser import parse_generic_tags
        from maque.ai_platform.metrics import export_eval_report

        def resolve_nested_col(df, col_name):
            """è§£æç‚¹å·åµŒå¥—å­—æ®µï¼Œå¦‚ 'metadata.content_label' â†’ ä» df['metadata'] dict åˆ—å– content_label"""
            if "." not in col_name:
                return col_name, col_name in df.columns
            parts = col_name.split(".", 1)
            root, sub_key = parts[0], parts[1]
            if root not in df.columns:
                return col_name, False
            # å±•å¼€åµŒå¥—å­—æ®µåˆ°æ–°åˆ—
            resolved_name = col_name.replace(".", "__")
            def _get_nested(val):
                if isinstance(val, dict):
                    # æ”¯æŒå¤šçº§åµŒå¥—
                    keys = sub_key.split(".")
                    cur = val
                    for k in keys:
                        if isinstance(cur, dict):
                            cur = cur.get(k)
                        else:
                            return None
                    return cur
                return None
            df[resolved_name] = df[root].apply(_get_nested)
            return resolved_name, True

        def parse_mapping(mapping_str):
            """è§£æ 'k1:v1,k2:v2' æ ¼å¼çš„æ˜ å°„"""
            m = {}
            for pair in mapping_str.split(","):
                pair = pair.strip()
                if ":" in pair:
                    k, v = pair.split(":", 1)
                    m[k.strip()] = v.strip()
            return m

        # --- åŠ è½½æ•°æ® ---
        data = jsonl_load(result_file)
        df = pd.DataFrame(data)
        safe_print(f"[cyan]åŠ è½½ {result_file}ï¼Œå…± {len(df)} æ¡[/cyan]")

        # åˆå¹¶ source æ–‡ä»¶
        if source:
            source_data = jsonl_load(source)
            source_df = pd.DataFrame(source_data)
            if len(source_df) != len(df):
                safe_print(f"[red]è¡Œæ•°ä¸ä¸€è‡´: result={len(df)}, source={len(source_df)}[/red]")
                return
            # å°† source ä¸­ä¸å­˜åœ¨äº df çš„åˆ—åˆå¹¶è¿‡æ¥
            for col in source_df.columns:
                if col not in df.columns:
                    df[col] = source_df[col].values
            safe_print(f"[dim]å·²åˆå¹¶ source æ–‡ä»¶: {source}[/dim]")

        # --- è§£æ response_colï¼ˆæ”¯æŒåµŒå¥—ï¼‰---
        response_col, found = resolve_nested_col(df, response_col)
        if not found:
            safe_print(f"[red]å“åº”åˆ— '{response_col}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {list(df.columns)}[/red]")
            return

        # --- è‡ªåŠ¨æ£€æµ‹ label_col ---
        if label_col is None:
            candidates = ["label", "labels", "content_label", "target", "ground_truth", "answer"]
            for c in candidates:
                if c in df.columns:
                    label_col = c
                    break
            # é¡¶å±‚æ‰¾ä¸åˆ°æ—¶ï¼Œæœç´¢ dict ç±»å‹åˆ—çš„åµŒå¥— key
            if label_col is None:
                for col in df.columns:
                    sample = df[col].dropna().iloc[0] if not df[col].dropna().empty else None
                    if isinstance(sample, dict):
                        for c in candidates:
                            if c in sample:
                                label_col = f"{col}.{c}"
                                break
                    if label_col is not None:
                        break
            if label_col is None:
                safe_print(f"[red]æœªæ‰¾åˆ°æ ‡ç­¾åˆ—ï¼Œè¯·é€šè¿‡ --label_col æŒ‡å®šã€‚å¯ç”¨åˆ—: {list(df.columns)}[/red]")
                return

        # è§£æ label_colï¼ˆæ”¯æŒåµŒå¥—ï¼‰
        label_col, found = resolve_nested_col(df, label_col)
        if not found:
            safe_print(f"[red]æ ‡ç­¾åˆ— '{label_col}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {list(df.columns)}[/red]")
            return

        safe_print(f"[dim]response_col={response_col}, label_col={label_col}, extract={extract}[/dim]")

        # --- é˜¶æ®µ1ï¼šè‡ªåŠ¨æ¸…æ´— ---
        def stage1_clean(text):
            if not isinstance(text, str):
                return str(text) if text is not None else ""
            text = strip_think_tags(text)
            # å°è¯•æå–ä»£ç å—å†…å®¹
            snippets = extract_code_snippets(text)
            if snippets:
                return snippets[-1]["code"]
            return text.strip()

        # --- é˜¶æ®µ2ï¼šç®¡é“å¼æå– ---
        def parse_pipeline(extract_str):
            """è§£æç®¡é“è¡¨è¾¾å¼ï¼ŒæŒ‰ ' | ' åˆ†å‰²"""
            return [op.strip() for op in extract_str.split(" | ") if op.strip()]

        def apply_op(text, op):
            """å¯¹å•ä¸ªå­—ç¬¦ä¸²æ‰§è¡Œå•ä¸ªç®—å­"""
            if op == "direct":
                return text
            elif op.startswith("tag:"):
                tag_name = op[4:]
                tags = parse_generic_tags(text)
                return tags.get(tag_name, text)
            elif op.startswith("json_key:"):
                key = op[9:]
                try:
                    import json5
                    obj = json5.loads(text)
                except Exception:
                    try:
                        import json
                        obj = json.loads(text)
                    except Exception:
                        return text
                if isinstance(obj, dict):
                    return str(obj.get(key, text))
                return text
            elif op.startswith("index:"):
                idx = int(op[6:])
                delimiter = sep if sep else ","
                parts = text.split(delimiter)
                if 0 <= idx < len(parts):
                    return parts[idx].strip()
                return text
            elif op.startswith("line:"):
                n = int(op[5:])
                text_lines = [l.strip() for l in text.splitlines() if l.strip()]
                if text_lines and -len(text_lines) <= n < len(text_lines):
                    return text_lines[n]
                return text
            elif op.startswith("regex:"):
                import re
                pattern = op[6:]
                m = re.search(pattern, text)
                if m:
                    return m.group(1) if m.lastindex else m.group(0)
                return text
            else:
                safe_print(f"[yellow]æœªçŸ¥ç®—å­: {op}ï¼Œè·³è¿‡[/yellow]")
                return text

        def run_pipeline(text, ops):
            """æ‰§è¡Œç®¡é“ï¼Œå¤„ç† lines å±•å¼€"""
            if "lines" in ops:
                pos = ops.index("lines")
                # lines å‰é¢çš„ç®—å­å…ˆæ‰§è¡Œ
                for op in ops[:pos]:
                    text = apply_op(text, op)
                # å±•å¼€ä¸ºå¤šè¡Œ
                items = [l.strip() for l in text.splitlines() if l.strip()]
                # æ¯è¡Œç‹¬ç«‹èµ°åç»­ç®¡é“
                rest_ops = ops[pos + 1:]
                results = []
                for item in items:
                    for op in rest_ops:
                        item = apply_op(item, op)
                    results.append(item)
                if not results:
                    return text
                return results if len(results) > 1 else results[0]
            else:
                for op in ops:
                    text = apply_op(text, op)
                return text

        # æ‰§è¡Œä¸¤é˜¶æ®µè§£æ
        ops = parse_pipeline(extract)
        pred_col = "__pred__"
        df[pred_col] = df[response_col].apply(lambda x: run_pipeline(stage1_clean(x), ops))

        # --- mapping é˜¶æ®µ ---
        if mapping:
            m = parse_mapping(mapping)
            # mapping ä¸­å€¼çš„å‡ºç°é¡ºåºå†³å®šä¼˜å…ˆçº§ï¼Œåå‡ºç°çš„ä¼˜å…ˆçº§æ›´é«˜
            priority = {}
            for i, v in enumerate(m.values()):
                priority[v] = i

            def map_value(x):
                if isinstance(x, list):
                    mapped = [m.get(v, v) for v in x]
                    return max(mapped, key=lambda v: priority.get(v, -1))
                return m.get(x, x) if isinstance(x, str) else m.get(str(x), x)

            df[pred_col] = df[pred_col].apply(map_value)
            df[label_col] = df[label_col].apply(map_value)

        # å°† pred å’Œ label éƒ½è½¬ä¸ºå­—ç¬¦ä¸²ä»¥ç¡®ä¿å¯æ¯”è¾ƒ
        df[pred_col] = df[pred_col].apply(lambda x: str(x).strip() if not isinstance(x, str) else x.strip())
        df[label_col] = df[label_col].apply(lambda x: str(x).strip() if not isinstance(x, str) else x.strip())

        # --- è°ƒç”¨ export_eval_report ---
        safe_print(f"\n[bold green]è¯„ä¼°ç»“æœ[/bold green]")
        export_eval_report(df, pred_col=pred_col, label_col=label_col, record_folder=output_dir)

    # ========== chat, models, test å·²ç§»è‡³ flexllm CLI ==========
    # è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ›¿ä»£:
    #   flexllm chat      - äº¤äº’å¼å¯¹è¯
    #   flexllm models    - åˆ—å‡ºå¯ç”¨æ¨¡å‹
    #   flexllm test      - æµ‹è¯•æœåŠ¡è¿æ¥
    # ============================================================

    def chain_analysis(
        self,
        query: str,
        steps: int = 3,
        model: str = None,
        base_url: str = None,
        api_key: str = None,
        temperature: float = 0.1,
        max_tokens: int = 2000,
        show_details: bool = False,
        **kwargs,
    ):
        """ä½¿ç”¨Chain of Thoughtè¿›è¡Œåˆ†ææ¨ç†
        
        Args:
            query: è¦åˆ†æçš„é—®é¢˜æˆ–å†…å®¹
            steps: åˆ†ææ­¥éª¤æ•°ï¼Œé»˜è®¤3æ­¥
            model: ä½¿ç”¨çš„æ¨¡å‹
            base_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            show_details: æ˜¯å¦æ˜¾ç¤ºæ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯
        """
        import asyncio
        from flexllm.chain_of_thought_client import ChainOfThoughtClient, LinearStep, ExecutionConfig
        from flexllm.openaiclient import OpenAIClient

        # ä»é…ç½®è·å–é»˜è®¤å€¼
        mllm_config = self.cli.maque_config.get("mllm", {})
        model = model or mllm_config.get("model", "gemma3:latest")
        base_url = base_url or mllm_config.get("base_url", "http://localhost:11434/v1")
        api_key = api_key or mllm_config.get("api_key", "EMPTY")

        async def run_chain_analysis():
            try:
                safe_print(f"[bold green]ğŸ” å¼€å§‹Chain of Thoughtåˆ†ææ¨ç†[/bold green]")
                safe_print(f"[cyan]ğŸ“ é—®é¢˜: {query}[/cyan]")
                safe_print(f"[dim]ğŸ”§ æ¨¡å‹: {model}, æ­¥éª¤æ•°: {steps}[/dim]\n")

                # åˆå§‹åŒ–å®¢æˆ·ç«¯
                openai_client = OpenAIClient(model=model, base_url=base_url, api_key=api_key)
                
                # é…ç½®æ‰§è¡Œå‚æ•°
                config = ExecutionConfig(
                    enable_monitoring=True,
                    enable_progress=show_details,
                    log_level="INFO" if show_details else "WARNING"
                )
                
                chain_client = ChainOfThoughtClient(openai_client, config)

                # å®šä¹‰åˆ†ææ­¥éª¤
                def create_analysis_step(step_num: int, step_name: str, prompt_template: str):
                    def prepare_messages(context):
                        previous_analysis = ""
                        if context.history:
                            previous_analysis = "\n\n".join([
                                f"æ­¥éª¤{i+1}: {step.response}" 
                                for i, step in enumerate(context.history)
                            ])
                        
                        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆ†æå¸ˆï¼Œæ­£åœ¨è¿›è¡Œç¬¬{step_num}æ­¥åˆ†æã€‚
è¯·æ ¹æ®é—®é¢˜å’Œä¹‹å‰çš„åˆ†æç»“æœï¼Œ{step_name}ã€‚
ä¿æŒé€»è¾‘æ¸…æ™°ï¼Œåˆ†ææ·±å…¥ã€‚"""

                        user_prompt = prompt_template.format(
                            query=context.query,
                            previous_analysis=previous_analysis
                        )

                        return [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ]
                    
                    return LinearStep(
                        name=f"analysis_step_{step_num}",
                        prepare_messages_fn=prepare_messages,
                        model_params={
                            "temperature": temperature,
                            "max_tokens": max_tokens,
                            **kwargs
                        }
                    )

                # åˆ›å»ºåˆ†æé“¾æ¡
                analysis_steps = []
                
                if steps >= 1:
                    analysis_steps.append(create_analysis_step(
                        1, "ç†è§£å’Œåˆ†è§£é—®é¢˜",
                        "è¯·ä»”ç»†åˆ†æè¿™ä¸ªé—®é¢˜ï¼š\n{query}\n\nè¯·åˆ†è§£è¿™ä¸ªé—®é¢˜çš„å…³é”®è¦ç´ ï¼Œæ˜ç¡®åˆ†æçš„æ–¹å‘å’Œé‡ç‚¹ã€‚"
                    ))
                
                if steps >= 2:
                    analysis_steps.append(create_analysis_step(
                        2, "æ·±å…¥åˆ†æå„ä¸ªæ–¹é¢",
                        "åŸºäºç¬¬ä¸€æ­¥çš„åˆ†æï¼š\n{previous_analysis}\n\nè¯·ä»å¤šä¸ªè§’åº¦æ·±å…¥åˆ†æé—®é¢˜ï¼Œæ¢è®¨å¯èƒ½çš„è§£å†³æ–¹æ¡ˆæˆ–ç­”æ¡ˆã€‚"
                    ))
                
                if steps >= 3:
                    analysis_steps.append(create_analysis_step(
                        3, "ç»¼åˆç»“è®ºå’Œå»ºè®®",
                        "åŸºäºå‰é¢çš„åˆ†æï¼š\n{previous_analysis}\n\nè¯·æ€»ç»“åˆ†æç»“æœï¼Œç»™å‡ºæ˜ç¡®çš„ç»“è®ºå’Œå®ç”¨çš„å»ºè®®ã€‚"
                    ))
                
                # å¦‚æœæ­¥éª¤è¶…è¿‡3æ­¥ï¼Œæ·»åŠ æ›´å¤šç»†åŒ–åˆ†æ
                for i in range(4, steps + 1):
                    analysis_steps.append(create_analysis_step(
                        i, f"è¿›ä¸€æ­¥ç»†åŒ–åˆ†æç¬¬{i-3}ä¸ªæ–¹é¢",
                        "ç»§ç»­æ·±åŒ–åˆ†æï¼š\n{previous_analysis}\n\nè¯·è¿›ä¸€æ­¥ç»†åŒ–å’Œè¡¥å……åˆ†æï¼Œæä¾›æ›´è¯¦ç»†çš„è§è§£ã€‚"
                    ))

                # åˆ›å»ºçº¿æ€§é“¾æ¡
                first_step = chain_client.create_linear_chain(analysis_steps, "analysis_chain")
                
                # æ‰§è¡Œé“¾æ¡
                context = chain_client.create_context({"query": query})
                result_context = await chain_client.execute_chain(
                    first_step, context, show_step_details=show_details
                )

                # æ˜¾ç¤ºç»“æœ
                if result_context.history:
                    safe_print(f"\n[bold blue]ğŸ¯ Chain of Thought åˆ†æç»“æœ[/bold blue]")
                    safe_print(f"[dim]{'=' * 60}[/dim]")
                    
                    for i, step_result in enumerate(result_context.history):
                        step_title = f"æ­¥éª¤ {i+1}"
                        if i == 0:
                            step_title += " - é—®é¢˜ç†è§£"
                        elif i == 1:
                            step_title += " - æ·±å…¥åˆ†æ"
                        elif i == 2:
                            step_title += " - ç»¼åˆç»“è®º"
                        else:
                            step_title += f" - ç»†åŒ–åˆ†æ {i-2}"
                            
                        safe_print(f"\n[bold cyan]{step_title}[/bold cyan]")
                        safe_print(f"[green]{step_result.response}[/green]")
                    
                    # æ‰§è¡Œæ‘˜è¦
                    summary = result_context.get_execution_summary()
                    safe_print(f"\n[dim]ğŸ“Š æ‰§è¡Œç»Ÿè®¡: {summary['total_steps']} ä¸ªæ­¥éª¤, "
                              f"è€—æ—¶ {summary['total_execution_time']:.2f}ç§’, "
                              f"æˆåŠŸç‡ {summary['success_rate']*100:.1f}%[/dim]")
                else:
                    safe_print("[red]âŒ åˆ†ææ‰§è¡Œå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆç»“æœ[/red]")

            except Exception as e:
                safe_print(f"[red]âŒ Chain of Thoughtåˆ†ææ‰§è¡Œå¤±è´¥: {e}[/red]")
                safe_print("[yellow]ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹é…ç½®å’Œç½‘ç»œè¿æ¥[/yellow]")

        return asyncio.run(run_chain_analysis())

    def chain_reasoning(
        self,
        query: str,
        model: str = None,
        base_url: str = None,
        api_key: str = None,
        temperature: float = 0.1,
        max_tokens: int = 2000,
        show_details: bool = False,
        **kwargs,
    ):
        """ä½¿ç”¨Chain of Thoughtè¿›è¡Œé€»è¾‘æ¨ç†
        
        Args:
            query: éœ€è¦æ¨ç†çš„é—®é¢˜æˆ–æƒ…å¢ƒ
            model: ä½¿ç”¨çš„æ¨¡å‹
            base_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            show_details: æ˜¯å¦æ˜¾ç¤ºæ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯
        """
        import asyncio
        from flexllm.chain_of_thought_client import ChainOfThoughtClient, LinearStep, ExecutionConfig
        from flexllm.openaiclient import OpenAIClient

        # ä»é…ç½®è·å–é»˜è®¤å€¼
        mllm_config = self.cli.maque_config.get("mllm", {})
        model = model or mllm_config.get("model", "gemma3:latest")
        base_url = base_url or mllm_config.get("base_url", "http://localhost:11434/v1")
        api_key = api_key or mllm_config.get("api_key", "EMPTY")

        async def run_chain_reasoning():
            try:
                safe_print(f"[bold green]ğŸ§  å¼€å§‹Chain of Thoughté€»è¾‘æ¨ç†[/bold green]")
                safe_print(f"[cyan]ğŸ’­ æ¨ç†é—®é¢˜: {query}[/cyan]")
                safe_print(f"[dim]ğŸ”§ æ¨¡å‹: {model}[/dim]\n")

                # åˆå§‹åŒ–å®¢æˆ·ç«¯
                openai_client = OpenAIClient(model=model, base_url=base_url, api_key=api_key)
                
                config = ExecutionConfig(
                    enable_monitoring=True,
                    enable_progress=show_details,
                    log_level="INFO" if show_details else "WARNING"
                )
                
                chain_client = ChainOfThoughtClient(openai_client, config)

                # å®šä¹‰æ¨ç†æ­¥éª¤
                def create_reasoning_step(step_name: str, prompt_template: str):
                    def prepare_messages(context):
                        previous_reasoning = ""
                        if context.history:
                            previous_reasoning = "\n\n".join([
                                f"[{step.step_name}]: {step.response}" 
                                for step in context.history
                            ])
                        
                        return [
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªé€»è¾‘æ¨ç†ä¸“å®¶ã€‚è¯·ä½¿ç”¨ä¸¥è°¨çš„é€»è¾‘æ€ç»´ï¼Œä¸€æ­¥ä¸€æ­¥åœ°åˆ†æå’Œæ¨ç†ã€‚æ¯ä¸€æ­¥éƒ½è¦æœ‰æ˜ç¡®çš„é€»è¾‘ä¾æ®ã€‚"},
                            {"role": "user", "content": prompt_template.format(
                                query=context.query,
                                previous_reasoning=previous_reasoning
                            )}
                        ]
                    
                    return LinearStep(
                        name=step_name,
                        prepare_messages_fn=prepare_messages,
                        model_params={
                            "temperature": temperature,
                            "max_tokens": max_tokens,
                            **kwargs
                        }
                    )

                # åˆ›å»ºæ¨ç†é“¾æ¡
                reasoning_steps = [
                    create_reasoning_step(
                        "observation",
                        "é¦–å…ˆï¼Œè®©æˆ‘è§‚å¯Ÿå’Œç†è§£è¿™ä¸ªé—®é¢˜ï¼š\n{query}\n\nè¯·ä»”ç»†è§‚å¯Ÿé—®é¢˜ä¸­çš„å…³é”®ä¿¡æ¯ã€å·²çŸ¥æ¡ä»¶å’Œè¦æ±‚è§£ç­”çš„å†…å®¹ã€‚åˆ—å‡ºæ‰€æœ‰é‡è¦çš„äº‹å®å’Œå‡è®¾ã€‚"
                    ),
                    create_reasoning_step(
                        "hypothesis",
                        "åŸºäºè§‚å¯Ÿåˆ°çš„ä¿¡æ¯ï¼š\n{previous_reasoning}\n\nç°åœ¨è¯·æå‡ºå¯èƒ½çš„å‡è®¾æˆ–è§£å†³æ–¹æ¡ˆã€‚è€ƒè™‘å¤šç§å¯èƒ½æ€§ï¼Œå¹¶è¯´æ˜æ¯ç§å‡è®¾çš„ä¾æ®ã€‚"
                    ),
                    create_reasoning_step(
                        "deduction",
                        "åŸºäºå‰é¢çš„è§‚å¯Ÿå’Œå‡è®¾ï¼š\n{previous_reasoning}\n\nç°åœ¨è¿›è¡Œé€»è¾‘æ¨å¯¼ã€‚ä½¿ç”¨æ¼”ç»æ¨ç†ï¼Œä»å·²çŸ¥æ¡ä»¶æ¨å¯¼å‡ºç»“è®ºã€‚ç¡®ä¿æ¯ä¸€æ­¥æ¨ç†éƒ½æœ‰æ˜ç¡®çš„é€»è¾‘å…³ç³»ã€‚"
                    ),
                    create_reasoning_step(
                        "verification",
                        "åŸºäºæ¨ç†è¿‡ç¨‹ï¼š\n{previous_reasoning}\n\nç°åœ¨éªŒè¯æ¨ç†ç»“æœã€‚æ£€æŸ¥é€»è¾‘æ˜¯å¦ä¸€è‡´ï¼Œç»“è®ºæ˜¯å¦åˆç†ï¼Œæ˜¯å¦é—æ¼äº†é‡è¦å› ç´ ã€‚å¦‚æœå‘ç°é—®é¢˜ï¼Œè¯·æŒ‡å‡ºå¹¶ä¿®æ­£ã€‚"
                    ),
                    create_reasoning_step(
                        "conclusion",
                        "ç»¼åˆæ•´ä¸ªæ¨ç†è¿‡ç¨‹ï¼š\n{previous_reasoning}\n\nè¯·ç»™å‡ºæœ€ç»ˆç»“è®ºã€‚æ€»ç»“æ¨ç†çš„å…³é”®æ­¥éª¤ï¼Œæ˜ç¡®å›ç­”åŸå§‹é—®é¢˜ï¼Œå¹¶è¯´æ˜ç»“è®ºçš„å¯ä¿¡åº¦ã€‚"
                    )
                ]

                # åˆ›å»ºå’Œæ‰§è¡Œé“¾æ¡
                first_step = chain_client.create_linear_chain(reasoning_steps, "reasoning_chain")
                context = chain_client.create_context({"query": query})
                result_context = await chain_client.execute_chain(
                    first_step, context, show_step_details=show_details
                )

                # æ˜¾ç¤ºæ¨ç†ç»“æœ
                if result_context.history:
                    safe_print(f"\n[bold blue]ğŸ¯ Chain of Thought æ¨ç†ç»“æœ[/bold blue]")
                    safe_print(f"[dim]{'=' * 60}[/dim]")
                    
                    step_names = {
                        "observation": "ğŸ” è§‚å¯Ÿåˆ†æ",
                        "hypothesis": "ğŸ’¡ å‡è®¾æå‡º", 
                        "deduction": "ğŸ”— é€»è¾‘æ¨å¯¼",
                        "verification": "âœ… éªŒè¯æ£€æŸ¥",
                        "conclusion": "ğŸ¯ æœ€ç»ˆç»“è®º"
                    }
                    
                    for step_result in result_context.history:
                        step_display = step_names.get(step_result.step_name, step_result.step_name)
                        safe_print(f"\n[bold cyan]{step_display}[/bold cyan]")
                        safe_print(f"[green]{step_result.response}[/green]")
                    
                    # æ‰§è¡Œæ‘˜è¦
                    summary = result_context.get_execution_summary()
                    safe_print(f"\n[dim]ğŸ“Š æ¨ç†ç»Ÿè®¡: {summary['total_steps']} ä¸ªæ­¥éª¤, "
                              f"è€—æ—¶ {summary['total_execution_time']:.2f}ç§’, "
                              f"æˆåŠŸç‡ {summary['success_rate']*100:.1f}%[/dim]")
                else:
                    safe_print("[red]âŒ æ¨ç†æ‰§è¡Œå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆç»“æœ[/red]")

            except Exception as e:
                safe_print(f"[red]âŒ Chain of Thoughtæ¨ç†æ‰§è¡Œå¤±è´¥: {e}[/red]")
                safe_print("[yellow]ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹é…ç½®å’Œç½‘ç»œè¿æ¥[/yellow]")

        return asyncio.run(run_chain_reasoning())

    def chain_run(
        self,
        config_file: str,
        input_data: str = None,
        model: str = None,
        base_url: str = None,
        api_key: str = None,
        show_details: bool = False,
        **kwargs,
    ):
        """è¿è¡Œè‡ªå®šä¹‰çš„Chain of Thoughté…ç½®æ–‡ä»¶
        
        Args:
            config_file: YAMLæ ¼å¼çš„é“¾æ¡é…ç½®æ–‡ä»¶è·¯å¾„
            input_data: è¾“å…¥æ•°æ®ï¼Œä¼šä½œä¸ºqueryä¼ å…¥
            model: ä½¿ç”¨çš„æ¨¡å‹ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰
            base_url: APIæœåŠ¡åœ°å€
            api_key: APIå¯†é’¥
            show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ‰§è¡Œä¿¡æ¯
        """
        import asyncio
        import yaml
        import os
        from pathlib import Path
        from flexllm.chain_of_thought_client import ChainOfThoughtClient, LinearStep, ExecutionConfig
        from flexllm.openaiclient import OpenAIClient

        async def run_chain_config():
            try:
                # è¯»å–é…ç½®æ–‡ä»¶
                config_path = Path(config_file)
                if not config_path.exists():
                    safe_print(f"[red]âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}[/red]")
                    return

                safe_print(f"[bold green]ğŸ“‹ è¿è¡ŒChain of Thoughté…ç½®[/bold green]")
                safe_print(f"[cyan]ğŸ“ é…ç½®æ–‡ä»¶: {config_file}[/cyan]")

                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)

                # ä»é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°åˆå¹¶è®¾ç½®
                mllm_config = self.cli.maque_config.get("mllm", {})
                
                # æ¨¡å‹é…ç½®ä¼˜å…ˆçº§: å‘½ä»¤è¡Œ > é…ç½®æ–‡ä»¶ > å…¨å±€é…ç½®
                final_model = model or config.get('model') or mllm_config.get("model", "gemma3:latest")
                final_base_url = base_url or config.get('base_url') or mllm_config.get("base_url", "http://localhost:11434/v1")
                final_api_key = api_key or config.get('api_key') or mllm_config.get("api_key", "EMPTY")

                # è·å–è¾“å…¥æ•°æ®
                query = input_data or config.get('query', '')
                if not query:
                    safe_print("[red]âŒ ç¼ºå°‘è¾“å…¥æ•°æ®ï¼Œè¯·é€šè¿‡ --input-data å‚æ•°æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­çš„ 'query' å­—æ®µæŒ‡å®š[/red]")
                    return

                safe_print(f"[cyan]ğŸ“ è¾“å…¥: {query}[/cyan]")
                safe_print(f"[dim]ğŸ”§ æ¨¡å‹: {final_model}[/dim]\n")

                # åˆå§‹åŒ–å®¢æˆ·ç«¯
                openai_client = OpenAIClient(model=final_model, base_url=final_base_url, api_key=final_api_key)
                
                # æ‰§è¡Œé…ç½®
                exec_config = ExecutionConfig(
                    enable_monitoring=config.get('enable_monitoring', True),
                    enable_progress=show_details,
                    log_level="INFO" if show_details else "WARNING",
                    step_timeout=config.get('step_timeout'),
                    chain_timeout=config.get('chain_timeout'),
                    max_retries=config.get('max_retries', 0),
                    retry_delay=config.get('retry_delay', 1.0)
                )
                
                chain_client = ChainOfThoughtClient(openai_client, exec_config)

                # æ„å»ºæ­¥éª¤
                steps = config.get('steps', [])
                if not steps:
                    safe_print("[red]âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰å®šä¹‰æ­¥éª¤[/red]")
                    return

                def create_config_step(step_config):
                    step_name = step_config['name']
                    system_prompt = step_config.get('system_prompt', '')
                    user_prompt = step_config.get('user_prompt', '')
                    
                    def prepare_messages(context):
                        # å¤„ç†æ¨¡æ¿å˜é‡
                        template_vars = {
                            'query': context.query,
                            'previous_responses': '\n\n'.join([f"[{s.step_name}]: {s.response}" for s in context.history])
                        }
                        
                        # æ·»åŠ è‡ªå®šä¹‰å˜é‡
                        custom_vars = context.get_custom_data('template_vars', {})
                        template_vars.update(custom_vars)
                        
                        messages = []
                        if system_prompt:
                            messages.append({
                                "role": "system", 
                                "content": system_prompt.format(**template_vars)
                            })
                        
                        messages.append({
                            "role": "user",
                            "content": user_prompt.format(**template_vars)
                        })
                        
                        return messages
                    
                    # è·å–æ¨¡å‹å‚æ•°
                    model_params = step_config.get('model_params', {})
                    model_params.update(kwargs)  # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
                    
                    return LinearStep(
                        name=step_name,
                        prepare_messages_fn=prepare_messages,
                        model_params=model_params
                    )

                # åˆ›å»ºæ‰€æœ‰æ­¥éª¤
                chain_steps = [create_config_step(step_config) for step_config in steps]
                
                # åˆ›å»ºå’Œæ‰§è¡Œé“¾æ¡
                chain_name = config.get('name', 'custom_chain')
                first_step = chain_client.create_linear_chain(chain_steps, chain_name)
                
                # æ·»åŠ è‡ªå®šä¹‰æ¨¡æ¿å˜é‡åˆ°ä¸Šä¸‹æ–‡
                context = chain_client.create_context({"query": query})
                if config.get('template_vars'):
                    context.add_custom_data('template_vars', config['template_vars'])
                
                result_context = await chain_client.execute_chain(
                    first_step, context, show_step_details=show_details
                )

                # æ˜¾ç¤ºç»“æœ
                if result_context.history:
                    safe_print(f"\n[bold blue]ğŸ¯ {config.get('name', 'Chain')} æ‰§è¡Œç»“æœ[/bold blue]")
                    safe_print(f"[dim]{'=' * 60}[/dim]")
                    
                    for step_result in result_context.history:
                        step_display = step_result.step_name.replace('_', ' ').title()
                        safe_print(f"\n[bold cyan]ğŸ“ {step_display}[/bold cyan]")
                        safe_print(f"[green]{step_result.response}[/green]")
                    
                    # æ‰§è¡Œæ‘˜è¦
                    summary = result_context.get_execution_summary()
                    safe_print(f"\n[dim]ğŸ“Š æ‰§è¡Œç»Ÿè®¡: {summary['total_steps']} ä¸ªæ­¥éª¤, "
                              f"è€—æ—¶ {summary['total_execution_time']:.2f}ç§’, "
                              f"æˆåŠŸç‡ {summary['success_rate']*100:.1f}%[/dim]")
                else:
                    safe_print("[red]âŒ é“¾æ¡æ‰§è¡Œå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆç»“æœ[/red]")

            except yaml.YAMLError as e:
                safe_print(f"[red]âŒ YAMLé…ç½®æ–‡ä»¶è§£æé”™è¯¯: {e}[/red]")
            except FileNotFoundError as e:
                safe_print(f"[red]âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {e}[/red]")
            except Exception as e:
                safe_print(f"[red]âŒ Chainæ‰§è¡Œå¤±è´¥: {e}[/red]")
                safe_print("[yellow]ğŸ’¡ è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æ ¼å¼å’Œæ¨¡å‹è¿æ¥[/yellow]")

        return asyncio.run(run_chain_config())
