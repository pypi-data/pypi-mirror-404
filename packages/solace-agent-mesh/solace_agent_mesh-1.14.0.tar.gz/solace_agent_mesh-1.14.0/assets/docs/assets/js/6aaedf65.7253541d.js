"use strict";(self.webpackChunksolace_agenitc_mesh_docs=self.webpackChunksolace_agenitc_mesh_docs||[]).push([[4795],{289:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>l,frontMatter:()=>o,metadata:()=>s,toc:()=>h});const s=JSON.parse('{"id":"documentation/components/speech","title":"Speech Integration","description":"Agent Mesh provides speech capabilities through integrated Speech-to-Text (STT) and Text-to-Speech (TTS) services. You can enable users to interact with agents through voice input and receive spoken responses, creating more natural and accessible conversational experiences.","source":"@site/docs/documentation/components/speech.md","sourceDirName":"documentation/components","slug":"/documentation/components/speech","permalink":"/solace-agent-mesh/docs/documentation/components/speech","draft":false,"unlisted":false,"editUrl":"https://github.com/SolaceLabs/solace-agent-mesh/edit/main/docs/docs/documentation/components/speech.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docSidebar","previous":{"title":"Prompt Library","permalink":"/solace-agent-mesh/docs/documentation/components/prompts"},"next":{"title":"Installing and Configuring Agent Mesh","permalink":"/solace-agent-mesh/docs/documentation/installing-and-configuring/"}}');var i=t(4848),r=t(8453);const o={},a="Speech Integration",c={},h=[{value:"Understanding Speech Integration",id:"understanding-speech-integration",level:2},{value:"Configuring Speech Services",id:"configuring-speech-services",level:2},{value:"Speech-to-Text Configuration",id:"speech-to-text-configuration",level:3},{value:"Text-to-Speech Configuration",id:"text-to-speech-configuration",level:3},{value:"Enabling Speech Features",id:"enabling-speech-features",level:2},{value:"Managing User Settings",id:"managing-user-settings",level:2},{value:"Monitoring Speech Usage",id:"monitoring-speech-usage",level:2},{value:"Troubleshooting Speech Issues",id:"troubleshooting-speech-issues",level:2},{value:"Security Considerations",id:"security-considerations",level:2},{value:"Integration Examples",id:"integration-examples",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"speech-integration",children:"Speech Integration"})}),"\n",(0,i.jsx)(n.p,{children:"Agent Mesh provides speech capabilities through integrated Speech-to-Text (STT) and Text-to-Speech (TTS) services. You can enable users to interact with agents through voice input and receive spoken responses, creating more natural and accessible conversational experiences."}),"\n",(0,i.jsx)(n.h2,{id:"understanding-speech-integration",children:"Understanding Speech Integration"}),"\n",(0,i.jsx)(n.p,{children:"The speech system consists of two complementary services that work together to enable voice interactions. The STT service converts spoken audio into text that agents can process, while the TTS service transforms agent responses into natural-sounding speech. Both services support multiple providers and can be configured independently based on your requirements."}),"\n",(0,i.jsx)(n.p,{children:"The system integrates with the WebUI gateway to provide seamless voice interactions in chat interfaces. When you enable speech features, users see microphone and speaker controls that allow them to speak their questions and hear agent responses without typing."}),"\n",(0,i.jsx)(n.h2,{id:"configuring-speech-services",children:"Configuring Speech Services"}),"\n",(0,i.jsxs)(n.p,{children:["You configure speech services in your gateway YAML file under the ",(0,i.jsx)(n.code,{children:"app_config.speech"})," section. The configuration defines which providers to use, authentication credentials, and service-specific settings that control behavior and quality."]}),"\n",(0,i.jsx)(n.h3,{id:"speech-to-text-configuration",children:"Speech-to-Text Configuration"}),"\n",(0,i.jsx)(n.p,{children:"The STT service transcribes audio input into text using either OpenAI's Whisper API or Azure Speech Services. You specify the provider and its credentials in your configuration:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'app_config:\n  speech:\n    stt:\n      provider: openai  # or "azure"\n      openai:\n        api_key: ${OPENAI_API_KEY}\n        url: https://api.openai.com/v1/audio/transcriptions\n        model: whisper-1\n'})}),"\n",(0,i.jsx)(n.p,{children:"When using Azure Speech Services, you provide your subscription key and region:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"app_config:\n  speech:\n    stt:\n      provider: azure\n      azure:\n        api_key: ${AZURE_SPEECH_KEY}\n        region: eastus\n        language: en-US\n"})}),"\n",(0,i.jsx)(n.p,{children:"The system validates audio files before transcription, rejecting files larger than 25MB or with unsupported formats. Supported formats include WAV, MP3, WebM, and OGG."}),"\n",(0,i.jsx)(n.h3,{id:"text-to-speech-configuration",children:"Text-to-Speech Configuration"}),"\n",(0,i.jsx)(n.p,{children:"The TTS service generates natural-sounding speech from text using either Google's Gemini or Azure Neural Voices. You configure the provider, voice selection, and quality settings:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'app_config:\n  speech:\n    tts:\n      provider: gemini  # or "azure"\n      gemini:\n        api_key: ${GEMINI_API_KEY}\n        model: gemini-2.5-flash-preview-tts\n        default_voice: Kore\n        voices:\n          - Kore\n          - Puck\n          - Charon\n          - Kore\n          - Fenrir\n          - Aoede\n'})}),"\n",(0,i.jsx)(n.p,{children:"Azure Neural Voices offer high-definition voices with natural prosody:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"app_config:\n  speech:\n    tts:\n      provider: azure\n      azure:\n        api_key: ${AZURE_SPEECH_KEY}\n        region: eastus\n        default_voice: en-US-Ava:DragonHDLatestNeural\n        voices:\n          - en-US-Ava:DragonHDLatestNeural\n          - en-US-Andrew:DragonHDLatestNeural\n          - en-US-Emma:DragonHDLatestNeural\n          - en-US-Brian:DragonHDLatestNeural\n"})}),"\n",(0,i.jsx)(n.p,{children:"The system automatically chunks long text into manageable segments for streaming playback, reducing latency and improving the user experience."}),"\n",(0,i.jsx)(n.h2,{id:"enabling-speech-features",children:"Enabling Speech Features"}),"\n",(0,i.jsxs)(n.p,{children:["Speech features are disabled by default and require explicit configuration to appear in the user interface. You control feature visibility through the ",(0,i.jsx)(n.code,{children:"frontend_feature_enablement"})," section:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"app_config:\n  frontend_feature_enablement:\n    speechToText: true\n    textToSpeech: true\n"})}),"\n",(0,i.jsx)(n.p,{children:"When you enable these flags, the WebUI displays microphone and speaker controls in the chat interface. Users can click the microphone to record voice input or the speaker icon to hear agent responses."}),"\n",(0,i.jsx)(n.h2,{id:"managing-user-settings",children:"Managing User Settings"}),"\n",(0,i.jsx)(n.p,{children:"Users can customize their speech experience through the settings panel. The system provides controls for voice selection, playback speed, and automatic playback behavior. You can set default values that users can override:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"app_config:\n  speech:\n    speechTab:\n      speechToText:\n        speechToText: true\n        engineSTT: external\n        languageSTT: en-US\n      textToSpeech:\n        textToSpeech: true\n        engineTTS: external\n        voice: Kore\n        playbackRate: 1.0\n"})}),"\n",(0,i.jsx)(n.h2,{id:"monitoring-speech-usage",children:"Monitoring Speech Usage"}),"\n",(0,i.jsx)(n.p,{children:"Speech services consume API credits based on audio duration and text length. OpenAI charges per minute of audio transcribed, while Gemini and Azure charge per character of text synthesized. You should monitor usage through your provider's dashboard and set appropriate rate limits to control costs."}),"\n",(0,i.jsx)(n.p,{children:"The system logs all speech operations, including transcription requests, TTS generation, and any errors encountered. You can use these logs to track usage patterns, identify issues, and optimize your configuration for better performance and cost efficiency."}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting-speech-issues",children:"Troubleshooting Speech Issues"}),"\n",(0,i.jsx)(n.p,{children:"When speech features do not appear in the interface, verify that you have enabled the feature flags in your configuration and that the gateway has restarted to load the new settings. Check the browser console for any JavaScript errors that might prevent the speech controls from rendering."}),"\n",(0,i.jsx)(n.p,{children:"If transcription fails, confirm that your API keys are valid and that you have sufficient credits with your provider. The system returns specific error messages for common issues like unsupported audio formats, files that are too large, or API authentication failures."}),"\n",(0,i.jsx)(n.p,{children:"For TTS problems, verify that your selected voice is available for your provider and region. Some voices require specific API versions or subscription tiers. The system falls back to default voices when requested voices are unavailable, but you should configure appropriate defaults to ensure consistent behavior."}),"\n",(0,i.jsx)(n.h2,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,i.jsx)(n.p,{children:"Audio data passes through your gateway to external speech providers. The system does not store audio recordings by default, but transcribed text becomes part of the conversation history. You should inform users about data handling practices and comply with relevant privacy regulations when processing voice data."}),"\n",(0,i.jsx)(n.h2,{id:"integration-examples",children:"Integration Examples"}),"\n",(0,i.jsxs)(n.p,{children:["For a complete working example, see the WebUI gateway configuration in ",(0,i.jsx)(n.code,{children:"templates/webui.yaml"}),". This configuration demonstrates all speech settings with appropriate defaults and shows how to structure your YAML for production use."]})]})}function l(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var s=t(6540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);