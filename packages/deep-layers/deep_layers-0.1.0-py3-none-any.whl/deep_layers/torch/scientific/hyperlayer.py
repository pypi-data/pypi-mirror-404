import torch
import torch.nn as nn
import torch.nn.functional as F
class HyperLayer(nn.Module):
    """
    Ha et al., 'HyperNetworks', 2016.
    
    Purpose
    -------
    Dynamically generates weights for a main layer based on a context vector.
    
    Description
    -----------
    A small 'hypernetwork' outputs the weights of a larger layer.
    
    Logic
    -----
        1. Input context z.
    2. HyperNetwork H_Î¸(z) outputs weight matrix W and bias b.
    3. Main layer computes y = Wx + b.
    
    HyperLSTM Cell implementation based on 'HyperNetworks' (Ha et al., 2016).
    This implements the 'Memory Efficient' version described in Equation 12.
    """
    def __init__(self, input_size, hidden_size, hyper_hidden_size):
        super(HyperLayer, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.hyper_hidden_size = hyper_hidden_size

        # --- Main LSTM Components (Static Weights) ---
        # We perform the linear transformations for the Main LSTM using static weights.
        # 4 * hidden_size because we compute input, forget, cell, and output gates simultaneously.
        self.main_lstm_x = nn.Linear(input_size, 4 * hidden_size, bias=True)
        self.main_lstm_h = nn.Linear(hidden_size, 4 * hidden_size, bias=False)
        
        # Layer Normalization for the Main LSTM (Eq 12)
        self.layer_norm = nn.LayerNorm(4 * hidden_size)
        self.layer_norm_c = nn.LayerNorm(hidden_size) # Add this line
        
        # --- HyperNetwork Components ---
        # The HyperNetwork is a smaller LSTM cell.
        # Input to HyperNet is concatenation of Main Input and Main Previous Hidden (Eq 6)
        hyper_input_size = input_size + hidden_size
        self.hyper_cell = nn.LSTMCell(hyper_input_size, hyper_hidden_size)

        # Weight Generators (Projections from HyperHidden to Scaling Vectors)
        # We need scaling vectors for both the input-to-hidden (d_x) and hidden-to-hidden (d_h)
        # components of the 4 gates.
        self.z_to_dx = nn.Linear(hyper_hidden_size, 4 * hidden_size)
        self.z_to_dh = nn.Linear(hyper_hidden_size, 4 * hidden_size)
        
        # Bias generator (optional, but consistent with Eq 12 scaling)
        self.z_to_db = nn.Linear(hyper_hidden_size, 4 * hidden_size)

    def forward(self, x, states):
        """
        Args:
            x: Input tensor (batch_size, input_size)
            states: Tuple containing ((main_h, main_c), (hyper_h, hyper_c))
        """
        (main_h, main_c), (hyper_h, hyper_c) = states

        # 1. Update HyperNetwork
        # Input is concatenation of x_t and h_{t-1} (Eq 6)
        hyper_input = torch.cat([x, main_h], dim=1)
        hyper_h_new, hyper_c_new = self.hyper_cell(hyper_input, (hyper_h, hyper_c))

        # 2. Generate Scaling Vectors (Dynamic Weights)
        # d_x, d_h, d_b in the paper (derived from z, which is derived from hyper_h)
        d_x = self.z_to_dx(hyper_h_new)
        d_h = self.z_to_dh(hyper_h_new)
        d_b = self.z_to_db(hyper_h_new)

        # 3. Compute Main LSTM Linear Projections (Static)
        # W_x * x + b
        proj_x = self.main_lstm_x(x) 
        # W_h * h
        proj_h = self.main_lstm_h(main_h)

        # 4. Apply Dynamic Scaling (Element-wise multiplication) - Eq 12
        # y = LN(d_x * (Wx) + d_h * (Wh) + d_b)
        # Note: We apply d_b as a scaling factor to the static bias implicitly contained in main_lstm_x
        # or as an additive term. The paper Eq 12 uses it as an additive bias generated by HyperNet.
        # Here we follow the element-wise scaling logic:
        
        gates = (d_x * proj_x) + (d_h * proj_h) + d_b
        
        # 5. Apply Layer Normalization
        gates = self.layer_norm(gates)

        # 6. Standard LSTM Gating Logic
        i, f, g, o = torch.chunk(gates, 4, dim=1)
        
        in_gate = torch.sigmoid(i)
        forget_gate = torch.sigmoid(f)
        cell_gate = torch.tanh(g)
        out_gate = torch.sigmoid(o)

        main_c_new = (forget_gate * main_c) + (in_gate * cell_gate)
        # Use the specific LN for the hidden size
        main_h_new = out_gate * torch.tanh(self.layer_norm_c(main_c_new))
        return main_h_new, ((main_h_new, main_c_new), (hyper_h_new, hyper_c_new))

class StaticHyperLinear(nn.Module):
    """
    A Static HyperNetwork layer for Fully Connected networks.
    Generates weights for a linear layer based on an embedding vector z.
    """
    def __init__(self, in_features, out_features, z_dim=32):
        super(StaticHyperLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.z_dim = z_dim

        # The learned embedding vector for this specific layer (Equation 1)
        self.z = nn.Parameter(torch.randn(z_dim))

        # HyperNetwork: Maps z -> Weights of main layer
        # For efficiency, we don't output (in * out) directly.
        # We project z -> hidden, then hidden -> weights.
        self.hyper_l1 = nn.Linear(z_dim, z_dim) # Hidden layer of hypernet
        self.hyper_l2_weights = nn.Linear(z_dim, in_features * out_features)
        self.hyper_l2_bias = nn.Linear(z_dim, out_features)

    def forward(self, x):
        # 1. Generate Weights
        h = F.elu(self.hyper_l1(self.z))
        
        # Generate Weight Matrix
        generated_weight = self.hyper_l2_weights(h)
        generated_weight = generated_weight.view(self.out_features, self.in_features)
        
        # Generate Bias
        generated_bias = self.hyper_l2_bias(h)

        # 2. Apply Linear Layer
        return F.linear(x, generated_weight, generated_bias)
