{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectrixDB Direct Embedders\n",
    "\n",
    "Access the embedding models directly for custom use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Dense Embedder\n",
    "\n",
    "Get vector embeddings directly without storing in a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T09:34:52.627035Z",
     "iopub.status.busy": "2026-02-01T09:34:52.626037Z",
     "iopub.status.idle": "2026-02-01T09:34:53.616398Z",
     "shell.execute_reply": "2026-02-01T09:34:53.615767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Machine learning is transforming industries.\n",
      "Embedding shape: (1, 384)\n",
      "First 5 values: [-0.05103514  0.01069377  0.02810179 -0.01240932  0.02174898]\n"
     ]
    }
   ],
   "source": [
    "from vectrixdb.models import DenseEmbedder\n",
    "\n",
    "# Initialize embedder (use bundled English model)\n",
    "embedder = DenseEmbedder(language=\"en\")\n",
    "\n",
    "# Embed single text\n",
    "text = \"Machine learning is transforming industries.\"\n",
    "embedding = embedder.embed(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"First 5 values: {embedding[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T09:34:53.675683Z",
     "iopub.status.busy": "2026-02-01T09:34:53.675683Z",
     "iopub.status.idle": "2026-02-01T09:34:54.386749Z",
     "shell.execute_reply": "2026-02-01T09:34:54.385237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch embeddings shape: (3, 384)\n",
      "Number of texts: 3\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Embed multiple texts (batch)\n",
    "texts = [\n",
    "    \"Python is great for data science.\",\n",
    "    \"JavaScript powers modern web apps.\",\n",
    "    \"Rust provides memory safety.\"\n",
    "]\n",
    "\n",
    "# embed() handles both single texts and lists\n",
    "embeddings = embedder.embed(texts)\n",
    "\n",
    "print(f\"Batch embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Number of texts: {len(texts)}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T09:34:54.391747Z",
     "iopub.status.busy": "2026-02-01T09:34:54.390747Z",
     "iopub.status.idle": "2026-02-01T09:34:55.310354Z",
     "shell.execute_reply": "2026-02-01T09:34:55.310354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between:\n",
      "  'I love machine learning.' and 'AI and ML are fascinating.': 0.8857\n",
      "  'I love machine learning.' and 'The weather is nice today.': 0.7552\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Compare texts\n",
    "text1 = \"I love machine learning.\"\n",
    "text2 = \"AI and ML are fascinating.\"\n",
    "text3 = \"The weather is nice today.\"\n",
    "\n",
    "# embed() returns 2D array (n_texts, dim), so use [0] to get 1D\n",
    "emb1 = embedder.embed(text1)[0]\n",
    "emb2 = embedder.embed(text2)[0]\n",
    "emb3 = embedder.embed(text3)[0]\n",
    "\n",
    "print(f\"Similarity between:\")\n",
    "print(f\"  '{text1}' and '{text2}': {cosine_similarity(emb1, emb2):.4f}\")\n",
    "print(f\"  '{text1}' and '{text3}': {cosine_similarity(emb1, emb3):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Sparse Embedder (BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T09:34:55.315362Z",
     "iopub.status.busy": "2026-02-01T09:34:55.314362Z",
     "iopub.status.idle": "2026-02-01T09:34:55.324373Z",
     "shell.execute_reply": "2026-02-01T09:34:55.324373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Python machine learning tutorial\n",
      "Sparse vector type: <class 'list'>\n",
      "Non-zero entries: N/A\n"
     ]
    }
   ],
   "source": [
    "from vectrixdb.models import SparseEmbedder\n",
    "\n",
    "# Initialize sparse embedder\n",
    "sparse = SparseEmbedder()\n",
    "\n",
    "# Get sparse representation\n",
    "text = \"Python machine learning tutorial\"\n",
    "sparse_vec = sparse.embed(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Sparse vector type: {type(sparse_vec)}\")\n",
    "print(f\"Non-zero entries: {len(sparse_vec) if isinstance(sparse_vec, dict) else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T09:34:55.327380Z",
     "iopub.status.busy": "2026-02-01T09:34:55.327380Z",
     "iopub.status.idle": "2026-02-01T09:34:56.872632Z",
     "shell.execute_reply": "2026-02-01T09:34:56.872632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning?\n",
      "\n",
      "Reranking scores:\n",
      "  1.0000: Deep learning is a subset of machine learning usin...\n",
      "  0.0000: Pizza is a popular Italian dish....\n",
      "  0.0000: Neural networks have multiple layers for feature e...\n",
      "  0.0000: The weather forecast predicts rain tomorrow....\n"
     ]
    }
   ],
   "source": [
    "from vectrixdb.models import RerankerEmbedder\n",
    "\n",
    "# Initialize reranker (use bundled English model)\n",
    "reranker = RerankerEmbedder(language=\"en\")\n",
    "\n",
    "# Query and candidate documents\n",
    "query = \"What is deep learning?\"\n",
    "documents = [\n",
    "    \"Deep learning is a subset of machine learning using neural networks.\",\n",
    "    \"The weather forecast predicts rain tomorrow.\",\n",
    "    \"Neural networks have multiple layers for feature extraction.\",\n",
    "    \"Pizza is a popular Italian dish.\"\n",
    "]\n",
    "\n",
    "# Score each document\n",
    "scores = reranker.score(query, documents)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Reranking scores:\")\n",
    "for doc, score in sorted(zip(documents, scores), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {score:.4f}: {doc[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model with Direct Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T09:34:56.877157Z",
     "iopub.status.busy": "2026-02-01T09:34:56.877157Z",
     "iopub.status.idle": "2026-02-01T09:34:57.611915Z",
     "shell.execute_reply": "2026-02-01T09:34:57.610408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (1, 384)\n",
      "Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# The embedder uses bundled models by default\n",
    "# You can also use custom ONNX models with model_dir parameter\n",
    "# Example: custom_embedder = DenseEmbedder(model_dir=\"/path/to/onnx/model\")\n",
    "\n",
    "# Using a different dimension is possible with custom models\n",
    "embedder_en = DenseEmbedder(language=\"en\")\n",
    "\n",
    "# Get embeddings\n",
    "text = \"Custom model embedding test.\"\n",
    "embedding = embedder_en.embed(text)\n",
    "\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Dimension: {embedding.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Finding Similar Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T09:34:57.616915Z",
     "iopub.status.busy": "2026-02-01T09:34:57.615916Z",
     "iopub.status.idle": "2026-02-01T09:34:59.265246Z",
     "shell.execute_reply": "2026-02-01T09:34:59.264237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'Python for ML'\n",
      "\n",
      "Most similar documents:\n",
      "  0.8862: Machine learning with Python is powerful.\n",
      "  0.8706: Python is excellent for data analysis.\n",
      "  0.8565: Data science uses Python extensively.\n",
      "  0.7538: React is a JavaScript framework.\n",
      "  0.7508: JavaScript enables interactive web pages.\n"
     ]
    }
   ],
   "source": [
    "# Document collection\n",
    "documents = [\n",
    "    \"Python is excellent for data analysis.\",\n",
    "    \"JavaScript enables interactive web pages.\",\n",
    "    \"Data science uses Python extensively.\",\n",
    "    \"React is a JavaScript framework.\",\n",
    "    \"Machine learning with Python is powerful.\"\n",
    "]\n",
    "\n",
    "# Embed all documents (embed() handles lists)\n",
    "doc_embeddings = embedder.embed(documents)\n",
    "\n",
    "# Find most similar to a query\n",
    "query = \"Python for ML\"\n",
    "query_emb = embedder.embed(query)\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = [cosine_similarity(query_emb[0], doc_emb) for doc_emb in doc_embeddings]\n",
    "\n",
    "# Rank by similarity\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Most similar documents:\")\n",
    "for doc, sim in sorted(zip(documents, similarities), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {sim:.4f}: {doc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
