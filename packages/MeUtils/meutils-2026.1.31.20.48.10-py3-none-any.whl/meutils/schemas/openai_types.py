#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Project      : AI.  @by PyCharm
# @File         : openai_types
# @Time         : 2024/6/7 17:30
# @Author       : betterme
# @WeChat       : meutils
# @Software     : PyCharm
# @Description  : https://github.com/Calcium-Ion/new-api/tree/main/dto

from meutils.pipe import *

from openai.types.chat import ChatCompletion as _ChatCompletion, ChatCompletionChunk as _ChatCompletionChunk
from openai.types.chat.chat_completion import Choice as _Choice, ChatCompletionMessage as _ChatCompletionMessage, \
    CompletionUsage as _CompletionUsage
from openai.types.chat.chat_completion_chunk import Choice as _ChunkChoice, ChoiceDelta
from openai._types import FileTypes
from openai.types import ImagesResponse as _ImagesResponse
from openai.types.shared_params import FunctionDefinition
from openai.types.chat import ChatCompletionToolParam

from meutils.schemas.image_types import ImageRequest, ImagesResponse

TOOLS = [
    {"type": "web_browser"},
    {"type": "code_interpreter"},
    {"type": "drawing_tool"},
]

BACKUP_MODEL = os.getenv("BACKUP_MODEL", "glm-4")


class Tool(BaseModel):
    # {"id": "", "type": "web_browser", "function": {}} => {"type": "web_browser", "function": None}
    function: Optional[dict] = None  # FunctionDefinition
    type: str

    def __init__(self, /, **data: Any):
        super().__init__(**data)
        self.function = self.function or None


class CompletionUsage(_CompletionUsage):  # todo åºŸå¼ƒ
    prompt_tokens: int = 1000
    completion_tokens: Optional[int] = None
    total_tokens: Optional[int] = None

    def __init__(self, /, **data: Any):
        super().__init__(**data)

        self.completion_tokens = self.completion_tokens or 0
        self.total_tokens = self.total_tokens or self.prompt_tokens + self.completion_tokens


class ChatCompletionMessage(_ChatCompletionMessage):
    role: Literal["assistant"] = "assistant"
    """The role of the author of this message."""


class Choice(_Choice):
    index: int = 0
    finish_reason: Literal["stop", "length", "tool_calls", "content_filter", "function_call"] = None


class ChatCompletion(_ChatCompletion):
    id: str = Field(default_factory=lambda: f"chatcmpl-{shortuuid.random()}")
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str = ""
    object: str = "chat.completion"
    usage: CompletionUsage = CompletionUsage()


class ChunkChoice(_ChunkChoice):
    index: int = 0


class ChatCompletionChunk(_ChatCompletionChunk):
    id: str = Field(default_factory=lambda: f"chatcmpl-{shortuuid.random()}")
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str = ""
    object: str = "chat.completion.chunk"


chat_completion = ChatCompletion(
    choices=[Choice(message=ChatCompletionMessage(reasoning_content="", content=""))]
)
chat_completion_chunk = ChatCompletionChunk(
    choices=[ChunkChoice(delta=ChoiceDelta(reasoning_content="", content=""))]
)
chat_completion_chunk_stop = ChatCompletionChunk(
    choices=[ChunkChoice(delta=ChoiceDelta(reasoning_content="", content=""), finish_reason="stop")]
)


# chat_completion.choices[0].message.content = "*"
# chat_completion_chunk.choices[0].delta.content = "*"

class CompletionRequest(BaseModel):
    """
    prompt_filter_result.content_filter_results
    choice.content_filter_results

    todo: ['messages', 'model', 'frequency_penalty', 'function_call', 'functions', 'logit_bias', 'logprobs', 'max_tokens', 'n', 'presence_penalty', 'response_format', 'seed', 'stop', 'stream', 'temperature', 'tool_choice', 'tools', 'top_logprobs', 'top_p', 'user']
    """
    model: str = ''  # "gpt-3.5-turbo-file-id"

    # [{'role': 'user', 'content': 'hi'}]
    # [{'role': 'user', 'content':  [{"type": "text", "text": ""}]]
    # [{'role': 'user', 'content': [{"type": "image_url", "image_url": ""}]}] # ä¹Ÿå…¼å®¹æ–‡ä»¶
    # [{'role': 'user', 'content': [{"type": "image_url", "image_url": {"url": ""}}]}] # ä¹Ÿå…¼å®¹æ–‡ä»¶
    # [{'role': 'user', 'content':  [{"type": "file", "file_url": ""}]]
    messages: Optional[List[Dict[str, Any]]] = None  # æ ‡å‡†åŒ–
    metadata: Optional[Dict[str, str]] = None

    stream: Optional[bool] = False
    stream_options: Optional[dict] = None

    top_p: Optional[float] = 0.7
    temperature: Optional[float] = 0.7

    n: Optional[int] = 1
    max_completion_tokens: Optional[int] = None
    max_tokens: Optional[int] = None
    stop: Optional[Union[str, List[str]]] = None

    frequency_penalty: Optional[float] = None
    user: Optional[str] = None

    reasoning_effort: Optional[Literal["low", "medium", "high"]] = None

    # Oneapi https://github.com/QuantumNous/new-api/blob/main/dto/openai_request.go
    extra_body: Optional[Any] = None
    enable_thinking: Optional[bool] = None  # ali

    thinking: Optional[dict] = None  # doubao "type": "disabled" "enabled" "auto"
    thinking_budget: Optional[int] = None  # æ€è€ƒé¢„ç®—

    # tools
    tools: Optional[Any] = None

    # tool_choice: Optional[Union[str, dict]] = None

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        self.max_completion_tokens = self.max_completion_tokens or self.max_tokens
        self.max_tokens = None

    @cached_property
    def system_instruction(self):
        # [{'role': 'system', 'content':  [{"type": "text", "text": ""}]]

        if message := self.messages and self.messages[0]:
            if message.get("role") == "system":
                return str(message.get("content", "")) or None

    @cached_property
    def last_message(self):
        return self.messages and self.messages[-1]

    @cached_property
    def last_user_content(self) -> str:
        """text"""
        for i, message in enumerate(self.messages[::-1], 1):
            if message.get("role") == "user":
                contents = message.get("content")
                if isinstance(contents, list):
                    for content in contents:
                        if content.get("type") == "text":
                            return content.get('text', "")

                else:
                    return str(contents)
        return ""

    @cached_property
    def last_assistant_content(self) -> str:
        """text"""
        for i, message in enumerate(self.messages[::-1], 1):
            if message.get("role") == "assistant":
                contents = message.get("content")
                if isinstance(contents, list):
                    for content in contents:
                        if content.get("type") == "text":
                            return content.get('text', "")
                else:
                    return str(contents)
        return ""

    @cached_property
    def last_urls(self):  # file_url å¤šè½®å¯¹è¯éœ€è¦  sum(request.last_urls.values(), []) # todo ä½ç½®ä¿¡æ¯
        """æœ€æ–°ä¸€è½®çš„ user url åˆ—è¡¨"""
        content_types = {
            "image_url",

            "audio_url", "input_audio",

            "video_url",

            "file_url", "file",

        }
        for i, message in enumerate(self.messages[::-1], 1):
            data = {}
            if message.get("role") == "user":  # æ¯ä¸€è½®è¿˜è¦å¤„ç†
                user_contents = message.get("content")
                if isinstance(user_contents, list):  # ç”¨æˆ· url
                    for content in user_contents:
                        content_type = content.get("type")
                        if content_type in content_types:
                            # logger.debug(content)
                            if _url := content.get(content_type, {}):  # {"type": "file", "file": fileid}
                                if isinstance(_url, str):  # å…¼å®¹äº†spark qwenai
                                    url = _url
                                else:
                                    url = _url.get("url") or _url.get("data")
                                url and data.setdefault(content_type, []).append(url)

            if data:
                data["audio_url"] = data.get("audio_url", []) + data.get("input_audio", [])
                data["file_url"] = data.get("file_url", []) + data.get("file", [])
                return data
        return {}

    # def create_message(self, text: str, content_type: Optional[str] = None):
    #     """
    #     æ¶ˆæ¯ç”Ÿæˆå™¨
    #     :param text:
    #     :param content_type:
    #     :return:
    #     """
    #     message = {
    #         'role': 'user',
    #         'content': [
    #             {
    #                 "type": "text",
    #                 "text": text
    #             },
    #         ]
    #     }
    #
    #     return message

    class Config:
        extra = "allow"

        json_schema_extra = {
            "examples": [
                {
                    "model": "deepseek-chat",
                    "messages": [
                        {
                            "role": "user",
                            "content": "hi"
                        }
                    ],
                    "stream": True
                },
            ]
        }


class ChatCompletionRequest(BaseModel):
    """
    prompt_filter_result.content_filter_results
    choice.content_filter_results

    todo: ['messages', 'model', 'frequency_penalty', 'function_call', 'functions', 'logit_bias', 'logprobs', 'max_tokens', 'n', 'presence_penalty', 'response_format', 'seed', 'stop', 'stream', 'temperature', 'tool_choice', 'tools', 'top_logprobs', 'top_p', 'user']
    """
    model: str = ''  # "gpt-3.5-turbo-file-id"

    # [{'role': 'user', 'content': 'hi'}]
    # [{'role': 'user', 'content':  [{"type": "text", "text": ""}]]
    # [{'role': 'user', 'content': [{"type": "image_url", "image_url": ""}]}] # ä¹Ÿå…¼å®¹æ–‡ä»¶
    # [{'role': 'user', 'content': [{"type": "image_url", "image_url": {"url": ""}}]}] # ä¹Ÿå…¼å®¹æ–‡ä»¶
    # [{'role': 'user', 'content':  [{"type": "file", "file_url": ""}]]
    messages: Optional[List[Dict[str, Any]]] = None

    stream: Optional[bool] = False
    stream_options: Optional[dict] = None

    top_p: Optional[float] = 0.7
    temperature: Optional[float] = 0.7

    n: Optional[int] = 1
    max_tokens: Optional[int] = None
    stop: Optional[Union[str, List[str]]] = None

    presence_penalty: Optional[float] = 0.0
    frequency_penalty: Optional[float] = None
    user: Optional[str] = None

    # tools
    response_format: Optional[Any] = None  # md json
    function_call: Optional[Any] = None
    functions: Optional[Any] = None
    tools: Optional[List[Tool]] = None  # ä¸ºäº†å…¼å®¹ oneapi
    tool_choice: Optional[Any] = None
    parallel_tool_calls: Optional[Any] = None

    # æ‹“å±•å­—æ®µ
    system_messages: Optional[list] = None
    last_content: Optional[Any] = None
    urls: List[str] = []

    system_fingerprint: Optional[str] = "ğŸ”¥"

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        self.messages = self.messages or [{'role': 'user', 'content': 'hi'}]
        self.system_messages = [m for m in self.messages if m.get("role") == "system"]

        # ç§»é™¤ ä¸æ”¯æŒçš„ content type: ä¼˜é›…å¤„ç†å¤šè½®

        last_message = self.messages[-1]
        self.last_content = last_message.get("content", "")  # å¤§éƒ¨åˆ†æ—¶é—´ç­‰ä»·äºuser_content
        """[{'role': 'user', 'content':  [{"type": "text", "text": ""}]]"""
        if isinstance(self.last_content, list) and self.last_content:
            # å¤šæ¨¡æ€ 'text','image_url','video_url' and 'video'
            urls = (
                    jsonpath.jsonpath(self.last_content, expr='$..url')
                    or jsonpath.jsonpath(self.last_content, expr='$..image_url')
                # or jsonpath.jsonpath(self.last_content, expr='$..audio_url')
                # or jsonpath.jsonpath(self.last_content, expr='$..video_url')
                # or jsonpath.jsonpath(self.last_content, expr='$..file_url')
            )
            self.urls = self.urls or urls or []

            # å–æœ€åä¸€ä¸ªæ–‡æœ¬æç¤ºè¯
            texts = jsonpath.jsonpath(self.last_content, expr='$..text') or []
            self.last_content = '\n'.join(texts)

        # å…¼å®¹ glm-4
        self.top_p = self.top_p is not None and np.clip(self.top_p, 0.01, 0.99)
        self.temperature = self.temperature is not None and np.clip(self.temperature, 0.01, 0.99)

        if self.model.startswith('o1'):
            self.top_p = 1
            self.temperature = 1

        if self.max_tokens:
            self.max_tokens = min(self.max_tokens, 4096)

    class Config:
        extra = "allow"

        json_schema_extra = {
            "examples": [
                {
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {
                            "role": "user",
                            "content": "hi"
                        }
                    ],
                    "stream": False
                },

                {
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {
                            "role": "user",
                            "content": "è¯·æŒ‰ç…§ä¸‹é¢æ ‡é¢˜ï¼Œå†™ä¸€ç¯‡400å­—çš„æ–‡ç« \nç‹å¿—æ–‡è¯´ï¼Œä¸€ä¸ªä¸ç†Ÿçš„äººæ‰¾ä½ å€Ÿé¥¯ï¼Œè¯´æ˜ä»–å·²ç»æŠŠç†Ÿäººå€Ÿéäº†ã€‚é™¤éä½ ä¸æƒ³è¦äº†ï¼Œå¦åˆ™ä¸è¦å€Ÿ"
                        }
                    ],
                    "stream": False
                },

                # url
                {
                    "model": "url-gpt-3.5-turbo",
                    "messages": [
                        {
                            "role": "user",
                            "content": "æ€»ç»“ä¸€ä¸‹https://mp.weixin.qq.com/s/Otl45GViytuAYPZw3m7q9w"
                        }
                    ],
                    "stream": False
                },

                # rag
                {
                    "messages": [
                        {
                            "content": "åˆ†åˆ«æ€»ç»“è¿™ä¸¤ç¯‡æ–‡ç« ",
                            "role": "user"
                        }
                    ],
                    "model": "gpt-3.5-turbo",
                    "stream": False,
                    "file_ids": ["cn2a0s83r07am0knkeag", "cn2a3ralnl9crebipv4g"]
                }

            ]
        }


class TTSRequest(BaseModel):
    model: Optional[Union[str, Literal["tts-1", "tts-1-hd"]]] = 'tts'
    voice: Optional[Union[str, Literal["alloy", "echo", "fable", "onyx", "nova", "shimmer",
    "male", "femal",
    ]]] = ""

    input: str
    instructions: Optional[str] = None
    emotion: Optional[Literal[
        "happy", "angry", "surprise", "coldness", "disgust", "fear", "excited", "hate", "sad", "fearful", "disgusted", "surprised", "neutral"]] = None

    speed: Optional[float] = None

    response_format: Union[
        str, Literal["mp3", "opus", "aac", "flac", "wav", "pcm", "b64_json", "url", "hex"]] = "b64_json"

    chunking_strategy: Optional[Any] = None
    include: Optional[Any] = None
    known_speaker_names: Optional[List[str]] = None
    known_speaker_references: Optional[List[Any]] = None
    language: str = ""
    prompt: str = ""
    stream: bool = False
    temperature: Optional[float] = None
    timestamp_granularities: Optional[List[Literal["word", "segment"]]] = None

    # https://github.com/QuantumNous/new-api/blob/main/dto/audio.go
    metadata: Optional[dict] = None

    class Config:
        extra = "allow"

        json_schema_extra = {
            "examples": [
                {
                    "model": "tts-1",
                    "voice": "7f92f8afb8ec43bf81429cc1c9199cb1",
                    "input": "ä½ å¥½ï¼Œæˆ‘æ˜¯AIåŠ©æ‰‹",
                }
            ]
        }


class STTRequest(BaseModel):  # ASR
    file: Union[bytes, str]
    model: str = "whisper-1"
    prompt: Optional[str] = None
    response_format: Literal["text", "srt", "verbose_json", "vtt"] = "text"
    language: Optional[str] = None
    temperature: Optional[float] = None
    timestamp_granularities: Optional[List[Literal["word", "segment"]]] = None


if __name__ == '__main__':
    pass

    # print(ChatCompletion(choices=[Choice(message=ChatCompletionMessage(content="ChatCompletion"))]))
    # print(ChatCompletionChunk(choices=[ChunkChoice(delta=ChoiceDelta(content="ChatCompletionChunk"))]))
    #
    # print(chat_completion)
    # print(chat_completion_chunk)
    # print(chat_completion_chunk_stop)

    # print(ChatCompletionRequest(temperature=0, top_p=1))
    # print(ChatCompletionRequest(temperature=1, top_p=0))

    # file = UploadFile(open("/Users/betterme/PycharmProjects/AI/ChatLLM/chatllm/api/routers/cocopilot.py", "rb"))
    # #
    # print(AudioRequest(file=file))
    # print(ImagesResponse(data=[]))
    # print(ChatCompletionRequest(tools=[
    #     {
    #         # "id": "",
    #         "type": "web_browser",
    #         # "function": {}
    #     }
    # ],
    #     max_tokens=None,
    # ).model_dump_json(indent=4))

    # åˆ›å»ºå®ä¾‹
    # req1 = ImageRequest(guidance_scale=5.0, prompt="ç”»æ¡ç‹—")
    # req2 = ImageRequest(guidance=5.0, prompt="ç”»æ¡ç‹—")
    #
    # print(req1.guidance_scale)  # è¾“å‡º: 5.0
    # print(req2.guidance_scale)  # è¾“å‡º: 5.0
    #
    # # ä¸¤ç§æ–¹å¼éƒ½å¯ä»¥è®¿é—®è¿™ä¸ªå€¼
    # print(req1.model_dump())  # è¾“å‡º: {'guidance_scale': 5.0}
    # print(req1.model_dump(by_alias=True))  # è¾“å‡º: {'guidance': 5.0}

    # class A(BaseModel):
    #     n: int = Field(1, ge=1, le=0)
    #
    #
    # print(A(n=11))
    messages = [
        {'role': 'user',
         'content': [{"type": "image_url", "image_url": "https://oss.ffire.cc/files/kling_watermark.png"}]},
        {'role': 'assistant', 'content': [{"type": "image_url", "image_url": "è¿™æ˜¯ä¸ªå›¾ç‰‡é“¾æ¥"}]},

        {'role': 'assistant', 'content': [{"type": "image_url", "image_url": "è¿™æ˜¯ä¸ªå›¾ç‰‡é“¾æ¥"}]},

        {'role': 'user', 'content': [
            {"type": "image_url", "image_url": {"url": "è¿™æ˜¯ä¸ªå›¾ç‰‡é“¾æ¥1"}},
            {"type": "file_url", "file_url": {"url": "è¿™æ˜¯ä¸ªfile_url"}},
            {"type": "file_url", "file_url": {"url": "è¿™æ˜¯ä¸ªfile_url"}},
            {"type": "file", "file": "è¿™æ˜¯ä¸ªfileid"},

            {"type": "audio_url", "audio_url": {"url": "è¿™æ˜¯ä¸ªfile_url"}},
            {"type": "video_url", "video_url": {"url": "è¿™æ˜¯ä¸ªvideo_url"}}
        ]},

        {'role': 'assistant', 'content': [{"type": "image_url", "image_url": "è¿™æ˜¯ä¸ªå›¾ç‰‡é“¾æ¥"}]},
        {'role': 'user', 'content': [
            {"type": "text", "text": "è¿™æ˜¯ä¸ªæ–‡æœ¬"},
            {"type": "image_url", "image_url": "è¿™æ˜¯ä¸ªå›¾ç‰‡é“¾æ¥"}
        ]},
    ]

    messages = [{'role': 'system',
                 'content': 'undefined\n Current date: 2025-03-13'},
                {'role': 'user',
                 'content': [{'type': 'text', 'text': 'è§£è¯»'},
                             {'type': 'image_url',
                              'image_url': 'https://ai.chatfire.cn/files/images/xx-1741850404348-339a12ba3.png'}]},
                {},
                {'role': 'user', 'content': 'æ€»ç»“ä¸€ä¸‹'}]

    messages = [{'role': 'system',
                 'content': 'undefined\n Current date: 2025-03-13'},
                {'role': 'user',
                 'content': [{'type': 'text', 'text': 'è§£è¯»'},
                             {'type': 'image_url',
                              'image_url': 'https://ai.chatfire.cn/files/images/xx-1741850404348-339a12ba3.png'}]},
                {},
                {'role': 'user', 'content': 'æ€»ç»“ä¸€ä¸‹'}]

    messages = [{'role': 'system',
                 'content': 'undefined\n Current date: 2025-03-13'},
                {'role': 'user',
                 'content': [{'type': 'text', 'text': 'ä¸€å¥è¯æ€»ç»“'},
                             {'type': 'image_url',
                              'image_url': 'https://ai.chatfire.cn/files/images/uniacessåˆ é™¤-1741849647273-c3f3be340.txt'}]},
                {},
                {'role': 'user', 'content': 'æ€»ç»“'}]

    messages = [{'role': 'system',
                 'content': 'undefined\n Current date: 2025-03-13'},
                {'role': 'user',
                 'content': [{'type': 'text', 'text': 'ä¸€å¥è¯æ€»ç»“'},
                             {'type': 'image_url',
                              'image_url': 'https://ai.chatfire.cn/files/images/uniacessåˆ é™¤-1741853464016-b176842ad.txt'}]},
                {},
                {'role': 'user',
                 'content': [{'type': 'text', 'text': 'ä¸€å¥è¯æ€»ç»“'},
                             {'type': 'image_url',
                              'image_url': 'https://ai.chatfire.cn/files/images/uniacessåˆ é™¤-1741853464016-b176842ad.txt'}]},
                {},
                {'role': 'assistant',
                 'content': [{'type': 'text', 'text': 'ä¸€å¥è¯æ€»ç»“'},
                             {'type': 'image_url',
                              'image_url': 'https://ai.chatfire.cn/files/images/uniacessåˆ é™¤-1741853464016-b176842ad.txt'}]},
                {},
                {'role': 'user',
                 'content': [{'type': 'text', 'text': 'ä¸€å¥è¯æ€»ç»“'},
                             {'type': 'image_url',
                              'image_url': 'https://ai.chatfire.cn/files/images/uniacessåˆ é™¤-1741853464016-b176842ad.txt'}]},
                {},
                {'role': 'user',
                 'content': [{'type': 'text', 'text': 'ä¸€å¥è¯æ€»ç»“'},
                             {'type': 'image_url',
                              'image_url': 'https://ai.chatfire.cn/files/images/uniacessåˆ é™¤-1741853464016-b176842ad.txt'}]},
                {},
                {'role': 'user',
                 'content': [
                     {'type': 'image_url',
                      'image_url': 'https://ai.chatfire.cn/files/images/uniacessåˆ é™¤-1741853464016-b176842ad.txt'},
                     {
                         "type": "input_audio",
                         "input_audio": {
                             "data": "base64_audio",
                             "format": "wav"
                         }
                     },
                     {'type': 'text', 'text': 'ä¸€å¥è¯æ€»ç»“'},

                 ],

                 },
                {},
                # {'role': 'user', 'content': 'æ€»ç»“'}
                ]
    #
    # r = ChatCompletionRequest(model="gpt-3.5-turbo", messages=messages)
    # r.messages[-1]['content'] = [{"type": "image_url", "image_url": {"url": r.urls[-1]}}]
    # print(r)

    # print(chat_completion_chunk)
    # print(chat_completion)
    # print(chat_completion_chunk_stop)

    # print(CompletionRequest(messages=messages).last_urls)
    # print(CompletionRequest(messages=messages).last_urls)

    r = CompletionRequest(messages=messages)

    # print(mesages)
    # print(CompletionRequest(messages=messages).last_assistant_content)

    # print(chat_completion_chunk)
    # print(chat_completion)

    # chat_completion_chunk.usage = dict(
    #     completion_tokens=10,
    #     prompt_tokens=10,
    #     total_tokens=20,
    # )
    #
    # print(chat_completion_chunk)

    # print(ImageRequest(prompt='xx'))
