#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Project      : AI.  @by PyCharm
# @File         : chatbot
# @Time         : 2023/10/27 14:31
# @Author       : betterme
# @WeChat       : meutils
# @Software     : PyCharm
# @Description  : https://nicedouble-streamlitantdcomponentsdemo-app-middmy.streamlit.app/
from langchain.prompts import ChatPromptTemplate

from chatllm.llmchain.decorators import llm_stream
from meutils.pipe import *
from meutils.ai_cv.latex_ocr import latex_ocr
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain

import streamlit as st
from streamlit_extras.streaming_write import write




class ChatMessage(BaseModel):
    name: str = "assistant"  # "user", "assistant", or str
    avatar: Optional[str] = None
    generator: Any = 'æˆ‘æ˜¯ä¸€æ¡å†…å®¹'


def chat_message(message: ChatMessage, help: Optional[str] = None, message_hook: Optional[Callable] = None):
    """
        chat_message(ChatMessage())
        chat_message(ChatMessage(name='assistant'))

        def chat_messages(messages: List[ChatMessage]):
            for msg in messages:
                chat_message(msg)

        chat_messages([ChatMessage()] * 10)
    """
    with st.chat_message(name=message.name, avatar=message.avatar):
        # message_placeholder = st.empty()
        # response = ''
        # for token in message.generator:
        #     # Display robot response in chat message container
        #     # time.sleep(0.1)
        #     # token = repr(f"""{token}""").strip("'")
        #     response += token
        #     message_placeholder.markdown(response + "â–Œ")
        #
        # message_placeholder.markdown(response, unsafe_allow_html=True, help=help)

        # def fn():
        #     yield from message.generator

        write(message.generator)

        if message_hook: message_hook()


def ai_reply(user_input):
    template = ChatPromptTemplate.from_messages([
        ("system",
         """
         ä½ æ˜¯ä¸€åæ•°å­¦ä¸“å®¶ï¼Œä½ çš„åå­—å«ç«å®ğŸ”¥
         è¦æ±‚ï¼š
         1. Let's think step by step. 
         2. å¦‚æœç­”æ¡ˆé‡åˆ°å…¬å¼è¯·ç”¨æ ‡å‡†çš„latexè¾“å‡ºï¼Œå…¬å¼ç”¨$åŒ…è£¹ï¼Œä¾‹å¦‚ï¼š$\sqrt{{x^2+y^2}}=1$
         3. åŠ¡å¿…ç”¨ä¸­æ–‡å›ç­”
         """.strip()),
        ('human', 'å¼€å§‹è§£é¢˜ï¼š\n```{user_input}```')
    ])

    llm = LLMChain(llm=ChatOpenAI(model_name="gpt-4-0613", temperature=0, streaming=True), prompt=template)
    output = llm_stream(llm.run)(user_input=user_input)  # "gpt-3.5-"
    return output


if __name__ == '__main__':
    st.markdown('### ğŸ”¥è§£é¢˜å°èƒ½æ‰‹')

    # st.markdown(r':green[$\text{æ±‚æé™}\lim_{x\to0}\frac{e^x-e^{-x}-2x}{x-\sin x}$]')
    # st.markdown(":green[$\sqrt{x^2+y^2}=1$] is a Pythagorean identity. :pencil:")
    # st.markdown(r'$\frac{d}{dx}(e^x + e^{-x} - 2) = e^x - e^{-x}] [\frac{d}{dx}(1 - \cos x) = \sin x$')

    # user_input = st.chat_input("    ğŸ¤” å¼€å§‹è§£é¢˜å§")

    file = st.file_uploader('ä¸Šä¼ é¢˜ç›®å›¾ç‰‡', type=[".jpg", ".jpeg", '.png'])

    # æ¬¢è¿è¯­
    chat_message(
        ChatMessage(generator='ğŸ˜˜ğŸ˜˜ğŸ˜˜ å—¨ï¼Œæˆ‘æ˜¯ä½ çš„è§£é¢˜å°èƒ½æ‰‹ï¼\n\n **å‚è€ƒç¤ºä¾‹**ï¼š'),
        message_hook=lambda: st.latex(r"\lim_{x\to0}\frac{e^x-e^{-x}-2x}{x-\sin x}")
    )

    if file:
        with st.spinner('AI æ­£åœ¨æ€è€ƒğŸ¤”'):
            ocr_text = latex_ocr(file)
            # ocr_text_ = r"\lim_{x\to0}\frac{e^x-e^{-x}-2x}{x-\sin x}"
            chat_message(
                ChatMessage(generator='è¯†åˆ«åˆ°çš„é¢˜ç›®ï¼š\n\n'),
                message_hook=lambda: st.latex(ocr_text or "`æœªè§£æåˆ°å…·ä½“é—®é¢˜`")
            )
        if ocr_text:
            with st.spinner('AI æ­£åœ¨è§£é¢˜ğŸ¤”'):
                output = ai_reply(ocr_text)
                chat_message(ChatMessage(generator=output))

                # st.markdown(repr(f'{ocr_text}').strip("\'"))

            # st.markdown(_)
