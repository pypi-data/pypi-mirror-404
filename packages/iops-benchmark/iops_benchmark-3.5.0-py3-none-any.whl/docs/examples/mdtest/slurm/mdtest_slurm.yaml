# =============================================================================
# IOPS Benchmark Configuration Template (SLURM-Ready)
# =============================================================================
#
# This is a comprehensive template showing all available IOPS configuration options.
# Copy this file and customize it for your benchmark needs.
#
# Quick Start:
#   1. Edit the configuration to match your needs
#   2. Validate: iops my_benchmark.yaml --check_setup
#   3. Dry run: iops my_benchmark.yaml --dry-run
#   4. Execute: iops my_benchmark.yaml
#
# Documentation: https://lgouveia.gitlabpages.inria.fr/iops/reference/yaml-schema/ 
# =============================================================================


# =============================================================================
# BENCHMARK CONFIGURATION
# =============================================================================
benchmark:
  # Required: Name of the benchmark (used in logs and job names)
  name: "mdtest Benchmark"

  # Optional: Description of what this benchmark measures
  description: "A benchmark to measure metadata performance using the mdtest tool"

  # Required: Working directory where all run outputs will be stored
  # Structure: <workdir>/run_NNN/runs/ and <workdir>/run_NNN/logs/
  workdir: "/path/to/your/workdir"

  # Optional: SQLite database for caching execution results
  # When --use_cache is specified, results are reused if parameters match
  # Set this to enable caching across multiple runs
  cache_file: "/path/to/your/cache.db"

  # Optional: Number of repetitions for each test case (default: 1)
  # Can be overridden per round. Higher values improve statistical validity
  repetitions: 3

  # Required: Test selection strategy
  # Options:
  #   - "exhaustive": Test all parameter combinations (full factorial sweep)
  #   - "random": Randomly sample N configurations from parameter space
  #   - "bayesian": Intelligent optimization using Bayesian methods (fewer tests)
  # When to use:
  #   - exhaustive: Small parameter spaces, need complete coverage
  #   - random: Large parameter spaces, quick exploration, budget constraints
  #   - bayesian: Expensive evaluations, smooth objective functions
  
  search_method: "exhaustive"
  
  # Uncomment to enable random sampling:
  #search_method: "random"  
  # random_config:
  #   # Option 1: Explicit number of samples (mutually exclusive with percentage)
  #   n_samples: 20  # Sample exactly 20 random configurations
  #
  #   # Option 2: Percentage of total space (mutually exclusive with n_samples)
  #   # percentage: 0.1  # Sample 10% of parameter space
  #
  #   # Optional: behavior when n_samples >= total_space (default: true)
  #   fallback_to_exhaustive: true  # Use full space if sample >= total

  # Uncomment to enable Bayesian optimization:
  #search_method: "bayesian"
  # bayesian_config:
  #   objective_metric: "file_creation_rate"  # Required: metric to optimize (from parser)
  #   objective: "maximize"                   # "maximize" or "minimize" (default: "minimize")
  #   n_initial_points: 5                     # Random samples before optimization starts
  #   n_iterations: 20                        # Total number of evaluations
  #   acquisition_func: "EI"                  # "EI" (Expected Improvement), "PI", or "LCB"
  


  # Required: Execution backend
  # Options:
  #   - "local": Run scripts locally using subprocess
  #   - "slurm": Submit jobs to SLURM cluster via sbatch
  
  # executor: "local"
  
  executor: "slurm"
  # ---------------------------------------------------------------------------
  # EXECUTOR OPTIONS (optional)
  # ---------------------------------------------------------------------------
  # For SLURM executor: Customize commands used for job management
  # Commands are templates that support {job_id} placeholder for runtime substitution
  # Useful when running on systems with command wrappers or custom SLURM installations
  #
  # Uncomment to customize SLURM commands:
  # ---------------------------------------------------------------------------
  # slurm_options:
  #   commands:
  #     # Default submit command (default: "sbatch")
  #     # Can be overridden per-script via scripts[].submit
  #     submit: "sbatch"
  #
  #     # Command to query job status (default: "squeue -j {job_id} --noheader --format=%T")
  #     # The {job_id} placeholder is replaced with the actual job ID at runtime
  #     status: "squeue -j {job_id} --noheader --format=%T"
  #
  #     # Command to get job information (default: "scontrol show job {job_id}")
  #     info: "scontrol show job {job_id}"
  #
  #     # Command to cancel jobs (default: "scancel {job_id}")
  #     cancel: "scancel {job_id}"
  #
  #   # Polling interval in seconds for SLURM job status checks (default: 30)
  #   # Controls how often IOPS queries job status during wait_and_collect
  #   poll_interval: 30
  #

  # Optional: Random seed for reproducible random operations (default: None)
  # Used by random and bayesian search methods
  random_seed: 42

  # Optional: Variables to exclude from cache key computation
  # Useful for path-based derived variables that shouldn't affect caching
  # Example: ["output_file", "test_dir"] - paths vary but don't affect results
  cache_exclude_vars: ["output_file", "test_dir"]

  # Optional: Variables to test exhaustively at each search point
  # For Bayesian/random search: these variables are fully expanded for each selected point
  # Use case: Analyze the impact of specific variables (e.g., ost_num) across all values
  #           while using intelligent search for other variables
  # Example: With search_method="bayesian" and exhaustive_vars=["ost_num"]:
  #   - Bayesian optimizer selects (nodes=4, block_size=64)
  #   - IOPS tests all ost_num values: (4, 64, ost_num=1), (4, 64, ost_num=2), etc.
  # For exhaustive search: this has no effect (all combinations already tested)
  # exhaustive_vars: ["ost_num"]

  # Optional: Budget configuration (SLURM only)
  # Maximum CPU core-hours budget - execution stops when budget is exhausted
  # Can be overridden with --max-core-hours CLI argument
  # max_core_hours: 1000

  # Optional: Jinja expression to compute cores used per execution (SLURM only)
  # Used with max_core_hours for budget tracking
  # Examples:
  #   - "{{ ntasks }}" - total tasks across all nodes
  #   - "{{ nodes * processes_per_node }}" - explicit calculation
  # cores_expr: "{{ ntasks }}"

  # Optional: Estimated execution time per test in seconds (SLURM only)
  # Used for dry-run budget analysis and scheduling estimates
  # Can be overridden with --estimated-time CLI argument (supports scenarios: "120,240,360")
  # estimated_time_seconds: 300

  # Optional: Variables to include in analysis reports
  # When using --analyze, only these variables will be used in plots
  # Useful when you have string variables that shouldn't be plotted
  # If omitted, all numeric swept variables are included
  report_vars: ["nodes", "processes_per_node", "files_per_task", "tree_depth", "branch_factor", "file_size_bytes"]

 


# =============================================================================
# VARIABLES CONFIGURATION
# =============================================================================
# Define parameters that will be swept or computed during execution.
# Each variable has:
#   - type: int | float | str
#   - sweep: defines how values vary (for swept variables)
#   - expr: computation from other variables (for derived variables)
#
# Variable types:
#   - Swept: Parameter values to test (generates execution matrix)
#   - Derived: Computed from other variables using expressions
#
# All variables are available in Jinja2 templates as {{ variable_name }}
# =============================================================================
vars:
  # ---------------------------------------------------------------------------
  # SWEPT VARIABLES (define parameter space)
  # ---------------------------------------------------------------------------

  # Number of compute nodes
  nodes:
    type: int
    sweep:
      mode: list  # Options: list | range
      values: [1, 2, 4]

  # MPI processes per node
  processes_per_node:
    type: int
    sweep:
      mode: list
      values: [4, 8, 16]

  # Number of files to create per MPI task
  # mdtest creates files_per_task * ntasks total files
  files_per_task:
    type: int
    sweep:
      mode: list
      values: [100, 1000, 10000]

  # Directory tree depth (0 = flat structure)
  tree_depth:
    type: int
    sweep:
      mode: list
      values: [0, 2]

  # Branching factor for directory tree (directories per level)
  # Only relevant when tree_depth > 0
  branch_factor:
    type: int
    sweep:
      mode: list
      values: [1, 10]

  # File size in bytes (mdtest focuses on metadata, so typically small)
  file_size_bytes:
    type: int
    sweep:
      mode: list
      values: [0, 4096]  # 0 = empty files, 4096 = 4KB

  # ---------------------------------------------------------------------------
  # DERIVED VARIABLES (computed from other variables)
  # ---------------------------------------------------------------------------

  # Total MPI tasks across all nodes
  ntasks:
    type: int
    expr: "{{ nodes * processes_per_node }}"

  # Total number of files created in the test
  total_files:
    type: int
    expr: "{{ files_per_task * ntasks }}"

  # Test directory path
  test_dir:
    type: str
    expr: "/path/to/testfs/mdtest_{{ execution_id }}"

  # Output file for mdtest results
  output_file:
    type: str
    expr: "{{ execution_dir }}/mdtest_output_{{ execution_id }}_{{ repetition }}.txt"


# Optional: Constraints to validate parameter combinations before execution
# Filters invalid configurations using Python expressions
# violation_policy: "skip" (filter out), "error" (fail), "warn" (log warning)
# constraints:
#   - name: "block_transfer_alignment"
#     rule: "block_size_mb % transfer_size == 0"
#     violation_policy: "skip"
#   - name: "max_processes"
#     rule: "ntasks <= 256"
#     violation_policy: "warn"


# =============================================================================
# COMMAND CONFIGURATION
# =============================================================================
# The benchmark command to execute. Uses Jinja2 templating.
# Available in templates:
#   - All variables from 'vars' section: {{ variable_name }}
#   - {{ execution_id }} - unique ID for each execution
#   - {{ repetition }} - current repetition number (1-based)
#   - {{ execution_dir }} - per-execution working directory
#   - {{ workdir }} - base working directory
#   - {{ round_name }}, {{ round_index }} - round information (if using rounds)
# =============================================================================
command:
  # The command template - can be multiline using YAML '>' or '|' syntax
  template: >
    mdtest -n {{ files_per_task }} -d {{ test_dir }}
    -z {{ tree_depth }} -b {{ branch_factor }}
    -w {{ file_size_bytes }} -F -C -T -r

  # Optional: User-defined labels to attach to results
  # These appear in output as labels.* columns
  # (metadata.* is reserved for IOPS internal fields)
  labels:
    benchmark_type: "metadata"
    operations: "create,stat,remove"


# =============================================================================
# SCRIPTS CONFIGURATION
# =============================================================================
# Define execution scripts (SLURM submission, parsing, etc.)
# Supports:
#   - External script files: script_template: path/to/script.sh
#   - Embedded scripts: script_template: | (inline YAML multiline string)
#
# NOTE: For better maintainability, use external script files referenced by path.
#       Embedded scripts are shown here for completeness but are harder to debug
#       and edit. See docs/examples/scripts/ for reference implementations.
# =============================================================================
scripts:
  # Script name (can define multiple scripts)
  - name: "mdtest"

    # submission method for this script (required for local and slurm executors)
    #submit: "bash"        # for local executor

    # ---------------------------------------------------------------------------
    # SCRIPT OPTIONS:
    # ---------------------------------------------------------------------------
    # OPTION 1: EXTERNAL SCRIPT FILE (use this when you have a separate script)
    #   Reference an external script file. This is the preferred approach.
    #   You can create a custom SLURM script for mdtest.
    #
    #   Customize these scripts for your cluster and benchmark needs.
    # ---------------------------------------------------------------------------
    # script_template: examples/scripts/mdtest_slurm.sh

    # ---------------------------------------------------------------------------
    # OPTION 2: EMBEDDED SCRIPT (active by default for this mdtest example)
    #   Embedded script directly in YAML for quick testing.
    # ---------------------------------------------------------------------------
    script_template: |
      #!/bin/bash

      #SBATCH --job-name=mdtest_{{ execution_id }}
      #SBATCH --ntasks={{ ntasks }}
      #SBATCH --nodes={{ nodes }}
      #SBATCH --ntasks-per-node={{ processes_per_node }}
      #SBATCH --time=02:00:00
      #SBATCH --chdir={{ execution_dir }}
      #SBATCH -o batch%j.out
      #SBATCH -e batch%j.err
      #SBATCH --exclusive

      # Uncomment and customize these SLURM directives as needed:
      # #SBATCH --partition=your_partition
      # #SBATCH --constraint=your_constraint
      # #SBATCH --account=your_account
      # #SBATCH --qos=your_qos

      # Load required modules (customize for your cluster)
      module purge
      module load mpi/openmpi/4.0.1
      # module load mdtest  # If mdtest is a module

      # Create test directory
      mkdir -p {{ test_dir }}

      # Execute the benchmark command
      # Note: SLURM sets MPI task count automatically via --ntasks
      # Redirect output to file since mdtest writes to stdout
      mpirun {{ command.template }} > {{ output_file }} 2>&1

      # Cleanup test directory after benchmark
      rm -rf {{ test_dir }}

    # ---------------------------------------------------------------------------
    # POST-EXECUTION SCRIPT (optional)
    # ---------------------------------------------------------------------------
    # Runs after the main script completes (success or failure)
    # Useful for cleanup, notifications, or logging
    # ---------------------------------------------------------------------------
    post:
      script: |
        #!/bin/bash
        echo "Job completed at $(date)"
        echo "Output file: {{ output_file }}"

    # ---------------------------------------------------------------------------
    # PARSER CONFIGURATION
    # ---------------------------------------------------------------------------
    # Defines how to extract metrics from output files
    # ---------------------------------------------------------------------------
    parser:
      # File to parse (typically a variable pointing to the output)
      file: "{{ output_file }}"

      # List of metrics to extract from mdtest output
      # Each metric name must be returned by the parser script
      metrics:
        - name: file_creation_rate    # Files created per second
        - name: file_stat_rate         # Files stat'd per second
        - name: file_removal_rate      # Files removed per second
        - name: tree_creation_rate     # Tree creation rate
        - name: tree_removal_rate      # Tree removal rate

      # ---------------------------------------------------------------------------
      # PARSER SCRIPT: EMBEDDED (for mdtest text output)
      # ---------------------------------------------------------------------------
      # Parses mdtest text output and extracts performance metrics
      # ---------------------------------------------------------------------------
      parser_script: |
        import re

        def parse(file_path: str):
            """
            Parse mdtest text output and extract performance metrics.

            mdtest outputs text with sections for different operations:
            - File creation
            - File stat
            - File removal
            - Tree creation/removal (if tree depth > 0)

            Args:
                file_path: Path to the mdtest output file

            Returns:
                dict: Metrics dictionary with operation rates
            """
            with open(file_path, "r") as f:
                content = f.read()

            metrics = {}

            # Parse file creation rate
            # Pattern: "File creation     :    XXX.XXX ops/sec"
            match = re.search(r"File creation\s*:\s*([\d.]+)\s*ops/sec", content, re.IGNORECASE)
            if match:
                metrics["file_creation_rate"] = float(match.group(1))

            # Parse file stat rate
            # Pattern: "File stat         :    XXX.XXX ops/sec"
            match = re.search(r"File stat\s*:\s*([\d.]+)\s*ops/sec", content, re.IGNORECASE)
            if match:
                metrics["file_stat_rate"] = float(match.group(1))

            # Parse file removal rate
            # Pattern: "File removal      :    XXX.XXX ops/sec"
            match = re.search(r"File removal\s*:\s*([\d.]+)\s*ops/sec", content, re.IGNORECASE)
            if match:
                metrics["file_removal_rate"] = float(match.group(1))

            # Parse tree creation rate (if applicable)
            # Pattern: "Tree creation     :    XXX.XXX ops/sec"
            match = re.search(r"Tree creation\s*:\s*([\d.]+)\s*ops/sec", content, re.IGNORECASE)
            if match:
                metrics["tree_creation_rate"] = float(match.group(1))
            else:
                metrics["tree_creation_rate"] = None

            # Parse tree removal rate (if applicable)
            # Pattern: "Tree removal      :    XXX.XXX ops/sec"
            match = re.search(r"Tree removal\s*:\s*([\d.]+)\s*ops/sec", content, re.IGNORECASE)
            if match:
                metrics["tree_removal_rate"] = float(match.group(1))
            else:
                metrics["tree_removal_rate"] = None

            return metrics


# =============================================================================
# OUTPUT CONFIGURATION
# =============================================================================
# Define how results should be stored
# Supports schema evolution: new columns are added automatically in append mode
# =============================================================================
output:
  sink:
    # Output format: csv | parquet | sqlite
    type: csv

    # Output path (supports Jinja2 templating)
    path: "{{ workdir }}/results.csv"

    # Optional: Exclude specific fields from output
    # Dot notation for nested fields: benchmark.name, vars.nodes, metrics.bwMiB
    # Wildcards supported: "vars.*", "labels.*", "metadata.*", "metrics.*"
    # Useful for removing verbose fields like descriptions
    exclude: ["benchmark.description"]

    # Optional: Table name (only for sqlite type)
    table: results


# =============================================================================
# REPORTING CONFIGURATION
# =============================================================================
# Automatic HTML report generation with interactive plots
# Reports include performance analysis, best configurations, and custom visualizations
# Manual: iops report /path/to/workdir/run_NNN
# Custom: iops report /path/to/workdir/run_NNN --report-config report.yaml
# Docs: https://lgouveia.gitlabpages.inria.fr/iops/user-guide/reporting/
# =============================================================================
reporting:
  # Enable automatic report generation after execution
  enabled: true

  # Output configuration
  output_filename: "mdtest_performance_report.html"
  # output_dir defaults to workdir if not specified

  # -------------------- Theme Configuration --------------------
  theme:
    style: "plotly_white"              # Clean white background
    # Other options: "plotly", "plotly_dark", "ggplot2", "seaborn", "simple_white"

    colors:
      - "#1f77b4"  # Blue
      - "#ff7f0e"  # Orange
      - "#2ca02c"  # Green
      - "#d62728"  # Red
      - "#9467bd"  # Purple

    font_family: "Segoe UI, Tahoma, Geneva, Verdana, sans-serif"

  # -------------------- Section Control --------------------
  sections:
    test_summary: true           # Execution statistics and parameter space
    best_results: true           # Top configurations per metric
    variable_impact: true        # Variable importance analysis
    parallel_coordinates: true   # Multi-dimensional visualization
    custom_plots: true           # User-defined plots

  # -------------------- Best Results Configuration --------------------
  best_results:
    top_n: 5                     # Show top 5 configurations per metric
    show_command: true           # Include the rendered command

  # -------------------- Per-Metric Custom Plots --------------------
  metrics:
    # File creation performance
    file_creation_rate:
      plots:
        - type: "line"
          x_var: "files_per_task"
          group_by: "nodes"
          title: "File Creation Rate by Files per Task"
          xaxis_label: "Files per Task"
          yaxis_label: "Creation Rate (ops/sec)"
          show_error_bars: true

        - type: "heatmap"
          x_var: "nodes"
          y_var: "processes_per_node"
          title: "File Creation Rate Heatmap"
          colorscale: "Viridis"

    # File stat performance
    file_stat_rate:
      plots:
        - type: "bar"
          x_var: "tree_depth"
          group_by: "branch_factor"
          title: "Stat Rate by Tree Depth"
          xaxis_label: "Tree Depth"
          yaxis_label: "Stat Rate (ops/sec)"

    # File removal performance
    file_removal_rate:
      plots:
        - type: "line"
          x_var: "files_per_task"
          group_by: "file_size_bytes"
          title: "File Removal Rate"
          xaxis_label: "Files per Task"
          yaxis_label: "Removal Rate (ops/sec)"

  # -------------------- Default Plots --------------------
  # For metrics without custom configuration
  default_plots:
    - type: "bar"
      per_variable: true         # One plot per swept variable
      show_error_bars: true


# =============================================================================
# ADVANCED: ROUNDS CONFIGURATION (optional)
# =============================================================================
# Multi-stage optimization workflow: optimize different parameters sequentially
# Each round tests a subset of variables, and the best result propagates forward
#
# Uncomment to enable rounds:
# =============================================================================
# rounds:
#   # Round 1: Optimize node count
#   - name: "optimize_nodes"
#
#     # Variables to sweep in this round (others use global defaults)
#     sweep_vars: ["nodes"]
#
#     # Optional: Override specific variables with fixed values for this round
#     fixed_overrides:
#       processes_per_node: 8
#       files_per_task: 1000
#       tree_depth: 0
#       branch_factor: 1
#       file_size_bytes: 0
#
#     # Search configuration for this round
#     search:
#       metric: "file_creation_rate"   # Metric to optimize
#       objective: "max"                # "max" or "min"
#
#     # Optional: Override repetitions for this round
#     repetitions: 3
#
#   # Round 2: Optimize processes per node using best node count from Round 1
#   - name: "optimize_processes"
#     sweep_vars: ["processes_per_node"]
#     search:
#       metric: "file_creation_rate"
#       objective: "max"
#     repetitions: 5


# =============================================================================
# ADDITIONAL NOTES
# =============================================================================
#
# Jinja2 Template Variables (available in all templates):
#   - {{ execution_id }}          - Unique execution identifier
#   - {{ repetition }}            - Current repetition number (1-based)
#   - {{ execution_dir }}         - Per-execution working directory
#   - {{ workdir }}               - Base working directory
#   - {{ round_name }}            - Round name (if using rounds)
#   - {{ round_index }}           - Round index (if using rounds)
#   - {{ vars.variable_name }}    - Any variable from 'vars' section
#   - {{ labels.key }}            - Any label from 'command.labels'
#   - {{ command.template }}      - The rendered command
#   - {{ benchmark.name }}        - Benchmark name
#
# Special Features:
#   - Cache: Use --use_cache to reuse results from previous runs
#   - Dry-run: Use --dry-run to preview execution without running tests
#   - Budget: Set max_core_hours and cores_expr for resource limits
#   - Reports: Use --analyze /path/to/run_NNN to generate HTML analysis
#   - Validation: Use --check_setup to validate configuration without execution
#
# Working Directory Structure:
#   <workdir>/
#   ├── run_001/
#   │   ├── runs/
#   │   │   ├── exec_0001/
#   │   │   │   └── repetition_001/
#   │   │   │       ├── run_mdtest.sh    # Generated script
#   │   │   │       ├── stdout           # Job stdout
#   │   │   │       ├── stderr           # Job stderr
#   │   │   │       └── batch12345.out   # SLURM output
#   │   │   └── ...
#   │   ├── logs/
#   │   │   └── iops.log                 # IOPS framework log
#   │   └── run_metadata.json            # Run metadata for reports
#   └── results.csv                      # Results output
#
# =============================================================================
