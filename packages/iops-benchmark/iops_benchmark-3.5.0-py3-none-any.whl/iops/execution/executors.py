"""
Executors for IOPS benchmark framework.

This module contains all executor implementations:
- BaseExecutor: Abstract base class with registry pattern
- LocalExecutor: Run benchmarks locally via subprocess
- SlurmExecutor: Submit and monitor jobs on SLURM clusters
"""

from __future__ import annotations

import json
import shlex
import subprocess
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import TYPE_CHECKING, Optional, Dict, Any

from iops.logger import HasLogger
from iops.config.models import GenericBenchmarkConfig
from iops.execution.matrix import ExecutionInstance
from iops.execution.parser import parse_metrics_from_execution, ParserError

if TYPE_CHECKING:
    pass

# Name of the system info file generated by the system probe
SYSINFO_FILENAME = "__iops_sysinfo.json"


# ============================================================================ #
# Base Executor
# ============================================================================ #

class BaseExecutor(ABC, HasLogger):
    """
    Abstract base class for all execution environments (e.g., SLURM, local).

    Contract:
    - submit(test): submits or executes the job and sets job-related information
      (e.g., job ID) into the `test` instance (typically `test.metadata`).
    - wait_and_collect(test): waits for completion and populates `test` with
      status / timing / executor info, then performs cleanup of temp files.
    """
    STATUS_SUCCEEDED = "SUCCEEDED"  # It was submitted and finished successfully
    STATUS_FAILED = "FAILED"  # It was submitted but failed
    STATUS_RUNNING = "RUNNING"  # It is currently running
    STATUS_PENDING = "PENDING"  # It is queued but not running yet
    STATUS_ERROR = "ERROR"  # There was an error before the submission
    STATUS_UNKNOWN = "UNKNOWN"  # Status is unknown

    # Log prefix for executor-specific messages (override in subclasses)
    _LOG_PREFIX = "Exec"

    # Initial status when job is submitted (override in subclasses)
    # - Local: RUNNING (runs immediately)
    # - SLURM: PENDING (goes to queue first)
    INITIAL_STATUS = "RUNNING"

    _registry: dict[str, type["BaseExecutor"]] = {}

    @classmethod
    def register(cls, name: str):
        def decorator(subclass: type["BaseExecutor"]):
            cls._registry[name.lower()] = subclass
            return subclass

        return decorator

    @classmethod
    def build(cls, cfg: GenericBenchmarkConfig, kickoff_path: Path = None) -> "BaseExecutor":
        executor_name = cfg.benchmark.executor.lower()

        # Check for single-allocation mode (SLURM only)
        if executor_name == "slurm":
            eo = cfg.benchmark.slurm_options
            if eo and eo.allocation:
                if eo.allocation.mode == "single":
                    # Single-allocation mode requires pre-generated kickoff path
                    if kickoff_path is None:
                        raise ValueError(
                            "KickoffSingleAllocationExecutor requires kickoff_path. "
                            "Use runner's single-allocation mode initialization."
                        )
                    return KickoffSingleAllocationExecutor(cfg, kickoff_path)

        # Default registry lookup
        executor_cls = cls._registry.get(executor_name)
        if executor_cls is None:
            raise ValueError(
                f"Executor '{executor_name}' is not registered."
            )
        return executor_cls(cfg)

    def __init__(self, cfg: GenericBenchmarkConfig):
        """
        Initialize executor with configuration.
        """
        super().__init__()
        self.cfg = cfg
        self.last_status: str | None = None
        self.runner = None  # Will be set by runner for job tracking

    def set_runner(self, runner):
        """Set reference to the runner for job tracking (used by SLURM for Ctrl+C cleanup)."""
        self.runner = runner

    # ------------------------------------------------------------------ #
    # Abstract API
    # ------------------------------------------------------------------ #
    @abstractmethod
    def submit(self, test: ExecutionInstance):
        """
        Submit / launch the job associated with `test`.

        Implementations MUST:
        - Use test.script_file
        - Set a job identifier in the test, e.g.:
            test.metadata["__jobid"] = <job id>
        """
        raise NotImplementedError

    # ------------------------------------------------------------------ #
    # Shared helpers
    # ------------------------------------------------------------------ #
    def _init_execution_metadata(self, test: ExecutionInstance) -> None:
        """
        Ensure the metadata dict has standard keys present.

        These keys are just a convention; you can extend them freely.
        """
        meta = test.metadata
        meta.setdefault("__jobid", None)
        meta.setdefault("__executor_status", None)
        meta.setdefault("__submission_time", None)
        meta.setdefault("__job_start", None)
        meta.setdefault("__end", None)
        meta.setdefault("__error", None)

    def _write_status_update(self, test: ExecutionInstance, status: str) -> None:
        """
        Write a status update to the status file.

        This enables real-time status tracking for watch mode. Called when
        the executor status changes (e.g., PENDING -> RUNNING).

        Args:
            test: ExecutionInstance with execution_dir set
            status: New status to write (e.g., "RUNNING", "PENDING")
        """
        if test.execution_dir is None:
            return

        status_file = test.execution_dir / "__iops_status.json"
        status_data = {
            "status": status,
            "error": test.metadata.get("__error"),
            "end_time": test.metadata.get("__end"),
            "cached": test.metadata.get("__cached", False),
            # Include timing for real-time wait time tracking in watch mode
            "submission_time": test.metadata.get("__submission_time"),
            "job_start": test.metadata.get("__job_start"),
        }

        try:
            with open(status_file, "w") as f:
                json.dump(status_data, f, indent=2, default=str)
            self.logger.debug(f"  [{self._LOG_PREFIX}] Status update: {status}")
        except Exception as e:
            self.logger.debug(f"  [{self._LOG_PREFIX}] Failed to write status update: {e}")

    # ------------------------------------------------------------------ #
    # Filesystem helpers with retry for HPC shared filesystems
    # ------------------------------------------------------------------ #

    # Default retry settings for filesystem operations on HPC systems
    _FS_RETRY_COUNT = 3
    _FS_RETRY_DELAY = 1.0  # seconds between retries

    def _safe_fs_check(self, path, check_fn, check_name: str = "check") -> bool:
        """
        Safely perform a filesystem check with retry logic.

        On HPC systems with network filesystems (NFS/Lustre), stat() calls
        can fail with OSError/PermissionError due to:
        - Stale file handles
        - Metadata sync delays (file just created on another node)
        - Transient permission issues during propagation

        This method retries the check to handle transient sync issues.

        Args:
            path: Path object or None to check
            check_fn: Function to call on path (e.g., lambda p: p.is_file())
            check_name: Name of check for logging purposes

        Returns:
            True if check passes, False otherwise (including errors after retries)
        """
        if path is None:
            return False

        last_error = None
        for attempt in range(self._FS_RETRY_COUNT):
            try:
                return check_fn(path)
            except OSError as e:
                last_error = e
                if attempt < self._FS_RETRY_COUNT - 1:
                    self.logger.warning(
                        f"Filesystem {check_name} error on {path} "
                        f"(attempt {attempt + 1}/{self._FS_RETRY_COUNT}): {e}. "
                        f"Retrying in {self._FS_RETRY_DELAY}s... "
                        f"(possible NFS/Lustre metadata sync delay)"
                    )
                    time.sleep(self._FS_RETRY_DELAY)

        # All retries exhausted
        self.logger.warning(
            f"Filesystem {check_name} failed after {self._FS_RETRY_COUNT} "
            f"attempts on {path}: {last_error}"
        )
        return False

    def _safe_is_file(self, path) -> bool:
        """
        Safely check if a path is a file, with retry for transient errors.

        Args:
            path: Path object or None to check

        Returns:
            True if path exists and is a file, False otherwise
        """
        return self._safe_fs_check(path, lambda p: p.is_file(), "is_file")

    def _safe_exists(self, path) -> bool:
        """
        Safely check if a path exists, with retry for transient errors.

        Args:
            path: Path object or None to check

        Returns:
            True if path exists, False otherwise
        """
        return self._safe_fs_check(path, lambda p: p.exists(), "exists")

    @abstractmethod
    def wait_and_collect(self, test: ExecutionInstance):
        """
        wait the execution to complete, collect the metrics
        """
        pass

    # ------------------------------------------------------------------ #
    # System Information Collection
    # ------------------------------------------------------------------ #

    def _collect_system_info(self, test: ExecutionInstance) -> Optional[Dict[str, Any]]:
        """
        Read system information collected by the system probe.

        The system probe (injected into scripts when probes.system_snapshot=True)
        writes a JSON file with information about the compute node:
        - hostname: Node hostname
        - cpu_model: CPU model string
        - cpu_cores: Number of CPU cores
        - memory_kb: Total memory in KB
        - kernel: Linux kernel version
        - os: Operating system name
        - ib_devices: InfiniBand devices (comma-separated)
        - filesystems: Parallel filesystems with mount points

        Args:
            test: ExecutionInstance with execution_dir set

        Returns:
            Dictionary with system info, or None if file not found/invalid
        """
        if test.execution_dir is None:
            return None

        sysinfo_path = Path(test.execution_dir) / SYSINFO_FILENAME

        if not self._safe_is_file(sysinfo_path):
            self.logger.debug(f"  [SysInfo] File not found: {sysinfo_path}")
            return None

        try:
            with open(sysinfo_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                # Remove trailing period if present (from filesystems field workaround)
                content = content.rstrip('.')
                if content.endswith('".'):
                    content = content[:-2] + '"'
                # Fix common JSON issues from shell escaping
                sysinfo = json.loads(content)

            # Validate expected fields
            expected_fields = ['hostname', 'cpu_model', 'cpu_cores', 'memory_kb', 'kernel']
            if not all(field in sysinfo for field in expected_fields[:2]):  # At least hostname and cpu_model
                self.logger.warning(f"  [SysInfo] Missing expected fields in {sysinfo_path}")
                return None

            self.logger.debug(f"  [SysInfo] Collected from {sysinfo.get('hostname', 'unknown')}")
            return sysinfo

        except json.JSONDecodeError as e:
            self.logger.warning(f"  [SysInfo] Failed to parse {sysinfo_path}: {e}")
            return None
        except Exception as e:
            self.logger.warning(f"  [SysInfo] Error reading {sysinfo_path}: {e}")
            return None

    def _store_system_info(self, test: ExecutionInstance) -> None:
        """
        Read system info and store it in test metadata.

        Call this at the end of wait_and_collect() to capture system info
        from the compute node after the job completes.

        Args:
            test: ExecutionInstance to update with system info
        """
        sysinfo = self._collect_system_info(test)
        if sysinfo:
            test.metadata["__sysinfo"] = sysinfo

    # ------------------------------------------------------------------ #
    # Post-processing and output helpers
    # ------------------------------------------------------------------ #

    def _truncate_output(self, text: str, max_lines: int = 10) -> str:
        """Truncate output to first and last N/2 lines."""
        if not text:
            return ""

        lines = text.splitlines()
        if len(lines) <= max_lines:
            return text

        half = max_lines // 2
        first_lines = lines[:half]
        last_lines = lines[-half:]

        return (
            "\n".join(first_lines)
            + f"\n... ({len(lines) - max_lines} lines omitted) ...\n"
            + "\n".join(last_lines)
        )

    def _run_post_script(self, test: ExecutionInstance) -> bool:
        """
        Execute post-processing script after main script succeeds.

        Returns:
            True if post script succeeded or doesn't exist, False if it failed
        """
        if not self._safe_is_file(test.post_script_file):
            return True  # No post script, that's okay

        if not test.execution_dir:
            self.logger.error(f"  [{self._LOG_PREFIX}] execution_dir not set, cannot run post script")
            return False

        stdout_path = test.execution_dir / "post_stdout"
        stderr_path = test.execution_dir / "post_stderr"

        test.metadata["__post_stdout_path"] = str(stdout_path)
        test.metadata["__post_stderr_path"] = str(stderr_path)

        cmd = ["bash", str(test.post_script_file)]

        try:
            self.logger.info(f"  [{self._LOG_PREFIX}] Running post-script: {test.post_script_file.name}")

            result = subprocess.run(
                cmd,
                cwd=test.execution_dir,
                capture_output=True,
                text=True,
            )

            # Save outputs
            stdout_path.write_text(result.stdout or "", encoding="utf-8", errors="replace")
            stderr_path.write_text(result.stderr or "", encoding="utf-8", errors="replace")

            test.metadata["__post_returncode"] = result.returncode

            # Log completion
            stdout_lines = len(result.stdout.splitlines()) if result.stdout else 0
            stderr_lines = len(result.stderr.splitlines()) if result.stderr else 0
            self.logger.debug(
                f"  [{self._LOG_PREFIX}] Post-script completed: returncode={result.returncode} "
                f"stdout={stdout_lines} lines, stderr={stderr_lines} lines"
            )

            if result.returncode != 0:
                msg = (
                    f"Post-processing script failed with code {result.returncode}.\n"
                    f"stdout: {stdout_path} ;\n"
                    f"stderr: {stderr_path}"
                )
                self.logger.error(f"  [{self._LOG_PREFIX}] Post-script FAILED: {msg}")

                # Show stderr preview
                if result.stderr:
                    stderr_preview = self._truncate_output(result.stderr, max_lines=10)
                    self.logger.debug(f"  [{self._LOG_PREFIX}] Post-script stderr preview:\n{stderr_preview}")

                test.metadata["__error"] = msg
                return False

            return True

        except Exception as e:
            msg = f"Error running post-processing script: {e}"
            self.logger.error(f"  [{self._LOG_PREFIX}] Post-script ERROR: {msg}")
            test.metadata["__error"] = msg
            return False


# ============================================================================ #
# Local Executor
# ============================================================================ #

@BaseExecutor.register("local")
class LocalExecutor(BaseExecutor):
    """
    Executor for running benchmark jobs locally.

    - Always captures stdout/stderr to files named after the script:
        <script_name>.stdout
        <script_name>.stderr
    - Marks FAILED only if returncode != 0.
    """

    _LOG_PREFIX = "LocalExec"

    def submit(self, test: ExecutionInstance):
        self._init_execution_metadata(test)
        self.logger.debug(
            f"  [LocalExec] Submit: exec_id={test.execution_id} "
            f"script={test.script_file.name if test.script_file else 'N/A'}"
        )

        if test.script_file is None or not isinstance(test.script_file, Path) or not self._safe_is_file(test.script_file):
            msg = "test.script_file is not set or invalid."
            self.logger.error(f"  [LocalExec] ERROR: {msg}")
            test.metadata["__executor_status"] = self.STATUS_ERROR
            test.metadata["__error"] = msg
            return

        # check test.execution_dir
        if test.execution_dir is None or not isinstance(test.execution_dir, Path):
            msg = "test.execution_dir is not set or invalid."
            self.logger.error(f"  [LocalExec] ERROR: {msg}")
            test.metadata["__executor_status"] = self.STATUS_ERROR
            test.metadata["__error"] = msg
            return

        script_path: Path = test.script_file

        stdout_path = test.execution_dir / f"stdout"
        stderr_path = test.execution_dir / f"stderr"

        test.metadata["__stdout_path"] = str(stdout_path)
        test.metadata["__stderr_path"] = str(stderr_path)

        # Prefer not using shell=True to avoid masking return codes
        # Equivalent of: bash /path/to/script.sh
        cmd = ["bash", str(script_path)]

        self.logger.debug(f"  [LocalExec] Executing: bash {script_path.name}")

        try:
            # For local executor, submission and job start are the same (no queue)
            now = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            test.metadata["__submission_time"] = now
            test.metadata["__job_start"] = now
            result = subprocess.run(
                cmd,
                cwd=test.execution_dir,
                capture_output=True,
                text=True,
            )
            test.metadata["__end"] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

            # Always persist outputs
            stdout_path.write_text(result.stdout or "", encoding="utf-8", errors="replace")
            stderr_path.write_text(result.stderr or "", encoding="utf-8", errors="replace")

            test.metadata["__jobid"] = "local"
            test.metadata["__returncode"] = result.returncode

            # Log completion
            stdout_lines = len(result.stdout.splitlines()) if result.stdout else 0
            stderr_lines = len(result.stderr.splitlines()) if result.stderr else 0
            self.logger.debug(
                f"  [LocalExec] Completed: returncode={result.returncode} "
                f"stdout={stdout_lines} lines, stderr={stderr_lines} lines"
            )

            if result.returncode != 0:
                msg = (
                    f"Local script failed with code {result.returncode}. \n"
                    f"stdout: {stdout_path} ; \n"
                    f"stderr: {stderr_path}"
                )
                self.logger.error(f"  [LocalExec] FAILED: {msg}")

                # Show first/last lines of stderr for debugging
                if result.stderr:
                    stderr_preview = self._truncate_output(result.stderr, max_lines=10)
                    self.logger.debug(f"  [LocalExec] stderr preview:\n{stderr_preview}")

                test.metadata["__executor_status"] = self.STATUS_FAILED
                test.metadata["__error"] = msg
                return

            test.metadata["__executor_status"] = self.STATUS_SUCCEEDED

            # Execute post-processing script if present
            if self._safe_is_file(test.post_script_file):
                post_success = self._run_post_script(test)
                if not post_success:
                    # Post script failed, mark entire test as failed
                    test.metadata["__executor_status"] = self.STATUS_FAILED

        except Exception as e:
            msg = f"Error running script {test.script_file}: {e}"
            self.logger.error(msg)
            test.metadata["__executor_status"] = self.STATUS_FAILED
            test.metadata["__error"] = msg

    def wait_and_collect(self, test: ExecutionInstance) -> None:
        # Always create a full metrics dict first (all keys, None values)
        
        metrics = {m.name: None for m in (test.parser.metrics if test.parser else [])}
        test.metadata["metrics"] = metrics  # <-- guarantee presence early

        # Only parse if succeeded
        if test.metadata.get("__executor_status") == self.STATUS_SUCCEEDED:
            if test.parser:
                self.logger.debug(f"  [LocalExec] Parsing metrics from output files")
                try:
                    results = parse_metrics_from_execution(test) or {}
                    parsed = results.get("metrics", {}) if isinstance(results, dict) else {}

                    for name, value in parsed.items():
                        if name in metrics:
                            metrics[name] = value
                except ParserError as e:
                    test.metadata["__executor_status"] = self.STATUS_FAILED
                    # Point to parser_stderr file if available, otherwise include error summary
                    stderr_path = test.metadata.get("__parser_stderr_path")
                    if stderr_path:
                        self.logger.warning(f"  [LocalExec] Parser failed. See: {stderr_path}")
                        test.metadata["__error"] = f"Parser error. See: {stderr_path}"
                    else:
                        self.logger.warning(f"  [LocalExec] Parser failed: {e}")
                        test.metadata["__error"] = f"Parser error: {e}"

        metric_count = len([v for v in metrics.values() if v is not None])
        self.logger.debug(
            f"  [LocalExec] Collected {metric_count}/{len(metrics)} metrics: "
            f"{list(metrics.keys())[:3]}{'...' if len(metrics) > 3 else ''}"
        )

        # Collect system info from compute node (if probe was enabled)
        self._store_system_info(test)


# ============================================================================ #
# SLURM Executor
# ============================================================================ #

@BaseExecutor.register("slurm")
class SlurmExecutor(BaseExecutor):
    """
    YAML-driven SLURM executor.

    Strategy:
      1) Submit using slurm_options.commands.submit (default: sbatch)
      2) Poll status via squeue while present
      3) When job leaves squeue:
           - try scontrol show job <jobid> to get JobState/ExitCode
           - if scontrol has no record (aged out), finalize by parser outcome:
                * if parser output exists and parsing succeeds -> SUCCEEDED
                * else -> FAILED

    Uses:
      - slurm_options.commands.submit (or default "sbatch")
      - test.script_file (rendered script already written)
      - test.execution_dir (work dir for the execution)

    Constraints honored:
      - does NOT use sacct
      - does NOT use sbatch --wait
      - does NOT require users to add sentinel files

    Finalization logic when job leaves squeue:
      - Prefer scontrol show job <jobid> (JobState/ExitCode)
      - If scontrol has no record, fall back to parser success.

    Configurable Commands:
      - Commands can be customized via slurm_options.commands in YAML
      - Useful for systems with command wrappers or custom SLURM installations
    """

    SLURM_ACTIVE_STATES = {
        "PENDING", "CONFIGURING", "RUNNING", "COMPLETING",
        "SUSPENDED", "REQUEUED", "RESIZING", "SIGNALING", "STAGE_OUT",
    }

    SLURM_FAIL_STATES = {
        "FAILED", "CANCELLED", "TIMEOUT", "NODE_FAIL", "OUT_OF_MEMORY",
        "PREEMPTED", "BOOT_FAIL",
    }

    _LOG_PREFIX = "SlurmExec"

    # SLURM jobs go to queue first, so initial status is PENDING
    INITIAL_STATUS = "PENDING"

    def __init__(self, cfg):
        """Initialize SLURM executor with configurable command templates and polling interval."""
        super().__init__(cfg)

        # Extract command overrides from slurm_options
        slurm_options = cfg.benchmark.slurm_options
        custom_commands = {}
        if slurm_options and slurm_options.commands:
            custom_commands = slurm_options.commands

        # Set command templates with defaults
        # Templates support {job_id} placeholder for runtime substitution
        self.cmd_submit = custom_commands.get("submit", "sbatch")
        self.cmd_status = custom_commands.get("status", "squeue -j {job_id} --noheader --format=%T")
        self.cmd_info = custom_commands.get("info", "scontrol show job {job_id}")
        self.cmd_srun = custom_commands.get("srun", "srun")
        self.cmd_cancel = custom_commands.get("cancel", "scancel {job_id}")

        # Set polling interval with fallback chain:
        # 1. slurm_options.poll_interval
        # 2. execution.status_check_delay
        # 3. default: 30 seconds
        if slurm_options and slurm_options.poll_interval is not None:
            self.poll_interval = slurm_options.poll_interval
        else:
            self.poll_interval = getattr(getattr(cfg, "execution", None), "status_check_delay", 30) 

    def submit(self, test) -> None:
        self._init_execution_metadata(test)

        # Validate script_file
        if test.script_file is None or not isinstance(test.script_file, Path) or not self._safe_is_file(test.script_file):
            msg = "test.script_file is not set or invalid."
            self.logger.error(f"  [SlurmExec] ERROR: {msg}")
            test.metadata["__executor_status"] = self.STATUS_ERROR
            test.metadata["__error"] = msg
            return

        # Validate execution_dir
        if test.execution_dir is None or not isinstance(test.execution_dir, Path):
            msg = "test.execution_dir is not set or invalid."
            self.logger.error(f"  [SlurmExec] ERROR: {msg}")
            test.metadata["__executor_status"] = self.STATUS_ERROR
            test.metadata["__error"] = msg
            return

        # Use submit command from slurm_options.commands.submit (or default "sbatch")
        cmd = shlex.split(self.cmd_submit)

        # Ensure the script path is included (unless user already put it in submit)
        script_str = str(test.script_file)
        if script_str not in cmd:
            cmd.append(script_str)

        # Log detailed execution information at debug level
        self.logger.debug(f"  [SlurmExec] ═══════════════════════════════════════════════════")
        self.logger.debug(f"  [SlurmExec] Execution ID: {test.execution_id}")
        self.logger.debug(f"  [SlurmExec] Repetition: {getattr(test, 'repetition', '?')}/{getattr(test, 'repetitions', '?')}")
        self.logger.debug(f"  [SlurmExec] Working directory: {test.execution_dir}")
        self.logger.debug(f"  [SlurmExec] Script file: {test.script_file}")
        self.logger.debug(f"  [SlurmExec] Submit command: {' '.join(cmd)}")
        self.logger.debug(f"  [SlurmExec] ═══════════════════════════════════════════════════")

        try:
            test.metadata["__submission_time"] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

            # NOTE: check=True would raise on non-zero exit,
            # but sbatch typically returns 0 on successful submission.
            r = subprocess.run(
                cmd,
                cwd=test.execution_dir,
                capture_output=True,
                text=True,
            )

            stdout = (r.stdout or "").strip()
            stderr = (r.stderr or "").strip()

            test.metadata["__slurm_submit_stdout"] = stdout
            test.metadata["__slurm_submit_stderr"] = stderr
            test.metadata["__submit_returncode"] = r.returncode

            if r.returncode != 0:
                msg = f"SLURM submission failed (rc={r.returncode}): stderr='{stderr}' stdout='{stdout}'"
                self.logger.error(msg)
                test.metadata["__executor_status"] = self.STATUS_ERROR
                test.metadata["__error"] = msg
                return

            job_id = self._parse_jobid(stdout)
            if not job_id:
                msg = f"Could not parse SLURM jobid from submission output: stdout='{stdout}' stderr='{stderr}'"
                self.logger.error(msg)
                test.metadata["__executor_status"] = self.STATUS_ERROR
                test.metadata["__error"] = msg
                return

            test.metadata["__jobid"] = job_id
            test.metadata["__executor_status"] = self.STATUS_PENDING
            self.logger.info("SLURM job submitted: %s", job_id)

            # Register job ID with runner for cleanup on interrupt (Ctrl+C)
            if self.runner and hasattr(self.runner, 'register_slurm_job'):
                self.runner.register_slurm_job(job_id)

        except Exception as e:
            msg = f"Unexpected SLURM submission error: {e}"
            self.logger.error(msg)
            test.metadata["__executor_status"] = self.STATUS_ERROR
            test.metadata["__error"] = msg
            return

    def wait_and_collect(self, test) -> None:
        """
        Poll squeue until the job disappears, then finalize with:
          1) scontrol show job (JobState/ExitCode)
          2) fallback to parser outcome (file exists + parse ok)
        Always initializes test.metadata["metrics"] first.
        """
        # Always create metrics dict early (handle parser=None safely)
        parser = test.parser
        metric_names = [m.name for m in (parser.metrics if parser else [])]
        metrics = {name: None for name in metric_names}
        test.metadata["metrics"] = metrics

        job_id = test.metadata.get("__jobid")
        if not job_id:
            msg = "wait_and_collect called but test.metadata['__jobid'] is not set."
            self.logger.error(msg)
            test.metadata["__executor_status"] = self.STATUS_ERROR
            test.metadata["__error"] = msg
            return

        self.logger.debug(f"  [SlurmExec] Waiting for job {job_id} (poll interval={self.poll_interval}s)")

        last_state = None
        while True:
            state = self._squeue_state(job_id)
            if state is None:
                break

            test.metadata["__slurm_state_live"] = state
            if state == "PENDING":
                new_status = self.STATUS_PENDING
            else:
                new_status = self.STATUS_RUNNING
                # Record when job transitions from PENDING to RUNNING (actual job start)
                if last_state == "PENDING" and "__job_start" not in test.metadata:
                    test.metadata["__job_start"] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
                    self.logger.debug(f"  [SlurmExec] Job {job_id} started running at {test.metadata['__job_start']}")

            # Update status in metadata
            old_status = test.metadata.get("__executor_status")
            test.metadata["__executor_status"] = new_status

            # Write status file when status changes (enables real-time watch)
            if new_status != old_status:
                self._write_status_update(test, new_status)

            if state != last_state:
                self.logger.info("SLURM job %s state: %s", job_id, state)
                last_state = state

            time.sleep(self.poll_interval)

        # Job left squeue - unregister from tracking (no longer needs cleanup)
        test.metadata["__end"] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

        # Unregister job from runner tracking (job completed, no need to cancel)
        if self.runner and hasattr(self.runner, 'submitted_job_ids') and job_id in self.runner.submitted_job_ids:
            self.runner.submitted_job_ids.discard(job_id)
            self.logger.debug(f"  [JobTracker] Unregistered completed job {job_id} (remaining tracked: {len(self.runner.submitted_job_ids)})")

        # 1) Prefer scontrol (best SLURM-native final status without accounting)
        info = self._scontrol_info(job_id)
        slurm_state = info.get("state")
        exitcode = info.get("exitcode")

        test.metadata["__slurm_state"] = slurm_state
        test.metadata["__slurm_exitcode"] = exitcode

        final = self._map_final_status(slurm_state, exitcode)

        # 2) If scontrol cannot provide final outcome (aged out), fall back to parser
        if final == self.STATUS_UNKNOWN:
            if parser is None:
                test.metadata.setdefault(
                    "__error",
                    "Job left squeue; scontrol has no record; no parser configured to validate completion."
                )
                test.metadata["__executor_status"] = self.STATUS_UNKNOWN
                return

            ok = self._try_parse_metrics(test, metrics)
            final = self.STATUS_SUCCEEDED if ok else self.STATUS_FAILED
            if not ok:
                test.metadata.setdefault(
                    "__error",
                    "Job left squeue; scontrol has no record; parsing failed or output missing."
                )

        test.metadata["__executor_status"] = final

        if final != self.STATUS_SUCCEEDED:
            return

        # Run post-processing script locally after SLURM job completes successfully
        if self._safe_is_file(test.post_script_file):
            post_success = self._run_post_script(test)
            if not post_success:
                # Post script failed, mark entire test as failed
                test.metadata["__executor_status"] = self.STATUS_FAILED
                return

        # On success, ensure parsing filled the metrics (if we haven't parsed yet)
        if parser is not None:
            # If we already parsed during fallback, this will be a no-op (still safe)
            self.logger.debug(f"  [SlurmExec] Parsing metrics from output files")
            self._try_parse_metrics(test, metrics)

        metric_count = len([v for v in metrics.values() if v is not None])
        self.logger.debug(
            f"  [SlurmExec] Collected {metric_count}/{len(metrics)} metrics: "
            f"{list(metrics.keys())[:3]}{'...' if len(metrics) > 3 else ''}"
        )

        # Collect system info from compute node (if probe was enabled)
        self._store_system_info(test)

    # ------------------------------------------------------------------ #
    # Helpers
    # ------------------------------------------------------------------ #

    def _parse_jobid(self, stdout: str) -> Optional[str]:
        """
        Supports:
          - sbatch --parsable  -> "12345" or "12345;something"
          - default sbatch     -> "Submitted batch job 12345"
        """
        if not stdout:
            return None

        token = stdout.splitlines()[-1].strip()

        # parsable form: "<jobid>[;...]"
        cand = token.split(";", 1)[0].strip()
        if cand.isdigit():
            return cand

        # classic form: "... 12345"
        parts = token.split()
        if parts and parts[-1].isdigit():
            return parts[-1]

        return None

    def _squeue_state(self, job_id: str) -> Optional[str]:
        """
        Returns job state string (e.g., PENDING/RUNNING/...) or None if not in queue.
        Uses cmd_status template with {job_id} placeholder.
        """
        try:
            # Format the command template with job_id
            cmd_str = self.cmd_status.format(job_id=job_id)
            cmd = shlex.split(cmd_str)

            r = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                check=True,
            )
            out = (r.stdout or "").strip()
            if not out:
                return None
            return out.splitlines()[0].strip()
        except subprocess.CalledProcessError as e:
            # treat failure as "not visible in queue" (best effort)
            self.logger.debug(
                f"  [SlurmExec] Status command failed for job {job_id}: {(e.stderr or str(e)).strip()}"
            )
            return None

    def _scontrol_info(self, job_id: str) -> Dict[str, Optional[str]]:
        """
        Best-effort final status without sacct.
        Uses cmd_info template with {job_id} placeholder.

        Returns:
          {"state": "...", "exitcode": "..."} when available,
          otherwise None values.
        """
        try:
            # Format the command template with job_id
            cmd_str = self.cmd_info.format(job_id=job_id)
            cmd = shlex.split(cmd_str)

            r = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                check=True,
            )
            txt = (r.stdout or "").strip()
            if not txt:
                return {"state": None, "exitcode": None}

            state = None
            exitcode = None

            # Key=Value tokens separated by spaces/newlines
            for tok in txt.replace("\n", " ").split():
                if tok.startswith("JobState="):
                    state = tok.split("=", 1)[1].strip()
                elif tok.startswith("ExitCode="):
                    exitcode = tok.split("=", 1)[1].strip()

            return {"state": state, "exitcode": exitcode}

        except subprocess.CalledProcessError as e:
            # common if job aged out: "Invalid job id specified"
            self.logger.debug(
                f"  [SlurmExec] Info command failed for job {job_id} (likely aged out): "
                f"{(e.stderr or str(e)).strip()}"
            )
            return {"state": None, "exitcode": None}

    def _map_final_status(self, state: Optional[str], exitcode: Optional[str]) -> str:
        """
        Map SLURM controller state + exit code into BaseExecutor status.
        """
        if state is None:
            return self.STATUS_UNKNOWN

        s = state.strip().upper()
        base = s.split()[0].split("+")[0]

        if base in self.SLURM_ACTIVE_STATES:
            if base == "PENDING":
                return self.STATUS_PENDING
            return self.STATUS_RUNNING

        if base == "COMPLETED":
            # ExitCode is usually "0:0" for success
            if exitcode is None or exitcode.strip() in {"", "0:0"}:
                return self.STATUS_SUCCEEDED
            return self.STATUS_FAILED

        if base in self.SLURM_FAIL_STATES:
            return self.STATUS_FAILED

        return self.STATUS_UNKNOWN

    def _try_parse_metrics(self, test, metrics: Dict[str, Any]) -> bool:
        """
        Parser-based success heuristic:
          - if parser.file exists AND parse_metrics_from_execution succeeds => True
          - else => False, and sets test.metadata["__error"].
        """
        parser = test.parser
        if parser is None:
            return False

        try:
            fpath = Path(parser.file)
        except Exception:
            fpath = None

        if not self._safe_is_file(fpath):
            test.metadata["__error"] = f"Parser file does not exist: {parser.file}"
            return False

        try:
            results = parse_metrics_from_execution(test) or {}
            parsed = results.get("metrics", {}) if isinstance(results, dict) else {}

            for name, value in parsed.items():
                if name in metrics:
                    metrics[name] = value
            return True

        except Exception as e:
            # Point to parser_stderr file if available, otherwise include error summary
            stderr_path = test.metadata.get("__parser_stderr_path")
            if stderr_path:
                self.logger.warning(f"  [SlurmExec] Parser failed. See: {stderr_path}")
                test.metadata["__error"] = f"Parser error. See: {stderr_path}"
            else:
                self.logger.warning(f"  [SlurmExec] Parser failed: {e}")
                test.metadata["__error"] = f"Parsing failed: {e}"
            return False


# ============================================================================ #
# Single-Allocation SLURM Executor
# ============================================================================ #

# Filename for the single-allocation execution script (written to run root)
KICKOFF_SCRIPT_FILENAME = "__iops_kickoff.sh"

# Status filename pattern (written by execution script to each test dir)
KICKOFF_STATUS_FILENAME = "__iops_status.json"


class KickoffSingleAllocationExecutor(SlurmExecutor):
    """
    SLURM executor for single-allocation mode.

    In single-allocation mode, a pre-generated script runs ALL tests sequentially
    within a single SLURM allocation. This eliminates per-test job submission
    overhead and queue wait times.

    Key features:
    - Tests are prepared upfront (folders, scripts, params) before allocation starts
    - An execution script is generated that sequences all tests
    - The execution script is submitted via sbatch once
    - Runner polls status files to track progress
    - User controls srun directly in script_template (no MPI wrapping)

    Execution flow:
    1. First submit(): Submit execution script via sbatch
    2. Subsequent submit(): Just register test, allocation already running
    3. wait_and_collect(): Poll status file until test completes

    The execution script writes status files as it progresses:
    - RUNNING when starting each test
    - SUCCEEDED/FAILED/TIMEOUT when test completes

    Inherits from SlurmExecutor:
    - SLURM_ACTIVE_STATES, SLURM_FAIL_STATES
    - _parse_jobid(), _squeue_state(), _scontrol_info()
    - Command setup (cmd_submit, cmd_status, cmd_cancel, poll_interval)
    """

    _LOG_PREFIX = "SingleAllocExec"

    def __init__(self, cfg: GenericBenchmarkConfig, kickoff_path: Path):
        """
        Initialize single-allocation executor.

        Args:
            cfg: Benchmark configuration
            kickoff_path: Path to the pre-generated execution script
        """
        # Initialize parent (sets up commands, poll_interval, etc.)
        super().__init__(cfg)

        # Extract allocation config
        self.allocation = cfg.benchmark.slurm_options.allocation
        self.kickoff_path = kickoff_path
        self.test_timeout = self.allocation.test_timeout

        # State tracking
        self.allocation_job_id: Optional[str] = None
        self.kickoff_submitted: bool = False
        self.kickoff_failed: bool = False
        self.kickoff_failure_reason: Optional[str] = None

        # Poll interval for status file checks (faster than SLURM polling)
        # Tests run sequentially, so we can poll quickly
        self.status_poll_interval = 0.5  # seconds

    def submit(self, test: ExecutionInstance) -> None:
        """
        Submit a test for execution within the single allocation.

        On first call, submits the execution script via sbatch.
        The execution script runs all tests sequentially; this method just
        records that we're waiting for this test.

        Args:
            test: ExecutionInstance to execute
        """
        self._init_execution_metadata(test)

        # Validate script_file
        if test.script_file is None or not isinstance(test.script_file, Path) or not self._safe_is_file(test.script_file):
            msg = "test.script_file is not set or invalid."
            self.logger.error(f"  [{self._LOG_PREFIX}] ERROR: {msg}")
            test.metadata["__executor_status"] = self.STATUS_ERROR
            test.metadata["__error"] = msg
            return

        # Validate execution_dir
        if test.execution_dir is None or not isinstance(test.execution_dir, Path):
            msg = "test.execution_dir is not set or invalid."
            self.logger.error(f"  [{self._LOG_PREFIX}] ERROR: {msg}")
            test.metadata["__executor_status"] = self.STATUS_ERROR
            test.metadata["__error"] = msg
            return

        # Check if allocation already failed
        if self.kickoff_failed:
            msg = f"Single allocation failed: {self.kickoff_failure_reason}"
            self.logger.error(f"  [{self._LOG_PREFIX}] ERROR: {msg}")
            test.metadata["__executor_status"] = self.STATUS_FAILED
            test.metadata["__error"] = msg
            return

        # Submit execution script on first test
        if not self.kickoff_submitted:
            try:
                self._submit_kickoff()
                self.kickoff_submitted = True
            except Exception as e:
                msg = f"Failed to submit single allocation: {e}"
                self.logger.error(f"  [{self._LOG_PREFIX}] ERROR: {msg}")
                self.kickoff_failed = True
                self.kickoff_failure_reason = str(e)
                test.metadata["__executor_status"] = self.STATUS_ERROR
                test.metadata["__error"] = msg
                return

        # Record that we're waiting for this test
        test.metadata["__jobid"] = self.allocation_job_id
        test.metadata["__submission_time"] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        test.metadata["__executor_status"] = self.STATUS_PENDING

        self.logger.debug(
            f"  [{self._LOG_PREFIX}] Registered test exec_id={test.execution_id} "
            f"rep={test.repetition} for single-allocation execution"
        )

    def wait_and_collect(self, test: ExecutionInstance) -> None:
        """
        Wait for test completion by polling its status file.

        The execution script writes status files as it progresses through tests.
        We poll until we see a terminal status (SUCCEEDED, FAILED, TIMEOUT, ERROR).

        Args:
            test: ExecutionInstance to collect results from
        """
        # Always create metrics dict early (handle parser=None safely)
        parser = test.parser
        metric_names = [m.name for m in (parser.metrics if parser else [])]
        metrics = {name: None for name in metric_names}
        test.metadata["metrics"] = metrics

        # If allocation failed before this test, bail out
        if self.kickoff_failed:
            test.metadata["__executor_status"] = self.STATUS_FAILED
            test.metadata["__error"] = f"Single allocation failed: {self.kickoff_failure_reason}"
            return

        # If test already marked as error (from submit), skip waiting
        if test.metadata.get("__executor_status") == self.STATUS_ERROR:
            return

        # Poll status file until completion
        status_file = test.execution_dir / KICKOFF_STATUS_FILENAME
        terminal_statuses = {"SUCCEEDED", "FAILED", "TIMEOUT", "ERROR"}

        self.logger.debug(
            f"  [{self._LOG_PREFIX}] Waiting for test exec_id={test.execution_id} "
            f"(polling {status_file})"
        )

        last_status = None
        wait_start = None  # Only start timing once allocation is RUNNING
        status_file_seen = False

        while True:
            # Check if allocation is still running (do this first to track state)
            alloc_state = None
            if self.allocation_job_id:
                alloc_state = self._squeue_state(self.allocation_job_id)

            # Start staleness timer once allocation is running
            # Don't count queue wait time (PENDING) against test_timeout
            if wait_start is None and alloc_state == "RUNNING":
                wait_start = time.time()
                self.logger.debug(
                    f"  [{self._LOG_PREFIX}] Allocation running, starting staleness timer"
                )

            # Staleness checks only apply once allocation is running
            if wait_start is not None:
                elapsed = time.time() - wait_start

                # Staleness check: if status file never appears, fail after test_timeout
                # This handles cases where the bash script stops before reaching this test
                if not status_file_seen and elapsed > self.test_timeout:
                    msg = (
                        f"Status file not found after {self.test_timeout}s. "
                        f"The execution script may have stopped before reaching this test. "
                        f"Expected: {status_file}"
                    )
                    self.logger.error(f"  [{self._LOG_PREFIX}] ERROR: {msg}")
                    test.metadata["__executor_status"] = self.STATUS_FAILED
                    test.metadata["__error"] = msg
                    break

                # Staleness check: if status file shows RUNNING for too long, fail
                # This handles cases where the bash script itself is hung
                if status_file_seen and elapsed > self.test_timeout:
                    current = test.metadata.get("__executor_status")
                    if current == self.STATUS_RUNNING:
                        msg = (
                            f"Test stuck in RUNNING state for {self.test_timeout}s. "
                            f"The execution script may be hung."
                        )
                        self.logger.error(f"  [{self._LOG_PREFIX}] ERROR: {msg}")
                        test.metadata["__executor_status"] = self.STATUS_FAILED
                        test.metadata["__error"] = msg
                        break

            # Handle allocation state changes
            if self.allocation_job_id:
                if alloc_state is None:
                    # Allocation finished - check if test completed
                    if self._check_status_file(test, status_file, terminal_statuses, metrics):
                        break
                    # Allocation ended without completing this test
                    msg = f"Allocation {self.allocation_job_id} ended before test completed"
                    self.logger.error(f"  [{self._LOG_PREFIX}] ERROR: {msg}")
                    test.metadata["__executor_status"] = self.STATUS_FAILED
                    test.metadata["__error"] = msg
                    self.kickoff_failed = True
                    self.kickoff_failure_reason = msg
                    break
                elif alloc_state in self.SLURM_FAIL_STATES:
                    msg = f"Allocation {self.allocation_job_id} failed with state: {alloc_state}"
                    self.logger.error(f"  [{self._LOG_PREFIX}] ERROR: {msg}")
                    test.metadata["__executor_status"] = self.STATUS_FAILED
                    test.metadata["__error"] = msg
                    self.kickoff_failed = True
                    self.kickoff_failure_reason = msg
                    break

            # Check status file
            if self._safe_is_file(status_file):
                status_file_seen = True
            if self._check_status_file(test, status_file, terminal_statuses, metrics):
                break

            # Log status changes
            current_status = test.metadata.get("__executor_status")
            if current_status != last_status:
                self.logger.debug(
                    f"  [{self._LOG_PREFIX}] Test exec_id={test.execution_id} "
                    f"status: {current_status}"
                )
                last_status = current_status

            time.sleep(self.status_poll_interval)

        # Record end time
        test.metadata["__end"] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

        # Parse metrics if test succeeded
        if test.metadata.get("__executor_status") == self.STATUS_SUCCEEDED:
            if parser is not None:
                self.logger.debug(f"  [{self._LOG_PREFIX}] Parsing metrics from output files")
                self._try_parse_metrics(test, metrics)

        metric_count = len([v for v in metrics.values() if v is not None])
        self.logger.debug(
            f"  [{self._LOG_PREFIX}] Collected {metric_count}/{len(metrics)} metrics: "
            f"{list(metrics.keys())[:3]}{'...' if len(metrics) > 3 else ''}"
        )

        # Collect system info from compute node (if probe was enabled)
        self._store_system_info(test)

    def _check_status_file(
        self,
        test: ExecutionInstance,
        status_file: Path,
        terminal_statuses: set,
        metrics: dict,
    ) -> bool:
        """
        Check the status file and update test metadata.

        Returns True if test reached a terminal status.
        """
        if not self._safe_is_file(status_file):
            return False

        try:
            with open(status_file, "r") as f:
                status_data = json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            self.logger.debug(
                f"  [{self._LOG_PREFIX}] Failed to read status file: {e}"
            )
            return False

        status = status_data.get("status", "UNKNOWN")
        test.metadata["__single_alloc_status"] = status_data

        # Capture job start time from status file (written when test starts running)
        if "__job_start" not in test.metadata and "start_time" in status_data:
            test.metadata["__job_start"] = status_data["start_time"]

        if status == "RUNNING":
            test.metadata["__executor_status"] = self.STATUS_RUNNING
            return False

        if status in terminal_statuses:
            # Map status file status to executor status
            if status == "SUCCEEDED":
                test.metadata["__executor_status"] = self.STATUS_SUCCEEDED
            elif status in ("FAILED", "TIMEOUT", "ERROR"):
                test.metadata["__executor_status"] = self.STATUS_FAILED
                test.metadata["__error"] = status_data.get(
                    "error", f"Test {status.lower()}"
                )
                if status == "TIMEOUT":
                    test.metadata["__error"] = (
                        f"Test timed out after {self.test_timeout}s"
                    )

            # Extract metrics from status file if present
            if "metrics" in status_data and status_data["metrics"]:
                for name, value in status_data["metrics"].items():
                    if name in metrics:
                        metrics[name] = value

            return True

        return False

    def _submit_kickoff(self) -> None:
        """
        Submit the single-allocation execution script via sbatch.

        Sets self.allocation_job_id on success.

        Raises:
            RuntimeError: If submission fails
        """
        workdir = self.cfg.benchmark.workdir

        # Use the pre-generated execution script
        if not self._safe_is_file(self.kickoff_path):
            raise RuntimeError(f"Execution script not found: {self.kickoff_path}")

        self.logger.info(
            f"  [{self._LOG_PREFIX}] Submitting single-allocation script: {self.kickoff_path}"
        )

        # Submit via sbatch
        cmd = shlex.split(self.cmd_submit)
        cmd.append(str(self.kickoff_path))

        self.logger.debug(f"  [{self._LOG_PREFIX}] Submit command: {' '.join(cmd)}")

        result = subprocess.run(
            cmd,
            cwd=workdir,
            capture_output=True,
            text=True,
        )

        stdout = (result.stdout or "").strip()
        stderr = (result.stderr or "").strip()

        if result.returncode != 0:
            raise RuntimeError(
                f"SLURM single-allocation submission failed (rc={result.returncode}): "
                f"stderr='{stderr}' stdout='{stdout}'"
            )

        job_id = self._parse_jobid(stdout)
        if not job_id:
            raise RuntimeError(
                f"Could not parse SLURM jobid from submission output: "
                f"stdout='{stdout}' stderr='{stderr}'"
            )

        self.allocation_job_id = job_id
        self.logger.info(f"  [{self._LOG_PREFIX}] Single allocation submitted: job_id={job_id}")

        # Register job with runner for cleanup on interrupt (Ctrl+C)
        if self.runner and hasattr(self.runner, "register_slurm_job"):
            self.runner.register_slurm_job(job_id)


# Module exports
__all__ = [
    "BaseExecutor",
    "LocalExecutor",
    "SlurmExecutor",
    "KickoffSingleAllocationExecutor",
    "SYSINFO_FILENAME",
    "KICKOFF_SCRIPT_FILENAME",
    "KICKOFF_STATUS_FILENAME",
]
