# =============================================================================
# IOPS Configuration - mdtest Benchmark (Local Execution)
# =============================================================================
#
# A simple configuration for running mdtest metadata benchmarks locally.
#
# Quick Start:
#   1. Update paths: workdir, test_dir
#   2. Validate: iops check my_config.yaml
#   3. Dry run:  iops run my_config.yaml --dry-run
#   4. Execute:  iops run my_config.yaml
#
# For a fully documented template with all options, run: iops generate --full
# =============================================================================

benchmark:
  name: "mdtest Benchmark"
  workdir: "./"
  repetitions: 3
  search_method: "exhaustive"
  executor: "local"
  cache_file: "./iops_cache.db"

vars:
  # Number of MPI processes
  processes:
    type: int
    sweep:
      mode: list
      values: [1, 2, 4]

  # Files to create per process
  files_per_proc:
    type: int
    sweep:
      mode: list
      values: [100, 1000, 10000]

  # Directory tree depth (0 = flat)
  tree_depth:
    type: int
    sweep:
      mode: list
      values: [0, 2]

  # Test directory path
  test_dir:
    type: str
    expr: "./mdtest_{{ execution_id }}"

  # Output file for mdtest results
  output_file:
    type: str
    expr: "{{ execution_dir }}/mdtest_output.txt"

command:
  template: >
    mdtest -n {{ files_per_proc }} -d {{ test_dir }}
    -z {{ tree_depth }} -F -C -T -r

scripts:
  - name: "mdtest"
    script_template: |
      #!/bin/bash
      set -euo pipefail

      echo "Running mdtest with {{ processes }} processes"

      # Create test directory
      mkdir -p {{ test_dir }}

      # Run mdtest and capture output
      mpirun -np {{ processes }} {{ command.template }} > {{ output_file }} 2>&1

      # Cleanup test directory
      rm -rf {{ test_dir }}

      echo "Completed repetition {{ repetition }}"

    parser:
      file: "{{ output_file }}"
      metrics:
        - name: file_creation_rate
        - name: file_stat_rate
        - name: file_removal_rate
      parser_script: |
        import re

        def parse(file_path: str):
            """Parse mdtest output and extract performance metrics."""
            with open(file_path, "r") as f:
                content = f.read()

            metrics = {}

            # Find the SUMMARY rate section
            summary_match = re.search(
                r"SUMMARY rate:.*?\n.*?Operation.*?Max.*?Min.*?Mean.*?Std Dev.*?\n.*?-+.*?\n(.*?)(?:\n\n|--\s+finished|\Z)",
                content,
                re.DOTALL | re.IGNORECASE,
            )

            if not summary_match:
                return {
                    "file_creation_rate": None,
                    "file_stat_rate": None,
                    "file_removal_rate": None,
                }

            summary_section = summary_match.group(1)

            for line in summary_section.strip().split("\n"):
                line = line.strip()
                if not line:
                    continue

                parts = line.split()
                if len(parts) < 4:
                    continue

                operation_name = " ".join(parts[:-4]).lower()

                try:
                    mean_value = float(parts[-2])
                except (ValueError, IndexError):
                    continue

                if "file creation" in operation_name:
                    metrics["file_creation_rate"] = mean_value
                elif "file stat" in operation_name:
                    metrics["file_stat_rate"] = mean_value
                elif "file removal" in operation_name:
                    metrics["file_removal_rate"] = mean_value

            # Ensure all expected metrics are present
            for key in ["file_creation_rate", "file_stat_rate", "file_removal_rate"]:
                if key not in metrics:
                    metrics[key] = None

            return metrics

output:
  sink:
    type: csv
    path: "{{ workdir }}/results.csv"
