# =============================================================================
# IOPS Benchmark Configuration - Bayesian Optimization Example
# =============================================================================
#
# This example demonstrates how to use Bayesian optimization to efficiently
# search for optimal parameter configurations that maximize a target metric
# (e.g., bandwidth, throughput, or minimize latency).
#
# Key Features:
# - Intelligent parameter space exploration using Bayesian optimization
# - Efficient search with fewer evaluations than exhaustive search
# - Automatic balancing of exploration vs exploitation
# - Configurable acquisition functions (EI, PI, LCB)
#
# Usage:
#   iops run example_bayesian.yaml
#   iops report <workdir>/run_XXX  # Generate analysis report after completion
#
# =============================================================================

benchmark:
  name: "IOR Bayesian Optimization"
  description: "Find optimal IOR parameters for maximum write bandwidth using Bayesian optimization"
  workdir: "/home/luan/workdir/"
  cache_file: "/home/luan/iops_bayesian.db"

  # Use Bayesian optimization search method
  search_method: "bayesian"

  # Bayesian optimization configuration
  bayesian_config:
    objective_metric: "bwMiB"         # Required: metric to optimize (must match parser metrics)
    objective: "maximize"             # "maximize" or "minimize"
    n_initial_points: 5               # Number of random explorations before optimization
    n_iterations: 15                 # Total number of parameter evaluations
    acquisition_func: "PI"            # Acquisition function: "EI", "PI", or "LCB"
                                      #   EI = Expected Improvement (balanced)
                                      #   PI = Probability of Improvement (exploitative)
                                      #   LCB = Lower Confidence Bound (explorative)

  # Global settings
  repetitions: 3                      # Repetitions per configuration (averaged for robustness)
  random_seed: 42                     # For reproducibility
  executor: "local"                   # Use "slurm" for cluster execution

  # Cache and budget configuration
  cache_exclude_vars: ["summary_file"]  # Exclude path-based derived vars from cache keys
  max_core_hours: 100                   # Maximum CPU core-hours budget
  cores_expr: "{{ processes_per_node }}"  # Jinja expression to compute cores
  estimated_time_seconds: 60            # Estimated execution time per test (for dry-run)
  report_vars: ["processes_per_node", "volume_size_gb"]  # Variables to include in reports

# -----------------------------------------------------------------------------
# Variables: Parameter space to search
# -----------------------------------------------------------------------------
# The Bayesian optimizer will search these parameter ranges/lists to find
# optimal configurations for the target metric.
# -----------------------------------------------------------------------------

vars:
  # Number of processes: discrete search space
  processes_per_node:
    type: int
    sweep:
      mode: range
      start: 1
      end: 8
      step: 1

  # Volume size: discrete search space
  volume_size_gb:
    type: int
    sweep:
      mode: list
      values: [1, 4, 8]

  # Derived variables (computed from swept variables)
  block_size_mb:
    type: int
    expr: "(volume_size_gb * 1024) / (processes_per_node)"

  summary_file:
    type: str
    expr: "{{ execution_dir }}/summary_{{ execution_id }}_{{ repetition }}.json"

# -----------------------------------------------------------------------------
# Command: Benchmark command template
# -----------------------------------------------------------------------------

command:
  template: >
    ior -w -b {{ block_size_mb }}mb -t 1mb
    -O summaryFile={{summary_file}} -O summaryFormat=JSON
    -o /home/luan/filesystem/output.ior

  labels:
    operation: "write"
    io_engine: "MPI-IO"
    access_pattern: "contiguous"

# -----------------------------------------------------------------------------
# Scripts: Job submission and execution
# -----------------------------------------------------------------------------

scripts:
  - name: "ior"

    script_template: |
      #!/bin/bash

      set -euo pipefail # exit on error, undefined var, or failed pipe

      mpirun --oversubscribe -np {{processes_per_node}} {{ command.template }}

      echo "repetition {{ repetition }} completed."

    # Optional: Post-processing script
    post:
      script: |
        #!/bin/bash
        echo "Job completed. Summary file located at {{ summary_file }}"

    # Parser: Extract metrics from output
    parser:
      file: "{{ summary_file }}"
      metrics:
        - name: bwMiB
      parser_script: ../scripts/ior_parser.py

# -----------------------------------------------------------------------------
# Output: Results storage
# -----------------------------------------------------------------------------

output:
  sink:
    type: "csv"
    path: "{{ workdir }}/results.csv"
    exclude: ["benchmark.name", "benchmark.description"]

# =============================================================================
# Notes:
# =============================================================================
#
# 1. Bayesian Optimization Process:
#    - First n_initial_points (5) will be random explorations
#    - Subsequent iterations use Gaussian Process model to predict promising points
#    - Acquisition function balances exploring new regions vs exploiting known good regions
#
# 2. Search Space:
#    - processes_per_node: 1 to 8 (continuous range, treated as discrete integers)
#    - volume_size_gb: [1, 4, 8] (discrete categorical values)
#    - block_size_mb: derived from volume_size_gb / processes_per_node
#
# 3. Objective Function:
#    - Metrics are averaged across repetitions for robust estimates
#    - "maximize" objective for bwMiB (write bandwidth in MiB/s)
#
# 4. Acquisition Functions:
#    - EI (Expected Improvement): Balanced exploration/exploitation (recommended)
#    - PI (Probability of Improvement): More exploitative, faster convergence
#    - LCB (Lower Confidence Bound): More explorative, broader search
#
# 5. After Completion:
#    - Run: iops report <workdir>/run_XXX
#    - Report shows convergence, best parameters, and optimization progress
#    - Best configuration will show optimal processes_per_node and volume_size_gb
#
# 6. Expected Behavior:
#    - Total evaluations: 20 configurations Ã— 3 repetitions = 60 runs
#    - First 5 configurations (15 runs) will be random exploration
#    - Next 15 configurations (45 runs) will use Bayesian optimization
#    - Final report will show which combination maximizes write bandwidth
#
# =============================================================================
