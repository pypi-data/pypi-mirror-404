# PagedAttention å“è³ªåŠ£åŒ–èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ

## æ¦‚è¦
é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆ2048+ tokensï¼‰ã§ã®æ¨è«–å“è³ªåŠ£åŒ–ã®åŸå› ã‚’èª¿æŸ»ã—ã¾ã—ãŸã€‚

## èª¿æŸ»ãƒ•ã‚¡ã‚¤ãƒ«
- `src/kernels/paged_attention.cu` - CUDA ã‚«ãƒ¼ãƒãƒ«
- `src/kernels/paged_attention.rs` - Rust ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
- `src/model/llama_4bit.rs` - generate_paged() ãƒ•ãƒ­ãƒ¼
- `src/paged_attention/block_manager.rs` - ãƒ–ãƒ­ãƒƒã‚¯ç®¡ç†
- `src/paged_attention/cache_engine.rs` - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ã‚¸ãƒ³

---

## ğŸ”´ Critical Issue 1: Shared Memory Overflow

### å ´æ‰€
`src/kernels/paged_attention.rs:155-156`

```rust
let max_context_len = 2048; // TODO: Make configurable
let shared_mem_bytes = ((head_dim + max_context_len) * std::mem::size_of::<f32>()) as u32;
```

### å•é¡Œ
- `max_context_len = 2048` ãŒãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰
- 2048ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¶…ãˆã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ã¯ã€qk_scores ã®æ›¸ãè¾¼ã¿ãŒ shared memory å¢ƒç•Œå¤–ã«ã‚¢ã‚¯ã‚»ã‚¹
- **çµæœ**: æœªå®šç¾©å‹•ä½œã€ã‚¬ãƒ™ãƒ¼ã‚¸å€¤ã€ã‚¯ãƒ©ãƒƒã‚·ãƒ¥

### ä¿®æ­£æ¡ˆ
```rust
// context_lens ã‹ã‚‰å®Ÿéš›ã®æœ€å¤§å€¤ã‚’å–å¾—
let context_lens_host: Vec<u32> = context_lens.to_vec1()?;
let max_context = *context_lens_host.iter().max().unwrap_or(&2048) as usize;
let shared_mem_bytes = ((head_dim + max_context) * std::mem::size_of::<f32>()) as u32;
```

---

## ğŸ”´ Critical Issue 2: Warp Reduction Bug

### å ´æ‰€
`src/kernels/paged_attention.cu:114-118`

```cuda
// Warp reduction for max (blockDim.x = 32)
for (int offset = 16; offset > 0; offset >>= 1) {
    max_score = fmaxf(max_score, __shfl_down_sync(0xffffffff, max_score, offset));
}
max_score = __shfl_sync(0xffffffff, max_score, 0);
```

### å•é¡Œ
- `__shfl_down_sync` ã¯åŒä¸€ warp (32 threads) å†…ã§ã®ã¿æœ‰åŠ¹
- `context_len > 32` ã®å ´åˆã€å„ã‚¹ãƒ¬ãƒƒãƒ‰ã¯è‡ªåˆ†ãŒæ‹…å½“ã—ãŸ token ã®ä¸­ã§ã®ã¿ max ã‚’è¨ˆç®—
- å…¨ä½“ã® max ã§ã¯ãªãã€éƒ¨åˆ†çš„ãª max ã§ softmax ã‚’è¨ˆç®—
- **çµæœ**: softmax ãŒä¸æ­£ç¢º â†’ attention weights ãŒæ­ªã‚€

### ä¿®æ­£æ¡ˆ
```cuda
// 1. å„ã‚¹ãƒ¬ãƒƒãƒ‰ã® local max ã‚’ shared memory ã«æ ¼ç´
__shared__ float max_scores_shared[32];
if (threadIdx.x < 32) max_scores_shared[threadIdx.x] = -FLT_MAX;
__syncthreads();

// 2. Atomic max (or reduction via shared memory)
atomicMax(&max_scores_shared[0], max_score); // Note: float atomicMax needs custom impl
__syncthreads();

// 3. Broadcast global max
max_score = max_scores_shared[0];
```

ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå®Ÿè£…ã¯ shared memory reduction:
```cuda
// Two-step reduction: warp-level -> block-level
float warp_max = warpReduceMax(max_score);
__shared__ float block_max[32]; // one per warp
if (lane_id == 0) block_max[warp_id] = warp_max;
__syncthreads();
if (warp_id == 0) warp_max = warpReduceMax(block_max[lane_id]);
if (threadIdx.x == 0) block_max[0] = warp_max;
__syncthreads();
max_score = block_max[0];
```

---

## ğŸŸ¡ Issue 3: Context Length Miscalculation

### å ´æ‰€
`src/paged_attention/block_manager.rs:76-79`

```rust
pub fn get_context_len(&self, seq_id: usize) -> usize {
    self.seq_to_blocks.get(&seq_id)
        .map(|blocks| blocks.len() * self.block_size)
        .unwrap_or(0)
}
```

### å•é¡Œ
- å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§ã¯ãªã `blocks * block_size` ã‚’è¿”ã™
- æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ãŒæº€æ¯ã§ãªã„å ´åˆã€éå¤§ãª context_len ãŒ CUDA ã‚«ãƒ¼ãƒãƒ«ã«æ¸¡ã•ã‚Œã‚‹
- ã‚«ãƒ¼ãƒãƒ«ã¯æœªåˆæœŸåŒ–é ˜åŸŸã® K/V ã‚’èª­ã‚€

### ä¿®æ­£æ¡ˆ
```rust
pub struct BlockManager {
    // ...
    /// Actual token count per sequence
    seq_token_counts: HashMap<usize, usize>,
}

pub fn get_context_len(&self, seq_id: usize) -> usize {
    self.seq_token_counts.get(&seq_id).copied().unwrap_or(0)
}

pub fn allocate_slots(&mut self, seq_id: usize, num_tokens: usize) -> Result<Vec<i64>> {
    // ... existing logic ...
    *self.seq_token_counts.entry(seq_id).or_insert(0) += num_tokens;
    // ...
}
```

---

## ğŸŸ¡ Issue 4: Softmax Sum Reduction Bug

### å ´æ‰€
`src/kernels/paged_attention.cu:125-128`

```cuda
// Warp reduction for sum
for (int offset = 16; offset > 0; offset >>= 1) {
    sum_exp += __shfl_down_sync(0xffffffff, sum_exp, offset);
}
sum_exp = __shfl_sync(0xffffffff, sum_exp, 0);
```

### å•é¡Œ
- Issue 2 ã¨åŒæ§˜ã€warp å†…ã§ã—ã‹ sum ãŒè¨ˆç®—ã•ã‚Œãªã„
- `context_len > 32` ã§ã¯å„ warp ãŒéƒ¨åˆ†å’Œã—ã‹æŒãŸãªã„
- **çµæœ**: æ­£è¦åŒ–ä¿‚æ•°ãŒä¸æ­£ç¢º â†’ attention weights ã®åˆè¨ˆãŒ 1 ã«ãªã‚‰ãªã„

---

## ğŸŸ¢ Minor Issue: RoPE Position Off-by-One

### å ´æ‰€
`src/model/llama_4bit.rs:564`

```rust
let cos = self.cos_cache.narrow(0, pos - 1, 1)?;
let sin = self.sin_cache.narrow(0, pos - 1, 1)?;
```

### å•é¡Œ
- decode ãƒ•ã‚§ãƒ¼ã‚ºã§ `pos - 1` ã‚’ä½¿ç”¨
- æ­£ã—ãã¯ `pos` (ç¾åœ¨ã®ä½ç½®) ã§ã‚ã‚‹ã¹ãå¯èƒ½æ€§
- é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ã¯ç´¯ç©çš„ãªã‚ºãƒ¬ãŒç™ºç”Ÿ

### æ¤œè¨¼æ–¹æ³•
- Prefill ã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ decode ã®æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã® position ã‚’æ¯”è¼ƒ
- HuggingFace å®Ÿè£…ã¨ã®å‡ºåŠ›æ¯”è¼ƒ

---

## æ¨å¥¨ä¿®æ­£å„ªå…ˆé †ä½

| å„ªå…ˆåº¦ | å•é¡Œ | å½±éŸ¿åº¦ | ä¿®æ­£ã‚³ã‚¹ãƒˆ |
|--------|------|--------|-----------|
| ğŸ”´ P0 | Warp reduction (max & sum) | Critical | ä¸­ |
| ğŸ”´ P0 | Shared memory overflow | Critical | ä½ |
| ğŸŸ¡ P1 | Context length calculation | High | ä½ |
| ğŸŸ¢ P2 | RoPE position | Medium | ä½ |

---

## ãƒ†ã‚¹ãƒˆè¨ˆç”»

### å›å¸°ãƒ†ã‚¹ãƒˆ
1. **Short sequence (128 tokens)**: æ—¢å­˜å‹•ä½œç¢ºèª
2. **Medium sequence (512 tokens)**: å¢ƒç•Œä»˜è¿‘
3. **Long sequence (2048 tokens)**: æ—§ max_context_len å¢ƒç•Œ
4. **Very long sequence (4096+ tokens)**: æ–°ã—ã„å¢ƒç•Œ

### å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
- Perplexity æ¯”è¼ƒ (traditional KV cache vs PagedAttention)
- Top-k token ä¸€è‡´ç‡
- Softmax weights ã®åˆè¨ˆå€¤ (1.0 ã«ãªã‚‹ã¹ã)

### æ•°å€¤ç²¾åº¦ãƒ†ã‚¹ãƒˆ
```python
# attention weights ã®åˆè¨ˆå€¤æ¤œè¨¼
attn_sum = attention_weights.sum(dim=-1)
assert torch.allclose(attn_sum, torch.ones_like(attn_sum), atol=1e-5)
```

---

## 0.5.0 ã§ä¿®æ­£ã™ã‚‹ã‚‚ã®

1. âœ… Warp reduction ã‚’ block-level reduction ã«ä¿®æ­£
2. âœ… max_context_len ã‚’å‹•çš„ã«è¨ˆç®—
3. âœ… BlockManager ã«å®Ÿãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¿½è·¡
4. â³ RoPE position ã®æ¤œè¨¼ (è¦èª¿æŸ»)

## å‚è€ƒè³‡æ–™

- [vLLM PagedAttention Paper](https://arxiv.org/abs/2309.06180)
- [vLLM CUDA Kernels](https://github.com/vllm-project/vllm/tree/main/csrc/attention)
- [FlashAttention](https://github.com/Dao-AILab/flash-attention)
