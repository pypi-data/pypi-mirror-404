//! BitLinear - 1.58-bit Quantized Linear Layer
//!
//! BitLinearå±¤ã¯ã€BitNet b1.58è«–æ–‡ã«åŸºã¥ã1.58ãƒ“ãƒƒãƒˆé‡å­åŒ–ç·šå½¢å±¤ã§ã™ã€‚
//! é‡ã¿ã‚’{-1, 0, +1}ã®ä¸‰å€¤ã«é‡å­åŒ–ã™ã‚‹ã“ã¨ã§ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã€
//! ä¹—ç®—ã‚’ãƒ“ãƒƒãƒˆæ¼”ç®—ã«ç½®ãæ›ãˆã‚‹ã“ã¨ã§é«˜é€Ÿãªæ¨è«–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
//!
//! This module implements the BitLinear layer based on the BitNet b1.58 paper.
//! By quantizing weights to ternary values {-1, 0, +1}, it significantly reduces
//! memory usage and replaces multiplications with bit operations for faster inference.
//!
//! # Architecture / ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
//!
//! - **Training (å­¦ç¿’æ™‚)**: Straight-Through Estimator (STE) ã§å‹¾é…ã‚’è¿‘ä¼¼
//! - **Inference (æ¨è«–æ™‚)**: Pre-packed weights ã§æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨
//!
//! # Quantization / é‡å­åŒ–æ–¹å¼
//!
//! ```text
//! W_quant = round(clamp(W / scale, -1, 1))
//! scale = mean(|W|)
//!
//! Note: Uses BitTTTError for unified error handling across the crate.
//! ```
//!
//! # Dual Kernel Support / ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚«ãƒ¼ãƒãƒ«å¯¾å¿œ
//!
//! - **CPU**: AVX2/FMAæœ€é©åŒ–ã•ã‚ŒãŸSIMDã‚«ãƒ¼ãƒãƒ«
//! - **CUDA**: BitNetå°‚ç”¨ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«

use candle_core::{Device, Result, Tensor};
use candle_nn::VarBuilder;

use crate::error::BitTTTError;
use crate::kernels::packing::PackedTensor;
use crate::kernels::{cpu::BitLinearCpu, cuda::BitLinearCuda};

/// Standard BitLinear layer implementing 1.58-bit quantization.
///
/// 1.58ãƒ“ãƒƒãƒˆé‡å­åŒ–ã‚’å®Ÿè£…ã™ã‚‹æ¨™æº–BitLinearå±¤ã€‚
/// æ¨è«–æ™‚ã¯pre-packedã‚¦ã‚§ã‚¤ãƒˆã«ã‚ˆã‚Šæœ€é©åŒ–ã•ã‚Œã¾ã™ã€‚
///
/// Optimized for inference with pre-packed weights that enable
/// efficient ternary matrix multiplication on both CPU and CUDA.
#[derive(Clone)]
pub struct BitLinear {
    pub weight: Tensor,
    /// Input feature dimension (retained for introspection/serialization)
    #[allow(dead_code)]
    pub in_features: usize,
    /// Output feature dimension (retained for introspection/serialization)
    #[allow(dead_code)]
    pub out_features: usize,
    /// Simply-packed weights for 1.58-bit kernels (Dual Device Support)
    pub packed_params: Option<PackedTensor>,
}

impl BitLinear {
    /// Load weights from a VarBuilder (checkpoint/safetensors).
    ///
    /// VarBuilderã‹ã‚‰ã‚¦ã‚§ã‚¤ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ/safetensorsã‹ã‚‰ï¼‰ã€‚
    /// mmapä½¿ç”¨æ™‚ã®ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§ã®ãŸã‚ã€CPUä¸Šã§ã¯æ˜ç¤ºçš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã™ã€‚
    ///
    /// # Arguments / å¼•æ•°
    /// - `in_dim`: Input dimension / å…¥åŠ›æ¬¡å…ƒ
    /// - `out_dim`: Output dimension / å‡ºåŠ›æ¬¡å…ƒ
    /// - `vb`: VarBuilder for loading weights / ã‚¦ã‚§ã‚¤ãƒˆãƒ­ãƒ¼ãƒ‰ç”¨VarBuilder
    /// - `device`: Target device (CPU/CUDA) / ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒã‚¤ã‚¹
    pub fn load(in_dim: usize, out_dim: usize, vb: VarBuilder, device: &Device) -> Result<Self> {
        let init = candle_nn::init::DEFAULT_KAIMING_NORMAL;
        let weight = vb.get_with_hints((out_dim, in_dim), "weight", init)?;

        // [Plan B] Explicit Mmap Detachment
        let weight = if device.is_cpu() {
            let data = weight.to_vec1::<f32>()?;
            Tensor::from_vec(data, weight.shape(), device)?
        } else {
            weight.to_device(device)?
        };
        Ok(Self {
            weight,
            in_features: in_dim,
            out_features: out_dim,
            packed_params: None,
        })
    }

    /// Load from pre-loaded weight tensor (legacy FP32/FP16 format).
    ///
    /// äº‹å‰ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚¦ã‚§ã‚¤ãƒˆãƒ†ãƒ³ã‚½ãƒ«ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼FP32/FP16å½¢å¼ï¼‰ã€‚
    ///
    /// # Arguments / å¼•æ•°
    /// - `weight`: Weight tensor `[out_dim, in_dim]`
    /// - `in_dim`: Input dimension / å…¥åŠ›æ¬¡å…ƒ
    /// - `out_dim`: Output dimension / å‡ºåŠ›æ¬¡å…ƒ
    /// - `device`: Target device / ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒã‚¤ã‚¹
    pub fn from_weight_tensor(
        weight: &Tensor,
        in_dim: usize,
        out_dim: usize,
        device: &Device,
    ) -> Result<Self> {
        // Move to device and ensure F32
        let weight = weight
            .to_dtype(candle_core::DType::F32)?
            .to_device(device)?;

        // Verify shape
        let dims = weight.dims();
        if dims != [out_dim, in_dim] {
            return Err(candle_core::Error::Msg(format!(
                "Weight shape mismatch: expected [{}, {}], got {:?}",
                out_dim, in_dim, dims
            )));
        }

        // Deep copy to detach from mmap
        let weight = if device.is_cpu() {
            let data = weight.flatten_all()?.to_vec1::<f32>()?;
            Tensor::from_vec(data, (out_dim, in_dim), device)?
        } else {
            weight
        };

        Ok(Self {
            weight,
            in_features: in_dim,
            out_features: out_dim,
            packed_params: None,
        })
    }

    /// Load from pre-loaded Bit-TTT tensors (weight_packed + scales).
    ///
    /// äº‹å‰ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®Bit-TTTãƒ†ãƒ³ã‚½ãƒ«ï¼ˆweight_packed + scalesï¼‰ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    ///
    /// This is the recommended way to load quantized models, as it avoids
    /// VarBuilder dtype issues with U8 tensors.
    ///
    /// # Arguments / å¼•æ•°
    /// - `weight_packed`: Packed weights `[out_dim, in_dim/4]` or `[out_dim, in_dim/4, n_bases]` as U8
    /// - `scales`: Per-base scales `[n_bases]` as F32
    /// - `device`: Target device (CPU/CUDA) / ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒã‚¤ã‚¹
    ///
    /// # Example
    /// ```ignore
    /// let tensors = candle_core::safetensors::load(&path, &device)?;
    /// let packed = tensors.get("layer.weight_packed").unwrap();
    /// let scales = tensors.get("layer.scales").unwrap();
    /// let layer = BitLinear::from_packed_tensors(packed, scales, &device)?;
    /// ```
    pub fn from_packed_tensors(
        weight_packed: &Tensor,
        scales: &Tensor,
        device: &Device,
    ) -> Result<Self> {
        let dims = weight_packed.dims();

        // Extract dimensions from packed tensor
        // [out_dim, in_dim/4] or [out_dim, in_dim/4, n_bases]
        let (out_dim, in_dim, _n_bases) = match dims.len() {
            2 => (dims[0], dims[1] * 4, 1usize),
            3 => (dims[0], dims[1] * 4, dims[2]),
            _ => {
                return Err(candle_core::Error::Msg(format!(
                    "Invalid weight_packed shape: expected 2D or 3D, got {:?}",
                    dims
                )))
            }
        };

        // Move tensors to device and ensure correct dtype
        // VarBuilder.get() may return F32 even for U8 safetensors, so we need to handle this
        let packed_data = if weight_packed.dtype() != candle_core::DType::U8 {
            // Convert F32 weights back to U8 (happens when VarBuilder auto-converts)
            eprintln!(
                "âš ï¸ [PACKED] Converting {:?} â†’ U8 (VarBuilder dtype issue)",
                weight_packed.dtype()
            );
            weight_packed
                .to_dtype(candle_core::DType::U8)?
                .to_device(device)?
        } else {
            weight_packed.to_device(device)?
        };
        let scales_data = scales
            .to_dtype(candle_core::DType::F32)?
            .to_device(device)?;

        // Create PackedTensor
        let packed_params =
            PackedTensor::from_loaded(packed_data, scales_data, out_dim, in_dim, device)?;

        // Create dummy weight tensor (not used in forward when packed_params exists)
        let weight = Tensor::zeros((out_dim, in_dim), candle_core::DType::F32, device)?;

        Ok(Self {
            weight,
            in_features: in_dim,
            out_features: out_dim,
            packed_params: Some(packed_params),
        })
    }

    /// Load from Bit-TTT quantized format via VarBuilder.
    ///
    /// **Note**: VarBuilder has issues with U8 tensors. Prefer `from_packed_tensors()`.
    ///
    /// Bit-TTTå½¢å¼ã®é‡å­åŒ–æ¸ˆã¿ã‚¦ã‚§ã‚¤ãƒˆã‚’VarBuilderçµŒç”±ã§ãƒ­ãƒ¼ãƒ‰ã€‚
    /// **æ³¨æ„**: VarBuilderã¯U8ãƒ†ãƒ³ã‚½ãƒ«ã«å•é¡ŒãŒã‚ã‚‹ãŸã‚ã€`from_packed_tensors()`ã‚’æ¨å¥¨ã€‚
    #[allow(dead_code)]
    pub fn load_packed(
        in_dim: usize,
        out_dim: usize,
        n_bases: usize,
        vb: VarBuilder,
        device: &Device,
    ) -> Result<Self> {
        // Try to load packed format
        let packed_shape = if n_bases == 1 {
            vec![out_dim, in_dim / 4]
        } else {
            vec![out_dim, in_dim / 4, n_bases]
        };

        let packed_result = vb.get(packed_shape.as_slice(), "weight_packed");
        let scales_result = vb.get(&[n_bases], "scales");

        match (packed_result, scales_result) {
            (Ok(packed_raw), Ok(scales)) => {
                // Ensure packed data is on correct device and dtype
                let packed_data = packed_raw.to_device(device)?;
                let scales_data = scales
                    .to_dtype(candle_core::DType::F32)?
                    .to_device(device)?;

                // Create PackedTensor from loaded data
                let packed_params =
                    PackedTensor::from_loaded(packed_data, scales_data, out_dim, in_dim, device)?;

                // Create dummy weight tensor (not used in forward when packed_params exists)
                let weight = Tensor::zeros((out_dim, in_dim), candle_core::DType::F32, device)?;

                Ok(Self {
                    weight,
                    in_features: in_dim,
                    out_features: out_dim,
                    packed_params: Some(packed_params),
                })
            }
            _ => {
                // Fallback to regular load for FP32/FP16 weights
                Self::load(in_dim, out_dim, vb, device)
            }
        }
    }

    /// Pre-compute packed weights for optimized inference via Dual Kernels.
    ///
    /// ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚«ãƒ¼ãƒãƒ«ç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸpacked weightsã‚’äº‹å‰è¨ˆç®—ã—ã¾ã™ã€‚
    /// ã“ã®é–¢æ•°ã¯æ¨è«–å‰ã«ä¸€åº¦ã ã‘å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚
    ///
    /// This quantizes the weights to ternary values and packs them into
    /// 2-bit format (4 weights per byte) for efficient SIMD/GPU processing.
    ///
    /// é‡ã¿ã‚’ä¸‰å€¤ã«é‡å­åŒ–ã—ã€2ãƒ“ãƒƒãƒˆå½¢å¼ï¼ˆ1ãƒã‚¤ãƒˆã‚ãŸã‚Š4ã‚¦ã‚§ã‚¤ãƒˆï¼‰ã«
    /// ãƒ‘ãƒƒã‚­ãƒ³ã‚°ã—ã¦SIMD/GPUå‡¦ç†ã‚’åŠ¹ç‡åŒ–ã—ã¾ã™ã€‚
    pub fn precompute_packed(&mut self) -> Result<()> {
        // This function quantizes the weights and packs them into 2-bit format.
        // It populates `self.packed_params`.
        let packed = PackedTensor::pack(&self.weight)?;
        self.packed_params = Some(packed);
        Ok(())
    }

    /// Forward pass: Y = X @ W^T (with automatic kernel dispatch).
    ///
    /// é †ä¼æ’­: Y = X @ W^Tï¼ˆè‡ªå‹•ã‚«ãƒ¼ãƒãƒ«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒä»˜ãï¼‰ã€‚
    ///
    /// # Execution Paths / å®Ÿè¡Œãƒ‘ã‚¹
    ///
    /// 1. **Dual Kernel Path** (æ¨è«–æ™‚æ¨å¥¨): `packed_params`ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€
    ///    ãƒ‡ãƒã‚¤ã‚¹ã«å¿œã˜ã¦æœ€é©åŒ–ã‚«ãƒ¼ãƒãƒ«ï¼ˆAVX2/CUDAï¼‰ã‚’è‡ªå‹•é¸æŠ
    /// 2. **Legacy STE Path** (å­¦ç¿’æ™‚): Straight-Through Estimatorã§å‹¾é…ã‚’è¿‘ä¼¼
    ///
    /// # Arguments / å¼•æ•°
    /// - `x`: Input tensor of shape `[..., in_features]`
    ///
    /// # Returns / æˆ»ã‚Šå€¤
    /// Output tensor of shape `[..., out_features]`
    pub fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // Handle Rank > 2 inputs (e.g. [Batch, Seq, Hidden]) via flattening
        let (input, original_shape) = if x.rank() > 2 {
            let dims = x.dims();
            let last_dim = dims[dims.len() - 1];
            let flattened_dim = x.elem_count() / last_dim;
            // flatten to [Batch*Seq, Hidden]
            (x.reshape(&[flattened_dim, last_dim])?, Some(dims.to_vec()))
        } else {
            (x.clone(), None)
        };

        // 1. Dual Kernel Path (Fastest, 1.58-bit Native)
        if let Some(packed) = &self.packed_params {
            // Automatic Dispatch based on device
            let result = match input.device() {
                Device::Cpu => {
                    // Use Optimized CPU Kernel (AVX2)
                    BitLinearCpu::forward(&input, packed)
                }
                Device::Cuda(_) => {
                    // Use Custom CUDA Kernel (BitNet)
                    BitLinearCuda::forward(&input, packed)
                }
                _ => {
                    // Fallback to legacy path if kernel not available for device
                    // But we don't have a fallback return here easily without code dupe or rearranging.
                    // For now, let's assume if packed exists, we must use kernel or fail.
                    // Or we can assume packing only happens if supported?
                    return Err(BitTTTError::kernel_error(
                        "Packed params present but Custom Kernel not implemented for this device",
                    )
                    .into());
                }
            }?;

            // Reshape back if needed
            if let Some(mut dims) = original_shape {
                let last_idx = dims.len() - 1;
                let (_total, out_dim) = result.dims2()?;
                dims[last_idx] = out_dim;
                return result.reshape(&dims[..]);
            } else {
                return Ok(result);
            }
        }

        // 3. Legacy Fallback (FP16/FP32 weights - no quantization)
        // Used for pre-quantized models that have been dequantized to FP16
        #[cfg(debug_assertions)]
        tracing::debug!("ğŸ“¦ BitLinear: Using legacy FP path (no STE quantization)");

        // Simple matmul without quantization simulation
        // The weights are already in their final form (FP16/FP32)
        let result = input.matmul(&self.weight.t()?)?;

        // Reshape back if needed
        if let Some(mut dims) = original_shape {
            let last_idx = dims.len() - 1;
            dims[last_idx] = self.out_features;
            result.reshape(&dims[..])
        } else {
            Ok(result)
        }
    }
}
