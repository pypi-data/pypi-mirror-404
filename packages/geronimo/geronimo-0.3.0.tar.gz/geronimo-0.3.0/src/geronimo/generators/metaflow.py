"""Metaflow flow generator for batch jobs.

Generates Metaflow flow files with deployment configuration for
Step Functions or Airflow backends.
"""

from pathlib import Path
from typing import Optional

from geronimo.config.schema import BatchBackend, BatchConfig, BatchJobConfig


class MetaflowGenerator:
    """Generate Metaflow flows and deployment configurations."""

    def __init__(self, project_name: str, batch_config: BatchConfig):
        """Initialize the generator.

        Args:
            project_name: Name of the project.
            batch_config: Batch configuration from geronimo.yaml.
        """
        self.project_name = project_name
        self.project_name_snake = project_name.replace("-", "_")
        self.config = batch_config

    def generate(self, output_dir: Path) -> list[Path]:
        """Generate all batch job artifacts.

        Args:
            output_dir: Directory to write generated files.

        Returns:
            List of generated file paths.
        """
        generated_files: list[Path] = []

        # Create flows directory
        flows_dir = output_dir / "flows"
        flows_dir.mkdir(exist_ok=True)

        # Generate flow files
        for job in self.config.jobs:
            flow_path = flows_dir / f"{job.name}.py"
            flow_content = self._generate_flow(job)
            flow_path.write_text(flow_content)
            generated_files.append(flow_path)

        # Generate deployment config based on backend
        if self.config.backend == BatchBackend.STEP_FUNCTIONS:
            deploy_path = output_dir / "step_functions_config.json"
            deploy_content = self._generate_step_functions_config()
            deploy_path.write_text(deploy_content)
            generated_files.append(deploy_path)
        else:
            dag_dir = output_dir / "dags"
            dag_dir.mkdir(exist_ok=True)
            for job in self.config.jobs:
                dag_path = dag_dir / f"{job.name}_dag.py"
                dag_content = self._generate_airflow_dag(job)
                dag_path.write_text(dag_content)
                generated_files.append(dag_path)

        return generated_files

    def _generate_flow(self, job: BatchJobConfig) -> str:
        """Generate a Metaflow flow file."""
        return f'''"""Metaflow flow for {job.name}.

Generated by Geronimo.
"""

from metaflow import FlowSpec, step, batch, Parameter


class {self._to_class_name(job.name)}Flow(FlowSpec):
    """Batch training/inference flow for {job.name}."""

    # Parameters can be passed at runtime
    input_path = Parameter(
        "input_path",
        help="S3 path to input data",
        default="s3://data-bucket/input/",
    )
    output_path = Parameter(
        "output_path",
        help="S3 path for output",
        default="s3://data-bucket/output/",
    )
    model_version = Parameter(
        "model_version",
        help="Model version for tracking",
        default="1.0.0",
    )
    capture_reference = Parameter(
        "capture_reference",
        help="Capture reference snapshot for drift detection",
        default=True,
        type=bool,
    )

    @step
    def start(self):
        """Initialize the flow."""
        print(f"Starting {{self.__class__.__name__}}")
        print(f"Input: {{self.input_path}}")
        self.next(self.load_data)

    @step
    def load_data(self):
        """Load input data."""
        import pandas as pd

        # TODO: Load data from self.input_path
        # Example: self.data = pd.read_parquet(self.input_path)
        self.data = pd.DataFrame()  # Replace with actual data loading

        print(f"Loaded {{len(self.data)}} rows")
        self.next(self.capture_baseline)

    @step
    def capture_baseline(self):
        """Capture reference snapshot for drift detection."""
        if self.capture_reference and len(self.data) > 0:
            from geronimo.monitoring.api import capture_reference_from_data

            self.reference_snapshot = capture_reference_from_data(
                data=self.data,
                project_name="{self.project_name}",
                model_version=self.model_version,
                deployment_type="batch",
            )
            print(f"Captured reference snapshot: {{self.reference_snapshot.id}}")
        else:
            self.reference_snapshot = None
            print("Skipping reference capture")

        self.next(self.process)

    @batch(cpu={job.cpu}, memory={job.memory})
    @step
    def process(self):
        """Main processing step.

        This runs on AWS Batch with the specified resources.
        """
        from {self.project_name_snake}.ml.predictor import ModelPredictor

        # Load model
        predictor = ModelPredictor()
        predictor.load()

        # TODO: Run predictions on self.data
        # TODO: Save results to self.output_path

        self.results = {{"status": "completed", "rows_processed": len(self.data)}}
        self.next(self.end)

    @step
    def end(self):
        """Finalize the flow."""
        print(f"Flow completed with results: {{self.results}}")
        if self.reference_snapshot:
            print(f"Reference snapshot: {{self.reference_snapshot.s3_path}}")


if __name__ == "__main__":
    {self._to_class_name(job.name)}Flow()
'''

    def _generate_step_functions_config(self) -> str:
        """Generate Step Functions deployment configuration."""
        import json

        config = {
            "METAFLOW_DATASTORE_SYSROOT_S3": self.config.step_functions.s3_root,
            "METAFLOW_DEFAULT_DATASTORE": "s3",
            "METAFLOW_DEFAULT_METADATA": "service",
        }
        if self.config.step_functions.batch_queue:
            config["METAFLOW_BATCH_JOB_QUEUE"] = self.config.step_functions.batch_queue

        return json.dumps(config, indent=2)

    def _generate_airflow_dag(self, job: BatchJobConfig) -> str:
        """Generate Airflow DAG for a job."""
        schedule = f'"{job.schedule}"' if job.schedule else "None"

        return f'''"""Airflow DAG for {job.name}.

Generated by Geronimo. Wraps Metaflow flow execution.
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator

default_args = {{
    "owner": "geronimo",
    "depends_on_past": False,
    "email_on_failure": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}}

with DAG(
    "{job.name}",
    default_args=default_args,
    description="Run {job.name} Metaflow flow",
    schedule={schedule},
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["metaflow", "batch", "{self.project_name}"],
) as dag:

    run_flow = KubernetesPodOperator(
        task_id="run_{job.name}_flow",
        name="{job.name}-flow",
        namespace="{self.config.airflow.namespace}",
        image="{{{{ var.value.metaflow_image }}}}",
        cmds=["python", "flows/{job.name}.py", "run"],
        arguments=["--with", "batch"],
        resources={{
            "request_cpu": "{job.cpu}",
            "request_memory": "{job.memory}Mi",
        }},
        get_logs=True,
        is_delete_operator_pod=True,
    )
'''

    def _to_class_name(self, name: str) -> str:
        """Convert job name to PascalCase class name."""
        return "".join(word.title() for word in name.replace("-", "_").split("_"))
