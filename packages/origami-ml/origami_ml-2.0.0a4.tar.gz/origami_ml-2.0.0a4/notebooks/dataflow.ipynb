{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Origami Data Flow\n",
    "\n",
    "This notebook traces the data transformations step-by-step through the Origami pipeline,\n",
    "from raw input data to model forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing with NumericDiscretizer\n",
    "\n",
    "The first step in the pipeline is preprocessing. When `numeric_mode=\"discretize\"`,\n",
    "high-cardinality numeric fields are binned into categories.\n",
    "\n",
    "**Input:** `list[dict]` - raw JSON objects\n",
    "\n",
    "**Output:** `list[dict]` - objects with high-cardinality numerics replaced by bin labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw input data:\n",
      "Length: 8\n",
      "\n",
      "Input Data:\n",
      "[0] {'price': 15000, 'category': 'sedan', 'tags': ['reliable', 'fuel-efficient'], 'seller': {'name': 'Alice', 'rating': 4.5}}\n",
      "[1] {'price': 25000, 'category': 'suv', 'tags': ['spacious'], 'seller': {'name': 'Bob', 'rating': 4.8}}\n",
      "[2] {'price': 18000, 'category': 'sedan', 'seller': {'name': 'Alice', 'rating': 4.5}}\n",
      "[3] {'price': 32000, 'category': 'truck', 'tags': [], 'seller': {'name': 'Carol', 'rating': 4.2}}\n",
      "[4] {'price': 28000, 'tags': ['luxury', 'fast', 'new'], 'seller': {'name': 'Bob', 'rating': 4.8}}\n",
      "[5] {'price': 12000, 'category': 'sedan', 'tags': ['budget']}\n",
      "[6] {'price': 45000, 'category': 'suv', 'tags': ['luxury', 'spacious'], 'seller': {'name': 'Carol', 'rating': 4.2}}\n",
      "[7] {'price': 22000, 'category': 'sedan', 'tags': ['reliable'], 'seller': {'name': 'Dave'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from origami.preprocessing import NumericDiscretizer\n",
    "\n",
    "# Sample input data - a list of dictionaries with varied structure:\n",
    "# - price: numeric field (high-cardinality, will be discretized)\n",
    "# - category: categorical field (low-cardinality, stays as-is)\n",
    "# - tags: array with 0-3 values\n",
    "# - seller: subdocument with 2 subfields (name, rating)\n",
    "# - some fields are missing in different objects\n",
    "\n",
    "raw_data = [\n",
    "    {\n",
    "        \"price\": 15000,\n",
    "        \"category\": \"sedan\",\n",
    "        \"tags\": [\"reliable\", \"fuel-efficient\"],\n",
    "        \"seller\": {\"name\": \"Alice\", \"rating\": 4.5},\n",
    "    },\n",
    "    {\n",
    "        \"price\": 25000,\n",
    "        \"category\": \"suv\",\n",
    "        \"tags\": [\"spacious\"],\n",
    "        \"seller\": {\"name\": \"Bob\", \"rating\": 4.8},\n",
    "    },\n",
    "    {\"price\": 18000, \"category\": \"sedan\", \"seller\": {\"name\": \"Alice\", \"rating\": 4.5}},  # no tags\n",
    "    {\n",
    "        \"price\": 32000,\n",
    "        \"category\": \"truck\",\n",
    "        \"tags\": [],\n",
    "        \"seller\": {\"name\": \"Carol\", \"rating\": 4.2},\n",
    "    },  # empty tags\n",
    "    {\n",
    "        \"price\": 28000,\n",
    "        \"tags\": [\"luxury\", \"fast\", \"new\"],\n",
    "        \"seller\": {\"name\": \"Bob\", \"rating\": 4.8},\n",
    "    },  # no category\n",
    "    {\"price\": 12000, \"category\": \"sedan\", \"tags\": [\"budget\"]},  # no seller\n",
    "    {\n",
    "        \"price\": 45000,\n",
    "        \"category\": \"suv\",\n",
    "        \"tags\": [\"luxury\", \"spacious\"],\n",
    "        \"seller\": {\"name\": \"Carol\", \"rating\": 4.2},\n",
    "    },\n",
    "    {\n",
    "        \"price\": 22000,\n",
    "        \"category\": \"sedan\",\n",
    "        \"tags\": [\"reliable\"],\n",
    "        \"seller\": {\"name\": \"Dave\"},\n",
    "    },  # seller missing rating\n",
    "]\n",
    "\n",
    "print(\"Raw input data:\")\n",
    "print(f\"Length: {len(raw_data)}\")\n",
    "print()\n",
    "\n",
    "\n",
    "def print_data(data: list[dict], title: str):\n",
    "    print(title)\n",
    "    for i, obj in enumerate(data):\n",
    "        print(f\"[{i}] {obj}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print_data(raw_data, \"Input Data:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumericDiscretizer config:\n",
      "  cat_threshold: 3\n",
      "  n_bins: 4\n",
      "  strategy: quantile\n"
     ]
    }
   ],
   "source": [
    "# Create NumericDiscretizer\n",
    "# - cat_threshold: fields with more unique values than this are considered high-cardinality\n",
    "# - n_bins: number of bins to discretize into\n",
    "# - strategy: binning strategy (\"quantile\" or \"uniform\")\n",
    "\n",
    "discretizer = NumericDiscretizer(\n",
    "    cat_threshold=3,  # price has 8 unique values -> discretized; rating has 3 -> kept as-is\n",
    "    n_bins=4,\n",
    "    strategy=\"quantile\",\n",
    ")\n",
    "\n",
    "print(f\"NumericDiscretizer config:\")\n",
    "print(f\"  cat_threshold: {discretizer.cat_threshold}\")\n",
    "print(f\"  n_bins: {discretizer.n_bins}\")\n",
    "print(f\"  strategy: {discretizer.strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fit_transform:\n",
      "Type: <class 'list'>\n",
      "Length: 8\n",
      "\n",
      "Fields identified as high-cardinality: {'price'}\n",
      "\n",
      "Transformed Data:\n",
      "[0] {'price': 14250.0, 'category': 'sedan', 'tags': ['reliable', 'fuel-efficient'], 'seller': {'name': 'Alice', 'rating': 4.5}}\n",
      "[1] {'price': 26750.0, 'category': 'suv', 'tags': ['spacious'], 'seller': {'name': 'Bob', 'rating': 4.8}}\n",
      "[2] {'price': 20000.0, 'category': 'sedan', 'seller': {'name': 'Alice', 'rating': 4.5}}\n",
      "[3] {'price': 37500.0, 'category': 'truck', 'tags': [], 'seller': {'name': 'Carol', 'rating': 4.2}}\n",
      "[4] {'price': 26750.0, 'tags': ['luxury', 'fast', 'new'], 'seller': {'name': 'Bob', 'rating': 4.8}}\n",
      "[5] {'price': 14250.0, 'category': 'sedan', 'tags': ['budget']}\n",
      "[6] {'price': 37500.0, 'category': 'suv', 'tags': ['luxury', 'spacious'], 'seller': {'name': 'Carol', 'rating': 4.2}}\n",
      "[7] {'price': 20000.0, 'category': 'sedan', 'tags': ['reliable'], 'seller': {'name': 'Dave'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit_transform: analyze the data and transform it\n",
    "preprocessed_data = discretizer.fit_transform(raw_data)\n",
    "\n",
    "print(\"After fit_transform:\")\n",
    "print(f\"Type: {type(preprocessed_data)}\")\n",
    "print(f\"Length: {len(preprocessed_data)}\")\n",
    "print()\n",
    "\n",
    "# Show which fields were discretized\n",
    "print(f\"Fields identified as high-cardinality: {discretizer.discretized_fields}\")\n",
    "print()\n",
    "\n",
    "print_data(discretizer.fit_transform(raw_data), \"Transformed Data:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin edges learned by discretizer:\n",
      "\n",
      "price:\n",
      "  Bin edges: [12000. 16500. 23500. 30000. 45000.]\n"
     ]
    }
   ],
   "source": [
    "# The discretizer stores bin edges for each field\n",
    "print(\"Bin edges learned by discretizer:\")\n",
    "for field, discretizer_obj in discretizer.discretizers.items():\n",
    "    print(f\"\\n{field}:\")\n",
    "    print(f\"  Bin edges: {discretizer_obj.bin_edges_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Step 1\n",
    "\n",
    "**What happened:**\n",
    "- `NumericDiscretizer.fit_transform()` analyzed the data\n",
    "- Fields with more than `cat_threshold` unique values were identified as high-cardinality\n",
    "- Only `price` was discretized (8 unique values > threshold of 3)\n",
    "- `seller.rating` was not discretized (only 3 unique values ≤ threshold)\n",
    "- Arrays, subdocuments, and categorical fields pass through unchanged\n",
    "\n",
    "**Data type transformation:**\n",
    "- Input: `list[dict]` with nested structure (arrays, subdocs, missing fields)\n",
    "- Output: `list[dict]` with same structure, but `price` values replaced by bin labels\n",
    "\n",
    "**Next step:** Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization\n",
    "\n",
    "The tokenizer converts JSON objects into token sequences with path information.\n",
    "\n",
    "**Input:** `list[dict]` - preprocessed JSON objects\n",
    "\n",
    "**Output:** `TokenizedInstance` per object, containing:\n",
    "- `tokens`: List of Token objects (grammar tokens, keys, values)\n",
    "- `paths`: Path for each token (location in JSON hierarchy)\n",
    "- `numeric_values`: Scaled float values for NUM tokens (used with continuous head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer fitted on 8 objects\n",
      "Vocabulary frozen: True\n"
     ]
    }
   ],
   "source": [
    "from origami.tokenizer import JSONTokenizer\n",
    "\n",
    "# Create and fit the tokenizer on preprocessed data\n",
    "tokenizer = JSONTokenizer(\n",
    "    max_depth=32,  # Maximum nesting depth for paths\n",
    "    max_array_index=256,  # Maximum array index supported\n",
    ")\n",
    "\n",
    "# fit() builds vocabulary from all keys and values in the data\n",
    "tokenizer.fit(preprocessed_data)\n",
    "\n",
    "print(f\"Tokenizer fitted on {len(preprocessed_data)} objects\")\n",
    "print(f\"Vocabulary frozen: {tokenizer.vocab.frozen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 37\n",
      "  - Grammar tokens (fixed): 10 (IDs 0-9)\n",
      "  - Dynamic tokens (keys + values): 27\n",
      "\n",
      "All tokens in vocabulary:\n",
      "--------------------------------------------------\n",
      "  ID  0: GrammarToken('START')\n",
      "  ID  1: GrammarToken('END')\n",
      "  ID  2: GrammarToken('OBJ_START')\n",
      "  ID  3: GrammarToken('OBJ_END')\n",
      "  ID  4: GrammarToken('ARRAY_START')\n",
      "  ID  5: GrammarToken('ARRAY_END')\n",
      "  ID  6: GrammarToken('PAD')\n",
      "  ID  7: GrammarToken('UNK_KEY')\n",
      "  ID  8: GrammarToken('UNK_VALUE')\n",
      "  ID  9: GrammarToken('NUM')\n",
      "  ID 10: KeyToken('price')\n",
      "  ID 11: ValueToken(14250.0)\n",
      "  ID 12: KeyToken('category')\n",
      "  ID 13: ValueToken('sedan')\n",
      "  ID 14: KeyToken('tags')\n",
      "  ID 15: ValueToken('reliable')\n",
      "  ID 16: ValueToken('fuel-efficient')\n",
      "  ID 17: KeyToken('seller')\n",
      "  ID 18: KeyToken('name')\n",
      "  ID 19: ValueToken('Alice')\n",
      "  ID 20: KeyToken('rating')\n",
      "  ID 21: ValueToken(4.5)\n",
      "  ID 22: ValueToken(26750.0)\n",
      "  ID 23: ValueToken('suv')\n",
      "  ID 24: ValueToken('spacious')\n",
      "  ID 25: ValueToken('Bob')\n",
      "  ID 26: ValueToken(4.8)\n",
      "  ID 27: ValueToken(20000.0)\n",
      "  ID 28: ValueToken(37500.0)\n",
      "  ID 29: ValueToken('truck')\n",
      "  ID 30: ValueToken('Carol')\n",
      "  ID 31: ValueToken(4.2)\n",
      "  ID 32: ValueToken('luxury')\n",
      "  ID 33: ValueToken('fast')\n",
      "  ID 34: ValueToken('new')\n",
      "  ID 35: ValueToken('budget')\n",
      "  ID 36: ValueToken('Dave')\n"
     ]
    }
   ],
   "source": [
    "# Explore the vocabulary\n",
    "vocab = tokenizer.vocab\n",
    "\n",
    "print(f\"Vocabulary size: {vocab.size}\")\n",
    "print(f\"  - Grammar tokens (fixed): 10 (IDs 0-9)\")\n",
    "print(f\"  - Dynamic tokens (keys + values): {vocab.size - 10}\")\n",
    "print()\n",
    "\n",
    "# Show all tokens in the vocabulary\n",
    "print(\"All tokens in vocabulary:\")\n",
    "print(\"-\" * 50)\n",
    "for token_id in range(vocab.size):\n",
    "    token = vocab.decode(token_id)\n",
    "    print(f\"  ID {token_id:2d}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object to tokenize:\n",
      "  {'price': 14250.0, 'category': 'sedan', 'tags': ['reliable', 'fuel-efficient'], 'seller': {'name': 'Alice', 'rating': 4.5}}\n",
      "\n",
      "TokenizedInstance:\n",
      "  Type: <class 'origami.tokenizer.json_tokenizer.TokenizedInstance'>\n",
      "  Length: 20 tokens\n",
      "  Tokens: [GrammarToken('START'), GrammarToken('OBJ_START'), KeyToken('price'), ValueToken(14250.0), KeyToken('category'), ValueToken('sedan'), KeyToken('tags'), GrammarToken('ARRAY_START'), ValueToken('reliable'), ValueToken('fuel-efficient'), GrammarToken('ARRAY_END'), KeyToken('seller'), GrammarToken('OBJ_START'), KeyToken('name'), ValueToken('Alice'), KeyToken('rating'), ValueToken(4.5), GrammarToken('OBJ_END'), GrammarToken('OBJ_END'), GrammarToken('END')]\n",
      "  Paths: [(), (), (), (KeyElement('price'),), (), (KeyElement('category'),), (), (KeyElement('tags'),), (KeyElement('tags'), IndexElement(0)), (KeyElement('tags'), IndexElement(1)), (KeyElement('tags'),), (), (KeyElement('seller'),), (KeyElement('seller'),), (KeyElement('seller'), KeyElement('name')), (KeyElement('seller'),), (KeyElement('seller'), KeyElement('rating')), (KeyElement('seller'),), (), ()]\n",
      "  Numeric Values: [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a single object\n",
    "# tokenize() returns a TokenizedInstance with tokens, paths, and numeric_values\n",
    "\n",
    "obj = preprocessed_data[0]\n",
    "print(f\"Object to tokenize:\")\n",
    "print(f\"  {obj}\")\n",
    "print()\n",
    "\n",
    "instance = tokenizer.tokenize(obj, shuffle=False)\n",
    "\n",
    "print(f\"TokenizedInstance:\")\n",
    "print(f\"  Type: {type(instance)}\")\n",
    "print(f\"  Length: {len(instance)} tokens\")\n",
    "\n",
    "print(f\"  Tokens: {instance.tokens}\")\n",
    "print(f\"  Paths: {instance.paths}\")\n",
    "print(f\"  Numeric Values: {instance.numeric_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token sequence with paths:\n",
      "----------------------------------------------------------------------\n",
      "Pos  Token                               Path                \n",
      "----------------------------------------------------------------------\n",
      "  0  GrammarToken('START')               (root)              \n",
      "  1  GrammarToken('OBJ_START')           (root)              \n",
      "  2  KeyToken('price')                   (root)              \n",
      "  3  ValueToken(14250.0)                 .price              \n",
      "  4  KeyToken('category')                (root)              \n",
      "  5  ValueToken('sedan')                 .category           \n",
      "  6  KeyToken('tags')                    (root)              \n",
      "  7  GrammarToken('ARRAY_START')         .tags               \n",
      "  8  ValueToken('reliable')              .tags[0]            \n",
      "  9  ValueToken('fuel-efficient')        .tags[1]            \n",
      " 10  GrammarToken('ARRAY_END')           .tags               \n",
      " 11  KeyToken('seller')                  (root)              \n",
      " 12  GrammarToken('OBJ_START')           .seller             \n",
      " 13  KeyToken('name')                    .seller             \n",
      " 14  ValueToken('Alice')                 .seller.name        \n",
      " 15  KeyToken('rating')                  .seller             \n",
      " 16  ValueToken(4.5)                     .seller.rating      \n",
      " 17  GrammarToken('OBJ_END')             .seller             \n",
      " 18  GrammarToken('OBJ_END')             (root)              \n",
      " 19  GrammarToken('END')                 (root)              \n"
     ]
    }
   ],
   "source": [
    "# Show the token sequence with paths\n",
    "# Each token has an associated path showing its location in the JSON hierarchy\n",
    "\n",
    "\n",
    "def format_path(path):\n",
    "    \"\"\"Format a path tuple as a readable string.\"\"\"\n",
    "    if not path:\n",
    "        return \"(root)\"\n",
    "    parts = []\n",
    "    for elem in path:\n",
    "        if hasattr(elem, \"key\"):  # KeyElement\n",
    "            parts.append(f\".{elem.key}\")\n",
    "        else:  # IndexElement\n",
    "            parts.append(f\"[{elem.index}]\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "print(\"Token sequence with paths:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Pos':>3}  {'Token':<35} {'Path':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for i, (token, path) in enumerate(zip(instance.tokens, instance.paths, strict=True)):\n",
    "    print(f\"{i:3d}  {str(token):<35} {format_path(path):<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Step 2\n",
    "\n",
    "**What happened:**\n",
    "- `JSONTokenizer.fit()` built a vocabulary from all keys and values in the data\n",
    "- Grammar tokens have fixed IDs (0-9): START, END, OBJ_START, OBJ_END, ARRAY_START, ARRAY_END, PAD, UNK_KEY, UNK_VALUE, NUM\n",
    "- Dynamic tokens (keys and values) are assigned IDs starting from 10\n",
    "- `tokenize()` converts a JSON object to a `TokenizedInstance`\n",
    "\n",
    "**Token sequence structure:**\n",
    "- Starts with `START`, ends with `END`\n",
    "- Objects: `OBJ_START` → key-value pairs → `OBJ_END`\n",
    "- Arrays: `ARRAY_START` → elements → `ARRAY_END`\n",
    "- Each token has an associated **path** showing its location in the JSON hierarchy\n",
    "\n",
    "**Paths are used for Key-Value Position Encoding (KVPE):**\n",
    "- Instead of standard positional encoding (1, 2, 3...), KVPE encodes the path through the JSON structure\n",
    "- This allows the model to understand hierarchical relationships\n",
    "\n",
    "**Next step:** Batch encoding (converting to tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Dataset Wrappers (OrigamiDataset)\n",
    "\n",
    "During training, preprocessed data is wrapped in a dataset class that handles:\n",
    "- **OrigamiDataset** with `shuffle=True` (training): Key-order shuffling for data augmentation\n",
    "- **OrigamiDataset** with `shuffle=False` (validation): Deterministic tokenization for reproducible evaluation\n",
    "\n",
    "**Input:** `list[dict]` - preprocessed JSON objects\n",
    "\n",
    "**Output:** `TokenizedInstance` when indexed (calls `tokenizer.tokenize()` on demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrigamiDataset (training):\n",
      "  Size: 8 objects\n",
      "  Shuffle: True\n"
     ]
    }
   ],
   "source": [
    "from origami.training.dataset import OrigamiDataset\n",
    "\n",
    "# Create an OrigamiDataset for training with key shuffling enabled\n",
    "train_dataset = OrigamiDataset(\n",
    "    data=preprocessed_data,\n",
    "    tokenizer=tokenizer,\n",
    "    shuffle=True,  # Shuffle key order each time\n",
    ")\n",
    "\n",
    "print(f\"OrigamiDataset (training):\")\n",
    "print(f\"  Size: {len(train_dataset)} objects\")\n",
    "print(f\"  Shuffle: {train_dataset.shuffle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key shuffling demonstration:\n",
      "Accessing the same object (index 0) multiple times:\n",
      "\n",
      "  Access 1: ['price', 'tags', 'category', 'seller']\n",
      "  Access 2: ['category', 'seller', 'name', 'rating']\n",
      "  Access 3: ['tags', 'category', 'price', 'seller']\n",
      "  Access 4: ['price', 'tags', 'category', 'seller']\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate key shuffling: access the same base object multiple times\n",
    "# Each access returns a TokenizedInstance with potentially different key order\n",
    "\n",
    "\n",
    "def extract_key_order(instance):\n",
    "    \"\"\"Extract the order of keys from token sequence.\"\"\"\n",
    "    from origami.tokenizer.vocabulary import KeyToken\n",
    "\n",
    "    keys = [t.key for t in instance.tokens if isinstance(t, KeyToken)]\n",
    "    # Only root-level keys (first 4 keys before nested ones)\n",
    "    return keys[:4]  # price, category, tags, seller (in some order)\n",
    "\n",
    "\n",
    "print(\"Key shuffling demonstration:\")\n",
    "print(\"Accessing the same object (index 0) multiple times:\\n\")\n",
    "\n",
    "for i in range(4):\n",
    "    inst = train_dataset[0]  # Always maps to base object 0\n",
    "    keys = extract_key_order(inst)\n",
    "    print(f\"  Access {i + 1}: {keys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Step 3a\n",
    "\n",
    "**What happened:**\n",
    "- `OrigamiDataset` wraps the preprocessed data with a tokenizer reference\n",
    "- With `shuffle=True`, `__getitem__` calls `tokenizer.tokenize(obj, shuffle=True)` on demand\n",
    "- Each access produces a fresh `TokenizedInstance` with randomized key order\n",
    "\n",
    "**Why key shuffling matters:**\n",
    "- JSON object keys have no inherent order\n",
    "- Without shuffling, model memorizes key positions instead of key semantics\n",
    "- Shuffling forces model to learn from key names, not positions\n",
    "\n",
    "**Data type:** Still `TokenizedInstance` (no tensors yet)\n",
    "\n",
    "**Next step:** Collation (batching + converting to tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Collation (OrigamiDataCollator)\n",
    "\n",
    "The collator converts a batch of `TokenizedInstance` objects into padded tensors ready for model input.\n",
    "\n",
    "**Input:** `list[TokenizedInstance]` - batch from DataLoader\n",
    "\n",
    "**Output:** `dict[str, Tensor]` with:\n",
    "- `input_ids`: Token IDs `(batch, seq_len)`\n",
    "- `path_types`: Path element types `(batch, seq_len, max_depth)`\n",
    "- `path_ids`: Path element IDs `(batch, seq_len, max_depth)`\n",
    "- `path_lengths`: Path depths `(batch, seq_len)`\n",
    "- `attention_mask`: Valid positions `(batch, seq_len)`\n",
    "- `labels`: Same as input_ids for autoregressive training\n",
    "- `numeric_values`, `numeric_mask`: For continuous head\n",
    "- `lengths`: Original sequence lengths\n",
    "\n",
    "**Key feature:** Uses **LEFT-PADDING** so all sequences end at the same position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of 8 TokenizedInstances:\n",
      "  [0] length=20 tokens  <- ['price', 'category', 'tags', 'seller']\n",
      "  [1] length=19 tokens  <- ['price', 'category', 'tags', 'seller']\n",
      "  [2] length=15 tokens  <- ['price', 'category', 'seller']\n",
      "  [3] length=18 tokens  <- ['price', 'category', 'tags', 'seller']\n",
      "  [4] length=19 tokens  <- ['price', 'tags', 'seller']\n",
      "  [5] length=12 tokens  <- ['price', 'category', 'tags']\n",
      "  [6] length=20 tokens  <- ['price', 'category', 'tags', 'seller']\n",
      "  [7] length=17 tokens  <- ['price', 'category', 'tags', 'seller']\n"
     ]
    }
   ],
   "source": [
    "from origami.training.collator import OrigamiDataCollator\n",
    "from origami.training.dataset import OrigamiDataset\n",
    "\n",
    "# Create a collator\n",
    "collator = OrigamiDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=None,  # No truncation\n",
    ")\n",
    "\n",
    "# Use OrigamiDataset with shuffle=False so we get deterministic sequences\n",
    "# This lets us see the varying lengths due to missing fields\n",
    "eval_dataset = OrigamiDataset(data=preprocessed_data, tokenizer=tokenizer, shuffle=False)\n",
    "\n",
    "# Get ALL 8 objects as a single batch\n",
    "batch_instances = [eval_dataset[i] for i in range(len(eval_dataset))]\n",
    "\n",
    "print(f\"Batch of {len(batch_instances)} TokenizedInstances:\")\n",
    "for i, inst in enumerate(batch_instances):\n",
    "    print(f\"  [{i}] length={len(inst):2d} tokens  <- {list(preprocessed_data[i].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collated batch (EncodedBatch dataclass):\n",
      "--------------------------------------------------\n",
      "  input_ids: shape=(8, 20), dtype=torch.int64\n",
      "  path_types: shape=(8, 20, 32), dtype=torch.int64\n",
      "  path_ids: shape=(8, 20, 32), dtype=torch.int64\n",
      "  path_lengths: shape=(8, 20), dtype=torch.int64\n",
      "  attention_mask: shape=(8, 20), dtype=torch.bool\n",
      "  numeric_values: shape=(8, 20), dtype=torch.float32\n",
      "  numeric_mask: shape=(8, 20), dtype=torch.bool\n",
      "  lengths: shape=(8,), dtype=torch.int64\n",
      "  labels: shape=(8, 20), dtype=torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Collate the batch into tensors\n",
    "batch = collator(batch_instances)\n",
    "\n",
    "print(\"Collated batch (EncodedBatch dataclass):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  input_ids: shape={tuple(batch.input_ids.shape)}, dtype={batch.input_ids.dtype}\")\n",
    "print(f\"  path_types: shape={tuple(batch.path_types.shape)}, dtype={batch.path_types.dtype}\")\n",
    "print(f\"  path_ids: shape={tuple(batch.path_ids.shape)}, dtype={batch.path_ids.dtype}\")\n",
    "print(f\"  path_lengths: shape={tuple(batch.path_lengths.shape)}, dtype={batch.path_lengths.dtype}\")\n",
    "print(\n",
    "    f\"  attention_mask: shape={tuple(batch.attention_mask.shape)}, dtype={batch.attention_mask.dtype}\"\n",
    ")\n",
    "print(\n",
    "    f\"  numeric_values: shape={tuple(batch.numeric_values.shape)}, dtype={batch.numeric_values.dtype}\"\n",
    ")\n",
    "print(f\"  numeric_mask: shape={tuple(batch.numeric_mask.shape)}, dtype={batch.numeric_mask.dtype}\")\n",
    "print(f\"  lengths: shape={tuple(batch.lengths.shape)}, dtype={batch.lengths.dtype}\")\n",
    "print(f\"  labels: shape={tuple(batch.labels.shape)}, dtype={batch.labels.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEFT-PADDING demonstration:\n",
      "================================================================================\n",
      "PAD=6, START=0, END=1, OBJ_START=2, OBJ_END=3, ARRAY_START=4, ARRAY_END=5\n",
      "\n",
      "[0] [0, 2, 10, 11, 12, 13, 14, 4, 15, 16, 5, 17, 2, 18, 19, 20, 21, 3, 3, 1]\n",
      "[1] [6, 0, 2, 10, 22, 12, 23, 14, 4, 24, 5, 17, 2, 18, 25, 20, 26, 3, 3, 1]\n",
      "[2] [6, 6, 6, 6, 6, 0, 2, 10, 27, 12, 13, 17, 2, 18, 19, 20, 21, 3, 3, 1]\n",
      "[3] [6, 6, 0, 2, 10, 28, 12, 29, 14, 4, 5, 17, 2, 18, 30, 20, 31, 3, 3, 1]\n",
      "[4] [6, 0, 2, 10, 22, 14, 4, 32, 33, 34, 5, 17, 2, 18, 25, 20, 26, 3, 3, 1]\n",
      "[5] [6, 6, 6, 6, 6, 6, 6, 6, 0, 2, 10, 11, 12, 13, 14, 4, 35, 5, 3, 1]\n",
      "[6] [0, 2, 10, 28, 12, 23, 14, 4, 32, 24, 5, 17, 2, 18, 30, 20, 31, 3, 3, 1]\n",
      "[7] [6, 6, 6, 0, 2, 10, 27, 12, 13, 14, 4, 15, 5, 17, 2, 18, 36, 3, 3, 1]\n",
      "\n",
      "All sequences end with token ID 1 (END) at the same position!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate LEFT-PADDING\n",
    "# Shorter sequences have PAD tokens at the START, content at the END\n",
    "# This allows batched prediction: logits[:, -1, :] gives next token for all sequences\n",
    "\n",
    "print(\"LEFT-PADDING demonstration:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"PAD=6, START=0, END=1, OBJ_START=2, OBJ_END=3, ARRAY_START=4, ARRAY_END=5\")\n",
    "print()\n",
    "\n",
    "input_ids = batch.input_ids\n",
    "\n",
    "for i in range(len(batch_instances)):\n",
    "    ids = input_ids[i].tolist()\n",
    "    print(f\"[{i}] {ids}\")\n",
    "\n",
    "print()\n",
    "print(\"All sequences end with token ID 1 (END) at the same position!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path encoding for sample 0:\n",
      "Object: {'price': 14250.0, 'category': 'sedan', 'tags': ['reliable', 'fuel-efficient'], 'seller': {'name': 'Alice', 'rating': 4.5}}\n",
      "\n",
      "Pos  TokID  Depth  Path Types (trimmed)  Path IDs (trimmed)\n",
      "---------------------------------------------------------------------------\n",
      "  0      0      0  [0, 0, 0]             [0, 0, 0]\n",
      "  1      2      0  [0, 0, 0]             [0, 0, 0]\n",
      "  2     10      0  [0, 0, 0]             [0, 0, 0]\n",
      "  3     11      1  [1, 0, 0]             [10, 0, 0]\n",
      "  4     12      0  [0, 0, 0]             [0, 0, 0]\n",
      "  5     13      1  [1, 0, 0]             [12, 0, 0]\n",
      "  6     14      0  [0, 0, 0]             [0, 0, 0]\n",
      "  7      4      1  [1, 0, 0]             [14, 0, 0]\n",
      "  8     15      2  [1, 2, 0]             [14, 0, 0]\n",
      "  9     16      2  [1, 2, 0]             [14, 1, 0]\n",
      " 10      5      1  [1, 0, 0]             [14, 0, 0]\n",
      " 11     17      0  [0, 0, 0]             [0, 0, 0]\n",
      " 12      2      1  [1, 0, 0]             [17, 0, 0]\n",
      " 13     18      1  [1, 0, 0]             [17, 0, 0]\n",
      " 14     19      2  [1, 1, 0]             [17, 18, 0]\n",
      " 15     20      1  [1, 0, 0]             [17, 0, 0]\n",
      " 16     21      2  [1, 1, 0]             [17, 20, 0]\n",
      " 17      3      1  [1, 0, 0]             [17, 0, 0]\n",
      " 18      3      0  [0, 0, 0]             [0, 0, 0]\n",
      " 19      1      0  [0, 0, 0]             [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Path encoding for sample 0\n",
    "# path_types: 0=pad, 1=key, 2=index\n",
    "# path_ids: vocab ID for keys, array index for indices\n",
    "# path_lengths: depth of path at each position\n",
    "\n",
    "sample_idx = 0\n",
    "print(f\"Path encoding for sample {sample_idx}:\")\n",
    "print(f\"Object: {preprocessed_data[sample_idx]}\")\n",
    "print()\n",
    "\n",
    "path_types = batch.path_types[sample_idx]\n",
    "path_ids = batch.path_ids[sample_idx]\n",
    "path_lengths = batch.path_lengths[sample_idx]\n",
    "ids = batch.input_ids[sample_idx]\n",
    "\n",
    "print(\n",
    "    f\"{'Pos':>3}  {'TokID':>5}  {'Depth':>5}  {'Path Types (trimmed)':20}  {'Path IDs (trimmed)'}\"\n",
    ")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for pos in range(len(ids)):\n",
    "    tok_id = ids[pos].item()\n",
    "    depth = path_lengths[pos].item()\n",
    "    types = path_types[pos, :3].tolist()  # Show first 3 elements\n",
    "    pids = path_ids[pos, :3].tolist()\n",
    "    print(f\"{pos:3d}  {tok_id:5d}  {depth:5d}  {str(types):20}  {pids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Step 3b\n",
    "\n",
    "**What happened:**\n",
    "- `OrigamiDataCollator` takes a list of `TokenizedInstance` objects\n",
    "- Converts tokens to integer IDs using the vocabulary\n",
    "- Encodes paths into `path_types`, `path_ids`, `path_lengths` tensors\n",
    "- Applies **LEFT-PADDING**: PAD tokens at start, content at end\n",
    "- Returns an `EncodedBatch` dataclass ready for `model.forward()`\n",
    "\n",
    "**Why LEFT-PADDING?**\n",
    "```\n",
    "Right-padding (standard):     Left-padding (Origami):\n",
    "[START, a, END, PAD, PAD]     [PAD, PAD, START, a, END]\n",
    "[START, a, b, c, END]         [START, a, b, c, END]\n",
    "       ↑                                         ↑\n",
    "  Ends differ                           All end at same position\n",
    "```\n",
    "With left-padding, `logits[:, -1, :]` gives the next-token prediction for ALL sequences in the batch simultaneously.\n",
    "\n",
    "**Data type transformation:**\n",
    "- Input: `list[TokenizedInstance]` (Python objects)\n",
    "- Output: `EncodedBatch` dataclass (PyTorch tensors with attribute access)\n",
    "\n",
    "**Next step:** Model forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: DataLoader\n",
    "\n",
    "In training, we use a PyTorch `DataLoader` to iterate over batches. The DataLoader:\n",
    "- Samples indices from the dataset\n",
    "- Calls `dataset[i]` for each index to get `TokenizedInstance` objects\n",
    "- Passes the list to the collator to produce the batch tensor dict\n",
    "\n",
    "Let's verify this produces the same output as calling the collator directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created:\n",
      "  Dataset size: 8\n",
      "  Batch size: 4\n",
      "  Number of batches: 2\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader with batch_size=4\n",
    "# This gives us 2 batches from our 8 samples\n",
    "data_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,  # Keep order deterministic for comparison\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created:\")\n",
    "print(f\"  Dataset size: {len(eval_dataset)}\")\n",
    "print(f\"  Batch size: 4\")\n",
    "print(f\"  Number of batches: {len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch from DataLoader:\n",
      "--------------------------------------------------\n",
      "  input_ids: shape=(4, 20)\n",
      "  path_types: shape=(4, 20, 32)\n",
      "  path_ids: shape=(4, 20, 32)\n",
      "  path_lengths: shape=(4, 20)\n",
      "  attention_mask: shape=(4, 20)\n",
      "  labels: shape=(4, 20)\n",
      "\n",
      "input_ids (first 4 samples, indices 0-3):\n",
      "[0] [0, 2, 10, 11, 12, 13, 14, 4, 15, 16, 5, 17, 2, 18, 19, 20, 21, 3, 3, 1]\n",
      "[1] [6, 0, 2, 10, 22, 12, 23, 14, 4, 24, 5, 17, 2, 18, 25, 20, 26, 3, 3, 1]\n",
      "[2] [6, 6, 6, 6, 6, 0, 2, 10, 27, 12, 13, 17, 2, 18, 19, 20, 21, 3, 3, 1]\n",
      "[3] [6, 6, 0, 2, 10, 28, 12, 29, 14, 4, 5, 17, 2, 18, 30, 20, 31, 3, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch from the DataLoader\n",
    "first_batch = next(iter(data_loader))\n",
    "\n",
    "print(\"First batch from DataLoader:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  input_ids: shape={tuple(first_batch.input_ids.shape)}\")\n",
    "print(f\"  path_types: shape={tuple(first_batch.path_types.shape)}\")\n",
    "print(f\"  path_ids: shape={tuple(first_batch.path_ids.shape)}\")\n",
    "print(f\"  path_lengths: shape={tuple(first_batch.path_lengths.shape)}\")\n",
    "print(f\"  attention_mask: shape={tuple(first_batch.attention_mask.shape)}\")\n",
    "print(f\"  labels: shape={tuple(first_batch.labels.shape)}\")\n",
    "\n",
    "print()\n",
    "print(\"input_ids (first 4 samples, indices 0-3):\")\n",
    "for i in range(4):\n",
    "    print(f\"[{i}] {first_batch.input_ids[i].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: DataLoader batch vs manual collation\n",
      "==================================================\n",
      "  input_ids: ✓\n",
      "  path_types: ✓\n",
      "  path_ids: ✓\n",
      "  path_lengths: ✓\n",
      "  attention_mask: ✓\n",
      "  numeric_values: ✓\n",
      "  numeric_mask: ✓\n",
      "  lengths: ✓\n",
      "  labels: ✓\n",
      "\n",
      "All tensors match! DataLoader + collator produces identical output.\n"
     ]
    }
   ],
   "source": [
    "# Compare with manually collated batch (first 4 samples)\n",
    "# The DataLoader batch should match samples 0-3 from our earlier batch of 8\n",
    "\n",
    "from dataclasses import fields\n",
    "\n",
    "import torch\n",
    "\n",
    "# Manually collate just the first 4 samples for comparison\n",
    "manual_batch = collator([eval_dataset[i] for i in range(4)])\n",
    "\n",
    "print(\"Comparison: DataLoader batch vs manual collation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_match = True\n",
    "for field in fields(first_batch):\n",
    "    key = field.name\n",
    "    first_val = getattr(first_batch, key)\n",
    "    manual_val = getattr(manual_batch, key)\n",
    "    if first_val is None and manual_val is None:\n",
    "        match = True\n",
    "    elif first_val is None or manual_val is None:\n",
    "        match = False\n",
    "    else:\n",
    "        match = torch.equal(first_val, manual_val)\n",
    "    status = \"✓\" if match else \"✗\"\n",
    "    print(f\"  {key}: {status}\")\n",
    "    if not match:\n",
    "        all_match = False\n",
    "\n",
    "print()\n",
    "if all_match:\n",
    "    print(\"All tensors match! DataLoader + collator produces identical output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Grammar Constraints (JSONGrammarPDA)\n",
    "\n",
    "During training, the model applies grammar constraints to mask out invalid tokens at each position. This ensures the model only learns to predict syntactically valid JSON.\n",
    "\n",
    "**Input:** `input_ids` tensor `(batch, seq_len)`\n",
    "\n",
    "**Output:** Boolean mask `(batch, seq_len, vocab_size)` where `True` = valid token for next position\n",
    "\n",
    "**Complexity:** O(n) for sequence length - single pass, no nested loops over positions.\n",
    "\n",
    "The grammar rules enforced:\n",
    "- After START: OBJ_START or ARRAY_START only\n",
    "- After OBJ_START: any key or OBJ_END\n",
    "- After key: value (primitive, OBJ_START, ARRAY_START)\n",
    "- After value in object: key or OBJ_END\n",
    "- After ARRAY_START: value or ARRAY_END\n",
    "- After root closes: END only\n",
    "- After END: PAD only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONGrammarPDA created:\n",
      "  Vocabulary size: 37\n",
      "  Max depth: 32\n",
      "  Number of keys: 7\n",
      "  Number of values: 23\n"
     ]
    }
   ],
   "source": [
    "from origami.constraints.json_grammar import JSONGrammarPDA\n",
    "\n",
    "# Create a grammar PDA with our vocabulary\n",
    "pda = JSONGrammarPDA(vocab=tokenizer.vocab, max_depth=32)\n",
    "\n",
    "print(f\"JSONGrammarPDA created:\")\n",
    "print(f\"  Vocabulary size: {tokenizer.vocab.size}\")\n",
    "print(f\"  Max depth: {pda.max_depth}\")\n",
    "print(f\"  Number of keys: {len(pda._key_ids)}\")\n",
    "print(f\"  Number of values: {len(pda._value_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar mask shape: torch.Size([8, 20, 37])\n",
      "  (batch=8, seq_len=20, vocab_size=37)\n",
      "\n",
      "mask[b, t, v] = True means token v is valid at position t+1 for sequence b\n"
     ]
    }
   ],
   "source": [
    "# Compute grammar mask for our batch\n",
    "# Use sample 0 (the full object with all fields)\n",
    "input_ids = batch.input_ids\n",
    "\n",
    "grammar_mask = pda.compute_valid_mask(input_ids)\n",
    "\n",
    "print(f\"Grammar mask shape: {grammar_mask.shape}\")\n",
    "print(\n",
    "    f\"  (batch={grammar_mask.shape[0]}, seq_len={grammar_mask.shape[1]}, vocab_size={grammar_mask.shape[2]})\"\n",
    ")\n",
    "print()\n",
    "print(\"mask[b, t, v] = True means token v is valid at position t+1 for sequence b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar mask for sample 5:\n",
      "Sequence: {'price': 14250.0, 'category': 'sedan', 'tags': ['budget']}\n",
      "\n",
      "Pos  Current Token                   Attn  # Valid  Valid Next Tokens\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "  0  GrammarToken('PAD')            False        1  GrammarToken('START')\n",
      "  1  GrammarToken('PAD')            False        1  GrammarToken('START')\n",
      "  2  GrammarToken('PAD')            False        1  GrammarToken('START')\n",
      "  3  GrammarToken('PAD')            False        1  GrammarToken('START')\n",
      "  4  GrammarToken('PAD')            False        1  GrammarToken('START')\n",
      "  5  GrammarToken('PAD')            False        1  GrammarToken('START')\n",
      "  6  GrammarToken('PAD')            False        1  GrammarToken('START')\n",
      "  7  GrammarToken('PAD')            False        1  GrammarToken('START')\n",
      "  8  GrammarToken('START')           True        2  GrammarToken('OBJ_START'), GrammarToken('ARRAY_START')\n",
      "  9  GrammarToken('OBJ_START')       True        8  GrammarToken('OBJ_END'), GrammarToken('UNK_KEY'), KeyToken('price'), KeyToken('category'), KeyToken('tags'), KeyToken('seller'), KeyToken('name'), KeyToken('rating')\n",
      " 10  KeyToken('price')               True       25  GrammarToken('OBJ_START'), GrammarToken('ARRAY_START'), GrammarToken('UNK_VALUE'), GrammarToken('NUM'), ValueToken(14250.0), ValueToken('sedan'), ValueToken('reliable'), ValueToken('fuel-efficient'), ValueToken('Alice'), ValueToken(4.5), ValueToken(26750.0), ValueToken('suv'), ValueToken('spacious'), ValueToken('Bob'), ValueToken(4.8), ValueToken(20000.0), ValueToken(37500.0), ValueToken('truck'), ValueToken('Carol'), ValueToken(4.2), ValueToken('luxury'), ValueToken('fast'), ValueToken('new'), ValueToken('budget'), ValueToken('Dave')\n",
      " 11  ValueToken(14250.0)             True        8  GrammarToken('OBJ_END'), GrammarToken('UNK_KEY'), KeyToken('price'), KeyToken('category'), KeyToken('tags'), KeyToken('seller'), KeyToken('name'), KeyToken('rating')\n",
      " 12  KeyToken('category')            True       25  GrammarToken('OBJ_START'), GrammarToken('ARRAY_START'), GrammarToken('UNK_VALUE'), GrammarToken('NUM'), ValueToken(14250.0), ValueToken('sedan'), ValueToken('reliable'), ValueToken('fuel-efficient'), ValueToken('Alice'), ValueToken(4.5), ValueToken(26750.0), ValueToken('suv'), ValueToken('spacious'), ValueToken('Bob'), ValueToken(4.8), ValueToken(20000.0), ValueToken(37500.0), ValueToken('truck'), ValueToken('Carol'), ValueToken(4.2), ValueToken('luxury'), ValueToken('fast'), ValueToken('new'), ValueToken('budget'), ValueToken('Dave')\n",
      " 13  ValueToken('sedan')             True        8  GrammarToken('OBJ_END'), GrammarToken('UNK_KEY'), KeyToken('price'), KeyToken('category'), KeyToken('tags'), KeyToken('seller'), KeyToken('name'), KeyToken('rating')\n",
      " 14  KeyToken('tags')                True       25  GrammarToken('OBJ_START'), GrammarToken('ARRAY_START'), GrammarToken('UNK_VALUE'), GrammarToken('NUM'), ValueToken(14250.0), ValueToken('sedan'), ValueToken('reliable'), ValueToken('fuel-efficient'), ValueToken('Alice'), ValueToken(4.5), ValueToken(26750.0), ValueToken('suv'), ValueToken('spacious'), ValueToken('Bob'), ValueToken(4.8), ValueToken(20000.0), ValueToken(37500.0), ValueToken('truck'), ValueToken('Carol'), ValueToken(4.2), ValueToken('luxury'), ValueToken('fast'), ValueToken('new'), ValueToken('budget'), ValueToken('Dave')\n",
      " 15  GrammarToken('ARRAY_START')     True       26  GrammarToken('OBJ_START'), GrammarToken('ARRAY_START'), GrammarToken('ARRAY_END'), GrammarToken('UNK_VALUE'), GrammarToken('NUM'), ValueToken(14250.0), ValueToken('sedan'), ValueToken('reliable'), ValueToken('fuel-efficient'), ValueToken('Alice'), ValueToken(4.5), ValueToken(26750.0), ValueToken('suv'), ValueToken('spacious'), ValueToken('Bob'), ValueToken(4.8), ValueToken(20000.0), ValueToken(37500.0), ValueToken('truck'), ValueToken('Carol'), ValueToken(4.2), ValueToken('luxury'), ValueToken('fast'), ValueToken('new'), ValueToken('budget'), ValueToken('Dave')\n",
      " 16  ValueToken('budget')            True       26  GrammarToken('OBJ_START'), GrammarToken('ARRAY_START'), GrammarToken('ARRAY_END'), GrammarToken('UNK_VALUE'), GrammarToken('NUM'), ValueToken(14250.0), ValueToken('sedan'), ValueToken('reliable'), ValueToken('fuel-efficient'), ValueToken('Alice'), ValueToken(4.5), ValueToken(26750.0), ValueToken('suv'), ValueToken('spacious'), ValueToken('Bob'), ValueToken(4.8), ValueToken(20000.0), ValueToken(37500.0), ValueToken('truck'), ValueToken('Carol'), ValueToken(4.2), ValueToken('luxury'), ValueToken('fast'), ValueToken('new'), ValueToken('budget'), ValueToken('Dave')\n",
      " 17  GrammarToken('ARRAY_END')       True        8  GrammarToken('OBJ_END'), GrammarToken('UNK_KEY'), KeyToken('price'), KeyToken('category'), KeyToken('tags'), KeyToken('seller'), KeyToken('name'), KeyToken('rating')\n",
      " 18  GrammarToken('OBJ_END')         True        1  GrammarToken('END')\n",
      " 19  GrammarToken('END')             True        1  GrammarToken('PAD')\n"
     ]
    }
   ],
   "source": [
    "# Visualize grammar mask for sample 5 (has left-padding)\n",
    "# Show: current token at position t, attention mask, and what tokens are valid for position t+1\n",
    "\n",
    "sample_idx = 5\n",
    "sample_ids = batch.input_ids[sample_idx]\n",
    "sample_mask = grammar_mask[sample_idx]\n",
    "sample_attn = batch.attention_mask[sample_idx]\n",
    "\n",
    "\n",
    "def get_valid_tokens(mask_row):\n",
    "    \"\"\"Get list of valid token IDs from a mask row.\"\"\"\n",
    "    return mask_row.nonzero(as_tuple=True)[0].tolist()\n",
    "\n",
    "\n",
    "def format_valid_tokens(valid_ids, vocab):\n",
    "    \"\"\"Format valid token IDs as readable string.\"\"\"\n",
    "    if len(valid_ids) == 0:\n",
    "        return \"(none)\"\n",
    "    return \", \".join(str(vocab.decode(tid)) for tid in valid_ids)\n",
    "\n",
    "\n",
    "print(f\"Grammar mask for sample {sample_idx}:\")\n",
    "print(f\"Sequence: {preprocessed_data[sample_idx]}\")\n",
    "print()\n",
    "print(f\"{'Pos':>3}  {'Current Token':<30} {'Attn':>5}  {'# Valid':>7}  {'Valid Next Tokens'}\")\n",
    "print(\"-\" * 130)\n",
    "\n",
    "for t in range(len(sample_ids)):\n",
    "    current_tok = vocab.decode(sample_ids[t].item())\n",
    "    attn = sample_attn[t].item()\n",
    "    valid_ids = get_valid_tokens(sample_mask[t])\n",
    "    valid_str = format_valid_tokens(valid_ids, vocab)\n",
    "    attn_str = \"True\" if attn else \"False\"\n",
    "    print(f\"{t:3d}  {str(current_tok):<30} {attn_str:>5}  {len(valid_ids):7d}  {valid_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Step 4\n",
    "\n",
    "**What happened:**\n",
    "- `JSONGrammarPDA.compute_valid_mask()` computes valid next-tokens for each position\n",
    "- Single loop over positions: O(n) complexity, vectorized over batch\n",
    "- Pre-computed mask patterns enable O(1) operations per position (no O(vocab_size) loops)\n",
    "\n",
    "**Grammar in action:**\n",
    "- After `START`: only `OBJ_START` or `ARRAY_START` valid (2 options)\n",
    "- After `OBJ_START`: any key or `OBJ_END` valid (~6 keys + 1 = 7 options)\n",
    "- After a key: any value or nested container valid (~21 values + 2 containers = 23 options)\n",
    "- After `END`: only `PAD` valid (1 option)\n",
    "\n",
    "**How it's used in training:**\n",
    "```python\n",
    "# In model._apply_grammar_mask():\n",
    "logits = logits.masked_fill(~grammar_mask, float(\"-inf\"))\n",
    "```\n",
    "Invalid tokens get `-inf` logits → 0 probability after softmax.\n",
    "\n",
    "**Training vs Inference:**\n",
    "- Training: Full mask computed for all positions (this O(n) method)\n",
    "- Inference: Grammar state updated incrementally, O(1) per generated token\n",
    "\n",
    "**Next step:** Model forward pass (embeddings → backbone → heads → loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "origami",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
