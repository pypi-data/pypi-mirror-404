Metadata-Version: 2.4
Name: vi-rag
Version: 0.1.4
Summary: Vietnamese Retrieval-Augmented Generation (RAG) Framework
Author-email: Vi-RAG Team <virag@example.com>
Project-URL: Homepage, https://github.com/NOT-erorr/PBL_2025_Vi-RAG_framework
Project-URL: Repository, https://github.com/NOT-erorr/PBL_2025_Vi-RAG_framework
Project-URL: Documentation, https://github.com/NOT-erorr/PBL_2025_Vi-RAG_framework/blob/main/README.md
Project-URL: Bug Tracker, https://github.com/NOT-erorr/PBL_2025_Vi-RAG_framework/issues
Keywords: rag,retrieval-augmented-generation,vietnamese-nlp,document-processing,vector-search,gemini,qdrant
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Text Processing :: Linguistic
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy<3.0.0,>=1.21.0
Requires-Dist: python-dotenv>=0.19.0
Requires-Dist: pypdf>=3.0.0
Requires-Dist: python-docx>=0.8.11
Requires-Dist: underthesea>=6.0.0
Requires-Dist: qdrant-client>=1.7.0
Requires-Dist: google-genai>=0.1.0
Requires-Dist: sentence-transformers>=2.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Provides-Extra: evaluation
Requires-Dist: ragas>=0.4.0; extra == "evaluation"
Requires-Dist: datasets>=4.0.0; extra == "evaluation"
Provides-Extra: all
Requires-Dist: vi-rag[dev,evaluation]; extra == "all"
Dynamic: license-file

# Vi-RAG Framework

**Vietnamese Retrieval-Augmented Generation Framework**

M·ªôt framework RAG to√†n di·ªán ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho ti·∫øng Vi·ªát, h·ªó tr·ª£ x·ª≠ l√Ω t√†i li·ªáu PDF, TXT, DOCX v·ªõi kh·∫£ nƒÉng chunking ph√¢n c·∫•p v√† t√¨m ki·∫øm ng·ªØ nghƒ©a.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## üåü T√≠nh NƒÉng Ch√≠nh

- ‚úÖ **H·ªó tr·ª£ ƒëa ƒë·ªãnh d·∫°ng**: PDF, TXT, DOCX
- ‚úÖ **Chunking th√¥ng minh**: Ph√¢n c·∫•p parent-child chunks v·ªõi overlap
- ‚úÖ **Vector Search**: T√≠ch h·ª£p Qdrant cho t√¨m ki·∫øm ng·ªØ nghƒ©a
- ‚úÖ **Gemini Integration**: S·ª≠ d·ª•ng Gemini API cho embedding v√† generation
- ‚úÖ **In-memory Caching**: Cache DocumentNode ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω
- ‚úÖ **Auto-chunking**: T·ª± ƒë·ªông load v√† chunk documents trong 1 b∆∞·ªõc
- ‚úÖ **Ti·∫øng Vi·ªát native**: ƒê∆∞·ª£c thi·∫øt k·∫ø t·ªëi ∆∞u cho ti·∫øng Vi·ªát

## üìÅ C·∫•u Tr√∫c Project

```
Vi-RAG/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ vi_rag/                  # Main package
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ core.py              # Core RAG functionality
‚îÇ       ‚îú‚îÄ‚îÄ utils.py             # Utility functions
‚îÇ       ‚îî‚îÄ‚îÄ py.typed             # Type hints marker
pyproject.toml               # Project configuration
README.md                    # This file
LICENSE                      # MIT License
.gitignore
```

## üöÄ C√†i ƒê·∫∑t Nhanh

### 0. C√†i ƒë·∫∑t Vi-RAG

```bash
pip install vi-rag
``` 

### 1. Clone Repository

```bash
git clone https://github.com/NOT-erorr/PBL_2025_Vi-RAG_framework.git
cd Vi-RAG
```

### 2. T·∫°o Virtual Environment

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 3. C·∫•u H√¨nh Environment Variables

```bash
# Copy file .env.example th√†nh .env
cp .env.example .env

# Ho·∫∑c tr√™n Windows
copy .env.example .env
```

Sau ƒë√≥, m·ªü file `.env` v√† ƒëi·ªÅn c√°c API keys c·ªßa b·∫°n:

```bash
# Required: Google Gemini API Key
GEMINI_API_KEY=your_gemini_api_key_here

# Required: Qdrant Vector Database Configuration  
QDRANT_API_KEY=your_qdrant_api_key_here
QDRANT_URL=your_qdrant_url_here
```

> **L∆∞u √Ω**: File `.env` ch·ª©a th√¥ng tin nh·∫°y c·∫£m v√† ƒë√£ ƒë∆∞·ª£c th√™m v√†o `.gitignore`. Kh√¥ng commit file n√†y l√™n Git!

**H∆∞·ªõng d·∫´n l·∫•y API keys:**
- **Gemini API Key**: [Google AI Studio](https://makersuite.google.com/app/apikey)
- **Qdrant**: [Qdrant Cloud](https://cloud.qdrant.io)


## üí° S·ª≠ D·ª•ng C∆° B·∫£n

### Example 1: Load v√† Chunk Document T·ª± ƒê·ªông

```python
from vi_rag import DocumentLoader

# Auto-chunking (khuy·∫øn ngh·ªã)
loader = DocumentLoader(
    "document.pdf",
    auto_chunk=True,
    parent_size=2000,
    child_size=400,
    overlap=50
)

# Load v√† chunk trong 1 b∆∞·ªõc
document, parents, children = loader.load_and_chunk()

print(f"Loaded: {document.title}")
print(f"Parent chunks: {len(parents)}")
print(f"Child chunks: {len(children)}")
```

### Example 2: Workflow Ho√†n Ch·ªânh RAG

```python
from vi_rag.ingestion import DocumentLoader
from vi_rag.models import GeminiEmbeddingModel, GeminiLLMClient
from vi_rag.retrieval import QdrantVectorStore
from vi_rag.config import settings
import uuid

# 1. Load v√† chunk document
loader = DocumentLoader("document.pdf", auto_chunk=True)
document, parents, children = loader.load_and_chunk()

# 2. Setup models (s·ª≠ d·ª•ng settings t·ª´ .env)
embedding_model = GeminiEmbeddingModel(
    settings.GEMINI_API_KEY, 
    output_dimensionality=settings.EMBEDDING_DIM
)
llm = GeminiLLMClient(settings.GEMINI_API_KEY, model_name="gemini-2.0-flash-exp")

# 3. Generate embeddings
child_texts = [child['text'] for child in children]
vectors = embedding_model.embed_documents(child_texts)

# 4. Setup v√† index v√†o vector store
vector_store = QdrantVectorStore(
    api_key=settings.QDRANT_API_KEY, 
    url=settings.QDRANT_URL
)
vector_store.connect()
vector_store.ensure_collection()

# Add IDs
for child in children:
    child['id'] = str(uuid.uuid4())

vector_store.add_vectors(
    vectors=vectors,
    payloads=children,
    ids=[c['id'] for c in children]
)

# 5. Query v√† generate answer
question = "T√†i li·ªáu n√†y n√≥i v·ªÅ g√¨?"
query_vector = embedding_model.embed_query(question)
results = vector_store.search(query_vector, top_k=settings.VECTOR_TOP_K)
context = "\n\n".join([r['text'] for r in results])

answer = llm.generate(query=question, context=context)
print(f"C√¢u h·ªèi: {question}")
print(f"Tr·∫£ l·ªùi: {answer}")
```

### Example 3: X·ª≠ L√Ω Document Cache

```python
from vi_rag.ingestion import DocumentLoader

loader = DocumentLoader("document.pdf")

# Check cache tr∆∞·ªõc khi load
cached = loader.check_document_loaded()
if cached:
    print("Document ƒë√£ ƒë∆∞·ª£c load tr∆∞·ªõc ƒë√≥!")
    document = cached
else:
    print("Loading document m·ªõi...")
    document = loader.load()
```

### Example 4: X·ª≠ L√Ω Nhi·ªÅu Documents

```python
from vi_rag.ingestion import DocumentLoader
import uuid

documents = ["doc1.pdf", "doc2.txt", "doc3.docx"]
all_children = []

# Load t·∫•t c·∫£ documents
for doc_path in documents:
    loader = DocumentLoader(doc_path, auto_chunk=True)
    doc, parents, children = loader.load_and_chunk()
    
    # Add source metadata
    for child in children:
        child['id'] = str(uuid.uuid4())
        child['source_file'] = doc_path
    
    all_children.extend(children)

print(f"Total chunks from all documents: {len(all_children)}")

# Embed v√† index t·∫•t c·∫£
texts = [c['text'] for c in all_children]
vectors = embedding_model.embed_documents(texts)
vector_store.add_vectors(vectors, all_children, [c['id'] for c in all_children])
```

### Example 5: Load Document Kh√¥ng Auto-Chunk

```python
from vi_rag.ingestion import DocumentLoader, HierarchicalChunker

# Load document only
loader = DocumentLoader("document.pdf", auto_chunk=False)
document, _, _ = loader.load_and_chunk()  # Empty lists returned

# Chunk th·ªß c√¥ng sau
chunker = HierarchicalChunker(
    parent_size=3000,  # Custom size
    child_size=500,
    overlap=100
)
parents, children = chunker.build_chunks(document)
```

### Example 6: Query v·ªõi Filtering

```python
from qdrant_client.models import Filter, FieldCondition, MatchValue

# Search v·ªõi filter theo source file
results = vector_store.client.search(
    collection_name=vector_store.collection_name,
    query_vector=query_vector,
    limit=5,
    query_filter=Filter(
        must=[
            FieldCondition(
                key="source_file",
                match=MatchValue(value="important_doc.pdf")
            )
        ]
    )
)
```

### Example 7: Multilingual - Ti·∫øng Vi·ªát

```python
from vi_rag.ingestion import DocumentLoader
from vi_rag.models import GeminiLLMClient
from vi_rag.config import settings

# Load Vietnamese document
loader = DocumentLoader("tai_lieu_tieng_viet.pdf", auto_chunk=True)
document, parents, children = loader.load_and_chunk()

# Query b·∫±ng ti·∫øng Vi·ªát
question = "N·ªôi dung ch√≠nh c·ªßa t√†i li·ªáu l√† g√¨?"
results = vector_store.search(query_vector, top_k=settings.VECTOR_TOP_K)
context = "\n\n".join([r['text'] for r in results])

# Generate v·ªõi instruction ti·∫øng Vi·ªát
llm = GeminiLLMClient(settings.GEMINI_API_KEY)
answer = llm.generate(
    query=question,
    context=context
)

print(f"Tr·∫£ l·ªùi: {answer}")
```

### Example 8: Batch Processing v·ªõi Retry

```python
from vi_rag.models import GeminiEmbeddingModel
from vi_rag.config import settings
import time

embedding_model = GeminiEmbeddingModel(settings.GEMINI_API_KEY)

def embed_with_retry(texts, max_retries=3):
    """Embed v·ªõi retry logic"""
    for attempt in range(max_retries):
        try:
            return embedding_model.embed_documents(texts)
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff
                print(f"Retry {attempt + 1}/{max_retries} sau {wait_time}s...")
                time.sleep(wait_time)
            else:
                raise e

# Batch processing
batch_size = 100
all_vectors = []

for i in range(0, len(child_texts), batch_size):
    batch = child_texts[i:i + batch_size]
    vectors = embed_with_retry(batch)
    all_vectors.extend(vectors)
    print(f"Processed {i + len(batch)}/{len(child_texts)}")
```

## üìñ V√≠ D·ª• Ho√†n Ch·ªânh

Th∆∞ m·ª•c `examples/` ch·ª©a c√°c v√≠ d·ª• ƒë·∫ßy ƒë·ªß v·ªÅ c√°ch s·ª≠ d·ª•ng Vi-RAG:

### Quick Start
```bash
python examples/quick_start.py
```

### Complete Workflow
```bash
python examples/complete_example.py
```

### Advanced Examples
```bash
python examples/advanced_examples.py
```

Xem chi ti·∫øt t·∫°i [examples/README.md](examples/README.md)

## üèóÔ∏è Ki·∫øn Tr√∫c H·ªá Th·ªëng

```mermaid

```

## üìä Key Components

### 1. Document Loading
- **PDFLoader**: X·ª≠ l√Ω PDF v·ªõi PyPDF ho·∫∑c PyMuPDF
- **TXTLoader**: H·ªó tr·ª£ nhi·ªÅu encoding
- **DOCXLoader**: X·ª≠ l√Ω Word documents
- **MD5 Caching**: T·ª± ƒë·ªông ph√°t hi·ªán duplicate documents

### 2. Chunking
- **HierarchicalChunker**: T·∫°o parent-child chunks
- **Configurable**: T√πy ch·ªânh size v√† overlap
- **Context Preservation**: Gi·ªØ ng·ªØ c·∫£nh qua overlap

### 3. Embedding
- **GeminiEmbeddingModel**: S·ª≠ d·ª•ng Gemini `embedding-001`
- **768 dimensions**: T·ªëi ∆∞u cho ti·∫øng Vi·ªát
- **Batch processing**: X·ª≠ l√Ω h√†ng lo·∫°t hi·ªáu qu·∫£

### 4. Vector Storage
- **QdrantVectorStore**: Integration v·ªõi Qdrant Cloud/Local
- **COSINE similarity**: ƒêo ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a
- **Metadata storage**: L∆∞u tr·ªØ th√¥ng tin b·ªï sung

### 5. Generation
- **GeminiLLMClient**: Multi-model support
- **PromptBuilder**: Template-based prompts
- **Context-aware**: Generate d·ª±a tr√™n retrieved context

## üß™ Testing

### Run Basic Tests

```bash
# Test document loading
python -m testing.code.demo.example_usage

# Test complete workflow
python -m testing.code.demo.complete_example
```

### Run Unit Tests (if available)

```bash
pytest tests/
```

## üìö Documentation

- **[QUICKSTART.md](QUICKSTART.md)**: H∆∞·ªõng d·∫´n b·∫Øt ƒë·∫ßu nhanh
- **[SYSTEM_LOGIC.md](docs/SYSTEM_LOGIC.md)**: Ki·∫øn tr√∫c chi ti·∫øt
- **[EVALUATION.md](docs/EVALUATION.md)**: ƒê√°nh gi√° v·ªõi RAGAS
- **[Workflow](\.agent\workflows\virag-workflow.md)**: Workflow ƒë·∫ßy ƒë·ªß

## üîß Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `GEMINI_API_KEY` | Gemini API key | Required |
| `QDRANT_API_KEY` | Qdrant API key | Required |
| `QDRANT_URL` | Qdrant server URL | Required |
| `QDRANT_COLLECTION_NAME` | Collection name | `rag_documents` |
| `EMBEDDING_DIM` | Embedding dimension | `768` |
| `QDRANT_VECTOR_DIM` | Vector dimension | `768` |
| `VECTOR_TOP_K` | Top K results | `5` |

### Chunking Parameters

```python
DocumentLoader(
    file_path="document.pdf",
    auto_chunk=True,
    parent_size=2000,    # Parent chunk size
    child_size=400,      # Child chunk size
    overlap=50           # Overlap between chunks
)
```

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## üìß Contact

- **Author**: Quoc Long
- **GitHub**: [NOT-erorr/PBL_2025_Vi-RAG_framework](https://github.com/NOT-erorr/PBL_2025_Vi-RAG_framework)

## üôè Acknowledgments

- Google Gemini API for embeddings and generation
- Qdrant for vector storage
- Contributors and testers

---

**Made with ‚ù§Ô∏è for Vietnamese NLP community**
