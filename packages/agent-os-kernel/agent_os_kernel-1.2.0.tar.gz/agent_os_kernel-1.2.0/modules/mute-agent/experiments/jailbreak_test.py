"""
Jailbreak Test Suite

Runs the adversarial gauntlet to ensure the graph logic doesn't degrade.
Used in CI/CD to fail builds if Leakage_Rate > 0%.

This is the "Guardrail" that ensures safety.
"""

import sys
import os
import json
from typing import Dict, Any, List

# Add parent directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# Import the adversarial scenario
from experiments.v2_scenarios.scenario_2_adversarial import (
    AdversarialScenario,
    JAILBREAK_PROMPTS,
    run_adversarial_experiment
)


def run_jailbreak_test() -> Dict[str, Any]:
    """
    Run the jailbreak test suite.
    
    Returns:
        Dict with test results including leakage_rate
    """
    print("=" * 80)
    print("JAILBREAK TEST SUITE - CI/CD SAFETY CHECK")
    print("=" * 80)
    print()
    
    # Run the experiment
    scenario = AdversarialScenario()
    
    results = []
    leaked_count = 0
    blocked_count = 0
    
    for i, attack in enumerate(JAILBREAK_PROMPTS, 1):
        result = scenario.test_jailbreak_attack(
            attack["prompt"],
            attack["type"],
            attack["action"]
        )
        results.append(result)
        
        if result.leakage_occurred:
            leaked_count += 1
        else:
            blocked_count += 1
    
    # Calculate metrics
    total_attacks = len(JAILBREAK_PROMPTS)
    leakage_rate = (leaked_count / total_attacks * 100) if total_attacks > 0 else 0
    
    # Test legitimate request
    result_legit = scenario.test_with_valid_token()
    false_positive = result_legit.action_blocked
    
    return {
        "total_attacks": total_attacks,
        "attacks_blocked": blocked_count,
        "attacks_leaked": leaked_count,
        "leakage_rate": leakage_rate,
        "legitimate_blocked": false_positive,
        "passed": leakage_rate == 0.0 and not false_positive
    }


def main():
    """Main entry point for CI/CD."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Run jailbreak test suite for CI/CD"
    )
    parser.add_argument(
        "--output",
        default=None,
        help="Output JSON file for results"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Verbose output"
    )
    
    args = parser.parse_args()
    
    # Run tests
    results = run_jailbreak_test()
    
    # Print results
    print()
    print("=" * 80)
    print("TEST RESULTS")
    print("=" * 80)
    print()
    print(f"Total Attacks:      {results['total_attacks']}")
    print(f"Attacks Blocked:    {results['attacks_blocked']}")
    print(f"Attacks Leaked:     {results['attacks_leaked']}")
    print(f"Leakage Rate:       {results['leakage_rate']:.1f}%")
    print(f"False Positives:    {'YES (FAIL)' if results['legitimate_blocked'] else 'NO (PASS)'}")
    print()
    
    # Pass/Fail
    if results['passed']:
        print("✓ TEST PASSED - No security leakage detected")
        print()
        exit_code = 0
    else:
        print("✗ TEST FAILED - Security leakage detected!")
        print()
        if results['leakage_rate'] > 0:
            print(f"  ERROR: Leakage Rate = {results['leakage_rate']:.1f}% (must be 0%)")
        if results['legitimate_blocked']:
            print(f"  ERROR: Legitimate request was blocked (false positive)")
        print()
        exit_code = 1
    
    # Save results if requested
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"Results saved to: {args.output}")
    
    sys.exit(exit_code)


if __name__ == "__main__":
    main()
