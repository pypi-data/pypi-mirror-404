% References for CaaS Paper

@inproceedings{lewis2020rag,
  title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{guu2020realm,
  title={REALM: Retrieval-Augmented Language Model Pre-Training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International Conference on Machine Learning},
  pages={3929--3938},
  year={2020}
}

@inproceedings{izacard2021leveraging,
  title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
  author={Izacard, Gautier and Grave, Edouard},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={874--880},
  year={2021}
}

@inproceedings{cohan2018hierarchical,
  title={A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents},
  author={Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
  booktitle={Proceedings of NAACL-HLT},
  pages={615--621},
  year={2018}
}

@inproceedings{liu2019hierarchical,
  title={Hierarchical Transformers for Multi-Document Summarization},
  author={Liu, Yang and Lapata, Mirella},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5070--5081},
  year={2019}
}

@inproceedings{kasai2022realtime,
  title={RealTime QA: What's the Answer Right Now?},
  author={Kasai, Jungo and Sakaguchi, Keisuke and Takahashi, Ronan and Le Bras, Ronan and Asai, Akari and Yu, Xinyan and Radev, Dragomir and Smith, Noah A and Choi, Yejin and Inui, Kentaro},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{lazaridou2021mind,
  title={Mind the Gap: Assessing Temporal Generalization in Neural Language Models},
  author={Lazaridou, Angeliki and Kuncoro, Adhiguna and Gribovskaya, Elena and Aber, Devang and Conneau, Alexis and Gonen, Hila and Cho, Jinhyuk and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29348--29363},
  year={2021}
}

@inproceedings{menick2022citation,
  title={Teaching Language Models to Support Answers with Verified Quotes},
  author={Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and McAleese, Nat},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{rashkin2021measuring,
  title={Measuring Attribution in Natural Language Generation Models},
  author={Rashkin, Hannah and Nikolaev, Vitaly and Lamm, Matthew and Aroyo, Lora and Collins, Michael and Das, Dipanjan and Petrov, Slav and Tomar, Gaurav Singh and Turc, Iulia and Reitter, David},
  journal={Computational Linguistics},
  year={2021}
}

@inproceedings{chevalier2023compression,
  title={Adapting Language Models to Compress Contexts},
  author={Chevalier, Alexis and Wettig, Alexander and Ajber, Anirudh and Chen, Danqi},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@inproceedings{dinan2019wizard,
  title={Wizard of Wikipedia: Knowledge-Powered Conversational Agents},
  author={Dinan, Emily and Roller, Stephen and Shuster, Kurt and Fan, Angela and Auli, Michael and Weston, Jason},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{zhang2020dialogpt,
  title={DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation},
  author={Zhang, Yizhe and Sun, Siqi and Galley, Michel and Chen, Yen-Chun and Brockett, Chris and Gao, Xiang and Gao, Jianfeng and Liu, Jingjing and Dolan, Bill},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={270--278},
  year={2020}
}

@article{wang2023selfrag,
  title={Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection},
  author={Wang, Liang and Yang, Nan and Wei, Furu},
  journal={arXiv preprint arXiv:2310.11511},
  year={2023}
}

@inproceedings{khattab2021baleen,
  title={Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval},
  author={Khattab, Omar and Potts, Christopher and Zaharia, Matei},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

% Accumulation Paradox References

@article{liu2023lost,
  title={Lost in the Middle: How Language Models Use Long Contexts},
  author={Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  year={2023},
  note={arXiv:2307.03172}
}

@inproceedings{xiao2024streaming,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  booktitle={International Conference on Learning Representations},
  year={2024},
  note={arXiv:2309.17453}
}

@article{li2024longcontext,
  title={Long-context LLMs Struggle with Long In-context Learning},
  author={Li, Tianle and Zhang, Ge and Do, Quy Duc and Yue, Xiang and Chen, Wenhu},
  journal={arXiv preprint arXiv:2404.02060},
  year={2024}
}

@article{packer2023memgpt,
  title={MemGPT: Towards LLMs as Operating Systems},
  author={Packer, Charles and Wooders, Sarah and Lin, Kevin and Fang, Vivian and Patil, Shishir G. and Stoica, Ion and Gonzalez, Joseph E.},
  journal={arXiv preprint arXiv:2310.08560},
  year={2023}
}
