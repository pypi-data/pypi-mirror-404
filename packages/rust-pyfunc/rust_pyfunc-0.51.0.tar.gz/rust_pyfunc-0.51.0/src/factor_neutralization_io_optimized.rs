use arrow::array::{Array, Float64Array, Int32Array, Int64Array, LargeStringArray, StringArray};
use chrono::Local;
use nalgebra::{DMatrix, DVector};
use parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder;
use pyo3::exceptions::PyRuntimeError;
use pyo3::prelude::*;
use rayon::prelude::*;
use std::collections::{HashMap, HashSet};
use std::fs;
use std::fs::File;
use std::io::{self, Write};
use std::path::{Path, PathBuf};
use std::sync::{
    atomic::{AtomicUsize, Ordering},
    Arc, Mutex,
};
use std::thread;
use std::time::{Duration, Instant};

/// I/Oä¼˜åŒ–çš„é£æ ¼æ•°æ®ç»“æ„
pub struct IOOptimizedStyleData {
    pub data_by_date: HashMap<i64, IOOptimizedStyleDayData>,
    // é¢„åŠ è½½çš„æ–‡ä»¶å†…å®¹ç¼“å­˜
    pub file_cache: Arc<Mutex<Vec<u8>>>,
}

/// I/Oä¼˜åŒ–çš„å•æ—¥é£æ ¼æ•°æ®
pub struct IOOptimizedStyleDayData {
    pub stocks: Vec<String>,
    pub style_matrix: DMatrix<f64>,
    pub regression_matrix: Option<Arc<DMatrix<f64>>>,
    pub stock_index_map: HashMap<String, usize>,
}

/// I/Oä¼˜åŒ–çš„å› å­æ•°æ®ç»“æ„ - æ”¯æŒæµå¼è¯»å–
pub struct IOOptimizedFactorData {
    pub dates: Vec<i64>,
    pub stocks: Vec<String>,
    pub values: DMatrix<f64>,
    pub stock_index_map: HashMap<String, usize>,
    // æ–‡ä»¶å…ƒæ•°æ®ç”¨äºå¿«é€Ÿè®¿é—®
    pub file_metadata: FactorFileMetadata,
}

/// å› å­æ–‡ä»¶å…ƒæ•°æ®
pub struct FactorFileMetadata {
    pub file_size: u64,
    pub row_count: usize,
    pub col_count: usize,
    pub has_nan_values: bool,
}

/// I/Oä¼˜åŒ–çš„æ–‡ä»¶æ‰¹é‡è¯»å–å™¨
pub struct BatchFileReader {
    // é¢„åˆ†é…çš„ç¼“å†²åŒº
    buffer_pool: Vec<Vec<u8>>,
    // å½“å‰å¯ç”¨ç¼“å†²åŒºç´¢å¼•
    available_buffers: Vec<usize>,
}

impl BatchFileReader {
    pub fn new(buffer_count: usize, buffer_size: usize) -> Self {
        let mut buffer_pool = Vec::with_capacity(buffer_count);
        let mut available_buffers = Vec::with_capacity(buffer_count);

        for i in 0..buffer_count {
            buffer_pool.push(vec![0u8; buffer_size]);
            available_buffers.push(i);
        }

        Self {
            buffer_pool,
            available_buffers,
        }
    }

    pub fn get_buffer(&mut self) -> Option<usize> {
        self.available_buffers.pop()
    }

    pub fn return_buffer(&mut self, index: usize) {
        if index < self.buffer_pool.len() {
            self.available_buffers.push(index);
        }
    }
}

impl IOOptimizedStyleData {
    /// I/Oä¼˜åŒ–çš„é£æ ¼æ•°æ®åŠ è½½ - ä½¿ç”¨ç¼“å†²è¯»å–å’Œé¢„åˆ†é…
    pub fn load_from_parquet_io_optimized(path: &str) -> PyResult<Self> {
        let start_time = Instant::now();
        println!("ğŸ”„ å¼€å§‹I/Oä¼˜åŒ–ç‰ˆé£æ ¼æ•°æ®åŠ è½½...");

        // è·å–æ–‡ä»¶å…ƒæ•°æ®ä»¥é¢„åˆ†é…å†…å­˜
        let file_metadata = fs::metadata(path)
            .map_err(|e| PyRuntimeError::new_err(format!("è·å–æ–‡ä»¶å…ƒæ•°æ®å¤±è´¥: {}", e)))?;
        let file_size = file_metadata.len();

        println!("ğŸ“ æ–‡ä»¶å¤§å°: {:.2}MB", file_size as f64 / 1024.0 / 1024.0);

        // ä½¿ç”¨æ ‡å‡†æ–‡ä»¶è¯»å–ï¼ˆparquetåº“ä¼˜åŒ–çš„è¯»å–æ–¹å¼ï¼‰
        let file = File::open(path)
            .map_err(|e| PyRuntimeError::new_err(format!("æ‰“å¼€é£æ ¼æ•°æ®æ–‡ä»¶å¤±è´¥: {}", e)))?;

        let builder = ParquetRecordBatchReaderBuilder::try_new(file)
            .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºparquetè¯»å–å™¨å¤±è´¥: {}", e)))?;

        // ä¼˜åŒ–æ‰¹å¤„ç†å¤§å° - æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´
        let optimal_batch_size = if file_size > 100 * 1024 * 1024 {
            // > 100MB
            32768 // å¤§æ–‡ä»¶ä½¿ç”¨æ›´å¤§çš„æ‰¹å¤„ç†
        } else if file_size > 10 * 1024 * 1024 {
            // > 10MB
            16384 // ä¸­ç­‰æ–‡ä»¶
        } else {
            8192 // å°æ–‡ä»¶
        };

        let reader = builder
            .with_batch_size(optimal_batch_size)
            .build()
            .map_err(|e| PyRuntimeError::new_err(format!("æ„å»ºè®°å½•æ‰¹æ¬¡è¯»å–å™¨å¤±è´¥: {}", e)))?;

        // é¢„åˆ†é…æ•°æ®ç»“æ„ä»¥å‡å°‘å†…å­˜åˆ†é…å¼€é”€
        let mut all_data = Vec::new();
        let mut total_rows = 0;

        // æ‰¹é‡è¯»å–æ‰€æœ‰æ•°æ®
        for batch_result in reader {
            let batch = batch_result
                .map_err(|e| PyRuntimeError::new_err(format!("è¯»å–è®°å½•æ‰¹æ¬¡å¤±è´¥: {}", e)))?;
            total_rows += batch.num_rows();
            all_data.push(batch);
        }

        println!(
            "ğŸ“Š è¯»å–å®Œæˆ: {}è¡Œæ•°æ®, {}ä¸ªæ‰¹æ¬¡",
            total_rows,
            all_data.len()
        );

        // é¢„åˆ†é…HashMapä»¥å‡å°‘é‡æ–°å“ˆå¸Œ
        let estimated_dates = (total_rows / 1000).max(100); // ä¼°ç®—æ—¥æœŸæ•°é‡
        let mut data_by_date: HashMap<i64, Vec<(String, Vec<f64>)>> =
            HashMap::with_capacity(estimated_dates);

        // ä½¿ç”¨å‘é‡åŒ–å¤„ç†ä¼˜åŒ–æ•°æ®è§£æ
        let parse_start = Instant::now();
        for batch in all_data {
            Self::process_batch_vectorized(&batch, &mut data_by_date)?;
        }
        let parse_time = parse_start.elapsed();

        println!("âš¡ æ•°æ®è§£æè€—æ—¶: {:.3}s", parse_time.as_secs_f64());

        // æ‰¹é‡è½¬æ¢ä¸ºæœ€ç»ˆæ•°æ®ç»“æ„
        let convert_start = Instant::now();
        let mut final_data_by_date = HashMap::with_capacity(data_by_date.len());
        let mut total_stocks_processed = 0;

        // å¹¶è¡Œå¤„ç†æ—¥æœŸæ•°æ®ï¼ˆå¦‚æœæ—¥æœŸæ•°é‡è¶³å¤Ÿå¤šï¼‰
        if data_by_date.len() > 10 {
            // å¤§é‡æ—¥æœŸæ—¶ä½¿ç”¨å¹¶è¡Œå¤„ç†
            let date_results: Vec<_> = data_by_date
                .into_par_iter()
                .filter_map(|(date, stock_data)| {
                    if stock_data.len() >= 12 {
                        if let Ok(day_data) = Self::convert_date_data_optimized(date, stock_data) {
                            Some((date, day_data))
                        } else {
                            None
                        }
                    } else {
                        None
                    }
                })
                .collect();

            for (date, day_data) in date_results {
                total_stocks_processed += day_data.stocks.len();
                final_data_by_date.insert(date, day_data);
            }
        } else {
            // å°‘é‡æ—¥æœŸæ—¶ä½¿ç”¨ä¸²è¡Œå¤„ç†
            for (date, stock_data) in data_by_date {
                if stock_data.len() >= 12 {
                    match Self::convert_date_data_optimized(date, stock_data) {
                        Ok(day_data) => {
                            total_stocks_processed += day_data.stocks.len();
                            final_data_by_date.insert(date, day_data);
                        }
                        Err(_) => {
                            println!("è­¦å‘Š: æ—¥æœŸ{}æ•°æ®è½¬æ¢å¤±è´¥", date);
                        }
                    }
                }
            }
        }

        let convert_time = convert_start.elapsed();
        println!("ğŸ”„ æ•°æ®è½¬æ¢è€—æ—¶: {:.3}s", convert_time.as_secs_f64());

        if final_data_by_date.is_empty() {
            return Err(PyRuntimeError::new_err(
                "é£æ ¼æ•°æ®ä¸ºç©ºæˆ–æ‰€æœ‰æ—¥æœŸçš„è‚¡ç¥¨æ•°é‡éƒ½å°‘äº12åª",
            ));
        }

        let total_time = start_time.elapsed();
        println!("âœ… I/Oä¼˜åŒ–ç‰ˆé£æ ¼æ•°æ®åŠ è½½å®Œæˆ!");
        println!(
            "   ğŸ“Š ç»Ÿè®¡: {}ä¸ªäº¤æ˜“æ—¥, {}åªè‚¡ç¥¨",
            final_data_by_date.len(),
            total_stocks_processed
        );
        println!("   â±ï¸  æ€»è€—æ—¶: {:.3}s", total_time.as_secs_f64());
        println!(
            "   ğŸš€ I/Oé€Ÿåº¦: {:.1}MB/s",
            file_size as f64 / 1024.0 / 1024.0 / total_time.as_secs_f64()
        );

        Ok(IOOptimizedStyleData {
            data_by_date: final_data_by_date,
            file_cache: Arc::new(Mutex::new(Vec::new())),
        })
    }

    /// å‘é‡åŒ–æ‰¹å¤„ç†æ•°æ®è§£æ
    fn process_batch_vectorized(
        batch: &arrow::record_batch::RecordBatch,
        data_by_date: &mut HashMap<i64, Vec<(String, Vec<f64>)>>,
    ) -> PyResult<()> {
        let date_column = batch.column(0);

        // æ‰¹é‡è§£ææ—¥æœŸåˆ—
        let batch_dates: Vec<i64> =
            if let Some(date_array_i64) = date_column.as_any().downcast_ref::<Int64Array>() {
                (0..date_array_i64.len())
                    .map(|i| date_array_i64.value(i))
                    .collect()
            } else if let Some(date_array_i32) = date_column.as_any().downcast_ref::<Int32Array>() {
                (0..date_array_i32.len())
                    .map(|i| date_array_i32.value(i) as i64)
                    .collect()
            } else {
                return Err(PyRuntimeError::new_err(
                    "æ—¥æœŸåˆ—ç±»å‹é”™è¯¯ï¼šæœŸæœ›Int64æˆ–Int32ç±»å‹",
                ));
            };

        // æ”¯æŒStringArrayå’ŒLargeStringArrayä¸¤ç§ç±»å‹
        let stock_column = batch.column(1);
        let get_stock_value = |row_idx: usize| -> String {
            if let Some(string_array) = stock_column.as_any().downcast_ref::<StringArray>() {
                string_array.value(row_idx).to_string()
            } else if let Some(large_string_array) =
                stock_column.as_any().downcast_ref::<LargeStringArray>()
            {
                large_string_array.value(row_idx).to_string()
            } else {
                panic!("è‚¡ç¥¨ä»£ç åˆ—ç±»å‹é”™è¯¯ï¼šæœŸæœ›StringArrayæˆ–LargeStringArrayç±»å‹");
            }
        };

        // æ‰¹é‡æå–é£æ ¼å› å­åˆ—å¼•ç”¨
        let style_columns: Result<Vec<&Float64Array>, _> = (2..13)
            .map(|i| {
                batch
                    .column(i)
                    .as_any()
                    .downcast_ref::<Float64Array>()
                    .ok_or_else(|| PyRuntimeError::new_err(format!("é£æ ¼å› å­åˆ—{}ç±»å‹é”™è¯¯", i - 2)))
            })
            .collect();
        let style_columns = style_columns?;

        // å‘é‡åŒ–å¤„ç†æ¯ä¸€è¡Œ
        for row_idx in 0..batch.num_rows() {
            let date = batch_dates[row_idx];
            let stock = get_stock_value(row_idx);

            // ä½¿ç”¨è¿­ä»£å™¨å’Œcollectä¼˜åŒ–é£æ ¼å€¼æå–
            let style_values: Vec<f64> = style_columns
                .iter()
                .map(|col| {
                    if col.is_null(row_idx) {
                        f64::NAN
                    } else {
                        col.value(row_idx)
                    }
                })
                .collect();

            data_by_date
                .entry(date)
                .or_insert_with(Vec::new)
                .push((stock, style_values));
        }

        Ok(())
    }

    /// ä¼˜åŒ–çš„å•æ—¥æ•°æ®è½¬æ¢
    fn convert_date_data_optimized(
        _date: i64,
        stock_data: Vec<(String, Vec<f64>)>,
    ) -> PyResult<IOOptimizedStyleDayData> {
        let n_stocks = stock_data.len();

        // é¢„åˆ†é…æ‰€æœ‰æ•°æ®ç»“æ„
        let mut stocks = Vec::with_capacity(n_stocks);
        let mut stock_index_map = HashMap::with_capacity(n_stocks);
        let mut style_matrix = DMatrix::zeros(n_stocks, 12);

        // å•æ¬¡éå†å¡«å……æ‰€æœ‰æ•°æ®ç»“æ„
        for (i, (stock, style_values)) in stock_data.into_iter().enumerate() {
            stock_index_map.insert(stock.clone(), i);
            stocks.push(stock);

            // ç›´æ¥å†™å…¥çŸ©é˜µï¼ˆé¿å…è¾¹ç•Œæ£€æŸ¥ï¼‰
            unsafe {
                for j in 0..11 {
                    *style_matrix.get_unchecked_mut((i, j)) = style_values[j];
                }
                *style_matrix.get_unchecked_mut((i, 11)) = 1.0;
            }
        }

        // é¢„è®¡ç®—å›å½’çŸ©é˜µ
        let regression_matrix = compute_regression_matrix_io_optimized(&style_matrix)?;

        Ok(IOOptimizedStyleDayData {
            stocks,
            style_matrix,
            regression_matrix: Some(Arc::new(regression_matrix)),
            stock_index_map,
        })
    }
}

/// I/Oä¼˜åŒ–çš„å›å½’çŸ©é˜µè®¡ç®—
fn compute_regression_matrix_io_optimized(style_matrix: &DMatrix<f64>) -> PyResult<DMatrix<f64>> {
    let xt = style_matrix.transpose();
    let xtx = &xt * style_matrix;

    let xtx_inv = xtx
        .try_inverse()
        .ok_or_else(|| PyRuntimeError::new_err("é£æ ¼å› å­çŸ©é˜µä¸å¯é€†ï¼Œå¯èƒ½å­˜åœ¨å¤šé‡å…±çº¿æ€§"))?;

    Ok(xtx_inv * xt)
}

/// I/Oä¼˜åŒ–çš„å› å­æ–‡ä»¶åŠ è½½
fn load_factor_file_io_optimized(
    file_path: &Path,
    log_detailed: bool,
) -> PyResult<IOOptimizedFactorData> {
    let start_time = Instant::now();

    // è·å–æ–‡ä»¶å…ƒæ•°æ®
    let file_metadata = fs::metadata(file_path)
        .map_err(|e| PyRuntimeError::new_err(format!("è·å–æ–‡ä»¶å…ƒæ•°æ®å¤±è´¥: {}", e)))?;
    let file_size = file_metadata.len();

    let file = File::open(file_path).map_err(|e| {
        PyRuntimeError::new_err(format!("æ‰“å¼€å› å­æ–‡ä»¶å¤±è´¥ {}: {}", file_path.display(), e))
    })?;

    // å‡†å¤‡ä½¿ç”¨I/Oä¼˜åŒ–çš„parquetè¯»å–

    let builder = ParquetRecordBatchReaderBuilder::try_new(file)
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºparquetè¯»å–å™¨å¤±è´¥: {}", e)))?;

    // è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°
    let batch_size = if file_size > 100 * 1024 * 1024 {
        32768
    } else if file_size > 10 * 1024 * 1024 {
        16384
    } else {
        8192
    };

    let reader = builder
        .with_batch_size(batch_size)
        .build()
        .map_err(|e| PyRuntimeError::new_err(format!("æ„å»ºè®°å½•æ‰¹æ¬¡è¯»å–å™¨å¤±è´¥: {}", e)))?;

    // é¢„åŠ è½½æ‰€æœ‰æ‰¹æ¬¡
    let mut all_batches = Vec::new();
    let mut total_rows = 0;
    for batch_result in reader {
        let batch = batch_result
            .map_err(|e| PyRuntimeError::new_err(format!("è¯»å–è®°å½•æ‰¹æ¬¡å¤±è´¥: {}", e)))?;
        total_rows += batch.num_rows();
        all_batches.push(batch);
    }

    if all_batches.is_empty() {
        return Err(PyRuntimeError::new_err("å› å­æ–‡ä»¶ä¸ºç©º"));
    }

    // è§£æschemaå’Œåˆ—æ˜ å°„
    let schema = all_batches[0].schema();
    let total_columns = schema.fields().len();
    let last_field = &schema.fields()[total_columns - 1];

    let (date_col_idx, stocks) = if last_field.name() == "date" {
        let stocks: Vec<String> = schema
            .fields()
            .iter()
            .take(total_columns - 1)
            .map(|f| f.name().clone())
            .collect();
        (total_columns - 1, stocks)
    } else {
        let stocks: Vec<String> = schema
            .fields()
            .iter()
            .skip(1)
            .map(|f| f.name().clone())
            .collect();
        (0, stocks)
    };

    let n_stocks = stocks.len();

    // é¢„åˆ†é…ç»“æœæ•°æ®ç»“æ„
    let mut all_data = Vec::with_capacity(total_rows);
    let mut dates = Vec::with_capacity(total_rows);
    let mut has_nan = false;

    // åˆ›å»ºè‚¡ç¥¨ç´¢å¼•æ˜ å°„
    let stock_index_map: HashMap<String, usize> = stocks
        .iter()
        .enumerate()
        .map(|(idx, stock)| (stock.clone(), idx))
        .collect();

    // é¢„æ„å»ºåˆ—æ˜ å°„
    let stock_col_map: HashMap<usize, usize> = (0..n_stocks)
        .filter_map(|stock_idx| {
            schema
                .fields()
                .iter()
                .position(|f| f.name() == &stocks[stock_idx])
                .map(|col_idx| (stock_idx, col_idx))
        })
        .collect();

    // å¹¶è¡Œå¤„ç†æ‰¹æ¬¡æ•°æ®ï¼ˆå¦‚æœæ‰¹æ¬¡æ•°é‡è¶³å¤Ÿå¤šï¼‰
    if all_batches.len() > 4 {
        // ä½¿ç”¨å¹¶è¡Œå¤„ç†
        let batch_results: Vec<_> = all_batches
            .into_par_iter()
            .map(|batch| {
                process_factor_batch_optimized(&batch, date_col_idx, &stock_col_map, n_stocks)
            })
            .collect();

        // åˆå¹¶ç»“æœ
        for result in batch_results {
            let (batch_data, batch_dates, batch_has_nan) = result?;
            all_data.extend(batch_data);
            dates.extend(batch_dates);
            has_nan = has_nan || batch_has_nan;
        }
    } else {
        // ä½¿ç”¨ä¸²è¡Œå¤„ç†
        for batch in all_batches {
            let (batch_data, batch_dates, batch_has_nan) =
                process_factor_batch_optimized(&batch, date_col_idx, &stock_col_map, n_stocks)?;
            all_data.extend(batch_data);
            dates.extend(batch_dates);
            has_nan = has_nan || batch_has_nan;
        }
    }

    // æ„å»ºæœ€ç»ˆçŸ©é˜µ
    let n_dates = dates.len();
    let mut values = DMatrix::zeros(n_dates, n_stocks);

    for (date_idx, row_values) in all_data.into_iter().enumerate() {
        for (stock_idx, value) in row_values.into_iter().enumerate() {
            values[(date_idx, stock_idx)] = value;
        }
    }

    let load_time = start_time.elapsed();
    let mb_per_sec = (file_size as f64 / 1024.0 / 1024.0) / load_time.as_secs_f64();

    // æ ¹æ®log_detailedå‚æ•°å†³å®šæ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
    if log_detailed {
        println!(
            "âœ… I/Oä¼˜åŒ–å› å­æ–‡ä»¶åŠ è½½: {}, {}è¡Œx{}åˆ—, {:.3}s, {:.1}MB/s",
            file_path.file_name().unwrap().to_string_lossy(),
            n_dates,
            n_stocks,
            load_time.as_secs_f64(),
            mb_per_sec
        );
    }

    Ok(IOOptimizedFactorData {
        dates,
        stocks,
        values,
        stock_index_map,
        file_metadata: FactorFileMetadata {
            file_size,
            row_count: n_dates,
            col_count: n_stocks,
            has_nan_values: has_nan,
        },
    })
}

/// ä¼˜åŒ–çš„æ‰¹æ¬¡å¤„ç†å‡½æ•°
fn process_factor_batch_optimized(
    batch: &arrow::record_batch::RecordBatch,
    date_col_idx: usize,
    stock_col_map: &HashMap<usize, usize>,
    n_stocks: usize,
) -> PyResult<(Vec<Vec<f64>>, Vec<i64>, bool)> {
    let date_column = batch.column(date_col_idx);

    let batch_dates: Vec<i64> =
        if let Some(date_array_i64) = date_column.as_any().downcast_ref::<Int64Array>() {
            (0..date_array_i64.len())
                .map(|i| date_array_i64.value(i))
                .collect()
        } else if let Some(date_array_i32) = date_column.as_any().downcast_ref::<Int32Array>() {
            (0..date_array_i32.len())
                .map(|i| date_array_i32.value(i) as i64)
                .collect()
        } else {
            return Err(PyRuntimeError::new_err(
                "æ—¥æœŸåˆ—ç±»å‹é”™è¯¯ï¼šæœŸæœ›Int64æˆ–Int32ç±»å‹",
            ));
        };

    let num_rows = batch.num_rows();

    // é¢„è·å–æ‰€æœ‰è‚¡ç¥¨åˆ—çš„å¼•ç”¨
    let mut stock_arrays: Vec<(usize, &Float64Array)> = Vec::with_capacity(stock_col_map.len());
    for (&stock_idx, &col_idx) in stock_col_map.iter() {
        let array = batch.column(col_idx);
        if let Some(float_array) = array.as_any().downcast_ref::<Float64Array>() {
            stock_arrays.push((stock_idx, float_array));
        }
    }

    let mut batch_data = Vec::with_capacity(num_rows);
    let mut has_nan = false;

    for row_idx in 0..num_rows {
        let mut row_values = vec![f64::NAN; n_stocks];

        // å‘é‡åŒ–å¤„ç†è¡Œæ•°æ®
        for &(stock_idx, float_array) in &stock_arrays {
            if !float_array.is_null(row_idx) {
                row_values[stock_idx] = float_array.value(row_idx);
            } else {
                has_nan = true;
            }
        }

        batch_data.push(row_values);
    }

    Ok((batch_data, batch_dates, has_nan))
}

/// I/Oä¼˜åŒ–çš„æˆªé¢æ’åº
fn cross_section_rank_io_optimized(values: &[f64]) -> Vec<f64> {
    let n = values.len();

    // é¢„åˆ†é…ç´¢å¼•å‘é‡
    let mut indexed_values = Vec::with_capacity(n);
    for (i, &v) in values.iter().enumerate() {
        if !v.is_nan() {
            indexed_values.push((i, v));
        }
    }

    // ä½¿ç”¨ä¸ç¨³å®šæ’åºæé«˜æ€§èƒ½
    indexed_values
        .sort_unstable_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));

    let mut ranks = vec![f64::NAN; n];

    // æ‰¹é‡èµ‹å€¼ranks
    for (rank, &(original_idx, _)) in indexed_values.iter().enumerate() {
        ranks[original_idx] = (rank + 1) as f64;
    }

    ranks
}

/// æ ¼å¼åŒ–æŒç»­æ—¶é—´ä¸º"å‡ å°æ—¶å‡ åˆ†é’Ÿå‡ ç§’"æ ¼å¼
fn format_duration(total_seconds: u64) -> String {
    let hours = total_seconds / 3600;
    let minutes = (total_seconds % 3600) / 60;
    let seconds = total_seconds % 60;

    if hours > 0 {
        format!("{}å°æ—¶{}åˆ†é’Ÿ{}ç§’", hours, minutes, seconds)
    } else if minutes > 0 {
        format!("{}åˆ†é’Ÿ{}ç§’", minutes, seconds)
    } else {
        format!("{}ç§’", seconds)
    }
}

/// I/Oä¼˜åŒ–çš„æ‰¹é‡å› å­ä¸­æ€§åŒ–å‡½æ•°
#[pyfunction]
pub fn batch_factor_neutralization_io_optimized(
    style_data_path: &str,
    factor_files_dir: &str,
    output_dir: &str,
    num_threads: Option<usize>,
    log_detailed: Option<bool>,
) -> PyResult<()> {
    let start_time = Instant::now();
    println!("ğŸš€ å¼€å§‹I/Oä¼˜åŒ–ç‰ˆæ‰¹é‡å› å­ä¸­æ€§åŒ–å¤„ç†...");

    // ä½¿ç”¨I/Oä¼˜åŒ–ç‰ˆæœ¬åŠ è½½é£æ ¼æ•°æ®
    println!("ğŸ“– æ­£åœ¨ä½¿ç”¨I/Oä¼˜åŒ–åŠ è½½é£æ ¼æ•°æ®...");
    let style_data = Arc::new(IOOptimizedStyleData::load_from_parquet_io_optimized(
        style_data_path,
    )?);

    // è·å–æ‰€æœ‰å› å­æ–‡ä»¶å¹¶æŒ‰å¤§å°æ’åºä»¥ä¼˜åŒ–å¤„ç†é¡ºåº
    let factor_dir = Path::new(factor_files_dir);
    let mut factor_files_with_size: Vec<(PathBuf, u64)> = fs::read_dir(factor_dir)
        .map_err(|e| PyRuntimeError::new_err(format!("è¯»å–å› å­ç›®å½•å¤±è´¥: {}", e)))?
        .filter_map(|entry| {
            let entry = entry.ok()?;
            let path = entry.path();
            if path.extension().and_then(|s| s.to_str()) == Some("parquet") {
                if let Ok(metadata) = fs::metadata(&path) {
                    Some((path, metadata.len()))
                } else {
                    Some((path, 0))
                }
            } else {
                None
            }
        })
        .collect();

    // æŒ‰æ–‡ä»¶å¤§å°æ’åº - å…ˆå¤„ç†å¤§æ–‡ä»¶ï¼Œåå¤„ç†å°æ–‡ä»¶ï¼ˆæ›´å¥½çš„è´Ÿè½½å¹³è¡¡ï¼‰
    factor_files_with_size.sort_unstable_by(|a, b| b.1.cmp(&a.1));
    let factor_files: Vec<PathBuf> = factor_files_with_size
        .into_iter()
        .map(|(path, _)| path)
        .collect();

    let total_files = factor_files.len();
    println!("ğŸ“ æ‰¾åˆ°{}ä¸ªå› å­æ–‡ä»¶ï¼ˆå·²æŒ‰å¤§å°æ’åºï¼‰", total_files);

    if total_files == 0 {
        return Err(PyRuntimeError::new_err("æœªæ‰¾åˆ°ä»»ä½•parquetå› å­æ–‡ä»¶"));
    }

    // åˆ›å»ºè¿›åº¦è®¡æ•°å™¨
    let processed_files = Arc::new(AtomicUsize::new(0));
    let error_files = Arc::new(AtomicUsize::new(0));

    // å¯åŠ¨è¿›åº¦ç›‘æ§çº¿ç¨‹
    let progress_counter = Arc::clone(&processed_files);
    let error_counter = Arc::clone(&error_files);
    let monitor_start_time = start_time;
    let progress_handle = thread::spawn(move || {
        loop {
            thread::sleep(Duration::from_secs(60));
            let processed = progress_counter.load(Ordering::Relaxed);
            let errors = error_counter.load(Ordering::Relaxed);
            let elapsed = monitor_start_time.elapsed();

            if processed >= total_files {
                break;
            }

            let success_count = processed - errors;
            let progress_percent = (processed as f64 / total_files as f64) * 100.0;
            let elapsed_minutes = elapsed.as_secs_f64() / 60.0;

            let estimated_total_minutes = if progress_percent > 0.0 {
                elapsed_minutes * 100.0 / progress_percent
            } else {
                0.0
            };
            let estimated_remaining_minutes = estimated_total_minutes - elapsed_minutes;

            // æ ¼å¼åŒ–å·²ç”¨æ—¶é—´
            let elapsed_seconds = elapsed.as_secs();
            let elapsed_time_str = format_duration(elapsed_seconds);

            // æ ¼å¼åŒ–é¢„è®¡å‰©ä½™æ—¶é—´
            let remaining_seconds = (estimated_remaining_minutes.max(0.0) * 60.0) as u64;
            let remaining_time_str = format_duration(remaining_seconds);

            // æ˜¾ç¤ºè¿›åº¦ï¼šæœ‰å¤„ç†è¿›å±•æˆ–è€…å·²ç»è¿è¡Œè¶…è¿‡5ç§’
            if processed > 0 || elapsed.as_secs() >= 5 {
                let current_time = Local::now().format("%Y-%m-%d %H:%M:%S");
                print!("\r[{}] ğŸ“Š å¤„ç†è¿›åº¦: {}/{} ({:.1}%) - æˆåŠŸ: {}, å¤±è´¥: {} - å·²ç”¨æ—¶é—´: {} - é¢„è®¡å‰©ä½™: {}", current_time, processed, total_files, progress_percent, success_count, errors, elapsed_time_str, remaining_time_str);
                io::stdout().flush().unwrap();
            }
        }
    });

    // åˆ›å»ºè¾“å‡ºç›®å½•
    fs::create_dir_all(output_dir)
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {}", e)))?;

    // ä¼˜åŒ–çº¿ç¨‹æ•°é…ç½®
    let optimal_threads = if let Some(threads) = num_threads {
        threads
    } else {
        // åŸºäºç³»ç»Ÿèµ„æºå’Œæ–‡ä»¶æ•°é‡è‡ªåŠ¨é€‰æ‹©çº¿ç¨‹æ•°
        let cpu_threads = rayon::current_num_threads();
        let memory_gb = sys_info::mem_info()
            .map(|info| info.total / 1024 / 1024)
            .unwrap_or(8);
        let memory_based_threads = (memory_gb / 2).min(16).max(1) as usize; // æ¯2GBå†…å­˜1ä¸ªçº¿ç¨‹

        std::cmp::min(
            std::cmp::min(cpu_threads, memory_based_threads),
            total_files,
        )
    };

    println!("âš¡ ä½¿ç”¨{}ä¸ªçº¿ç¨‹è¿›è¡ŒI/Oä¼˜åŒ–å¹¶è¡Œå¤„ç†", optimal_threads);

    // åˆ›å»ºI/Oä¼˜åŒ–çš„çº¿ç¨‹æ± 
    let pool = rayon::ThreadPoolBuilder::new()
        .num_threads(optimal_threads)
        .thread_name(|index| format!("io-optimized-worker-{}", index))
        .build()
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºçº¿ç¨‹æ± å¤±è´¥: {}", e)))?;

    // ä½¿ç”¨I/Oä¼˜åŒ–ç‰ˆæœ¬å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶
    let processed_counter = Arc::clone(&processed_files);
    let error_counter = Arc::clone(&error_files);

    let results: Vec<_> = pool.install(|| {
        factor_files
            .into_par_iter()
            .map(|file_path| {
                let style_data = Arc::clone(&style_data);
                let output_dir = Path::new(output_dir);
                let processed_counter = Arc::clone(&processed_counter);
                let error_counter = Arc::clone(&error_counter);

                let file_start_time = Instant::now();
                let result = (|| -> PyResult<()> {
                    // ä½¿ç”¨I/Oä¼˜åŒ–ç‰ˆæœ¬åŠ è½½å› å­æ•°æ®
                    let factor_data =
                        load_factor_file_io_optimized(&file_path, log_detailed.unwrap_or(false))?;

                    // æ‰§è¡Œä¸­æ€§åŒ–å¤„ç†
                    let neutralized_result =
                        neutralize_single_factor_io_optimized(factor_data, &style_data)?;

                    // æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
                    let output_filename = file_path
                        .file_name()
                        .ok_or_else(|| PyRuntimeError::new_err("æ— æ•ˆçš„æ–‡ä»¶å"))?;
                    let output_path = output_dir.join(output_filename);

                    // ä¿å­˜ç»“æœ
                    save_neutralized_result_io_optimized(neutralized_result, &output_path)?;

                    Ok(())
                })();

                // æ¡ä»¶åŒ–è¯¦ç»†æ—¥å¿—è¾“å‡º
                if log_detailed.unwrap_or(false) {
                    let file_time = file_start_time.elapsed();
                    if let Err(e) = &result {
                        eprintln!(
                            "âŒ I/Oä¼˜åŒ–å¤„ç†å¤±è´¥: {} ({:.3}s) - {}",
                            file_path.file_name().unwrap().to_string_lossy(),
                            file_time.as_secs_f64(),
                            e
                        );
                    } else {
                        println!(
                            "âœ… I/Oä¼˜åŒ–å®Œæˆ: {} ({:.3}s)",
                            file_path.file_name().unwrap().to_string_lossy(),
                            file_time.as_secs_f64()
                        );
                    }
                }

                // æ›´æ–°è®¡æ•°å™¨
                processed_counter.fetch_add(1, Ordering::Relaxed);
                if result.is_err() {
                    error_counter.fetch_add(1, Ordering::Relaxed);
                }

                result
            })
            .collect()
    });

    // ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
    progress_handle.join().expect("è¿›åº¦ç›‘æ§çº¿ç¨‹å¼‚å¸¸ç»“æŸ");

    // ç»Ÿè®¡å¤„ç†ç»“æœ
    let success_count = results.iter().filter(|r| r.is_ok()).count();
    let error_count = results.len() - success_count;

    let total_time = start_time.elapsed();
    println!("\nğŸ‰ I/Oä¼˜åŒ–ç‰ˆæ‰¹é‡å› å­ä¸­æ€§åŒ–å¤„ç†å®Œæˆ!");
    println!("{}", "=".repeat(60));
    println!("ğŸ“Š å¤„ç†ç»Ÿè®¡:");
    println!("   æ€»æ–‡ä»¶æ•°: {}", total_files);
    println!(
        "   æˆåŠŸå¤„ç†: {} ({:.1}%)",
        success_count,
        success_count as f64 / total_files as f64 * 100.0
    );
    println!("   å¤±è´¥æ–‡ä»¶: {}", error_count);
    println!(
        "   æ€»ç”¨æ—¶: {:.1}åˆ†é’Ÿ ({:.1}ç§’)",
        total_time.as_secs_f64() / 60.0,
        total_time.as_secs_f64()
    );
    println!(
        "   å¹³å‡å¤„ç†é€Ÿåº¦: {:.1} æ–‡ä»¶/åˆ†é’Ÿ",
        total_files as f64 / (total_time.as_secs_f64() / 60.0)
    );
    println!(
        "   å¹³å‡å•æ–‡ä»¶ç”¨æ—¶: {:.3}ç§’",
        total_time.as_secs_f64() / total_files as f64
    );
    println!("   I/Oä¼˜åŒ–æ•ˆæœ: âš¡ ç¼“å†²è¯»å– + ğŸ”„ æ‰¹å¤„ç†ä¼˜åŒ– + ğŸ“Š è‡ªé€‚åº”é…ç½®");

    if error_count > 0 {
        println!("âš ï¸  è­¦å‘Š: {}ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯æ—¥å¿—", error_count);
    }

    Ok(())
}

/// I/Oä¼˜åŒ–çš„å•å› å­ä¸­æ€§åŒ–
fn neutralize_single_factor_io_optimized(
    factor_data: IOOptimizedFactorData,
    style_data: &IOOptimizedStyleData,
) -> PyResult<IOOptimizedNeutralizationResult> {
    // ä½¿ç”¨åŸæœ‰çš„ä¸­æ€§åŒ–é€»è¾‘ï¼Œä½†åº”ç”¨I/Oä¼˜åŒ–çš„æ•°æ®ç»“æ„
    let n_dates = factor_data.dates.len();

    if n_dates == 0 {
        return Err(PyRuntimeError::new_err("å› å­æ•°æ®ä¸ºç©ºï¼šæ²¡æœ‰æ—¥æœŸæ•°æ®"));
    }

    if factor_data.stocks.is_empty() {
        return Err(PyRuntimeError::new_err("å› å­æ•°æ®ä¸ºç©ºï¼šæ²¡æœ‰è‚¡ç¥¨æ•°æ®"));
    }

    // è·å–è‚¡ç¥¨äº¤é›†
    let mut all_stocks_set = HashSet::new();
    for day_data in style_data.data_by_date.values() {
        for stock in &day_data.stocks {
            all_stocks_set.insert(stock.clone());
        }
    }

    let factor_stocks_set: HashSet<String> = factor_data.stocks.iter().cloned().collect();
    let mut union_stocks: Vec<String> = all_stocks_set
        .intersection(&factor_stocks_set)
        .cloned()
        .collect();
    union_stocks.sort_unstable();

    let n_union_stocks = union_stocks.len();
    let mut neutralized_values = DMatrix::from_element(n_dates, n_union_stocks, f64::NAN);

    // å¤„ç†æ¯ä¸ªæ—¥æœŸçš„ä¸­æ€§åŒ–
    for (date_idx, &date) in factor_data.dates.iter().enumerate() {
        if let Some(day_data) = style_data.data_by_date.get(&date) {
            if let Ok(day_values) =
                process_single_date_io_optimized(date_idx, &factor_data, day_data, &union_stocks)
            {
                for (union_idx, value) in day_values {
                    neutralized_values[(date_idx, union_idx)] = value;
                }
            }
        }
    }

    Ok(IOOptimizedNeutralizationResult {
        dates: factor_data.dates,
        stocks: union_stocks,
        neutralized_values,
    })
}

/// I/Oä¼˜åŒ–çš„å•æ—¥å¤„ç†
fn process_single_date_io_optimized(
    date_idx: usize,
    factor_data: &IOOptimizedFactorData,
    day_data: &IOOptimizedStyleDayData,
    union_stocks: &[String],
) -> PyResult<Vec<(usize, f64)>> {
    let mut daily_factor_values = Vec::new();
    let mut valid_union_indices = Vec::new();
    let mut valid_style_indices = Vec::new();

    for (union_idx, union_stock) in union_stocks.iter().enumerate() {
        if let Some(&factor_stock_idx) = factor_data.stock_index_map.get(union_stock) {
            if let Some(&style_stock_idx) = day_data.stock_index_map.get(union_stock) {
                let value = factor_data.values[(date_idx, factor_stock_idx)];
                if !value.is_nan() {
                    daily_factor_values.push(value);
                    valid_union_indices.push(union_idx);
                    valid_style_indices.push(style_stock_idx);
                }
            }
        }
    }

    if daily_factor_values.len() < 12 {
        return Ok(Vec::new());
    }

    let ranked_values = cross_section_rank_io_optimized(&daily_factor_values);

    if let Some(regression_matrix) = &day_data.regression_matrix {
        let mut selected_regression_cols = Vec::with_capacity(valid_style_indices.len());
        for &style_idx in &valid_style_indices {
            selected_regression_cols.push(regression_matrix.column(style_idx).clone_owned());
        }

        let selected_regression_matrix = DMatrix::from_columns(&selected_regression_cols);
        let aligned_y_vector = DVector::from_vec(ranked_values.clone());

        let beta = &selected_regression_matrix * &aligned_y_vector;

        let mut result_values = Vec::new();
        for (i, &union_idx) in valid_union_indices.iter().enumerate() {
            let style_idx = valid_style_indices[i];

            let mut predicted_value = 0.0;
            for j in 0..12 {
                predicted_value += day_data.style_matrix[(style_idx, j)] * beta[j];
            }

            let residual = ranked_values[i] - predicted_value;
            result_values.push((union_idx, residual));
        }

        Ok(result_values)
    } else {
        Ok(Vec::new())
    }
}

/// I/Oä¼˜åŒ–çš„ä¸­æ€§åŒ–ç»“æœ
pub struct IOOptimizedNeutralizationResult {
    pub dates: Vec<i64>,
    pub stocks: Vec<String>,
    pub neutralized_values: DMatrix<f64>,
}

/// I/Oä¼˜åŒ–çš„ç»“æœä¿å­˜
fn save_neutralized_result_io_optimized(
    result: IOOptimizedNeutralizationResult,
    output_path: &Path,
) -> PyResult<()> {
    use arrow::array::{ArrayRef, Float64Array, Int64Array};
    use arrow::datatypes::{DataType, Field, Schema};
    use arrow::record_batch::RecordBatch;
    use parquet::arrow::ArrowWriter;
    use parquet::basic::{Compression, Encoding};
    use parquet::file::properties::WriterProperties;

    // ä¼˜åŒ–çš„Schemaæ„å»º
    let mut fields = Vec::with_capacity(result.stocks.len() + 1);
    fields.push(Field::new("date", DataType::Int64, false));
    for stock in &result.stocks {
        fields.push(Field::new(stock, DataType::Float64, true));
    }
    let schema = Arc::new(Schema::new(fields));

    // ä¼˜åŒ–çš„æ•°ç»„æ„å»º
    let mut arrays: Vec<ArrayRef> = Vec::with_capacity(result.stocks.len() + 1);

    // æ—¥æœŸæ•°ç»„
    arrays.push(Arc::new(Int64Array::from(result.dates.clone())));

    // å¹¶è¡Œæ„å»ºè‚¡ç¥¨æ•°æ®æ•°ç»„
    let stock_arrays: Vec<ArrayRef> = (0..result.stocks.len())
        .into_par_iter()
        .map(|stock_idx| {
            let column_data: Vec<Option<f64>> = (0..result.dates.len())
                .map(|date_idx| {
                    let value = result.neutralized_values[(date_idx, stock_idx)];
                    if value.is_nan() {
                        None
                    } else {
                        Some(value)
                    }
                })
                .collect();
            Arc::new(Float64Array::from(column_data)) as ArrayRef
        })
        .collect();

    arrays.extend(stock_arrays);

    let batch = RecordBatch::try_new(schema.clone(), arrays)
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºRecordBatchå¤±è´¥: {}", e)))?;

    // I/Oä¼˜åŒ–çš„å†™å…¥é…ç½®
    let props = WriterProperties::builder()
        .set_compression(Compression::LZ4) // ä½¿ç”¨æ›´å¿«çš„å‹ç¼©ç®—æ³•
        .set_encoding(Encoding::PLAIN)
        .set_max_row_group_size(200000) // æ›´å¤§çš„è¡Œç»„
        .set_write_batch_size(10000) // ä¼˜åŒ–å†™å…¥æ‰¹æ¬¡å¤§å°
        .build();

    let file = File::create(output_path)
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤±è´¥: {}", e)))?;

    let mut writer = ArrowWriter::try_new(file, schema, Some(props))
        .map_err(|e| PyRuntimeError::new_err(format!("åˆ›å»ºArrowå†™å…¥å™¨å¤±è´¥: {}", e)))?;

    writer
        .write(&batch)
        .map_err(|e| PyRuntimeError::new_err(format!("å†™å…¥æ•°æ®å¤±è´¥: {}", e)))?;

    writer
        .close()
        .map_err(|e| PyRuntimeError::new_err(format!("å…³é—­å†™å…¥å™¨å¤±è´¥: {}", e)))?;

    Ok(())
}
