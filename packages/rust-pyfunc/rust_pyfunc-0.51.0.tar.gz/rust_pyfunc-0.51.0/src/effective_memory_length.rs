use pyo3::prelude::*;
use numpy::{PyReadonlyArray1, PyArray1};
use std::collections::HashMap;

/// 基于信息论的有效记忆长度(EML)计算 - 针对成交量序列优化
///
/// # 参数
/// * `data` - 成交量或其他一维数据序列 (numpy array)
/// * `window_size` - 统计窗口大小
/// * `max_lag` - 最大回顾长度
/// * `threshold_ratio` - 满意度阈值比例, 默认0.9 (90%)
/// * `quantile` - 离散化分位数, 默认0.5 (中位数)
///
/// # 返回
/// * 有效记忆长度 (eml)
///
/// # 离散化方法
/// 将数据按指定分位数二值化：
/// - 大于分位数: 1 (高成交量)
/// - 小于等于分位数: 0 (低成交量)
///
/// # 分位数说明
/// - quantile=0.5: 中位数分割 (默认)
/// - quantile=0.9: 90%分位数分割，只有前10%最高成交量为1
/// - quantile=0.1: 10%分位数分割，只有前10%最低成交量为1
#[pyfunction]
#[pyo3(signature = (data, window_size, max_lag, threshold_ratio=0.9, quantile=0.5))]
pub fn calculate_effective_memory_length(
    data: PyReadonlyArray1<f64>,
    window_size: usize,
    max_lag: usize,
    threshold_ratio: f64,
    quantile: f64,
) -> PyResult<f64> {
    let data_slice = data.as_slice()?;

    if data_slice.len() < window_size + max_lag + 1 {
        return Ok(0.0);
    }

    // 步骤1: 二值离散化 (基于指定分位数)
    let symbols = discretize_binary(data_slice, quantile);

    // 步骤2: 使用最后一个窗口进行计算
    let start_idx = symbols.len() - window_size - max_lag;
    let window = &symbols[start_idx..];

    // 步骤3-7: 计算EML
    let eml = compute_eml_single_window(window, max_lag, threshold_ratio);

    Ok(eml)
}

/// 批量计算有效记忆长度 (滚动窗口) - 带步长优化的版本
///
/// # 性能优化技术
/// 1. 支持步长参数，减少计算量
/// 2. 全局离散化，避免重复计算
/// 3. lag=1特殊优化路径
///
/// # 步长说明
/// step=1: 计算每个窗口（最精确，但最慢）
/// step=10: 每隔10个点计算一次（速度提升10倍）
///
/// # 分位数说明
/// quantile=0.5: 中位数分割 (默认)
/// quantile=0.9: 90%分位数分割，只有前10%最高成交量为1
#[pyfunction]
#[pyo3(signature = (data, window_size, max_lag, threshold_ratio=0.9, quantile=0.5, step=1))]
pub fn rolling_effective_memory_length(
    py: Python,
    data: PyReadonlyArray1<f64>,
    window_size: usize,
    max_lag: usize,
    threshold_ratio: f64,
    quantile: f64,
    step: usize,
) -> PyResult<Py<PyArray1<f64>>> {
    let data_slice = data.as_slice()?;
    let n = data_slice.len();

    if n < window_size + max_lag + 1 {
        return Ok(PyArray1::from_vec(py, vec![0.0; n]).into());
    }

    // 步骤1: 全局二值离散化 (基于指定分位数)
    let symbols = discretize_binary(data_slice, quantile);

    // 步骤2: 预分配结果向量
    let mut result = vec![0.0; n];

    // 步骤3: 按步长计算EML
    let mut i = window_size + max_lag;
    while i < n {
        let start_idx = i - window_size - max_lag;
        let window = &symbols[start_idx..i];

        result[i] = compute_eml_single_window(window, max_lag, threshold_ratio);

        i += step;
    }

    Ok(PyArray1::from_vec(py, result).into())
}

/// 将连续序列二值离散化 (基于指定分位数)
///
/// # 参数
/// * `data` - 连续数据序列
/// * `quantile` - 分位数 (0.0-1.0)，如0.9表示90%分位数
///
/// # 返回
/// * 二值化序列：大于分位数为1，小于等于分位数为0
fn discretize_binary(data: &[f64], quantile: f64) -> Vec<u8> {
    if data.is_empty() {
        return Vec::new();
    }

    // 找到指定分位数
    let mut sorted = data.to_vec();
    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap());

    // 限制quantile在[0, 1]范围内
    let q = quantile.max(0.0).min(1.0);
    let idx = ((sorted.len() as f64) * q).floor() as usize;
    let idx = idx.min(sorted.len() - 1);
    let threshold = sorted[idx];

    // 二值化
    data.iter()
        .map(|&x| if x > threshold { 1 } else { 0 })
        .collect()
}

/// 对单个窗口计算有效记忆长度 - 优化版本
fn compute_eml_single_window(window: &[u8], max_lag: usize, threshold_ratio: f64) -> f64 {
    if window.is_empty() {
        return 0.0;
    }

    // 步骤1: 计算基准熵 H(X)
    let base_entropy = compute_entropy(window);

    // 步骤2: 计算每个lag的信息增益，同时跟踪最大增益
    let mut max_gain = 0.0f64;
    let mut gains = [0.0f64; 32]; // 最多支持32个lag

    for lag in 1..=max_lag.min(32) {
        let conditional_entropy = compute_conditional_entropy_fast(window, lag);
        let gain = base_entropy - conditional_entropy;
        gains[lag - 1] = gain;
        if gain > max_gain {
            max_gain = gain;
        }
    }

    if max_gain <= 0.0 {
        return max_lag as f64;
    }

    // 步骤3: 计算阈值并找到第一个超过阈值的lag
    let threshold = max_gain * threshold_ratio;

    for lag in 1..=max_lag.min(32) {
        if gains[lag - 1] >= threshold {
            return lag as f64;
        }
    }

    max_lag as f64
}

/// 计算二值序列的熵 H(X)
fn compute_entropy(symbols: &[u8]) -> f64 {
    if symbols.is_empty() {
        return 0.0;
    }

    let n = symbols.len() as f64;
    let count_1 = symbols.iter().filter(|&&s| s == 1).count() as f64;

    if count_1 == 0.0 || count_1 == n {
        return 0.0;
    }

    let p = count_1 / n;
    let q = 1.0 - p;

    -p * p.log2() - q * q.log2()
}

/// 计算条件熵 H(X|X_lag) - 极度优化版本
///
/// 优化策略：
/// 1. lag=1使用固定数组，避免HashMap
/// 2. lag>1使用位模式+HashMap
/// 3. 预分配HashMap容量
fn compute_conditional_entropy_fast(symbols: &[u8], lag: usize) -> f64 {
    let n = symbols.len();

    if n <= lag {
        return 0.0;
    }

    // 特殊情况：lag=1时，使用固定数组优化
    if lag == 1 {
        return compute_conditional_entropy_lag1_optimized(symbols);
    }

    // 对于lag>1，使用位模式
    let effective_lag = lag.min(32);

    // 使用HashMap统计实际出现的模式
    let mut pattern_count: HashMap<u64, usize> = HashMap::new();
    let mut transition_count: HashMap<(u64, u8), usize> = HashMap::new();

    // 预分配容量以减少重新哈希
    pattern_count.reserve(256);
    transition_count.reserve(512);

    // 单次遍历构建统计
    for i in effective_lag..n {
        // 提取长度为lag的位模式
        let mut pattern: u64 = 0;
        for j in 0..effective_lag {
            pattern = (pattern << 1) | (symbols[i - effective_lag + j] as u64);
        }

        let next_symbol = symbols[i];

        *pattern_count.entry(pattern).or_insert(0) += 1;
        *transition_count.entry((pattern, next_symbol)).or_insert(0) += 1;
    }

    // 计算条件熵
    let mut conditional_entropy = 0.0;
    let total_windows = (n - effective_lag) as f64;

    for (&pattern, &p_count) in &pattern_count {
        if p_count == 0 {
            continue;
        }

        let pattern_prob = p_count as f64;

        // 计算该模式下的条件熵
        let mut inner_entropy = 0.0;

        let count_0 = transition_count.get(&(pattern, 0)).copied().unwrap_or(0);
        let count_1 = transition_count.get(&(pattern, 1)).copied().unwrap_or(0);

        if count_0 > 0 {
            let p0 = count_0 as f64 / pattern_prob;
            inner_entropy -= p0 * p0.log2();
        }

        if count_1 > 0 {
            let p1 = count_1 as f64 / pattern_prob;
            inner_entropy -= p1 * p1.log2();
        }

        conditional_entropy += (pattern_prob / total_windows) * inner_entropy;
    }

    conditional_entropy
}

/// lag=1时的极度优化版本 - 使用固定数组
fn compute_conditional_entropy_lag1_optimized(symbols: &[u8]) -> f64 {
    let n = symbols.len();

    if n <= 1 {
        return 0.0;
    }

    // 统计: count[prev][next]
    // 使用固定数组，避免HashMap开销
    let mut count = [[0usize; 2]; 2];

    for i in 1..n {
        let prev = symbols[i - 1] as usize;
        let next = symbols[i] as usize;
        count[prev][next] += 1;
    }

    let total = (n - 1) as f64;
    let mut conditional_entropy = 0.0;

    // prev=0的情况
    let count0 = count[0][0] + count[0][1];
    if count0 > 0 {
        let prob0 = count0 as f64;
        let mut inner = 0.0;
        if count[0][0] > 0 {
            let p = count[0][0] as f64 / prob0;
            inner -= p * p.log2();
        }
        if count[0][1] > 0 {
            let p = count[0][1] as f64 / prob0;
            inner -= p * p.log2();
        }
        conditional_entropy += (prob0 / total) * inner;
    }

    // prev=1的情况
    let count1 = count[1][0] + count[1][1];
    if count1 > 0 {
        let prob1 = count1 as f64;
        let mut inner = 0.0;
        if count[1][0] > 0 {
            let p = count[1][0] as f64 / prob1;
            inner -= p * p.log2();
        }
        if count[1][1] > 0 {
            let p = count[1][1] as f64 / prob1;
            inner -= p * p.log2();
        }
        conditional_entropy += (prob1 / total) * inner;
    }

    conditional_entropy
}

/// 计算信息增益序列 - 返回每个lag的信息增益
///
/// # 参数
/// * `data` - 成交量或其他一维数据序列 (numpy array)
/// * `window_size` - 统计窗口大小
/// * `max_lag` - 最大回顾长度
/// * `quantile` - 离散化分位数, 默认0.5 (中位数)
///
/// # 返回
/// * 二维数组，每一行是一个时间点，每一列是对应lag的信息增益
///
/// # 返回值说明
/// 返回形状为 (n, max_lag) 的二维数组：
/// - 每一行对应一个时间点
/// - 每一列对应一个lag（lag=1在第一列，lag=max_lag在最后一列）
/// - 前 window_size + max_lag 行全为0（数据不足）
#[pyfunction]
#[pyo3(signature = (data, window_size, max_lag, quantile=0.5, step=1))]
pub fn rolling_information_gain(
    py: Python,
    data: PyReadonlyArray1<f64>,
    window_size: usize,
    max_lag: usize,
    quantile: f64,
    step: usize,
) -> PyResult<Py<PyArray1<f64>>> {
    let data_slice = data.as_slice()?;
    let n = data_slice.len();

    if n < window_size + max_lag + 1 {
        // 数据不足，返回全0数组
        let empty_result = vec![0.0; n * max_lag];
        return Ok(PyArray1::from_vec(py, empty_result).into());
    }

    // 步骤1: 全局二值离散化
    let symbols = discretize_binary(data_slice, quantile);

    // 步骤2: 预分配结果向量 (扁平化的二维数组)
    let mut result = vec![0.0; n * max_lag];

    // 步骤3: 按步长计算每个窗口的信息增益
    let mut i = window_size + max_lag;
    let max_lag_min = max_lag.min(32);
    let row_size = max_lag;

    while i < n {
        let start_idx = i - window_size - max_lag;
        let window = &symbols[start_idx..i];

        // 计算基准熵
        let base_entropy = compute_entropy(window);

        // 计算每个lag的信息增益
        let base_idx = i * row_size;
        for lag in 1..=max_lag_min {
            let conditional_entropy = compute_conditional_entropy_fast(window, lag);
            let gain = base_entropy - conditional_entropy;

            // 存储到扁平化数组中
            result[base_idx + (lag - 1)] = gain;
        }

        i += step;
    }

    Ok(PyArray1::from_vec(py, result).into())
}


// ============================================================
// 优化版本：批量计算所有lag的条件熵
// ============================================================

/// 批量计算所有lag的条件熵（超高性能优化版本）
///
/// # 优化策略
/// 1. 单次遍历同时计算所有lag的统计信息
/// 2. 使用固定数组替代HashMap（针对lag<=8）
/// 3. 对lag>8使用HashMap，但批量计算避免重复遍历
/// 4. 预分配所有计数器避免动态扩容
///
/// # 性能预期
/// - 相比逐个计算lag的方式，提升max_lag倍（通常10-30倍）
fn compute_all_conditional_entropies_ultra_fast(window: &[u8], max_lag: usize) -> Vec<f64> {
    let max_lag = max_lag.min(32);
    let n = window.len();

    if n <= max_lag {
        return vec![0.0; max_lag];
    }

    // 分界点：lag<=8用数组，lag>8用HashMap
    let small_lag_threshold = 30;

    // 对每个lag分别进行统计（保持与原版逻辑一致）
    let mut conditional_entropies = Vec::with_capacity(max_lag);

    // 计算小lag的条件熵（使用固定数组优化）
    for lag in 1..=max_lag.min(small_lag_threshold) {
        let num_patterns = 1_usize << lag;
        let mut pattern_counts = vec![0usize; num_patterns];
        let mut transition_counts = vec![[0usize; 2]; num_patterns];

        // 从lag开始遍历（与原版一致）
        for i in lag..n {
            let mut pattern: u64 = 0;
            for j in 0..lag {
                pattern = (pattern << 1) | (window[i - lag + j] as u64);
            }
            let pattern_idx = pattern as usize;
            pattern_counts[pattern_idx] += 1;
            transition_counts[pattern_idx][window[i] as usize] += 1;
        }

        // 计算条件熵
        let mut entropy = 0.0;
        let total = (n - lag) as f64;

        for pattern_idx in 0..num_patterns {
            let p_count = pattern_counts[pattern_idx];
            if p_count == 0 {
                continue;
            }

            let p_prob = p_count as f64;
            let mut inner_entropy = 0.0;

            let count_0 = transition_counts[pattern_idx][0];
            let count_1 = transition_counts[pattern_idx][1];

            if count_0 > 0 {
                let p0 = count_0 as f64 / p_prob;
                inner_entropy -= p0 * p0.log2();
            }

            if count_1 > 0 {
                let p1 = count_1 as f64 / p_prob;
                inner_entropy -= p1 * p1.log2();
            }

            entropy += (p_prob / total) * inner_entropy;
        }

        conditional_entropies.push(entropy);
    }

    // 计算大lag的条件熵（使用HashMap）
    for lag in (small_lag_threshold + 1)..=max_lag {
        let mut pattern_counts: HashMap<u64, usize> = HashMap::new();
        let mut transition_counts: HashMap<(u64, u8), usize> = HashMap::new();

        // 预分配容量
        pattern_counts.reserve(256);
        transition_counts.reserve(512);

        // 从lag开始遍历（与原版一致）
        for i in lag..n {
            let mut pattern: u64 = 0;
            for j in 0..lag {
                pattern = (pattern << 1) | (window[i - lag + j] as u64);
            }
            *pattern_counts.entry(pattern).or_insert(0) += 1;
            *transition_counts.entry((pattern, window[i])).or_insert(0) += 1;
        }

        // 计算条件熵
        let mut entropy = 0.0;
        let total = (n - lag) as f64;

        for (&pattern, &p_count) in &pattern_counts {
            if p_count == 0 {
                continue;
            }

            let p_prob = p_count as f64;
            let mut inner_entropy = 0.0;

            let count_0 = transition_counts.get(&(pattern, 0)).copied().unwrap_or(0);
            let count_1 = transition_counts.get(&(pattern, 1)).copied().unwrap_or(0);

            if count_0 > 0 {
                let p0 = count_0 as f64 / p_prob;
                inner_entropy -= p0 * p0.log2();
            }

            if count_1 > 0 {
                let p1 = count_1 as f64 / p_prob;
                inner_entropy -= p1 * p1.log2();
            }

            entropy += (p_prob / total) * inner_entropy;
        }

        conditional_entropies.push(entropy);
    }

    conditional_entropies
}

/// 滚动窗口计算信息增益序列（高性能优化版本）
///
/// # 性能优化
/// - 批量计算所有lag的条件熵，避免重复遍历
/// - 使用固定数组替代HashMap
/// - 预期性能提升：10-30倍
///
/// # 参数
/// * `data` - 成交量或其他一维数据序列
/// * `window_size` - 统计窗口大小
/// * `max_lag` - 最大回顾长度
/// * `quantile` - 离散化分位数，默认0.5
/// * `step` - 计算步长，默认1
///
/// # 返回
/// * 扁平化的二维数组，形状为 (n, max_lag)
#[pyfunction]
#[pyo3(signature = (data, window_size, max_lag, quantile=0.5, step=1))]
pub fn rolling_information_gain_fast(
    py: Python,
    data: PyReadonlyArray1<f64>,
    window_size: usize,
    max_lag: usize,
    quantile: f64,
    step: usize,
) -> PyResult<Py<PyArray1<f64>>> {
    let data_slice = data.as_slice()?;
    let n = data_slice.len();

    if n < window_size + max_lag + 1 {
        // 数据不足，返回全0数组
        let empty_result = vec![0.0; n * max_lag];
        return Ok(PyArray1::from_vec(py, empty_result).into());
    }

    // 步骤1: 全局二值离散化
    let symbols = discretize_binary(data_slice, quantile);

    // 步骤2: 预分配结果向量 (扁平化的二维数组)
    let mut result = vec![0.0; n * max_lag];

    // 步骤3: 按步长计算每个窗口的信息增益
    let mut i = window_size + max_lag;
    let max_lag_min = max_lag.min(32);
    let row_size = max_lag;

    while i < n {
        let start_idx = i - window_size - max_lag;
        let window = &symbols[start_idx..i];

        // 计算基准熵
        let base_entropy = compute_entropy(window);

        // 批量计算所有lag的条件熵（核心优化）
        let conditional_entropies = compute_all_conditional_entropies_ultra_fast(window, max_lag_min);

        // 计算并存储信息增益
        let base_idx = i * row_size;
        for lag in 1..=max_lag_min {
            let gain = base_entropy - conditional_entropies[lag - 1];
            result[base_idx + (lag - 1)] = gain;
        }

        i += step;
    }

    Ok(PyArray1::from_vec(py, result).into())
}
