THIRD PARTY SOFTWARE NOTICES AND INFORMATION

This file contains notices and information regarding third-party software components used in the lk_moe project.

1. KVCache.AI Derived Code

The following files are derived from code originally developed by KVCache.AI:

- csrc/lk_moe/moe.h
  Original source: https://github.com/kvcache-ai/ktransformers/blob/v0.2.3/ktransformers/ktransformers_ext/operators/llamafile/moe.h
  Modifications: Significant changes made to support NUMA architecture and CPU-GPU hybrid inference
  Original copyright: Copyright (c) 2024 by KVCache.AI
  Modified by: guqiong9696@gmail.com (2025)

- csrc/lk_moe/moe.cpp
  Original source: https://github.com/kvcache-ai/ktransformers/blob/v0.2.3/ktransformers/ktransformers_ext/operators/llamafile/moe.cpp
  Modifications: Significant changes made to support NUMA architecture and CPU-GPU hybrid inference
  Original copyright: Copyright (c) 2024 by KVCache.AI
  Modified by: guqiong9696@gmail.com (2025)

- csrc/lk_moe/conversion.h
  Original source: [Original source URL if available]
  Modifications: Changes made to integrate with NUMA extension
  Original copyright: Copyright (c) 2024 by KVCache.AI
  Modified by: guqiong9696@gmail.com (2025)

- csrc/lk_moe/backend_numa.cpp
  Original source: [Original source URL if available]
  Modifications: Significant changes to implement NUMA-aware backend
  Original copyright: Copyright (c) 2024 by KVCache.AI
  Modified by: guqiong9696@gmail.com (2025)

2. llama_cpp Dependency

The project includes a dependency on llama_cpp, located in the .deps/ directory:

- .deps/llama_cpp-src/
  Source: llama_cpp library
  License: MIT License
  Original copyright: Copyright (c) 2023-2024 The ggml authors
  Description: A library for LLM inference

3. llamafile Code

The following files are copied from Mozilla-Ocho/llamafile:

- csrc/lk_moe/llamafile/
  Original source: https://github.com/Mozilla-Ocho/llamafile
  Description: Collection of optimized matrix multiplication implementations for various CPU architectures
  Modifications: Integrated into the lk_moe project for accelerated computation
  Special thanks to: Mozilla-Ocho team
 
4. vllm Dependency

- csrc/lk_moe/cpu/
  Original source: https://github.com/vllm-project/vllm
  Description: High-performance LLM inference engine
  Modifications: Integrated into the lk_moe project for accelerated computation
  Special thanks to: vllm team

Note: The original KVCache.AI code used in this project is subject to their respective license terms. All modifications and adaptations made to this code are subject to the proprietary license of the lk_moe project as specified in the LICENSE file.