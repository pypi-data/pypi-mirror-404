# -*- coding:utf-8 -*-

import os
import random
import re
import string
import pandas as pd
import numpy as np
import pandas as pd
import numpy as np
import json
import joblib 
from pathlib import Path
from typing import List, Dict, Optional, Callable, Tuple, Any 
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from tpf.d1 import DataDeal as dt
from tpf.data.read import get_features

from tpf import pkl_save,pkl_load
from tpf.d1 import DataDeal as dt
from tpf.d1 import read,write
from tpf.box.fil import  parentdir
# from tpf.link.toolml import str_pd
# from tpf.link.feature import FeatureEval
# from tpf.link.toolml import null_deal_pandas
# from tpf.link.toolml import std7

from tpf.conf.common import ParamConfig, CommonConfig
from tpf import pkl_save,pkl_load

# Check if torch is available
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

from tpf import pkl_load,pkl_save
from datetime import date, timedelta
# from tpf.nlp.text import TextEmbedding as tte 

from sklearn.base import BaseEstimator, TransformerMixin

class MinMaxScalerCustom(BaseEstimator, TransformerMixin):
    """è‡ªå®šä¹‰ MinMaxScalerï¼Œæ”¯æŒåŠ¨æ€æ›´æ–° min/max"""
    def __init__(self):
        self.min_ = None
        self.max_ = None

    def fit(self, X):
        self.min_ = X.min()
        self.max_ = X.max()
        return self

    def transform(self, X):
        return (X - self.min_) / (self.max_ - self.min_)

    def partial_fit(self, X):
        """å¢é‡æ›´æ–° min/max"""
        if self.min_ is None:
            self.min_ = X.min()
            self.max_ = X.max()
        else:
            self.min_ = min(self.min_, X.min())
            self.max_ = max(self.max_, X.max())


class DataDeal():
    def __init__(self):
        """ 
        1. s1_data_classify,å­—æ®µåˆ†ç±»ï¼ŒåŒºåˆ†å‡ºæ ‡è¯†ï¼Œæ•°å­—ï¼Œå­—ç¬¦ï¼Œæ—¥æœŸç­‰åˆ†ç±»çš„åˆ— ï¼Œä¸åŒçš„åˆ—æŒ‰ä¸åŒçš„æ–¹å¼å¤„ç†
        2. s2_pd_split,s2_data_split,è®­ç»ƒé›†æµ‹è¯•é›†æŒ‰æ ‡ç­¾æ‹†åˆ† 
        3. s3_min_max_scaler,æ•°å­—ç±»å‹å½’ä¸€åŒ–å¤„ç†
        """
        pass
    
    @staticmethod
    def append_csv(new_data, file_path):
        """è¿½åŠ å†™csvæ–‡ä»¶ï¼Œé€‚åˆå°æ•°æ®é‡
        
        """
        if os.path.exists(file_path):
            # è¯»å–ç°æœ‰çš„ CSV æ–‡ä»¶
            existing_df = pd.read_csv(file_path)
        
            # å°†æ–°æ•°æ®è¿½åŠ åˆ°ç°æœ‰çš„ DataFrame
            updated_df = pd.concat([existing_df, new_data], ignore_index=True)
        else:
            updated_df = new_data
        
        # å°†æ›´æ–°åçš„ DataFrame å†™å›åˆ° CSV æ–‡ä»¶
        updated_df.to_csv(file_path, index=False)
    
    @staticmethod
    def append_txt(data_base_path,
                    data_new_path=None,row=None):
        """
        ä»¥æ–‡æœ¬æ–¹å¼è¯»å–ä¸¤ä¸ªCSVæ–‡ä»¶ï¼Œå¹¶å°†æ–°æ–‡ä»¶å†…å®¹è¿½åŠ åˆ°åŸºç¡€æ–‡ä»¶å°¾éƒ¨

        å‚æ•°:
        data_base_path: åŸºç¡€CSVæ–‡ä»¶è·¯å¾„
        data_new_path: è¦è¿½åŠ çš„æ–°CSVæ–‡ä»¶è·¯å¾„
        row: è¦ç›´æ¥è¿½åŠ çš„å•è¡Œæ•°æ®ï¼ˆåˆ—è¡¨æˆ–å­—ç¬¦ä¸²ï¼‰

        è¿”å›:
        bool: æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        try:
            # å½“rowä¸ä¸ºNoneæ—¶ï¼Œç›´æ¥å°†ä¸€è¡Œæ•°æ®è¿½åŠ åˆ°æ–‡ä»¶å°¾éƒ¨
            if row is not None:
                with open(data_base_path, 'a', encoding='utf-8', newline='') as file:
                    if isinstance(row, list):
                        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºCSVæ ¼å¼å­—ç¬¦ä¸²
                        line = ','.join(str(item) for item in row)
                    else:
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                        line = str(row)

                    # ç¡®ä¿ä¸ä»¥æ¢è¡Œç¬¦ç»“å°¾çš„æƒ…å†µä¸‹æ·»åŠ æ¢è¡Œç¬¦
                    if not line.endswith('\n'):
                        line += '\n'

                    file.write(line)

                print(f"æˆåŠŸå°†æ•°æ®è¡Œè¿½åŠ åˆ° {data_base_path}")
                return True

            # å¦‚æœrowä¸ºNoneï¼ŒæŒ‰åŸæœ‰é€»è¾‘å¤„ç†æ–‡ä»¶è¿½åŠ 
            if data_new_path is None:
                print("é”™è¯¯: æœªæä¾›è¦è¿½åŠ çš„æ•°æ®æº")
                return False

            # æ£€æŸ¥åŸºç¡€æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            import os
            if not os.path.exists(data_base_path):
                # å¦‚æœåŸºç¡€æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨å¹¶ç›´æ¥å¤åˆ¶æ–°æ–‡ä»¶å†…å®¹åˆ°åŸºç¡€æ–‡ä»¶
                base_dir = os.path.dirname(data_base_path)
                if base_dir and not os.path.exists(base_dir):
                    os.makedirs(base_dir, exist_ok=True)
                    pc.lg(f"åˆ›å»ºç›®å½•: {base_dir}")

                pc.lg(f"åŸºç¡€æ–‡ä»¶ {data_base_path} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
                with open(data_new_path, 'r', encoding='utf-8') as new_file:
                    new_content = new_file.read()
                with open(data_base_path, 'w', encoding='utf-8') as output_file:
                    output_file.write(new_content)
                pc.lg(f"æˆåŠŸå°† {data_new_path} å¤åˆ¶åˆ° {data_base_path}")
                return True

            # è¯»å–åŸºç¡€æ–‡ä»¶å†…å®¹
            with open(data_base_path, 'r', encoding='utf-8') as base_file:
                base_content = base_file.read()

            # è¯»å–æ–°æ–‡ä»¶å†…å®¹
            with open(data_new_path, 'r', encoding='utf-8') as new_file:
                new_content = new_file.read()

            # åˆ†ç¦»åŸºç¡€æ–‡ä»¶çš„æ ‡é¢˜è¡Œå’Œæ•°æ®è¡Œ
            base_lines = base_content.split('\n')
            if len(base_lines) < 2:
                print("åŸºç¡€æ–‡ä»¶æ ¼å¼é”™è¯¯æˆ–ä¸ºç©º")
                return False

            header = base_lines[0]
            base_data = '\n'.join(base_lines[1:])

            # åˆ†ç¦»æ–°æ–‡ä»¶çš„æ ‡é¢˜è¡Œå’Œæ•°æ®è¡Œ
            new_lines = new_content.split('\n')
            if len(new_lines) < 2:
                print("æ–°æ–‡ä»¶æ ¼å¼é”™è¯¯æˆ–ä¸ºç©º")
                return False

            new_header = new_lines[0]
            new_data = '\n'.join(new_lines[1:])

            # éªŒè¯æ ‡é¢˜è¡Œæ˜¯å¦ä¸€è‡´
            if header.strip() != new_header.strip():
                print(f"è­¦å‘Š: æ–‡ä»¶æ ‡é¢˜ä¸ä¸€è‡´\nåŸºç¡€æ–‡ä»¶: {header}\næ–°æ–‡ä»¶: {new_header}")
                return False

            # åˆå¹¶å†…å®¹
            combined_content = header + '\n'
            if base_data.strip():
                combined_content += base_data + '\n'
            if new_data.strip():
                combined_content += new_data + '\n'

            # å†™å›åŸºç¡€æ–‡ä»¶
            with open(data_base_path, 'w', encoding='utf-8') as output_file:
                output_file.write(combined_content)

            print(f"æˆåŠŸå°† {data_new_path} å†…å®¹è¿½åŠ åˆ° {data_base_path}")
            return True

        except FileNotFoundError as e:
            print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
            return False
        except Exception as e:
            print(f"æ–‡ä»¶è¿½åŠ è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return False

    @staticmethod
    def csv_quchong(col_name,df=None, data_path=None, is_save=False):
        """
        ğŸš€ CSVæ•°æ®å»é‡å¤„ç†å·¥å…· - æŒ‰æŒ‡å®šåˆ—æ¶ˆé™¤é‡å¤è®°å½•,ä¿ç•™æœ€åä¸€æ¬¡å‡ºç°çš„åˆ—

        âš¡ æ ¸å¿ƒåŠŸèƒ½ï¼š
        æ™ºèƒ½åŒ–å»é‡å·¥å…·ï¼Œæ”¯æŒDataFrameå’ŒCSVæ–‡ä»¶ä¸¤ç§è¾“å…¥æ–¹å¼
        é€šè¿‡æŒ‡å®šåˆ—çš„å€¼æ¥è¯†åˆ«é‡å¤è®°å½•ï¼Œä¿ç•™æœ€åå‡ºç°çš„è®°å½•
        å®ç°"æ–°æ•°æ®è¦†ç›–æ—§æ•°æ®"çš„å¢é‡æ›´æ–°ç­–ç•¥ï¼Œç¡®ä¿æ•°æ®æ—¶æ•ˆæ€§

        ğŸ¯ è¾“å…¥æ–¹å¼ï¼ˆäºŒé€‰ä¸€ï¼‰ï¼š
        ğŸ”¹ DataFrameç›´å¤„ç†: ç›´æ¥ä¼ å…¥DataFrameå¯¹è±¡ï¼Œå†…å­˜æ“ä½œ
        ğŸ”¹ CSVæ–‡ä»¶è¯»å–: ä¼ å…¥data_pathè·¯å¾„ï¼Œè‡ªåŠ¨è¯»å–æ–‡ä»¶

        âš™ï¸ å‚æ•°é…ç½®ï¼š

        ğŸ“Š df (pandas.DataFrame, å¯é€‰)
            è¦å»é‡çš„DataFrameå¯¹è±¡
            ä¸data_pathäºŒé€‰ä¸€ï¼Œä¼˜å…ˆä½¿ç”¨df
            å¸¸ç”¨: deduplicate_csv_data(df=my_dataframe)

        ğŸ“ data_path (str, å¯é€‰)
            CSVæ–‡ä»¶è·¯å¾„ï¼Œå½“dfä¸ºNoneæ—¶ä½¿ç”¨
            æ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
            ç¤ºä¾‹: "data.csv", "/path/to/file.csv"

        ğŸ’¾ is_save (bool)
            æ˜¯å¦å°†ç»“æœå†™å›åŸæ–‡ä»¶ï¼ˆä»…å¯¹CSVæ–‡ä»¶æœ‰æ•ˆï¼‰
            False: å†…å­˜å¤„ç†ï¼Œä»…è¿”å›DataFrameï¼ˆæ¨èç”¨äºæµ‹è¯•ï¼‰
            True: è¦†ç›–åŸæ–‡ä»¶ï¼ˆç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼‰
            é»˜è®¤: False

        ğŸ¯ col_name (str)
            å»é‡åŸºå‡†åˆ—ï¼Œå¿…é¡»æ˜¯æ•°æ®ä¸­å­˜åœ¨çš„åˆ—å
            é»˜è®¤: 'text'ï¼ˆé€‚ç”¨äºæ–‡æœ¬å»é‡ï¼‰
            å¸¸ç”¨: 'text', 'label', 'id', 'user_id', 'order_id'

        ğŸ“ˆ ç®—æ³•åŸç†ï¼š
        é‡‡ç”¨ pandas.groupby(col_name).last() é«˜æ•ˆç®—æ³•ï¼š
        1. æŒ‰ col_name å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„
        2. æ¯ç»„ä¿ç•™æœ€åä¸€æ¡è®°å½•ï¼ˆæœ€æ–°æ•°æ®ï¼‰
        3. é‡ç½®ç´¢å¼•ä¿æŒæ•°æ®ç»“æ„å®Œæ•´
        4. æ—¶é—´å¤æ‚åº¦: O(n), ç©ºé—´å¤æ‚åº¦: O(k) (kä¸ºå”¯ä¸€å€¼æ•°é‡)


        ğŸ“Š è¿”å›å€¼ï¼š
        âœ… æˆåŠŸ: pandas.DataFrameï¼ˆå·²å»é‡çš„å®Œæ•´æ•°æ®é›†ï¼‰
        âŒ å¤±è´¥: Noneï¼ˆé”™è¯¯ä¿¡æ¯ä¼šæ‰“å°åˆ°æ§åˆ¶å°ï¼‰


        ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹ï¼š

        ğŸ”§ æ–¹å¼1: DataFrameç›´å¤„ç†ï¼ˆæ¨èï¼‰
        >>> import pandas as pd
        >>> # å·²æœ‰DataFrame
        >>> df_data = pd.read_csv("complaints.csv")
        >>> df_clean = deduplicate_csv_data(df=df_data, col_name='text')
        >>> print(f"å»é‡å: {len(df_clean)} æ¡è®°å½•")

        ğŸ”§ æ–¹å¼2: CSVæ–‡ä»¶å¤„ç†
        >>> # ç›´æ¥å¤„ç†æ–‡ä»¶
        >>> df_clean = deduplicate_csv_data(data_path="complaints.csv")
        >>> print(f"æ–‡ä»¶å»é‡å: {len(df_clean)} æ¡è®°å½•")

        ğŸ”§ ç”Ÿäº§ç¯å¢ƒä¿å­˜
        >>> # å¤„ç†å¹¶ä¿å­˜ç»“æœ
        >>> df_clean = deduplicate_csv_data(
        ...     data_path="complaints.csv",
        ...     col_name='label',
        ...     is_save=True
        ... )
        >>> # åŸæ–‡ä»¶å·²è¢«æ›´æ–°

        ğŸ”§ æ‰¹é‡å¤„ç†æ¨¡å¼
        >>> files = ["day1.csv", "day2.csv", "day3.csv"]
        >>> for file in files:
        ...     df = deduplicate_csv_data(data_path=file, col_name='text')
        ...     if df is not None:
        ...         print(f"{file}: {len(df)} æ¡å”¯ä¸€è®°å½•")

        ğŸ”§ æ•°æ®ç®¡é“é›†æˆ
        >>> def process_data_pipeline(raw_df):
        ...     # æ­¥éª¤1: å»é‡
        ...     df_dedup = deduplicate_csv_data(df=raw_df, col_name='user_id')
        ...
        ...     # æ­¥éª¤2: è¿›ä¸€æ­¥å¤„ç†
        ...     if df_dedup is not None:
        ...         return df_dedup.groupby('category').agg({
        ...             'amount': 'sum',
        ...             'count': 'len'
        ...         })
        ...     return None

        âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
        â€¢ å»é‡æ“ä½œä¸å¯é€†ï¼Œé‡è¦æ•°æ®è¯·å…ˆå¤‡ä»½
        â€¢ å¤§æ•°æ®é‡å¤„ç†æ—¶æ³¨æ„å†…å­˜ä½¿ç”¨æƒ…å†µ
        â€¢ ç¡®ä¿å»é‡åˆ—çš„æ•°æ®è´¨é‡ï¼Œé¿å…Noneå€¼å½±å“
        â€¢ is_save=True ä¼šç›´æ¥è¦†ç›–åŸæ–‡ä»¶ï¼Œè¯·è°¨æ…ä½¿ç”¨
        â€¢ DataFrameå’Œdata_pathåŒæ—¶æä¾›æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨DataFrame
        """
        try:
            if df is None:
                # è¯»å–åˆå¹¶åçš„CSVæ–‡ä»¶
                df = pd.read_csv(data_path)
            print(f"åŸæœ‰: {len(df)} æ¡è®°å½•")

            # æŒ‰æŒ‡å®šåˆ—å»é‡ï¼Œä¿ç•™æœ€åä¸€æ¬¡å‡ºç°çš„è®°å½•(æ–°æ•°æ®è¦†ç›–æ—§æ•°æ®)
            if col_name not in df.columns:
                print(f"é”™è¯¯ï¼šåˆ— '{col_name}' ä¸å­˜åœ¨äºæ–‡ä»¶ä¸­ï¼Œå¯ç”¨åˆ—ï¼š{list(df.columns)}")
                return None

            df_deduplicated = df.groupby(col_name, as_index=False).last()
            print(f"å»é‡åè®°å½•æ•°: {len(df_deduplicated)}")

            if is_save:
                # ä¿å­˜å»é‡åçš„æ•°æ®
                df_deduplicated.to_csv(data_path, index=False)
                print(f"å»é‡åçš„æ•°æ®å·²ä¿å­˜åˆ°: {data_path}")

            return df_deduplicated

        except FileNotFoundError:
            print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {data_path}")
            return None
        except Exception as e:
            print(f"å»é‡å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return None

    
    @staticmethod
    def columns_by_max_value(df: pd.DataFrame, 
                           condition: str = 'less', 
                           threshold: float = 100,
                           include_numeric_only: bool = True,
                           skipna: bool = True) -> List[str]:
        """
        æ ¹æ®æœ€å¤§å€¼æ¡ä»¶è·å–åˆ—åç§°
        
        å‚æ•°:
        df: pandas DataFrame
            è¾“å…¥çš„æ•°æ®æ¡†
        condition: str, å¯é€‰ 'less', 'less_equal', 'greater', 'greater_equal', 'equal'
            æ¯”è¾ƒæ¡ä»¶
        threshold: float, é»˜è®¤ 100
            é˜ˆå€¼
        include_numeric_only: bool, é»˜è®¤ True
            æ˜¯å¦åªåŒ…å«æ•°å­—åˆ—
        skipna: bool, é»˜è®¤ True
            æ˜¯å¦å¿½ç•¥NaNå€¼
        
        è¿”å›:
        List[str]: ç¬¦åˆæ¡ä»¶çš„åˆ—åç§°åˆ—è¡¨
        
        
        ç¤ºä¾‹:
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        import pandas as pd 
        df = pd.DataFrame({
            'small_values': [10, 20, 30, 40],       # æœ€å¤§å€¼40 < 100
            'large_values': [150, 200, 50, 300],    # æœ€å¤§å€¼300 > 100
            'mixed_values': [5, 15, 25, 35],        # æœ€å¤§å€¼35 < 100
            'string_col': ['a', 'b', 'c', 'd'],     # éæ•°å­—åˆ—
            'edge_case': [80, 90, 95, 99],          # æœ€å¤§å€¼99 < 100
            'exactly_100': [10, 50, 100, 30],       # æœ€å¤§å€¼100 = 100
            'with_nan': [10, np.nan, 30, 40],       # åŒ…å«NaNï¼Œæœ€å¤§å€¼40 < 100
            'all_nan': [np.nan, np.nan, np.nan, np.nan]     # å…¨ä¸ºNaN
        })
        
  
    from tpf.data.deal import DataDeal as dtl
    
    # ä½¿ç”¨å¢å¼ºç‰ˆæœ¬
    print("\nä½¿ç”¨å¢å¼ºç‰ˆæœ¬:")
    result_less = dtl.columns_by_max_value(df, condition='less', threshold=100)
    result_less_equal = dtl.columns_by_max_value(df, condition='less_equal', threshold=100)
    result_greater = dtl.columns_by_max_value(df, condition='greater', threshold=50)

    print(f"æœ€å¤§å€¼å°äº100çš„åˆ—: {result_less}")
    print(f"æœ€å¤§å€¼å°äºç­‰äº100çš„åˆ—: {result_less_equal}")
    print(f"æœ€å¤§å€¼å¤§äº50çš„åˆ—: {result_greater}")
    
    ä½¿ç”¨å¢å¼ºç‰ˆæœ¬:
    æœ€å¤§å€¼å°äº100çš„åˆ—: ['small_values', 'mixed_values', 'edge_case', 'with_nan']
    æœ€å¤§å€¼å°äºç­‰äº100çš„åˆ—: ['small_values', 'mixed_values', 'edge_case', 'exactly_100', 'with_nan']
    æœ€å¤§å€¼å¤§äº50çš„åˆ—: ['large_values', 'edge_case', 'exactly_100']
            
        
        """
        
        # éªŒè¯æ¡ä»¶å‚æ•°
        valid_conditions = ['less', 'less_equal', 'greater', 'greater_equal', 'equal']
        if condition not in valid_conditions:
            raise ValueError(f"condition must be one of {valid_conditions}")
        
        try:
            # å¦‚æœåªå¤„ç†æ•°å­—åˆ—
            if include_numeric_only:
                numeric_df = df.select_dtypes(include=[np.number])
                if numeric_df.empty:
                    return []
                max_values = numeric_df.max(skipna=skipna)
            else:
                max_values = df.apply(lambda x: pd.to_numeric(x, errors='coerce').max(skipna=skipna) 
                                    if x.dtype == 'object' else x.max(skipna=skipna))
            
            # æ ¹æ®æ¡ä»¶ç­›é€‰
            if condition == 'less':
                mask = max_values < threshold
            elif condition == 'less_equal':
                mask = max_values <= threshold
            elif condition == 'greater':
                mask = max_values > threshold
            elif condition == 'greater_equal':
                mask = max_values >= threshold
            elif condition == 'equal':
                mask = max_values == threshold
            
            result_columns = max_values[mask].index.tolist()
            if result_columns is None or len(result_columns) == 0:
                return []
            
            return result_columns
            
        except Exception as e:
            print(f"Error occurred: {e}")
            return []

    @staticmethod
    def col_split(df, col='key', sep='_', maxsplit=1, prefix=None, suffix=None,
                  drop_original=False, handle_missing='ignore'):
        """
        åˆ—æ‹†åˆ†ï¼Œå°†åˆ—æŒ‰æŒ‡å®šåˆ†éš”ç¬¦æ‹†åˆ†ä¸ºå¤šä¸ªåˆ—ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        Args:
            df: pandas DataFrame
            col: è¦æ‹†åˆ†çš„åˆ—å
            sep: åˆ†éš”ç¬¦ï¼Œæ”¯æŒå­—ç¬¦ä¸²æˆ–æ­£åˆ™è¡¨è¾¾å¼
            maxsplit: æœ€å¤§æ‹†åˆ†æ¬¡æ•°ï¼Œ-1è¡¨ç¤ºæ‹†åˆ†æ‰€æœ‰
            prefix: æ–°åˆ—åå‰ç¼€ï¼Œé»˜è®¤ä½¿ç”¨åŸåˆ—å
            suffix: æ–°åˆ—ååç¼€ï¼Œé»˜è®¤ä½¿ç”¨æ•°å­—åºå·
            drop_original: æ˜¯å¦åˆ é™¤åŸåˆ—
            handle_missing: å¤„ç†ç¼ºå¤±å€¼çš„æ–¹å¼ ('ignore', 'fill_empty', 'drop')

        Returns:
            pandas DataFrame: æ‹†åˆ†åçš„æ•°æ®æ¡†

        Raises:
            ValueError: å½“åˆ—ä¸å­˜åœ¨æˆ–å‚æ•°æ— æ•ˆæ—¶
        """
        import pandas as pd
        import numpy as np

        # è¾“å…¥éªŒè¯
        if col not in df.columns:
            raise ValueError(f"åˆ— '{col}' ä¸å­˜åœ¨")
        if not isinstance(sep, str):
            raise ValueError("åˆ†éš”ç¬¦å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
        if handle_missing not in ['ignore', 'fill_empty', 'drop']:
            raise ValueError("handle_missing å¿…é¡»æ˜¯ 'ignore', 'fill_empty' æˆ– 'drop'")

        # å¤„ç†ç¼ºå¤±å€¼
        if handle_missing == 'drop':
            df = df.dropna(subset=[col]).copy()
        elif handle_missing == 'fill_empty':
            df = df.fillna({col: ''}).copy()
        else:
            df = df.copy()

        # ä¼˜åŒ–çš„æ‹†åˆ†é€»è¾‘
        if maxsplit == -1:
            # æ‹†åˆ†æ‰€æœ‰å¯èƒ½çš„åˆ†éš”ç¬¦
            split_data = df[col].str.split(sep, expand=True)
        else:
            # é™åˆ¶æ‹†åˆ†æ¬¡æ•°ï¼Œæé«˜æ€§èƒ½
            split_data = df[col].str.split(sep, n=maxsplit, expand=True)

        # è®¾ç½®åˆ—å
        num_cols = len(split_data.columns)
        if prefix is None:
            prefix = col

        if suffix is None:
            new_columns = [f"{prefix}_{i+1}" for i in range(num_cols)]
        else:
            new_columns = [f"{prefix}{suffix}" for suffix in range(1, num_cols + 1)]

        split_data.columns = new_columns

        # åˆå¹¶åˆ°åŸæ•°æ®æ¡†
        result = pd.concat([df, split_data], axis=1)

        # åˆ é™¤åŸåˆ—ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if drop_original:
            result = result.drop(columns=[col])

        return result
        
    
    
        
    @staticmethod
    def col2index(df, identity=[], classify_type=[], classify_type2=[],
                  dict_file="dict_file.dict", is_pre=False, 
                  word2id=None, start_index=1):
        """ç±»åˆ«ç‰¹å¾ç¼–ç ï¼šå°†æ–‡æœ¬ç±»åˆ«è½¬æ¢ä¸ºæ•°å€¼ç´¢å¼•

        è¯¥æ–¹æ³•æ˜¯å¯¹TextEmbedding.col2indexçš„å°è£…ï¼Œæä¾›ç±»åˆ«ç‰¹å¾çš„æ•°å€¼ç¼–ç åŠŸèƒ½ã€‚
        æ”¯æŒç‹¬ç«‹ç¼–ç å’Œå…±äº«ç¼–ç ä¸¤ç§æ¨¡å¼ï¼Œé€‚ç”¨äºæ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹ã€‚
        æ ‡è¯†åˆ—ä¸ä¼šè¢«ç¼–ç ï¼Œä¿æŒåŸå§‹å€¼ç”¨äºæ•°æ®è¿½è¸ªã€‚

        Args:
            df: è¾“å…¥æ•°æ®è¡¨ï¼ŒåŒ…å«éœ€è¦ç¼–ç çš„ç±»åˆ«åˆ—
            identity: list, æ ‡è¯†åˆ—ååˆ—è¡¨ï¼Œè¿™äº›åˆ—ä¸ä¼šè¢«ç¼–ç ï¼Œç”¨äºå”¯ä¸€æ ‡è®°æ•°æ®è¡Œ
            classify_type: ç‹¬ç«‹ç¼–ç çš„åˆ—ååˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºæˆ–Noneåˆ™è‡ªåŠ¨æ¨æ–­é™¤æ•°å€¼ã€æ—¥æœŸå’Œæ ‡è¯†åˆ—å¤–çš„æ‰€æœ‰åˆ—
            classify_type2: å…±äº«ç¼–ç ç»„åˆ—è¡¨ï¼Œå¦‚[['From', 'To']]è¡¨ç¤ºä¸¤åˆ—å…±äº«ç¼–ç ç©ºé—´
            dict_file: ç¼–ç å­—å…¸ä¿å­˜è·¯å¾„
            is_pre: æ˜¯å¦ä¸ºæ¨ç†æ¨¡å¼ï¼ˆTrue=åŠ è½½å·²æœ‰å­—å…¸ï¼ŒFalse=åˆ›å»ºæ–°å­—å…¸ï¼‰
            word2id: é¢„åŠ è½½çš„ç¼–ç å­—å…¸
            start_index: ç¼–ç èµ·å§‹ç´¢å¼•ï¼Œé»˜è®¤ä¸º1

        Returns:
            DataFrame: ç¼–ç åçš„æ•°æ®è¡¨ï¼Œç±»åˆ«åˆ—æ›¿æ¢ä¸ºæ•°å€¼ç´¢å¼•ï¼Œæ ‡è¯†åˆ—ä¿æŒä¸å˜ï¼Œåˆ—é¡ºåºä¸è¾“å…¥ä¸€è‡´

        Example:
            # åŸºç¡€ä½¿ç”¨ - è‡ªåŠ¨æ¨æ–­ç±»åˆ«åˆ—ï¼Œæ’é™¤æ ‡è¯†åˆ—
            df_encoded = DataDeal.col2index(
                df,
                identity=['id', 'account_number'],
                dict_file='categories.dict'
            )

            # ç‹¬ç«‹ç¼–ç  + æ ‡è¯†åˆ—ä¿æŠ¤
            df_encoded = DataDeal.col2index(
                df,
                identity=['user_id', 'transaction_id'],
                classify_type=['currency', 'payment_type'],
                dict_file='category.dict'
            )

            # å…±äº«ç¼–ç  + æ ‡è¯†åˆ—ä¿æŠ¤
            df_encoded = DataDeal.col2index(
                df,
                identity=['record_id'],
                classify_type=['transaction_type'],
                classify_type2=[['from_account', 'to_account']],
                dict_file='shared.dict'
            )

            # å®Œå…¨è‡ªåŠ¨æ¨æ–­ - è‡ªåŠ¨æ’é™¤æ•°å€¼åˆ—ã€æ—¥æœŸåˆ—å’Œæ ‡è¯†åˆ—
            df_encoded = DataDeal.col2index(
                df,
                identity=['id', 'timestamp'],
                dict_file='auto_inferred.dict'
            )

        Note:
            - æ ‡è¯†åˆ—ä¸ä¼šè¢«ç¼–ç ï¼Œä¿æŒåŸå§‹å€¼ç”¨äºæ•°æ®è¿½è¸ª
            - æ”¯æŒå®Œå…¨è‡ªåŠ¨æ¨æ–­æˆ–éƒ¨åˆ†æ‰‹åŠ¨æŒ‡å®šç±»åˆ«åˆ—
            - åˆ—é¡ºåºä¿æŒï¼šæ–¹æ³•ä¼šè‡ªåŠ¨ä¿æŒè¾“å…¥DataFrameçš„åˆ—é¡ºåºï¼Œç¡®ä¿è¾“å‡ºç»“æœçš„åˆ—é¡ºåºä¸è¾“å…¥ä¸€è‡´
            - è‡ªåŠ¨æ¨æ–­æ—¶ä¼šæ’é™¤æ•°å€¼ç±»å‹ã€æ—¥æœŸç±»å‹å’Œæ ‡è¯†åˆ—
        """
        # åˆå§‹åŒ–å‚æ•°ï¼Œç¡®ä¿ä¸ºåˆ—è¡¨ç±»å‹
        identity = identity or []

        # è‡ªåŠ¨æ¨æ–­ç±»åˆ«åˆ—ï¼šå¦‚æœclassify_typeä¸ºç©ºæˆ–Noneï¼Œåˆ™é€‰æ‹©é™¤æ•°å€¼ç±»å‹ã€æ—¥æœŸç±»å‹å’Œæ ‡è¯†åˆ—å¤–çš„æ‰€æœ‰åˆ—
        if classify_type is None or len(classify_type) == 0:
            # è·å–æ•°å€¼ç±»å‹åˆ—
            numeric_cols = df.select_dtypes('number').columns.tolist()
            # è·å–æ—¥æœŸç±»å‹åˆ—
            date_cols = df.select_dtypes(['datetime', 'datetimetz', 'datetime64']).columns.tolist()
            # ä»æ‰€æœ‰åˆ—ä¸­æ’é™¤æ•°å€¼åˆ—ã€æ—¥æœŸåˆ—å’Œæ ‡è¯†åˆ—ï¼Œå‰©ä½™çš„ä½œä¸ºç±»åˆ«åˆ—
            all_cols = df.columns.tolist()
            classify_type = [col for col in all_cols if col not in numeric_cols and col not in date_cols and col not in identity]
        else:
            # å¦‚æœæŒ‡å®šäº†classify_typeï¼Œåˆ™éœ€è¦ä»ä¸­æ’é™¤æ ‡è¯†åˆ—
            classify_type = [col for col in classify_type if col not in identity]

        # å¤„ç†å…±äº«ç¼–ç ç»„ï¼šä»classify_type2çš„æ¯ä¸ªç»„ä¸­ç§»é™¤æ ‡è¯†åˆ—
        if classify_type2:
            filtered_classify_type2 = []
            for group in classify_type2:
                filtered_group = [col for col in group if col not in identity]
                if filtered_group:  # åªä¿ç•™éç©ºçš„ç»„
                    filtered_classify_type2.append(filtered_group)
            classify_type2 = filtered_classify_type2

        # ä¿å­˜åŸå§‹åˆ—é¡ºåºï¼Œç¡®ä¿è¿”å›ç»“æœçš„åˆ—é¡ºåºä¸è¾“å…¥ä¸€è‡´
        original_columns = df.columns.tolist()

        # è°ƒç”¨TextEmbeddingè¿›è¡Œå®é™…çš„ç¼–ç å¤„ç†
        TextEmbedding.col2index(df, classify_type=classify_type,
            classify_type2=classify_type2,
            dict_file=dict_file,
            is_pre=is_pre,
            word2id=word2id,
            start_index=start_index)

        # æ¢å¤åŸå§‹åˆ—é¡ºåº
        df = df[original_columns] 
        return df

    
    @staticmethod
    def data_classify(data, col_type, pc, dealnull=False,dealstd=False,deallowdata=False,lowdata=10,deallog=False):
        """å°†pandasæ•°è¡¨çš„ç±»å‹è½¬æ¢ä¸ºç‰¹å®šçš„ç±»å‹
        - float64è½¬æ¢ä¸ºfloat32
        - å¸ƒå°”è½¬ä¸ºint64
        - å­—ç¬¦ä¸²æ—¥æœŸè½¬ä¸ºpandasæ—¥æœŸ
        
        
        æ•°æ®åˆ†ç±»å¤„ç†
        - æ—¥æœŸå¤„ç†ï¼šå­—ç¬¦ä¸²æ—¥æœŸè½¬ä¸ºpandas æ—¥æœŸ
        - objectè½¬string
        - ç©ºå€¼å¤„ç†ï¼šæ•°å­—ç©ºå…¨éƒ¨è½¬ä¸º0ï¼Œå­—ç¬¦ç©ºå…¨éƒ¨è½¬ä¸º'<PAD>'
        - å¸ƒå°”å¤„ç†ï¼šå¸ƒå°”0ä¸1å…¨éƒ¨è½¬ä¸ºint64
        - æ•°å­—å¤„ç†
            - æ ¼å¼ï¼šå…¨éƒ¨è½¬float32
            - è¾¹ç•Œï¼šæå°-èˆå¼ƒ10ï¿¥ä»¥ä¸‹äº¤æ˜“ï¼Œæå¤§-é‡ç½®è¶…è¿‡7å€å‡å€¼çš„é‡‘é¢
            - åˆ†å¸ƒï¼šLog10åæ ‡å‡†åŒ–
            - æœ€ç»ˆçš„æ•°æ®å€¼ä¸å¤§ï¼Œå¹¶ä¸”æ˜¯ä»¥0ä¸ºä¸­å¿ƒçš„æ­£æ€åˆ†å¸ƒ

        - å¤„ç†åçš„æ•°æ®ç±»å‹ï¼šæ•°å­—ï¼Œæ—¥æœŸï¼Œå­—ç¬¦
        -
        
        params
        --------------------------------
        - data:pandasæ•°è¡¨
        - col_type:pcå‚æ•°é…ç½®ä¸­çš„col_type
        - pc:å‚æ•°é…ç½®
        - dealnull:æ˜¯å¦å¤„ç†ç©ºå€¼
        - dealstd:æ˜¯å¦æ ‡å‡†åŒ–å¤„ç†
        - deallog:æ˜¯å¦å¯¹æ•°å­—åˆ—log10å¤„ç†
        - deallowdata:é‡‘é¢ä½äº10ï¿¥çš„æ•°æ®å…¨ç½®ä¸º0
        
        example
        ----------------------------------
        data_classify_deal(data,pc.col_type_nolable,pc)
        
        """
        column_all = data.columns
        
        
        ### æ—¥æœŸ
        date_type = [col for col in col_type.date_type if col in column_all] 
        data = DataDeal.str_pd(data, date_type)
        for col in date_type:
            data[col] = pd.to_datetime(data[col], errors='coerce')  

        ### æ•°å­—
        num_type = [col for col in col_type.num_type if col in column_all] 
        data[num_type] = data[num_type].astype(np.float32)
        
        
        bool_type = [col for col in col_type.bool_type if col in column_all]
        data[bool_type] = (data[bool_type].astype(np.float32)).astype(int)  # ä¸ºäº†å¤„ç†'0.00000000'

        ### å­—ç¬¦-èº«ä»½æ ‡è¯†ç±»
        cname_str_identity = pc.cname_str_identity 
        str_identity = [col for col in column_all if col in cname_str_identity]
        col_type.str_identity = str_identity
        data = DataDeal.str_pd(data,str_identity)

        ### å­—ç¬¦-åˆ†ç±»ï¼Œç”¨äºåˆ†ç±»çš„åˆ—ï¼Œæ¯”å¦‚æ¸ é“ï¼Œäº¤æ˜“ç±»å‹,å•†æˆ·ï¼Œåœ°åŒºç­‰
        str_classification = [col for col in data.columns if col not in str_identity and col not in num_type and col not in date_type and col not in bool_type]
        col_type.str_classification = str_classification
        data = DataDeal.str_pd(data,str_classification)

        #ç©ºå€¼å¤„ç†
        if dealnull:
            data = DataDeal.null_deal_pandas(data,cname_num_type=num_type,cname_str_type=str_classification,num_padding=0, str_padding = '<PAD>')

        if len(num_type)>0:
            if deallowdata:
                #æ•°å­—ç‰¹å¾-æå°å€¼å¤„ç†
                #å°†å°äº10ï¿¥çš„é‡‘é¢å…¨éƒ¨ç½®ä¸º0ï¼Œå³ä¸è€ƒè™‘10ï¿¥ä»¥ä¸‹çš„äº¤æ˜“
                for col_name in num_type:
                    data.loc[data[col_name]<lowdata,col_name] = lowdata
            
                #å°†lowdataä»¥ä¸‹äº¤æ˜“å‰”é™¤
                data.drop(data[data.CNY_AMT.eq(10)].index, inplace=True)
            if deallog:
                #é˜²æ­¢åé¢ç‰¹å¾ç»„åˆæ—¶ï¼Œä¸¤ä¸ªæœ¬æ¥å°±å¾ˆå¤§çš„æ•°æ®ç›¸ä¹˜åå˜ä¸ºinf
                data[num_type] = np.log10(data[num_type])
        
            if dealstd:
                # æ•°å­—ç‰¹å¾-å½’ä¸€åŒ–åŠæå¤§å€¼å¤„ç†
                #éœ€è¦ä¿å­˜ï¼Œé¢„æµ‹æ—¶ä½¿ç”¨
                means = data[num_type].mean()
                stds = data[num_type].std()
                
                data = DataDeal.std7(data, num_type, means, stds)
        

        return data
        
        
    @staticmethod
    def data_pre_deal(df,fe,date_type,num_type,classify_type,classify_type2=[],bool_type=[],
                  save_file=None,dict_file=None,is_num_std=True, 
                  is_pre=False,num_scaler_file="scaler_num.pkl",
                  date_scaler_file="scaler_date.pkl",max_date='2035-01-01'):
        """ç›¸æ¯”æ™®é€šçš„æ•°æ®é¢„å¤„ç†ï¼Œæœ¬æ–¹æ³•å¤šäº†ä¸€ä¸ªç±»åˆ«ç±»å‹ç¼–ç ï¼›åŒæ—¶æ•°å­—å½’ä¸€åŒ–æ—¶ï¼Œä¼šè‡ªåŠ¨å°†æ•°å­—åˆ—çš„æå€¼æ›´æ–°ä¸ºè®­ç»ƒæ—¶ä½¿ç”¨çš„æå€¼ï¼›æ›´é€‚ç”¨äºæ‰¹é‡è®­ç»ƒ
        - æ—¥æœŸå½’ä¸€åŒ–ï¼Œæœ‰æ–‡ä»¶ä¼šè‡ªåŠ¨åº”ç”¨
        - ç±»å‹ç¼–ç ï¼Œéœ€è¦æŒ‡å®šis_pre=False
        - classify_type2:å¤šåˆ—å…±ç”¨ä¸€ä¸ªå­—å…¸æ—¶ï¼Œå…¶å…ƒç´ ä¸ºå…±ç”¨åŒä¸€ä¸ªå­—å…¸çš„åˆ—çš„åˆ—è¡¨
        - num_scaler_file:å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”æ˜¯è®­ç»ƒé˜¶æ®µï¼Œåˆ™æ›´æ–°å…ƒç´ çš„æå€¼ 
        - fe:ç‰¹å¾å¤„ç†ç±»
        """
        #å¦‚æœä¿å­˜è¿‡æ•°æ®ï¼Œåˆ™ç›´æ¥è¯»å–
        if save_file and os.path.exists(save_file):
            df = pd.read_csv(save_file)
            return df 


        #å­—æ®µåˆ†ç±»
        print("---------------------------")
        print(f"classify_type={classify_type}")
        df = DataDeal.data_type_change(df, num_type=num_type,classify_type=classify_type,date_type=date_type)

        print("---------------------------")
        print(f"å­—æ®µåˆ†ç±»ä¹‹å ,\n{df.info()}")

        
        #ç±»å‹å­—æ®µç´¢å¼•ç¼–ç 
        fe.col2index(df,classify_type=classify_type,
                    classify_type2=classify_type2,
                    dict_file=dict_file,
                    is_pre=is_pre,
                    word2id=None)

        print(f"å­—æ®µç´¢å¼•ç¼–ç ä¹‹å \n{df[:3]}")

        ## æ•°å­—å½’ä¸€åŒ–
        if is_num_std:
            fe.min_max_scaler(df, num_type=num_type, model_path=num_scaler_file, reuse=True,log10=True)

        ## æ—¥æœŸå½’ä¸€åŒ–
        if date_scaler_file is not None or max_date is not None:
            df = DataDeal.min_max_scaler_dt(df,
                date_type=date_type,
                scaler_file=date_scaler_file,
                max_date=max_date,
                adjust=True)
        
        #ä¿å­˜æ•°æ®
        if save_file:
            df.to_csv(save_file,index=False)
        
        return df 

        

    @staticmethod
    def data_dl_deal(df, date_type, num_type, 
                        classify_type, classify_type2=[], bool_type=[],
                    save_file=None,dict_file=None,is_num_std=True, 
                    is_pre=False,num_scaler_file="scaler_num.pkl",
                    date_scaler_file="scaler_date.pkl", max_date='2035-01-01'):
        """å¯¹äºæ•°å­—åŠç±»åˆ«ç¼–ç ï¼Œåœ¨è®­ç»ƒé˜¶æ®µæ˜¯ä¼šè‡ªåŠ¨æ›´æ–°å­—å…¸çš„;é€‚ç”¨äºæ•°æ®é›†ä¸å…¨ï¼Œä¸æ–­æ”¶é›†æ‰¹æ¬¡æ•°æ®çš„æå€¼
        - æ—¥æœŸå½’ä¸€åŒ–ï¼Œæœ‰æ–‡ä»¶ä¼šè‡ªåŠ¨åº”ç”¨
        - ç±»å‹ç¼–ç ï¼Œéœ€è¦æŒ‡å®šis_pre=False
        - classify_type2:å¤šåˆ—å…±ç”¨ä¸€ä¸ªå­—å…¸æ—¶ï¼Œå…¶å…ƒç´ ä¸ºå…±ç”¨åŒä¸€ä¸ªå­—å…¸çš„åˆ—çš„åˆ—è¡¨
        - num_scaler_file:å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”æ˜¯è®­ç»ƒé˜¶æ®µï¼Œåˆ™æ›´æ–°å…ƒç´ çš„æå€¼ 
        
        """
        if save_file and os.path.exists(save_file):
            df = pd.read_csv(save_file)
            return df 
        
        if dict_file is None:
            raise Exception("è¯·è¾“å…¥å­—å…¸æ–‡ä»¶dict_fileçš„è·¯å¾„")

        
        #å­—æ®µåˆ†ç±»
        print(f"classify_type={classify_type}")
        df = DataDeal.data_type_change(df, num_type=num_type,classify_type=classify_type,date_type=date_type)
        print(df.info())
        

        #ç±»å‹å­—æ®µç´¢å¼•ç¼–ç ,å¦‚æœæ˜¯è®­ç»ƒåˆ™ä¿å­˜å­—å…¸
        DataDeal.col2index(df,classify_type=classify_type,
                    classify_type2=classify_type2,
                    dict_file=dict_file,
                    is_pre=is_pre,
                    word2id=None)


        ## æ•°å­—å½’ä¸€åŒ–
        if is_num_std:
            # fe.min_max_scaler(df, num_type=pc.col_type.num_type, model_path=num_scaler_file, reuse=True,log10=True)
            DataDeal.min_max_update(df, num_type=num_type,num_scaler_file=num_scaler_file, is_pre=is_pre,log10=True)

        if date_scaler_file is not None or max_date is not None:
            ## æ—¥æœŸå½’ä¸€åŒ–
            df = DataDeal.min_max_scaler_dt(df,
                date_type=date_type,
                scaler_file=date_scaler_file,
                max_date=max_date,
                adjust=True)

        if save_file:
            df.to_csv(save_file,index=False)
        
        return df 

    
    @staticmethod
    def data_split_pd(X, y,test_split=0.2, random_state=42):
        """æŒ‰æ ‡ç­¾ç±»åˆ«ç­‰æ¯”éšæœºé‡‡æ ·ï¼Œç¡®ä¿æµ‹è¯•é›†ä¸­æ¯ç±»æ ‡ç­¾çš„æ•°æ®ä¸è®­ç»ƒé›†ä¿æŒç­‰æ¯”"""
        copied_index = X.index.copy()
        X_test = pd.DataFrame(columns=X.columns)
        y_test = pd.DataFrame()
        unique_labels = y.unique()
        
        for label in unique_labels:
            label_indices = y[y == label].index
            num_samples_to_select = int(len(label_indices) * test_split)
            resampled_indices = resample(label_indices, replace=False, n_samples=num_samples_to_select, random_state=random_state)
            copied_index = copied_index.difference(resampled_indices)
            
            X_label_test = X.loc[resampled_indices]
            y_label_test = y.loc[resampled_indices]
            
            if X_test.shape[0] == 0:
                X_test = X_label_test
                y_test = y_label_test
            else:
                X_test = pd.concat([X_test, X_label_test], ignore_index=True)
                y_test = pd.concat([y_test, y_label_test], ignore_index=True)
                
        X_train = X.loc[copied_index]
        y_train = y.loc[copied_index]
        return X_train, y_train, X_test, y_test

    @staticmethod
    def data_type_change(data,num_type=None,classify_type=None,date_type=None):
        """
        å°†pandasæ•°è¡¨çš„ç±»å‹è½¬æ¢ä¸ºç‰¹å®šçš„ç±»å‹

        Args:
            data: pandas DataFrame, è¾“å…¥çš„æ•°æ®è¡¨
            num_type: list, éœ€è¦è½¬æ¢ä¸ºæ•°å€¼ç±»å‹çš„åˆ—ååˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºæˆ–Noneåˆ™è‡ªåŠ¨æ¨æ–­æ‰€æœ‰æ•°å€¼åˆ—
            classify_type: list, éœ€è¦è½¬æ¢ä¸ºç±»åˆ«ç±»å‹çš„åˆ—ååˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºæˆ–Noneåˆ™è‡ªåŠ¨æ¨æ–­å‰©ä½™éæ•°å€¼éæ—¥æœŸåˆ—
            date_type: list, éœ€è¦è½¬æ¢ä¸ºæ—¥æœŸç±»å‹çš„åˆ—ååˆ—è¡¨

        Returns:
            DataFrame: ç±»å‹è½¬æ¢åçš„æ•°æ®è¡¨ï¼Œåˆ—é¡ºåºä¸è¾“å…¥ä¸€è‡´

        è¯¥æ–¹æ³•æ˜¯ç‰¹å¾å·¥ç¨‹æ•°æ®é¢„å¤„ç†çš„æ ¸å¿ƒæ­¥éª¤ï¼Œè´Ÿè´£ç»Ÿä¸€æ•°æ®ç±»å‹ï¼š
        - ç±»åˆ«ç‰¹å¾åˆ—è½¬æ¢ä¸ºpandas stringç±»å‹ï¼Œä¾¿äºåç»­çš„ç¼–ç å¤„ç†
        - æ•°å€¼ç‰¹å¾åˆ—è½¬æ¢ä¸ºfloat64ç±»å‹ï¼Œç¡®ä¿æ•°å€¼è®¡ç®—çš„ç²¾åº¦
        - æ—¥æœŸç‰¹å¾åˆ—è½¬æ¢ä¸ºdatetimeç±»å‹ï¼Œæ”¯æŒæ—¶é—´åºåˆ—åˆ†æ


        å¤„ç†é€»è¾‘ï¼š
        1. è‡ªåŠ¨æ¨æ–­æ•°å€¼åˆ—ï¼šå¦‚æœnum_typeä¸ºç©ºæˆ–Noneï¼Œåˆ™è‡ªåŠ¨æ¨æ–­
           - ä½¿ç”¨data.select_dtypes('number')é€‰æ‹©æ‰€æœ‰æ•°å€¼ç±»å‹çš„åˆ—
           - åŒ…æ‹¬int, float, boolç­‰æ•°å€¼ç±»å‹ï¼Œç¡®ä¿æ•°å€¼æ•°æ®ç»Ÿä¸€å¤„ç†

        2. è‡ªåŠ¨æ¨æ–­ç±»åˆ«åˆ—ï¼šå¦‚æœclassify_typeä¸ºç©ºæˆ–Noneï¼Œåˆ™è‡ªåŠ¨æ¨æ–­
           - ä»æ‰€æœ‰åˆ—ä¸­æ’é™¤num_typeå’Œdate_typeåˆ—ï¼Œå‰©ä½™çš„ä½œä¸ºç±»åˆ«åˆ—
           - è¿™ç§æ–¹å¼å¯ä»¥ç®€åŒ–è°ƒç”¨ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®šæ‰€æœ‰ç±»åˆ«åˆ—

        3. ç±»åˆ«åˆ—å¤„ç†ï¼šå°†æŒ‡å®šçš„ç±»åˆ«åˆ—è½¬æ¢ä¸ºpandas stringç±»å‹
           - ä½¿ç”¨astype("string")è€Œä¸æ˜¯astype(str)ï¼Œä»¥è·å¾—æ›´å¥½çš„å†…å­˜æ•ˆç‡
           - é€šè¿‡é›†åˆæ“ä½œç¡®ä¿åªå¤„ç†å®é™…å­˜åœ¨çš„åˆ—

        4. æ•°å€¼åˆ—å¤„ç†ï¼šå°†æŒ‡å®šçš„æ•°å€¼åˆ—è½¬æ¢ä¸ºfloat64ç±»å‹
           - float64æä¾›äº†è¶³å¤Ÿçš„æ•°å€¼ç²¾åº¦ï¼Œé€‚ç”¨äºå¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•
           - ç»Ÿä¸€æ•°å€¼ç±»å‹æœ‰åŠ©äºåç»­çš„å½’ä¸€åŒ–å’Œæ ‡å‡†åŒ–å¤„ç†

        5. æ—¥æœŸåˆ—å¤„ç†ï¼šå°†æŒ‡å®šçš„æ—¥æœŸåˆ—è½¬æ¢ä¸ºdatetimeç±»å‹
           - é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯datetimeç±»å‹ï¼Œé¿å…é‡å¤è½¬æ¢
           - ä½¿ç”¨errors='coerce'å‚æ•°ï¼Œæ— æ•ˆæ—¥æœŸè½¬ä¸ºNaTè€ŒéæŠ¥é”™
           - æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼çš„è‡ªåŠ¨è¯†åˆ«å’Œè½¬æ¢

        Note:
            - è¯¥æ–¹æ³•ä¸ä¼šä¿®æ”¹åŸå§‹DataFrameï¼Œè€Œæ˜¯è¿”å›å¤„ç†åçš„å‰¯æœ¬
            - ç±»å‹è½¬æ¢æ˜¯æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹çš„é‡è¦å‰ç½®æ­¥éª¤
            - ç»Ÿä¸€çš„æ•°æ®ç±»å‹æœ‰åŠ©äºæé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç¨³å®šæ€§
            - åˆ—é¡ºåºä¿æŒï¼šæ–¹æ³•ä¼šè‡ªåŠ¨ä¿æŒè¾“å…¥DataFrameçš„åˆ—é¡ºåºï¼Œç¡®ä¿è¾“å‡ºç»“æœçš„åˆ—é¡ºåºä¸è¾“å…¥ä¸€è‡´

        Example:
            # å®šä¹‰å„ç±»å‹åˆ—
            num_cols = ['amount', 'age', 'score']
            cat_cols = ['gender', 'city', 'product_type']
            date_cols = ['transaction_date', 'birth_date']

            # æ‰§è¡Œç±»å‹è½¬æ¢
            df_processed = DataDeal.data_type_change(
                df, num_cols, cat_cols, date_cols
            )
        """
        # åˆå§‹åŒ–å‚æ•°ï¼Œç¡®ä¿ä¸ºåˆ—è¡¨ç±»å‹
        date_type = date_type or []

        # ä¿å­˜åŸå§‹åˆ—é¡ºåºï¼Œç¡®ä¿è¿”å›ç»“æœçš„åˆ—é¡ºåºä¸è¾“å…¥ä¸€è‡´
        original_columns = data.columns.tolist()

        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrameï¼Œç”¨äºå­˜å‚¨å¤„ç†åçš„æ•°æ®
        df = pd.DataFrame()

        # è·å–æ‰€æœ‰åˆ—åï¼Œç”¨äºåç»­çš„é›†åˆæ“ä½œ
        col_all = data.columns.tolist()

        # è‡ªåŠ¨æ¨æ–­æ•°å€¼åˆ—ï¼šå¦‚æœnum_typeä¸ºç©ºæˆ–Noneï¼Œåˆ™é€‰æ‹©æ‰€æœ‰æ•°å€¼ç±»å‹çš„åˆ—
        if num_type is None or len(num_type) == 0:
            num_type = data.select_dtypes('number').columns.tolist()

        # è‡ªåŠ¨æ¨æ–­ç±»åˆ«åˆ—ï¼šå¦‚æœclassify_typeä¸ºç©ºæˆ–Noneï¼Œåˆ™ä»æ‰€æœ‰åˆ—ä¸­æ’é™¤æ•°å€¼åˆ—å’Œæ—¥æœŸåˆ—
        if classify_type is None or len(classify_type) == 0:
            # ä½¿ç”¨é›†åˆæ“ä½œï¼Œä»æ‰€æœ‰åˆ—ä¸­æ’é™¤æ•°å€¼åˆ—å’Œæ—¥æœŸåˆ—
            exclude_cols = set(num_type) | set(date_type)
            classify_type = list(set(col_all) - exclude_cols)

        # å¤„ç†ç±»åˆ«ç±»å‹åˆ— - è½¬æ¢ä¸ºstringç±»å‹
        if len(classify_type)>0:
            # å°†ç±»åˆ«åˆ—è½¬æ¢ä¸ºpandas stringç±»å‹ï¼ˆæ¯”strç±»å‹æ›´èŠ‚çœå†…å­˜ï¼‰
            df[classify_type] = data[classify_type].astype("string")

        # å¤„ç†æ•°å€¼ç±»å‹åˆ— - è½¬æ¢ä¸ºfloat64ç±»å‹
        if len(num_type)>0:
            df[num_type] = data[num_type].astype(np.float64)

        # å¤„ç†æ—¥æœŸç±»å‹åˆ— - è½¬æ¢ä¸ºdatetimeç±»å‹
        if len(date_type)>0:
            for col in date_type:
                # åªæœ‰å½“åˆ—ä¸æ˜¯datetimeç±»å‹æ—¶æ‰è¿›è¡Œè½¬æ¢ï¼Œé¿å…é‡å¤æ“ä½œ
                if not pd.api.types.is_datetime64_any_dtype(data[col]):
                    # errors='coerce'å°†æ— æ•ˆæ—¥æœŸè½¬ä¸ºNaT(Not a Time)ï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                    df[col] = pd.to_datetime(data[col], errors='coerce')
                else:
                    # å¦‚æœå·²ç»æ˜¯datetimeç±»å‹ï¼Œç›´æ¥å¤åˆ¶
                    df[col] = data[col]

        # æ¢å¤åŸå§‹åˆ—é¡ºåº
        df = df[original_columns]

        # è¿”å›å¤„ç†åçš„DataFrame
        return df

    @staticmethod
    def date_deal(data,date_type=[]):
        """to_datetimeå¤„ç†æ—¥æœŸåˆ—
        """
        column_all = data.columns
        
        ### æ—¥æœŸ
        date_type = [col for col in date_type if col in column_all] 
        # data = str_pd(data, date_type)
        for col in date_type:
            if pd.api.types.is_datetime64_any_dtype(data[col]):
                continue
            data[col] = pd.to_datetime(data[col], errors='coerce') 

    @staticmethod
    def drop_cols(df_all, columns=["dt"]):
        """å¤šä½™å­—æ®µåˆ é™¤"""
        # å¤šäº†ä¸€ä¸ªdtæ—¥æœŸ è¿™é‡Œåšåˆ é™¤å¤„ç†
        df_all.drop(columns=columns,inplace=True)

    @staticmethod
    def file_counter(file_path, add_num=0.01, reset0=False, format_float=None, return_int=False, max_float_count=4):
        """ä¸´æ—¶æ–‡ä»¶è®¡æ•°å™¨
        - file_path:æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        - add_num: æ¯æ¬¡è¯»å–å¢åŠ çš„æ•°å€¼
        - reset0:ä¸ºTrueä¼šå°†æ–‡ä»¶çš„æ•°å­—ç½®ä¸º0
        - format_float:æŒ‡å®šå°æ•°ä½æ ¼å¼ï¼Œæ¯”å¦‚ï¼Œ".2f"ï¼Œæ•ˆæœç±»ä¼¼0.10ï¼Œæœ€åä¸€ä½æ˜¯0ä¹Ÿä¼šä¿ç•™
        -return_int:è¿”å›æ•´æ•°
        - max_float_count:æœ€å¤§å°æ•°ä½ï¼Œæœ€å¤šä¿ç•™å‡ ä½å°æ•°

        examples
        -------------------------
        file_path = '.tmp_model_count.txt'
        count = file_counter(file_path, add_num=0.01, reset0=False)

        count = file_counter(file_path, add_num=0.01, reset0=False, format_float=".2f")
        """
        if reset0:
            write(0, file_path)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºæ–‡ä»¶å¹¶å†™å…¥0
            write(0, file_path)
            current_count = 0
        else:
            # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œåˆ™è¯»å–æ–‡ä»¶ä¸­çš„æ•°å­—ï¼Œç„¶å+1
            current_count = read(file_path)
            current_count += add_num
            # å°†+1åçš„æ•°å­—å†™å…¥æ–‡ä»¶
            current_count=round(current_count, max_float_count)
            write(current_count, file_path)
        if return_int:
            return round(current_count)
        elif format_float is not None:
            return  f"{current_count:.2f}"

        # è¿”å›+1åçš„æ•°å­—
        return  current_count




    @staticmethod
    def getXy(data_path: str,
            label_name: str,
            identity_cols: List[str],
            sep: str = '~',
            is_train: bool = True,
            usecols: Optional[List[str]] = None,
            drop_columns: Optional[List[str]] = None,
            dtype_mapping: Optional[Dict[str, Any]] = None,
            is_categorical_func: Optional[Callable[[str], bool]] = None,
            date_type=[],
            bool_type = [],
            ) -> Tuple[pd.DataFrame, Optional[pd.Series]]:
        
        """
        æ•°æ®åŠ è½½ä¸ç‰¹å¾æå–ï¼šä»åŸå§‹æ•°æ®æ–‡ä»¶ä¸­åŠ è½½ç‰¹å¾æ•°æ®ã€æ ‡ç­¾å’Œå­—æ®µåˆ†ç±»ä¿¡æ¯
        è¿”å›æ ‡è¯†åˆ—+å‡åºçš„ç‰¹å¾åˆ—
        
        è¯¥æ–¹æ³•æ˜¯å¯¹ get_features å‡½æ•°çš„å°è£…ï¼Œæä¾›ç®€åŒ–çš„æ¥å£ç”¨äºå¸¸è§çš„æ•°æ®åŠ è½½åœºæ™¯
        
        Parameters:
        -----------
        data_path : str
            æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ CSV æ ¼å¼
        label_name : str  
            æ ‡ç­¾åˆ—åï¼Œç”¨äºæå–ç›®æ ‡å˜é‡
        identity_cols : List[str]
            èº«ä»½æ ‡è¯†åˆ—åˆ—è¡¨ï¼Œè¿™äº›åˆ—ä¸å‚ä¸å»ºæ¨¡ä½†éœ€è¦ä¿ç•™åœ¨ç»“æœä¸­
        sep : str, default='~'
            æ•°æ®æ–‡ä»¶åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º '~'
        is_train : bool, default=True
            æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼ï¼ŒTrue æ—¶æå–æ ‡ç­¾ yï¼ŒFalse æ—¶ä¸æå–
        usecols : List[str], optional
            æŒ‡å®šè¦è¯»å–çš„åˆ—ååˆ—è¡¨ï¼Œå¦‚æœä¸º None åˆ™è¯»å–æ‰€æœ‰åˆ—
        drop_columns : List[str], optional  
            è¦æ˜ç¡®ä¸¢å¼ƒçš„åˆ—ååˆ—è¡¨ï¼Œå¦‚ä¸´æ—¶å­—æ®µæˆ–ä¸éœ€è¦çš„åˆ—
        dtype_mapping : Dict[str, Any], optional
            å¼ºåˆ¶æŒ‡å®šæŸäº›åˆ—çš„æ•°æ®ç±»å‹ï¼Œå¦‚ {"Amount": "float32", "age": "int32"}
        is_categorical_func : Callable[[str], bool], optional
            åˆ¤æ–­åˆ—æ˜¯å¦ä¸ºç±»åˆ«å‹çš„å‡½æ•°ï¼Œè¾“å…¥åˆ—åè¿”å›å¸ƒå°”å€¼
            å¦‚æœä¸º Noneï¼Œé»˜è®¤ä½¿ç”¨ "is_" å¼€å¤´çš„åˆ—ä½œä¸ºç±»åˆ«åˆ—
            
        Returns:
        --------
        Tuple[pd.DataFrame, Optional[pd.Series]]
            è®­ç»ƒæ¨¡å¼ (is_train=True): (X, y, col_types) - ç‰¹å¾ DataFrameã€æ ‡ç­¾ Series å’Œå­—æ®µç±»å‹å­—å…¸
            é¢„æµ‹æ¨¡å¼ (is_train=False): (X, NOne,col_types) - ç‰¹å¾ DataFrame å’Œå­—æ®µç±»å‹å­—å…¸
            
        Notes:
        ------
        - è¿”å›çš„ X åŒ…å« identity_cols åˆ—
        - å†…éƒ¨è‡ªåŠ¨æ¨æ–­å­—æ®µç±»å‹ï¼šæ•°å€¼å‹ã€ç±»åˆ«å‹ã€å¸ƒå°”å‹ç­‰
        - æ”¯æŒè‡ªå®šä¹‰ç±»åˆ«åˆ¤æ–­é€»è¾‘ï¼Œé€‚åº”ä¸åŒæ•°æ®é›†çš„ç‰¹ç‚¹
        - æ ‡ç­¾åˆ—è‡ªåŠ¨ä»ç‰¹å¾ä¸­åˆ†ç¦»ï¼Œä¸åŒ…å«åœ¨ X ä¸­
        
        Examples:
        ---------
        # 1. åŸºæœ¬ä½¿ç”¨ - è®­ç»ƒæ•°æ®åŠ è½½
        X, y, col_types = DataDeal.getXy(
            data_path="train_data.csv",
            label_name="Is_Fraud", 
            identity_cols=["Account", "Bank"],
            sep='~',
            is_train=True
        )
        
        # 2. é¢„æµ‹æ•°æ®åŠ è½½  
        X_pred, col_types = DataDeal.getXy(
            data_path="test_data.csv",
            label_name="Is_Fraud",
            identity_cols=["Account"], 
            sep='~',
            is_train=False
        )
        
        # 3. è‡ªå®šä¹‰ç±»åˆ«åˆ¤æ–­ + ç±»å‹æ˜ å°„
        X, y, col_types = DataDeal.getXy(
            data_path="data.csv",
            label_name="Target",
            identity_cols=["ID"],
            usecols=["ID", "Amount", "risk_flag", "Target"],
            dtype_mapping={"Amount": "float32"},
            is_categorical_func=lambda col: col.endswith("_flag") or col.startswith("is_")
        )
        
        print("Numeric columns:", col_types['num_type'])
        print("Categorical columns:", col_types['classify_type'])
        """

        X, y, col_types = get_features(
            data_path=data_path,
            label_name=label_name,
            identity_cols=identity_cols,
            sep=sep,
            is_categorical_func=is_categorical_func or (lambda c: c.lower().startswith("is_")),
            date_type=date_type,
            bool_type=bool_type,
            usecols=usecols,
            drop_columns=drop_columns,
            dtype_mapping=dtype_mapping,
            is_train=is_train
        )
        return (X, y, col_types) if is_train else (X, None, col_types)

    @staticmethod
    def min_max_scaler(X, num_type=[], model_path=f"min_max_scaler.pkl", 
                       reuse=False, col_sort=True, force_rewrite=False):
        """
        MinMaxScalerå½’ä¸€åŒ–å¤„ç†ï¼šå°†æ•°å€¼ç‰¹å¾ç¼©æ”¾åˆ°[0,1]åŒºé—´ï¼Œæ”¯æŒæ¨¡å‹å¤ç”¨å’Œè‡ªåŠ¨ä¿å­˜ã€‚

        Args:
            X: pandas DataFrame, è¾“å…¥çš„æ•°æ®è¡¨
            num_type: list, éœ€è¦å½’ä¸€åŒ–çš„æ•°å€¼åˆ—ååˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºåˆ™è‡ªåŠ¨é€‰æ‹©æ‰€æœ‰æ•°å€¼ç±»å‹çš„åˆ—
            model_path: str, å½’ä¸€åŒ–æ¨¡å‹ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸º'min_max_scaler.pkl'
            reuse: bool, æ˜¯å¦å¤ç”¨å·²æœ‰æ¨¡å‹ï¼ŒFalseæ—¶ä¸ä¿å­˜æ–‡ä»¶ï¼ŒTrueæ—¶æ”¯æŒæ¨¡å‹æŒä¹…åŒ–
            col_sort: bool, æ˜¯å¦å¯¹åˆ—åè¿›è¡Œæ’åºï¼Œç¡®ä¿å¤„ç†é¡ºåºçš„ä¸€è‡´æ€§
            force_rewrite: bool, æ˜¯å¦å¼ºåˆ¶é‡å†™æ¨¡å‹æ–‡ä»¶ï¼ŒTrueæ—¶æ€»æ˜¯é‡æ–°è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹

        Returns:
            DataFrame: å½’ä¸€åŒ–å¤„ç†åçš„æ•°æ®è¡¨

        å¤„ç†é€»è¾‘ï¼š
        1. åˆ—é€‰æ‹©ä¸éªŒè¯ï¼š
           - å¦‚æœnum_typeä¸ºç©ºï¼Œè‡ªåŠ¨é€‰æ‹©æ‰€æœ‰æ•°å€¼ç±»å‹çš„åˆ—
           - å¯é€‰åˆ—åæ’åºï¼Œç¡®ä¿å¤„ç†é¡ºåºçš„ä¸€è‡´æ€§

        2. æ¨¡å‹å¤ç”¨ç­–ç•¥ï¼š
           - reuse=Falseï¼šæ¯æ¬¡é‡æ–°è®­ç»ƒï¼Œä¸ä¿å­˜æ¨¡å‹ï¼Œé€‚ç”¨äºä¸´æ—¶åˆ†æ
           - reuse=Trueï¼šæ£€æŸ¥model_pathæ˜¯å¦å­˜åœ¨ï¼Œå­˜åœ¨åˆ™åŠ è½½å¤ç”¨ï¼Œä¸å­˜åœ¨åˆ™è®­ç»ƒå¹¶ä¿å­˜
           - force_rewrite=Trueï¼šå¼ºåˆ¶é‡æ–°è®­ç»ƒå¹¶è¦†ç›–å·²æœ‰æ¨¡å‹ï¼Œå¿½ç•¥reuseçš„å¤ç”¨é€»è¾‘

        3. ç›®å½•å®‰å…¨æ£€æŸ¥ï¼š
           - ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨ï¼Œé¿å…å†™å…¥å¤±è´¥

        4. å½’ä¸€åŒ–å¤„ç†ï¼š
           - ä½¿ç”¨MinMaxScalerå°†æ•°æ®ç¼©æ”¾åˆ°[0,1]åŒºé—´
           - ä¿æŒåŸå§‹æ•°æ®çš„åˆ†å¸ƒç‰¹å¾ï¼Œä»…è¿›è¡Œçº¿æ€§å˜æ¢

        å‚æ•°è¯¦ç»†è¯´æ˜ï¼š
        - num_type: æ”¯æŒè‡ªåŠ¨æ¨æ–­æ•°å€¼åˆ—ï¼Œä½¿ç”¨select_dtypes('number')è‡ªåŠ¨è¯†åˆ«
        - reuse: False=ä¸´æ—¶æ¨¡å¼(ä¸ä¿å­˜)ï¼ŒTrue=ç”Ÿäº§æ¨¡å¼(ä¿å­˜/å¤ç”¨)
        - model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒè‡ªå®šä¹‰å‘½åå’Œè·¯å¾„ç®¡ç†
        - col_sort: æ’åºç¡®ä¿åˆ—å¤„ç†é¡ºåºä¸€è‡´ï¼Œæé«˜ç»“æœçš„å¯é‡ç°æ€§
        - force_rewrite: True=å¼ºåˆ¶é‡å†™æ¨¡å¼(æ€»æ˜¯é‡æ–°è®­ç»ƒ)ï¼ŒFalse=æ­£å¸¸æ¨¡å¼(éµå¾ªreuseé€»è¾‘)

        Examples:
            # åŸºç¡€ä½¿ç”¨ - è‡ªåŠ¨é€‰æ‹©æ‰€æœ‰æ•°å€¼åˆ—
            X_normalized = DataDeal.min_max_scaler(X)

            # æŒ‡å®šç‰¹å®šåˆ—è¿›è¡Œå½’ä¸€åŒ–
            X_normalized = DataDeal.min_max_scaler(
                X,
                num_type=['amount', 'age', 'score'],
                model_path='financial_scaler.pkl'
            )

            # è®­ç»ƒé˜¶æ®µ - ä¿å­˜æ¨¡å‹ä¾›åç»­ä½¿ç”¨
            X_train_normalized = DataDeal.min_max_scaler(
                X_train,
                num_type=['feature1', 'feature2'],
                model_path='model/scaler.pkl',
                reuse=True  # é¦–æ¬¡è®­ç»ƒå¹¶ä¿å­˜
            )

            # æµ‹è¯•é˜¶æ®µ - å¤ç”¨å·²æœ‰æ¨¡å‹
            X_test_normalized = DataDeal.min_max_scaler(
                X_test,
                num_type=['feature1', 'feature2'],
                model_path='model/scaler.pkl',
                reuse=True  # åŠ è½½å·²æœ‰æ¨¡å‹
            )

            # ä¸´æ—¶åˆ†æ - ä¸ä¿å­˜æ¨¡å‹
            X_temp_normalized = DataDeal.min_max_scaler(
                X,
                reuse=False  # ä¸´æ—¶ä½¿ç”¨ï¼Œä¸ä¿å­˜
            )

            # å¼ºåˆ¶é‡å†™ - é‡æ–°è®­ç»ƒå¹¶è¦†ç›–å·²æœ‰æ¨¡å‹
            X_retrained = DataDeal.min_max_scaler(
                X,
                num_type=['feature1', 'feature2'],
                model_path='model/scaler.pkl',
                reuse=True,
                force_rewrite=True  # å¼ºåˆ¶é‡æ–°è®­ç»ƒï¼Œå¿½ç•¥å·²æœ‰æ¨¡å‹
            )

            # æ¨¡å‹æ›´æ–° - å½“æ•°æ®åˆ†å¸ƒå‘ç”Ÿå˜åŒ–æ—¶å¼ºåˆ¶é‡å†™
            X_updated = DataDeal.min_max_scaler(
                X_new_data,
                model_path='model/old_scaler.pkl',
                force_rewrite=True  # ä½¿ç”¨æ–°æ•°æ®é‡æ–°è®­ç»ƒå¹¶è¦†ç›–
            )

        Note:
            - å½’ä¸€åŒ–åçš„æ•°æ®èŒƒå›´åœ¨[0,1]ä¹‹é—´ï¼Œä¿ç•™äº†åŸå§‹æ•°æ®çš„åˆ†å¸ƒç‰¹å¾
            - æ¨¡å‹å¤ç”¨ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä½¿ç”¨ç›¸åŒçš„ç¼©æ”¾å‚æ•°ï¼Œé¿å…æ•°æ®æ³„éœ²
            - å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨reuse=Trueä»¥ç¡®ä¿æ•°æ®é¢„å¤„ç†çš„ä¸€è‡´æ€§
            - force_rewrite=Trueé€‚ç”¨äºæ•°æ®åˆ†å¸ƒå˜åŒ–ã€æ¨¡å‹å‚æ•°è°ƒæ•´æˆ–éœ€è¦é‡æ–°è®­ç»ƒçš„åœºæ™¯
            - force_rewriteå‚æ•°ä¼˜å…ˆçº§é«˜äºreuseï¼Œå½“force_rewrite=Trueæ—¶ä¼šå¿½ç•¥reuseçš„å¤ç”¨é€»è¾‘
            - åˆ—é¡ºåºä¿æŒï¼šæ–¹æ³•ä¼šè‡ªåŠ¨ä¿æŒè¾“å…¥DataFrameçš„åˆ—é¡ºåºï¼Œç¡®ä¿è¾“å‡ºç»“æœçš„åˆ—é¡ºåºä¸è¾“å…¥ä¸€è‡´
        """
        # è°ƒè¯•ä¿¡æ¯ï¼šè¾“å‡ºæ•°æ®ç±»å‹å’Œå½¢çŠ¶ï¼ˆæ³¨é‡Šæ‰ï¼Œéœ€è¦æ—¶å¯å–æ¶ˆæ³¨é‡Šï¼‰
        # print(type(X),X.shape)

        # è‡ªåŠ¨æ¨æ–­æ•°å€¼åˆ—ï¼šå¦‚æœæœªæŒ‡å®šï¼Œåˆ™é€‰æ‹©æ‰€æœ‰æ•°å€¼ç±»å‹çš„åˆ—
        if len(num_type) == 0:
            num_type = X.select_dtypes('number').columns.tolist()

        # åˆ—åæ’åºï¼šç¡®ä¿å¤„ç†é¡ºåºçš„ä¸€è‡´æ€§ï¼Œæé«˜ç»“æœçš„å¯é‡ç°æ€§
        if col_sort:
            num_type = sorted(num_type)

        # ç›®å½•å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨ï¼Œé¿å…å†™å…¥å¤±è´¥
        p_dir = parentdir(model_path)
        if not os.path.exists(p_dir):
            raise Exception(f"The file directory {p_dir} does not exist, unable to write files to it ")

        # æ¨¡å‹å¤ç”¨ç­–ç•¥ï¼šæ ¹æ®reuseå’Œforce_rewriteå‚æ•°å†³å®šæ¨¡å‹å¤„ç†æ–¹å¼
        if force_rewrite:
            # å¼ºåˆ¶é‡å†™æ¨¡å¼ï¼šæ€»æ˜¯é‡æ–°è®­ç»ƒå¹¶è¦†ç›–å·²æœ‰æ¨¡å‹
            scaler_train = preprocessing.MinMaxScaler().fit(X[num_type])
            if reuse:  # åªæœ‰åœ¨reuse=Trueæ—¶æ‰ä¿å­˜æ¨¡å‹
                pkl_save(scaler_train, file_path=model_path, use_joblib=True)
        elif reuse:
            if os.path.exists(model_path):
                # å¤ç”¨æ¨¡å¼ï¼šåŠ è½½å·²ä¿å­˜çš„å½’ä¸€åŒ–æ¨¡å‹
                scaler_train = pkl_load(file_path=model_path, use_joblib=True)
            else:
                # è®­ç»ƒæ¨¡å¼ï¼šæ‹Ÿåˆæ–°çš„å½’ä¸€åŒ–æ¨¡å‹å¹¶ä¿å­˜
                scaler_train = preprocessing.MinMaxScaler().fit(X[num_type])
                pkl_save(scaler_train, file_path=model_path, use_joblib=True)
        else:
            # ä¸´æ—¶æ¨¡å¼ï¼šæ¯æ¬¡é‡æ–°è®­ç»ƒï¼Œä¸ä¿å­˜æ¨¡å‹
            scaler_train = preprocessing.MinMaxScaler().fit(X[num_type])

        # åº”ç”¨å½’ä¸€åŒ–è½¬æ¢ï¼šå°†æŒ‡å®šåˆ—ç¼©æ”¾åˆ°[0,1]åŒºé—´
        X[num_type] = scaler_train.transform(X[num_type])

        return X

    @staticmethod 
    def min_max_scaler_log(df, num_type=[], model_path=f"min_max_scaler.pkl", reuse=False,
                       log=False,log2=False,log10=False):
        """é’ˆå¯¹æŒ‡å®šçš„æ•°å­—æ•°æ®ç±»å‹åšmin max scalerï¼Œé€šå¸¸æ˜¯float32ï¼Œfloat64,int64ç±»å‹çš„æ•°æ®
        
        params
        ---------------------------
        - num_type:éœ€è¦åšå½’ä¸€åŒ–çš„æ•°å­—åˆ—ï¼Œå¦‚æœä¸ºç©ºï¼Œåˆ™å–æ•°æ®Xçš„æ‰€æœ‰åˆ—
        - reuse:Falseå°±ä¸éœ€è¦å¤ç”¨ï¼Œä¹Ÿä¸ä¼šä¿å­˜æ–‡ä»¶ï¼Œæ­¤æ—¶model_pathå‚æ•°ä¸èµ·ä½œç”¨ï¼Œæ¯”å¦‚ä¸€äº›æ— ç›‘ç£ï¼Œç‰¹å¾é€‰æ‹©ç­‰åœºæ™¯
        
        examples
        -------------------------------------------------
        # è®­ç»ƒé›†æ•°å­—ç±»å‹å½’ä¸€åŒ–, reuse=Trueæ—¶ï¼Œé¦–æ¬¡æ‰§è¡Œå› model_pathä¸å­˜åœ¨ä¼šä¿å­˜preprocessing.MinMaxScaler().fitçš„ç»“æœ
        ddl.s3_min_max_scaler(X, num_type=pc.col_type.num_type, model_path=pc.scale_path, reuse=True)

        #reuse=Trueä¸”model_pathå­˜åœ¨æ—¶ï¼Œç›´æ¥åŠ è½½æ–‡ä»¶ï¼Œç„¶åtransform
        ddl.s3_min_max_scaler(X_test, num_type=pc.col_type.num_type,model_path=pc.scale_path, reuse=True)
        
        """
        
        if log:
            df[num_type] = df[num_type].clip(lower=1)
            if TORCH_AVAILABLE:
                df.loc[:,num_type] = torch.log(torch.tensor(df[num_type].values, dtype=torch.float32)).numpy()
            else:
                df.loc[:,num_type] = np.log(df[num_type].values)
        if log2:
            df[num_type] = df[num_type].clip(lower=1)
            if TORCH_AVAILABLE:
                df.loc[:,num_type] = torch.log2(torch.tensor(df[num_type].values, dtype=torch.float32)).numpy()
            else:
                df.loc[:,num_type] = np.log2(df[num_type].values)
        if log10:
            df[num_type] = df[num_type].clip(lower=1)
            if TORCH_AVAILABLE:
                df.loc[:,num_type] = torch.log10(torch.tensor(df[num_type].values, dtype=torch.float32)).numpy()
            else:
                df.loc[:,num_type] = np.log10(df[num_type].values)
        
        DataDeal.min_max_scaler(df, num_type=num_type, model_path=model_path, reuse=reuse)


    @staticmethod
    def min_max_scaler_dt(X, date_type=[], scaler_file=None, max_date=None, adjust=True):
        """
        å¯¹pandasæ•°æ®è¡¨ä¸­çš„æ—¥æœŸåˆ—è¿›è¡Œå½’ä¸€åŒ–å¤„ç†;æ¯æ®µæ—¥æœŸéƒ½æ˜¯ä»0åˆ°1çš„åŒºé—´å†…ï¼Œå°†ä¸€æ®µæ—¶é—´çº³å…¥[0,1]
        
        å‚æ•°:
        - X: pandas DataFrame, éœ€è¦å¤„ç†çš„æ•°æ®è¡¨
        - date_type: list, éœ€è¦å½’ä¸€åŒ–çš„æ—¥æœŸåˆ—ååˆ—è¡¨
        - scaler_file: str, ç”¨äºä¿å­˜æˆ–åŠ è½½å½’ä¸€åŒ–å‚æ•°çš„jsonæ–‡ä»¶è·¯å¾„
        - max_date: str, æŒ‡å®šå½’ä¸€åŒ–ä½¿ç”¨çš„æœ€å¤§æ—¥æœŸï¼ˆå¦‚'2099-01-01'ï¼‰;å› ä¸ºé¢„æµ‹æ—¶çš„æ—¥æœŸæ˜¯æœªæ¥çš„ï¼Œåœ¨è®­ç»ƒæ—¶æ˜¯æ²¡æœ‰ï¼Œå› æ­¤æ”¯æŒæŒ‡å®š
        - adjust:å°†è¿‡äºå°çš„æ•°ï¼Œè°ƒæ•´å¤§ä¸€ç‚¹ï¼Œåªæœ‰ä½¿ç”¨äº†max_dateæ‰ä¼šç”Ÿæ•ˆï¼Œè¿™æ˜¯ç¼“å†²max_dateè®¾ç½®è¿‡å¤§å¸¦æ¥çš„å½’ä¸€åŒ–åæ•°å€¼è¿‡å°çš„å½±å“
        
        è¿”å›:
        - å¤„ç†åçš„DataFrame

        examples
        -----------------------------------------
        # ä¸ä½¿ç”¨max_dateï¼ˆä½¿ç”¨æ•°æ®å®é™…æœ€å¤§å€¼ï¼‰
        df_normalized = dt_min_max_scaler(df, date_type=['date_column'])
        
        # ä½¿ç”¨max_dateæŒ‡å®šæœ€å¤§æ—¥æœŸ
        df_normalized = dt_min_max_scaler(df, date_type=['date_column'], max_date='2099-01-01')
        
        # åŒæ—¶ä½¿ç”¨scaler_fileå’Œmax_date
        df_normalized = dt_min_max_scaler(df, 
                                        date_type=['date_column'], 
                                        scaler_file='scaler_params.json',
                                        max_date='2099-01-01')
                                    
        """
        
        # å¦‚æœdate_typeä¸ºç©ºï¼Œç›´æ¥è¿”å›åŸæ•°æ®
        if not date_type:
            return X
        
        # å¦‚æœæä¾›äº†scaler_fileä¸”æ–‡ä»¶å­˜åœ¨ï¼Œåˆ™åŠ è½½å½’ä¸€åŒ–å‚æ•°
        if scaler_file and Path(scaler_file).exists():
            # with open(scaler_file, 'r') as f:
                # scaler_params = json.load(f)
            scaler_params = read(scaler_file)
        else:
            scaler_params = {}
 
        # å¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸDataFrame
        df = X.copy()
        
        # å°†max_dateè½¬æ¢ä¸ºæ—¶é—´æˆ³æ•°å€¼ï¼ˆå¦‚æœæä¾›äº†ï¼‰
        max_date_value = pd.to_datetime(max_date).value if max_date else None
        
        for col in date_type:
            # ç¡®ä¿åˆ—å­˜åœ¨
            if col not in df.columns:
                continue
                
            # è½¬æ¢ä¸ºdatetimeç±»å‹
            df[col] = pd.to_datetime(df[col])
            
            # è½¬æ¢ä¸ºæ—¶é—´æˆ³æ•°å€¼
            df[col] = df[col].apply(lambda x: x.value)
            
            # å¦‚æœscaler_fileå­˜åœ¨ä¸”åŒ…å«å½“å‰åˆ—çš„å‚æ•°ï¼Œåˆ™ä½¿ç”¨ä¿å­˜çš„å‚æ•°
            adjust_val = 1
            if scaler_params and (col in scaler_params):
                min_val = scaler_params[col]['min']
                max_val = scaler_params[col]['max']
            else:
                min_val = df[col].min()
                # å¦‚æœæä¾›äº†max_dateåˆ™ä½¿ç”¨å®ƒï¼Œå¦åˆ™ä½¿ç”¨æ•°æ®çš„æœ€å¤§å€¼
                max_val = max_date_value if max_date_value else df[col].max()
                scaler_params[col] = {'min': min_val, 'max': max_val}
            
            # æ‰§è¡Œå½’ä¸€åŒ–
            range_val = max_val - min_val
            if range_val > 0:  # é¿å…é™¤ä»¥0
                df[col] = (df[col] - min_val) / range_val
            else:
                df[col] = 0.0  # å¦‚æœæ‰€æœ‰å€¼ç›¸åŒï¼Œå½’ä¸€åŒ–ä¸º0

            if max_date and adjust:
                if "adjust_val" in scaler_params[col].keys():
                    adjust_val = scaler_params[col]["adjust_val"]
                    df[col] = df[col]*adjust_val
                else:
                    df_col_max = df[col].max()
                    df_col_max = np.abs(df_col_max)
                    if df_col_max<0.00001:
                        adjust_val = 100000
                        df[col] = df[col]*adjust_val
                    elif df_col_max<0.0001:
                        adjust_val = 10000
                        df[col] = df[col]*adjust_val
                    elif df_col_max<0.001:
                        adjust_val = 1000
                        df[col] = df[col]*adjust_val
                    elif df_col_max<0.01:
                        adjust_val = 100
                        df[col] = df[col]*adjust_val
                    scaler_params[col]["adjust_val"] = float(adjust_val)
                
        # å¦‚æœæŒ‡å®šäº†scaler_fileï¼Œåˆ™ä¿å­˜å½’ä¸€åŒ–å‚æ•°
        if scaler_file:
            write(scaler_params,file_path=scaler_file)
        return df
    
    @staticmethod
    def min_max_scale_sample(df, col, min_val, max_val):
        """Min-max scale a column"""
        return (df[col] - min_val) / (max_val - min_val)
    
    @staticmethod
    def min_max_update(df, num_type=[],num_small=[], 
                       is_pre=False, num_scaler_file=None,
                       log=False,log2=False,log10=False):
        """
        MinMaxScalerCustomç±»æ¯æ¬¡partial_fitæ—¶æ›´æ–°min-maxå€¼ï¼Œå› æ­¤æ¯æ¬¡fitæ—¶éƒ½ä¿å­˜min-maxå€¼
        - æ¯ä¸ªåˆ—éƒ½éœ€å•ç‹¬å¤„ç†ï¼Œä¸€ä¸ªå•ç‹¬çš„MinMaxScalerCustomå®ä¾‹
        
        Parameters:
        - df: DataFrame to process
        - dict_file: Not used in this implementation (kept for compatibility)
        - is_pre: Whether in preprocessing mode
        - num_scaler_file: File to store/load min-max scaler values (using joblib)
        - log: Whether to apply log transformation
        - log2: Whether to apply log2 transformation
        - log10: Whether to apply log10 transformation
        - num_small: List of column names to exclude from log transformations
        
        Returns:
        - Processed DataFrame
        """
        
        if num_scaler_file is None:
            raise ValueError("num_scaler_file must be specified")
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦logå˜æ¢çš„åˆ—ï¼Œä¸€æ¬¡æ€§å¤„ç†é¿å…DataFrameç¢ç‰‡åŒ–
        log_transformations = []
        
        if log:
            log_cols = [col for col in num_type if col not in num_small]
            if log_cols:
                df[log_cols] = df[log_cols].clip(lower=1)
                if TORCH_AVAILABLE:
                    log_transformations.append((log_cols, torch.log))
                else:
                    log_transformations.append((log_cols, np.log))

        if log2:
            log_cols = [col for col in num_type if col not in num_small]
            if log_cols:
                df[log_cols] = df[log_cols].clip(lower=1)
                if TORCH_AVAILABLE:
                    log_transformations.append((log_cols, torch.log2))
                else:
                    log_transformations.append((log_cols, np.log2))

        if log10:
            log_cols = [col for col in num_type if col not in num_small]
            if log_cols:
                df[log_cols] = df[log_cols].clip(lower=1)
                if TORCH_AVAILABLE:
                    log_transformations.append((log_cols, torch.log10))
                else:
                    log_transformations.append((log_cols, np.log10))
        
        # ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰logå˜æ¢ï¼Œé¿å…å¤šæ¬¡drop/addæ“ä½œå¯¼è‡´çš„ç¢ç‰‡åŒ–
        if log_transformations:
            # åˆ›å»ºæ–°çš„åˆ—æ•°æ®å­—å…¸
            new_columns = {}
            for log_cols, log_func in log_transformations:
                if TORCH_AVAILABLE:
                    df_tmp = log_func(torch.tensor(df[log_cols].values, dtype=torch.float32)).numpy()
                else:
                    df_tmp = log_func(df[log_cols].values)
                for i, col in enumerate(log_cols):
                    if df_tmp.ndim == 1:
                        new_columns[col] = df_tmp
                    else:
                        new_columns[col] = df_tmp[:, i]
            
            # ä¸€æ¬¡æ€§ç§»é™¤æ—§åˆ—å¹¶æ·»åŠ æ–°åˆ—
            cols_to_remove = [col for log_cols, _ in log_transformations for col in log_cols]
            df = df.drop(columns=cols_to_remove)
            
            # ä½¿ç”¨concatä¸€æ¬¡æ€§æ·»åŠ æ‰€æœ‰æ–°åˆ—ï¼Œé¿å…ç¢ç‰‡åŒ–
            if new_columns:
                new_df = pd.DataFrame(new_columns, index=df.index)
                df = pd.concat([df, new_df], axis=1)
            
        
        # Initialize scaler dictionary (column_name -> MinMaxScalerCustom)
        scaler_dict = {}
        
        # If not preprocessing mode, we need to update scaler values
        if not is_pre:
            # Load existing scaler values if file exists
            if os.path.exists(num_scaler_file):
                scaler_dict = joblib.load(num_scaler_file)
            
            # Update scaler values with new data
            if len(num_type) == 0:
                num_type = df.select_dtypes(include=['number']).columns
            for col in num_type:
                col_data = df[col].values
                
                if col in scaler_dict:
                    # Update min/max if current is lower/higher
                    scaler_dict[col].partial_fit(col_data)
                else:
                    # Initialize new column scaler
                    scaler = MinMaxScalerCustom()
                    scaler.fit(col_data)
                    scaler_dict[col] = scaler
            
            # Save updated scaler values
            joblib.dump(scaler_dict, num_scaler_file)
        
        else:
            # In preprocessing mode, just load the scaler values
            if not os.path.exists(num_scaler_file):
                raise FileNotFoundError(f"Scaler file {num_scaler_file} not found for preprocessing")
            
            scaler_dict = joblib.load(num_scaler_file)
        # print(scaler_dict)
        # Apply min-max scaling
        processed_df = df.copy()
        for col, scaler in scaler_dict.items():
            if col in processed_df.columns:
                if scaler.max_ != scaler.min_:  # Avoid division by zero
                    processed_df[col] = scaler.transform(processed_df[col].values)
                else:
                    processed_df[col] = 0.0  # Default value for constant columns
        
        return processed_df

    
    @staticmethod
    def num_deal(data, num_type):
        column_all = data.columns
        ### æ•°å­—
        num_type = [col for col in num_type if col in column_all] 
        data[num_type] = data[num_type].astype(np.float32)
    
    @staticmethod
    def num_describe(df,pc:ParamConfig=None):
        # åªæ£€æŸ¥æ•°å€¼åˆ—
        numeric_cols = df.select_dtypes(include='number').columns
        
        # åˆ é™¤å€¼å…¨ä¸º0çš„æ•°å€¼åˆ—
        tmp_num_df = df[numeric_cols]
        df_with_nan = tmp_num_df.copy()

        # åˆ é™¤å…¨ä¸º0æˆ–NaNçš„åˆ—
        df_cleaned_nan = df_with_nan.loc[:, df_with_nan.notna().any() & (df_with_nan != 0).any()]
        
        if pc:
            pc.lg(f"df_cleaned_nan.shape={df_cleaned_nan.shape}")
            pc.lg(f"num describe:\n{df_cleaned_nan[:3]}")
            pc.lg(f"num fenbu:\n{df_cleaned_nan.describe()}")
            
        return df_cleaned_nan
            
    @staticmethod
    def null_deal_pandas(data,cname_num_type=[], cname_str_type=[], num_padding=0, str_padding = '<PAD>'):
        """
        params
        ----------------------------------
        - data:pandasæ•°è¡¨
        - cname_num_typeï¼šæ•°å­—ç±»å‹åˆ—è¡¨
        - cname_str_typeï¼šå­—ç¬¦ç±»å‹åˆ—è¡¨
        - num_padding:æ•°å­—ç±»å‹ç©ºå€¼å¡«å……
        - str_padding:å­—ç¬¦ç±»å‹ç©ºå€¼å¡«å……
        
        example
        -----------------------------------
        #ç©ºå€¼å¤„ç†
        data = null_deal_pandas(data,cname_num_type=num_type,cname_str_type=str_classification,num_padding=0,str_padding = '<PAD>')

        """
        if len(cname_num_type)>0:
            # æ•°å­—ç½®ä¸º0
            for col in cname_num_type:
                data.loc[data[col].isna(),col]=num_padding
        
        if len(cname_str_type)>0:
            #objectè½¬strï¼Œä»…å¤„ç†åˆ†ç±»ç‰¹å¾ï¼Œèº«ä»½è®¤è¯ç±»ç‰¹å¾ä¸å‚ä¸è®­ç»ƒ
            data[cname_str_type] = data[cname_str_type].astype(str)
            data[cname_str_type] = data[cname_str_type].astype("string")
            
            for col in cname_str_type:
                data.loc[data[col].isna(),col]=str_padding

            # nanè¢«è½¬ä¸ºäº†å­—ç¬¦ä¸²ï¼Œä½†åœ¨pandasä¸­ä»ç„¶æ˜¯ä¸ªç‰¹æ®Šå­˜åœ¨ï¼Œè½¬ä¸ºç‰¹å®šå­—ç¬¦ä¸²ï¼Œä»¥é˜²Pandasè‡ªåŠ¨å¤„ç†
            # åˆ›å»ºä¸€ä¸ªæ›¿æ¢æ˜ å°„å­—å…¸  
            type_mapping = {  
                'nan': str_padding,   
                '': str_padding
            }  
                
            # ä½¿ç”¨.replace()æ–¹æ³•æ›¿æ¢'åˆ—çš„ç±»å‹'åˆ—ä¸­çš„å€¼  
            data[cname_str_type] = data[cname_str_type].replace(type_mapping)  
                
            nu = data[cname_str_type].isnull().sum()
            for col_name,v in nu.items():
                if v > 0 :
                    print("å­˜åœ¨ç©ºå€¼çš„åˆ—:\n")
                    print(col_name,v)
            return data


    
    
    @staticmethod
    def rolling_windows(data_path=None, df=None,col_time='DT_TIME',
                    interval=7, win_len=10):
        """
        ç”Ÿæˆå™¨ï¼šæ¯æ¬¡ yield (window_start, window_end, sub_df)
        ä»æœ€æ—©æ—¥æœŸå¼€å§‹ï¼Œæ¯éš” interval å¤©å–ä¸€ä¸ª win_len å¤©çš„çª—å£
        """
        if df is None:
            df = pd.read_csv(data_path, parse_dates=[col_time])
        DataDeal.date_deal(df,date_type=[col_time])
        date_col = df[col_time].dt.date

        min_date = date_col.min()
        max_date = date_col.max()

        # å½“å‰çª—å£èµ·ç‚¹
        cur_start = min_date
        while cur_start + timedelta(days=win_len-1) <= max_date:
            cur_end = cur_start + timedelta(days=win_len-1)
            mask = date_col.between(cur_start, cur_end)
            yield cur_start, cur_end, df[mask]
            cur_start += timedelta(days=interval)
    
    @staticmethod
    def std7(df, cname_num, means=None, stds=None, set_7mean=True):
        if set_7mean: #å°†è¶…è¿‡7å€å‡å€¼çš„æ•°æ®ç½®ä¸º7å€å‡å€¼
            # éå†DataFrameçš„æ¯ä¸€åˆ—,
            for col in cname_num:  
                # è·å–å½“å‰åˆ—çš„å‡å€¼  
                mean_val = means[col]  
                # åˆ›å»ºä¸€ä¸ªå¸ƒå°”ç´¢å¼•ï¼Œç”¨äºæ ‡è®°å“ªäº›å€¼è¶…è¿‡äº†å‡å€¼çš„7å€  
                mask = df[col] > (7 * mean_val)  
                # å°†è¿™äº›å€¼é‡ç½®ä¸ºå‡å€¼çš„7å€  
                df.loc[mask, col] = 7 * mean_val  

        df[cname_num] = (df[cname_num] - means)/stds  #æ ‡å‡†åŒ–
        
        return df  

    
    @staticmethod
    def str_deal(data, pc, classify_type=[]):
        """æ ‡è¯†åˆ—åŠç±»åˆ«åˆ—å¤„ç†
        - classify_type:æŒ‡å®šå€¼åˆ™ç±»åˆ«åˆ—ä¸ºæŒ‡å®šçš„å€¼ï¼Œå¦åˆ™ä½¿ç”¨æ’é™¤æ³•ï¼Œæ’é™¤æ•°å­—ï¼Œå¸ƒå°”ï¼Œæ ‡è¯†åˆ—ï¼Œå‰©ä¸‹çš„åˆ—ä¸ºç±»åˆ«åˆ—
        
        """
        column_all = data.columns
        identity = pc.col_type.identity
        ### å­—ç¬¦-èº«ä»½æ ‡è¯†ç±»
        str_identity = [col for col in column_all if col in identity]
        print("str_identity:",str_identity)
        DataDeal.str_pd(data,str_identity)

        ### å­—ç¬¦-åˆ†ç±»ï¼Œç”¨äºåˆ†ç±»çš„åˆ—ï¼Œæ¯”å¦‚æ¸ é“ï¼Œäº¤æ˜“ç±»å‹,å•†æˆ·ï¼Œåœ°åŒºç­‰
        if len(classify_type)==0:
            str_classification = [col for col in data.columns if col not in str_identity and col not in pc.col_type.num_type and col not in pc.col_type.date_type and col not in pc.col_type.bool_type]
        else:
            str_classification = classify_type
        pc.col_type.classify_type = str_classification
        DataDeal.str_pd(data,str_classification)
    
    @staticmethod
    def str_pd(data,cname_date_type):
        """pandasæ•°è¡¨åˆ—è½¬å­—ç¬¦ç±»å‹"""
        data[cname_date_type] = data[cname_date_type].astype(str)
        data[cname_date_type] = data[cname_date_type].astype("string")
        return data
    
    @staticmethod
    def time_between(df, start_date, end_date, time_col='key_2', date_format=None,
                    inclusive='both', copy=True, timezone=None,
                    return_mask=False, validate_dates=True):
        """
        ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        Args:
            df: pandas DataFrame
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ”¯æŒå¤šç§æ ¼å¼ (str, datetime, Timestamp)
            end_date: ç»“æŸæ—¥æœŸï¼Œæ”¯æŒå¤šç§æ ¼å¼ (str, datetime, Timestamp)
            time_col: æ—¶é—´åˆ—å
            date_format: æ—¥æœŸæ ¼å¼å­—ç¬¦ä¸²ï¼Œå¦‚ '%Y-%m-%d %H:%M:%S'
            inclusive: åŒ…å«è¾¹ç•Œ ['both', 'neither', 'left', 'right']
            copy: æ˜¯å¦å¤åˆ¶æ•°æ®æ¡†ï¼Œé¿å…ä¿®æ”¹åŸæ•°æ®
            timezone: æ—¶åŒºè®¾ç½®ï¼Œå¦‚ 'Asia/Shanghai'
            return_mask: æ˜¯å¦è¿”å›å¸ƒå°”æ©ç è€Œä¸æ˜¯è¿‡æ»¤åçš„æ•°æ®
            validate_dates: æ˜¯å¦éªŒè¯æ—¥æœŸæ ¼å¼å’ŒèŒƒå›´

        Returns:
            pandas DataFrame æˆ– numpy array: è¿‡æ»¤åçš„æ•°æ®æ¡†æˆ–å¸ƒå°”æ©ç 

        Raises:
            ValueError: å½“æ—¥æœŸæ ¼å¼æ— æ•ˆã€åˆ—ä¸å­˜åœ¨æˆ–æ—¥æœŸèŒƒå›´é”™è¯¯æ—¶
            TypeError: å½“è¾“å…¥å‚æ•°ç±»å‹é”™è¯¯æ—¶
        """
        import pandas as pd
        import numpy as np
        from datetime import datetime
        import warnings

        # è¾“å…¥éªŒè¯
        if time_col not in df.columns:
            raise ValueError(f"æ—¶é—´åˆ— '{time_col}' ä¸å­˜åœ¨")
        if inclusive not in ['both', 'neither', 'left', 'right']:
            raise ValueError("inclusive å¿…é¡»æ˜¯ 'both', 'neither', 'left', æˆ– 'right'")

        if validate_dates:
            if pd.isna(start_date) or pd.isna(end_date):
                raise ValueError("å¼€å§‹å’Œç»“æŸæ—¥æœŸä¸èƒ½ä¸ºç©º")
            if start_date > end_date:
                raise ValueError("å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ")

        # å¤åˆ¶æ•°æ®æ¡†ï¼ˆé¿å…ä¿®æ”¹åŸæ•°æ®ï¼‰
        if copy:
            df = df.copy()

        try:
            # ä¼˜åŒ–çš„æ—¥æœŸè½¬æ¢
            if date_format:
                # æŒ‡å®šæ ¼å¼è½¬æ¢ï¼Œæ›´å¿«æ›´å‡†ç¡®
                time_series = pd.to_datetime(df[time_col], format=date_format, errors='coerce')
                start_ts = pd.to_datetime(start_date, format=date_format)
                end_ts = pd.to_datetime(end_date, format=date_format)
            else:
                # è‡ªåŠ¨æ¨æ–­æ ¼å¼
                time_series = pd.to_datetime(df[time_col], errors='coerce')
                start_ts = pd.to_datetime(start_date)
                end_ts = pd.to_datetime(end_date)

            # æ—¶åŒºå¤„ç†
            if timezone:
                if time_series.dt.tz is None:
                    time_series = time_series.dt.tz_localize(timezone)
                else:
                    time_series = time_series.dt.tz_convert(timezone)

                if start_ts.tz is None:
                    start_ts = start_ts.tz_localize(timezone)
                if end_ts.tz is None:
                    end_ts = end_ts.tz_localize(timezone)

            # å¤„ç†è½¬æ¢å¤±è´¥çš„æ—¥æœŸ
            invalid_dates = time_series.isna()
            if invalid_dates.any():
                if validate_dates:
                    warnings.warn(f"å‘ç° {invalid_dates.sum()} ä¸ªæ— æ•ˆæ—¥æœŸï¼Œå°†è¢«æ’é™¤")

                # å¯¹äºæ— æ•ˆæ—¥æœŸï¼Œåœ¨æ©ç ä¸­è®¾ä¸ºFalse
                time_series = time_series.fillna(pd.NaT)

            # åˆ›å»ºè¿‡æ»¤æ©ç ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼šé¿å…åˆ›å»ºä¸­é—´åˆ—ï¼‰
            if inclusive == 'both':
                mask = (time_series >= start_ts) & (time_series <= end_ts)
            elif inclusive == 'left':
                mask = (time_series >= start_ts) & (time_series < end_ts)
            elif inclusive == 'right':
                mask = (time_series > start_ts) & (time_series <= end_ts)
            else:  # neither
                mask = (time_series > start_ts) & (time_series < end_ts)

            # ç¡®ä¿æ— æ•ˆæ—¥æœŸä¸è¢«åŒ…å«
            mask = mask & ~invalid_dates

            if return_mask:
                return mask.values
            else:
                # ç›´æ¥ä½¿ç”¨æ©ç è¿‡æ»¤ï¼Œé¿å…åˆ›å»ºä¸­é—´åˆ—
                return df.loc[mask]

        except Exception as e:
            raise ValueError(f"æ—¥æœŸè½¬æ¢å¤±è´¥: {str(e)}") from e

    @staticmethod
    def time_between_multiple(df, date_ranges, time_col='key_2', **kwargs):
        """
        æ”¯æŒå¤šä¸ªæ—¶é—´èŒƒå›´çš„è¿‡æ»¤ï¼ˆæ‰©å±•åŠŸèƒ½ï¼‰

        Args:
            df: pandas DataFrame
            date_ranges: æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [(start1, end1), (start2, end2), ...]
            time_col: æ—¶é—´åˆ—å
            **kwargs: ä¼ é€’ç»™ time_between çš„å…¶ä»–å‚æ•°

        Returns:
            pandas DataFrame: è¿‡æ»¤åçš„æ•°æ®æ¡†
        """
        if not date_ranges:
            return df.copy()

        combined_mask = None
        for start_date, end_date in date_ranges:
            mask = DataDeal.time_between(
                df, start_date, end_date, time_col,
                return_mask=True, **kwargs
            )

            if combined_mask is None:
                combined_mask = mask
            else:
                combined_mask = combined_mask | mask

        return df.loc[combined_mask].copy()

    @classmethod
    def get_col_names(cls, df, col_type='object'):
        """
        æ ¹æ®æŒ‡å®šçš„æ•°æ®ç±»å‹è·å–DataFrameä¸­çš„åˆ—å

        Args:
            df: pandas DataFrame
            col_type: æ•°æ®ç±»å‹è¿‡æ»¤æ¡ä»¶ï¼Œå¯é€‰å€¼ï¼š
                - 'object': objectç±»å‹åˆ—
                - 'cat': åˆ†ç±»ç±»å‹åˆ—ï¼ˆåŒ…æ‹¬str, string, categoryï¼‰
                - 'num': æ•°å€¼ç±»å‹åˆ—ï¼ˆåŒ…æ‹¬æ‰€æœ‰æ•°å­—ç±»å‹ï¼‰
                - 'date': æ—¥æœŸç±»å‹åˆ—ï¼ˆåŒ…æ‹¬datetime64[ns], datetime64ï¼‰
                - 'datetime64[ns]': ç²¾ç¡®çš„datetime64[ns]ç±»å‹
                - 'datetime64': datetime64ç±»å‹
                - 'int': æ•´æ•°ç±»å‹åˆ—
                - 'float': æµ®ç‚¹æ•°ç±»å‹åˆ—
                - 'bool': å¸ƒå°”ç±»å‹åˆ—
                - 'str': å­—ç¬¦ä¸²ç±»å‹åˆ—
                - 'category': åˆ†ç±»ç±»å‹åˆ—
                - 'all': è¿”å›æ‰€æœ‰åˆ—åï¼ˆé»˜è®¤è¡Œä¸ºï¼‰

        Returns:
            list: ç¬¦åˆæŒ‡å®šç±»å‹çš„åˆ—ååˆ—è¡¨
        """
        import pandas as pd
        import numpy as np

        # å¦‚æœè¯·æ±‚æ‰€æœ‰åˆ—ï¼Œç›´æ¥è¿”å›
        if col_type == 'all':
            return list(df.columns)

        # å®šä¹‰ç±»å‹æ˜ å°„å…³ç³»
        type_mapping = {
            # æ•°å€¼ç±»å‹
            'num': ['int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', 'uint32', 'uint64',
                   'float16', 'float32', 'float64', 'number'],
            'int': ['int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', 'uint32', 'uint64', 'integer'],
            'float': ['float16', 'float32', 'float64', 'floating'],

            # å­—ç¬¦ä¸²å’Œåˆ†ç±»ç±»å‹
            'cat': ['object', 'string', 'category', 'str'],
            'str': ['object', 'string'],
            'category': ['category'],

            # æ—¥æœŸæ—¶é—´ç±»å‹
            'date': ['datetime64[ns]', 'datetime64', 'datetime'],
            'datetime64[ns]': ['datetime64[ns]'],
            'datetime64': ['datetime64', 'datetime64[ns]'],

            # å¸ƒå°”ç±»å‹
            'bool': ['bool', 'boolean'],

            # å¯¹è±¡ç±»å‹ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            'object': ['object'],
        }

        # è·å–è¯·æ±‚çš„ç±»å‹åˆ—è¡¨
        target_types = type_mapping.get(col_type, [col_type])

        # æ”¶é›†ç¬¦åˆæ¡ä»¶çš„åˆ—å
        result_columns = []

        for col in df.columns:
            col_dtype = str(df[col].dtype).lower()

            # æ£€æŸ¥åˆ—çš„æ•°æ®ç±»å‹æ˜¯å¦åŒ¹é…ç›®æ ‡ç±»å‹
            for target_type in target_types:
                if target_type.lower() in col_dtype:
                    result_columns.append(col)
                    break

        return result_columns

    @classmethod
    def get_col_names_by_pattern(cls, df, pattern='.*'):
        """
        æ ¹æ®æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼è·å–åˆ—å

        Args:
            df: pandas DataFrame
            pattern: æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œé»˜è®¤åŒ¹é…æ‰€æœ‰åˆ—

        Returns:
            list: åŒ¹é…æ¨¡å¼çš„åˆ—ååˆ—è¡¨
        """
        import re
        return [col for col in df.columns if re.match(pattern, col)]

    @classmethod
    def get_col_types_summary(cls, df):
        """
        è·å–DataFrameä¸­å„åˆ—çš„æ•°æ®ç±»å‹æ±‡æ€»

        Args:
            df: pandas DataFrame

        Returns:
            dict: æ•°æ®ç±»å‹åˆ°åˆ—ååˆ—è¡¨çš„æ˜ å°„
        """
        type_summary = {}
        for col in df.columns:
            dtype = str(df[col].dtype)
            if dtype not in type_summary:
                type_summary[dtype] = []
            type_summary[dtype].append(col)
        return type_summary

class FileDb():
    def __init__(self,path):
        self.path = path

    def read(self):
        return read(self.path)
    
    def write(self,data):
        return write(data,self.path)
    
    def list_append(self,data, del_dup=False):
        ll = self.read()
        if ll is None or len(str(ll)) == 0 or len(ll) == 0:
            ll = []
        ll.append(data)
        if del_dup:
            ll = list(set(ll))
        write(ll,self.path)
        
    
    
# -*- coding:utf-8 -*-
"""
æ·±åº¦å­¦ä¹ é€šç”¨æ•°æ®é¢„å¤„ç†
- æŠŠ bank+id åˆå¹¶æˆå”¯ä¸€ id
- æ•°å€¼åˆ—å½’ä¸€åŒ– / æ ‡å‡†å·®
- ç±»åˆ«åˆ— LabelEncoder
- æ—¶é—´åˆ—è½¬è·åŸºçº¿å¤©æ•°
- æ”¯æŒä¿å­˜/åŠ è½½ transformerï¼Œä¿è¯ç¦»çº¿/åœ¨çº¿ä¸€è‡´
"""
import os
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from typing import List, Optional, Dict, Any
import joblib
import datetime


"""
æ”¹è¿›ç‰ˆæ·±åº¦å­¦ä¹ æ•°æ®é¢„å¤„ç†ç±»
- æ”¯æŒå­—æ®µç±»å‹è½¬æ¢
- identity åˆå¹¶ï¼ˆbank + idï¼‰
- ç±»åˆ«åˆ—ï¼šLabelEncoderï¼ˆæ”¯æŒå¤šåˆ—å…±äº«ç¼–ç å™¨ï¼‰
- æ•°å€¼åˆ—ï¼šMinMaxScalerï¼ˆæ”¯æŒ log10 + å¢é‡æ›´æ–°ï¼‰
- æ—¥æœŸåˆ—ï¼šè½¬ä¸ºè· max_date çš„å¤©æ•°åå½’ä¸€åŒ–
- ä¿å­˜/åŠ è½½æ‰€æœ‰ transformerï¼Œä¿è¯ç¦»çº¿/åœ¨çº¿ä¸€è‡´
"""


import os
import pandas as pd
import joblib
from sklearn.preprocessing import MinMaxScaler
from typing import List, Dict, Any, Optional
import numpy as np
import datetime


class DataDealDL:
    def __init__(
        self,
        identity_cols: List[str] = None,
        classify_cols: List[str] = None,
        classify_shared_groups: List[List[str]] = None,
        num_cols: List[str] = None,
        num_small: List[str] = None,
        date_cols: List[str] = None,
        bool_cols: List[str] = None,
        log10_transform: bool = False,
        pc:ParamConfig = None
    ):
        """
        åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ•°æ®å¤„ç†å™¨ï¼Œç”¨äºæ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹ã€‚
        
        Args:
            identity_cols: èº«ä»½æ ‡è¯†åˆ—ååˆ—è¡¨ï¼Œå¦‚ ['Bank', 'Account'] ä¼šåˆå¹¶ä¸ºå”¯ä¸€ID
            classify_cols: éœ€è¦ç‹¬ç«‹ç¼–ç çš„ç±»åˆ«åˆ—ååˆ—è¡¨
            classify_shared_groups: å…±äº«ç¼–ç å™¨çš„åˆ—ç»„ï¼Œå¦‚ [['From', 'To']] è¡¨ç¤ºå¤šåˆ—å…±ç”¨ä¸€ä¸ªç¼–ç å™¨
            num_cols: éœ€è¦å½’ä¸€åŒ–çš„æ•°å€¼åˆ—ååˆ—è¡¨
            num_small: æ•°å€¼åˆ—ä¸­ä¸è¿›è¡Œlog10å˜æ¢çš„å°æ•°å€¼åˆ—ååˆ—è¡¨
            date_cols: éœ€è¦è½¬æ¢å’Œå½’ä¸€åŒ–çš„æ—¥æœŸåˆ—ååˆ—è¡¨ï¼ˆå°†è½¬æ¢ä¸ºå¤©æ•°åå½’ä¸€åŒ–ï¼‰
            bool_cols: å¸ƒå°”åˆ—ååˆ—è¡¨ï¼ˆè½¬æ¢ä¸º0/1ï¼‰
            cls_dict_file: ç±»åˆ«å­—å…¸æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºä¿å­˜å’ŒåŠ è½½ç¼–ç å™¨
            log10_transform: æ˜¯å¦å¯¹æ•°å€¼åˆ—è¿›è¡Œlog10å˜æ¢ï¼Œé»˜è®¤False
            file_num: æ–‡ä»¶ç¼–å·ï¼Œç”¨äºåŒºåˆ†åŒä¸€æ¨¡å‹å¯¹åº”çš„å¤šä¸ªæ–‡ä»¶ï¼Œé»˜è®¤1
            pc: å‚æ•°é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ¨¡å‹ä¿å­˜è·¯å¾„ã€ç®—æ³•ç±»å‹ã€æ¨¡å‹ç¼–å·ç­‰ä¿¡æ¯
        """
        self.identity_cols = identity_cols or []
        self.classify_cols = classify_cols or []
        self.classify_shared_groups = classify_shared_groups or []
        self.num_cols = num_cols or []
        self.num_small = num_small or []
        self.date_cols = date_cols or []
        self.bool_cols = bool_cols or []
        
        self.file_num    = pc.file_num
        self.scaler_root = pc.model_save_dir
        self.alg_type    = pc.alg_type
        self.batch_num   = pc.model_num
        
        self.is_merge_identity = pc.is_merge_identity
        self.num_scaler_file   = pc.num_scaler_file()
        self.date_scaler_file  = pc.date_scaler_file()
        self.cls_dict_file     = pc.dict_file()  # ç±»åˆ«å­—å…¸æ–‡ä»¶
        
        self.log10_transform   = log10_transform
        self.max_date = pd.to_datetime(pc.max_date).date()
        self._split="_._"

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(self.scaler_root, exist_ok=True)

        # å­˜å‚¨ encoder å’Œ scaler
        self._le_dict: Dict[str, Any] = {}  # å•ç‹¬åˆ—çš„ encoder
        self._shared_le: Dict[str, Any] = {}  # å…±äº«ç»„çš„ encoderï¼Œkey å¯ä¸º "From_To"
        self._num_scaler = MinMaxScaler()
        self._date_scaler = MinMaxScaler()


    @staticmethod
    def data_deal(df,pc:ParamConfig):
        """
        æ·±åº¦å­¦ä¹ æ•°æ®é¢„å¤„ç†çš„ç»Ÿä¸€å…¥å£æ–¹æ³•

        ä¸»è¦è®¡ç®—é€»è¾‘ï¼š
        1. æ¨¡å‹åŠ è½½/åˆå§‹åŒ–ç­–ç•¥
           - é¦–å…ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿å­˜çš„é¢„å¤„ç†æ¨¡å‹æ–‡ä»¶
           - å¦‚æœå­˜åœ¨ï¼šç›´æ¥åŠ è½½pickleåºåˆ—åŒ–çš„DataDealDLå¯¹è±¡
           - å¦‚æœä¸å­˜åœ¨ï¼šåˆ›å»ºæ–°çš„DataDealDLå®ä¾‹ï¼ŒåŒ…å«æ‰€æœ‰é¢„å¤„ç†ç»„ä»¶

        2. DataDealDLåˆå§‹åŒ–å‚æ•°ï¼ˆéœ€è¦12ä¸ªæ ¸å¿ƒå‚æ•°ï¼‰
           - identity_cols: èº«ä»½æ ‡è¯†åˆ—ï¼ˆå¦‚['Bank', 'Account']ï¼‰ä¼šåˆå¹¶ä¸ºå”¯ä¸€ID
           - classify_cols: ç‹¬ç«‹ç¼–ç çš„ç±»åˆ«åˆ—åˆ—è¡¨
           - classify_shared_groups: å…±äº«ç¼–ç å™¨çš„åˆ—ç»„ï¼ˆå¦‚[['From', 'To']]ï¼‰
           - num_cols: éœ€è¦å½’ä¸€åŒ–çš„æ•°å€¼åˆ—åˆ—è¡¨
           - num_small: ä¸è¿›è¡Œlog10å˜æ¢çš„å°æ•°å€¼åˆ—åˆ—è¡¨
           - date_cols: æ—¥æœŸåˆ—åˆ—è¡¨ï¼ˆè½¬æ¢ä¸ºå¤©æ•°åå½’ä¸€åŒ–ï¼‰
           - bool_cols: å¸ƒå°”åˆ—åˆ—è¡¨ï¼ˆè½¬æ¢ä¸º0/1ï¼‰
           - log10_transform: æ˜¯å¦å¯¹æ•°å€¼åˆ—è¿›è¡Œlog10å˜æ¢
           - alg_type: ç®—æ³•ç±»å‹ï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰
           - model_save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
           - model_num: æ¨¡å‹ç¼–å·
           - file_num: æ–‡ä»¶ç¼–å·

        3. è®­ç»ƒ/æ¨ç†åˆ†æ”¯å¤„ç†
           - è®­ç»ƒæ¨¡å¼ (pc.is_train=True):
             * è°ƒç”¨fit_transform()ï¼šæ‹Ÿåˆå¹¶è½¬æ¢æ•°æ®
             * æ›´æ–°æ•°å€¼åˆ—çš„æå€¼èŒƒå›´
             * ä¿å­˜å®Œæ•´çš„é¢„å¤„ç†æ¨¡å‹åˆ°æ–‡ä»¶
           - æ¨ç†æ¨¡å¼ (pc.is_train=False):
             * è°ƒç”¨transform()ï¼šä»…ä½¿ç”¨å·²æœ‰æ¨¡å‹è½¬æ¢æ•°æ®
             * ä¿æŒé¢„å¤„ç†å‚æ•°ä¸å˜ï¼Œç¡®ä¿ä¸€è‡´æ€§

        4. é¢„å¤„ç†æµç¨‹ï¼ˆåœ¨DataDealDLå†…éƒ¨ï¼‰
           - æ­¥éª¤1: ç±»å‹è½¬æ¢å’Œç»Ÿä¸€
           - æ­¥éª¤2: èº«ä»½æ ‡è¯†åˆ—åˆå¹¶
           - æ­¥éª¤3: å¸ƒå°”åˆ—å¤„ç†ï¼ˆ0/1è½¬æ¢ï¼‰
           - æ­¥éª¤4: ç±»åˆ«åˆ—ç¼–ç ï¼ˆLabelEncoderï¼‰
           - æ­¥éª¤5: æ—¥æœŸåˆ—å½’ä¸€åŒ–
           - æ­¥éª¤6: æ•°å€¼åˆ—å½’ä¸€åŒ–ï¼ˆlog10 + MinMaxï¼‰

        Args:
            df: è¾“å…¥çš„pandas DataFrame
            pc: ParamConfigå‚æ•°é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰é¢„å¤„ç†å‚æ•°

        Returns:
            DataFrame: é¢„å¤„ç†åçš„æ•°æ®è¡¨ï¼Œå¯ç›´æ¥ç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ

        Example:
            # é…ç½®å‚æ•°
            pc.col_type.identity = ['Bank', 'Account']
            pc.col_type.num_type = ['amount', 'risk_score']
            pc.col_type.classify_type = ['currency', 'payment_type']
            pc.is_train = True

            # æ‰§è¡Œé¢„å¤„ç†
            df_processed = DataDealDL.data_deal(df, pc)

        Note:
            - è¯¥æ–¹æ³•æ”¯æŒå¢é‡å­¦ä¹ ï¼Œè®­ç»ƒæ—¶ä¼šæ›´æ–°æ•°å€¼æå€¼
            - æ‰€æœ‰é¢„å¤„ç†ç»„ä»¶éƒ½ä¼šä¿å­˜ï¼Œç¡®ä¿ç¦»çº¿/åœ¨çº¿ä¸€è‡´æ€§
            - æ”¯æŒå¤šåˆ—å…±äº«ç¼–ç å™¨ï¼Œé€‚åˆå›¾ç¥ç»ç½‘ç»œç­‰åœºæ™¯
        """
        # æ•°æ®é¢„å¤„ç†
        # è¿™é‡Œä¸èƒ½ç›´æ¥ä¿å­˜DataDealDLï¼Œå› ä¸ºè®­ç»ƒæ—¶åˆå§‹åŒ–çš„æ˜¯è®­ç»ƒçš„å‚æ•°ï¼Œé¢„æµ‹æ—¶éœ€è¦é‡æ–°åˆå§‹åŒ–å‚æ•°
        # if os.path.exists(pc.data_deal_model_path()):
        #     print("æ¨¡å‹å·²å­˜åœ¨ï¼ŒåŠ è½½æ¨¡å‹-------1----------")
        #     data_deal = pkl_load(file_path=pc.data_deal_model_path())
        # else :
        #     print("æ¨¡å‹ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–æ¨¡å‹-------2----------")
        data_deal = DataDealDL(  # åˆå§‹åŒ–
            identity_cols=pc.col_type.identity,
            classify_cols=pc.col_type.classify_type,
            classify_shared_groups=pc.col_type.classify_type_pre,
            num_cols=pc.col_type.num_type,
            num_small=pc.col_type.num_small,
            date_cols=pc.col_type.date_type,
            bool_cols=pc.col_type.bool_type,
            log10_transform=True,
            pc=pc
        )

        # è®­ç»ƒé˜¶æ®µ
        if pc.is_train:  #æ•°å­—æå€¼æ›´æ–°è®­ç»ƒ
            df_processed = data_deal.fit_transform(df)
            # pkl_save(data_deal,file_path=pc.data_deal_model_path())
        else:
            df_processed = data_deal.transform(df)

        return df_processed


    # ---------- public ----------
    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®­ç»ƒé˜¶æ®µï¼šæ‹Ÿåˆå¹¶è½¬æ¢æ•°æ®"""
        df = df.copy()
        
        df = self._type_conversion(df)
        print(f"self.identity_cols={self.identity_cols}-----------------")
 
        # 1. åˆå¹¶ identity
        df = self._merge_identity(df)
        # print(1,df[:3])
        

        # 2. å¸ƒå°”åˆ—å¤„ç†
        df = self._process_bool(df, fit=True)
        # print(2,df[:3])

        # 3. ç±»åˆ«ç¼–ç ï¼ˆå•ç‹¬ + å…±äº«ï¼‰
        # df = self._encode_category(df, fit=True)
        df = self._col2index(df, fit=True)
   


        # 4. æ—¥æœŸå¤„ç†ï¼ˆè½¬å¤©æ•° + å½’ä¸€åŒ–ï¼‰
        df = self._scale_date(df, fit=True)

        # 5. æ•°å€¼å½’ä¸€åŒ–ï¼ˆlog10 + MinMaxï¼‰

        df = self._min_max_update(df, fit=True)
 

        return df

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¨ç†é˜¶æ®µï¼šä»…è½¬æ¢ï¼Œä½¿ç”¨å·²ä¿å­˜çš„ encoder/scaler"""
        df = df.copy()
        df = self._type_conversion(df)
        # print(df.info())
        
        # 1. åˆå¹¶ identity
        df = self._merge_identity(df)
        # print(1,df[:3])
        
        # 2. å¸ƒå°”åˆ—
        df = self._process_bool(df, fit=False)
        # print(2,df[:3])

        # 3. ç±»åˆ«ç¼–ç 
        # df = self._encode_category(df, fit=False)
        df = self._col2index(df, fit=False)


        # 4. æ—¥æœŸ
        df = self._scale_date(df, fit=False)
  

        # 5. æ•°å€¼
        # df = self._scale_numeric(df, fit=False)
        df = self._min_max_update(df, fit=False)

        return df
    
        
    
    # ---------- private ----------
    def _type_conversion(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç»Ÿä¸€å­—æ®µç±»å‹"""
        for col in self.num_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        for col in self.date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        for col in self.classify_cols + self.identity_cols:
            if col in df.columns:
                df[col] = df[col].astype(str)
        for group in self.classify_shared_groups:
            for col in group:
                if col in df.columns:
                    df[col] = df[col].astype(str)
        return df

    def _merge_identity(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆå¹¶ identity åˆ—ä¸ºä¸€ä¸ªå­—æ®µï¼ˆå¦‚ Bank_Accountï¼‰"""
        if self.is_merge_identity:
            if len(self.identity_cols) <= 1:
                return df
            new_id = df[self.identity_cols[0]].astype(str)
            for c in self.identity_cols[1:]:
                new_id += self._split + df[c].astype(str)
            df[self.identity_cols[0]] = new_id
            
            return df.drop(columns=self.identity_cols[1:])
        else:
            return df

    def _process_bool(self, df: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        """å¸ƒå°”åˆ—è½¬ 0/1"""
        for col in self.bool_cols:
            if col in df.columns:
                df[col] = (df[col].astype(bool)).astype(int)
        return df

    def _col2index(self, df: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        #ç±»å‹å­—æ®µç´¢å¼•ç¼–ç ,å¦‚æœæ˜¯è®­ç»ƒåˆ™ä¿å­˜å­—å…¸
        is_pre = not fit 
        if len(self.classify_cols) == 0 and len(self.classify_shared_groups) == 0:
            return df
        DataDeal.col2index(df,classify_type=self.classify_cols ,
                    classify_type2=self.classify_shared_groups,
                    dict_file=self.cls_dict_file,
                    is_pre=is_pre,
                    word2id=None) 
        return df
    def _encode_category(self, df: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        """ç±»åˆ«ç¼–ç ï¼Œæ”¯æŒå…±äº« encoder"""
        # å•ç‹¬ç¼–ç åˆ—
        for col in self.classify_cols:
            if col not in df.columns:
                continue
            if fit:
                le = joblib.load(self.scaler_root + f"/le_{col}.pkl") if os.path.exists(
                    self.scaler_root + f"/le_{col}.pkl") else None
                if le is None:
                    le = LabelEncoder()
                    # å¤„ç†ç©ºå€¼
                    df[col] = df[col].fillna("<UNK>")
                    le.fit(df[col])
                    joblib.dump(le, os.path.join(self.scaler_root, f"le_{col}.pkl"))
                self._le_dict[col] = le
            else:
                le = joblib.load(os.path.join(self.scaler_root, f"le_{col}.pkl"))
                self._le_dict[col] = le

            # è½¬æ¢
            unknown = -1
            df[col] = df[col].fillna("<UNK>").map(
                lambda x: le.transform([x])[0] if x in le.classes_ else unknown
            ).astype(int)

        # å…±äº«ç¼–ç ç»„
        for group in self.classify_shared_groups:
            name_key = "_".join(group)
            shared_le = None
            if fit:
                if os.path.exists(os.path.join(self.scaler_root, f"le_shared_{name_key}.pkl")):
                    shared_le = joblib.load(os.path.join(self.scaler_root, f"le_shared_{name_key}.pkl"))
                else:
                    shared_le = LabelEncoder()
                    all_vals = pd.concat([df[col].dropna() for col in group if col in df.columns], ignore_index=True)
                    all_vals = pd.Series(all_vals).fillna("<UNK>").astype(str)
                    shared_le.fit(all_vals)
                    joblib.dump(shared_le, os.path.join(self.scaler_root, f"le_shared_{name_key}.pkl"))
                self._shared_le[name_key] = shared_le
            else:
                shared_le = joblib.load(os.path.join(self.scaler_root, f"le_shared_{name_key}.pkl"))
                self._shared_le[name_key] = shared_le

            for col in group:
                if col not in df.columns:
                    continue
                unknown = -1
                df[col] = df[col].fillna("<UNK>").map(
                    lambda x: shared_le.transform([x])[0] if x in shared_le.classes_ else unknown
                ).astype(int)

        return df

    def _scale_date(self, df: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        """æ—¥æœŸå¤„ç†ï¼Œä½¿ç”¨DataDeal.date_dealæ–¹æ³•"""
   
        if not self.date_cols or len(self.date_cols) == 0:
            return df
        
        # ä½¿ç”¨DataDeal.min_max_scaler_dtæ–¹æ³•å¤„ç†æ—¥æœŸ
        df = DataDeal.min_max_scaler_dt(df, 
                                 date_type=self.date_cols,
                                 scaler_file=self.date_scaler_file,
                                 max_date=self.max_date)
        
        
        return df
    
    def _min_max_update(self, df: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        
        is_pre = not fit
        df = DataDeal.min_max_update(df, 
                                num_type=self.num_cols,
                                num_small=self.num_small,
                                num_scaler_file=self.num_scaler_file, 
                                is_pre=is_pre,
                                log10=self.log10_transform)
        return df
         

    def _scale_numeric(self, df: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        """æ•°å€¼åˆ—ï¼šlog10 + MinMaxScalerï¼Œæ”¯æŒå¢é‡æ›´æ–°"""
        if not self.num_cols:
            return df

        # log10 å˜æ¢
        for col in self.num_cols:
            if col in df.columns:
                # é¿å… log(0)
                df[col] = np.log10(df[col] + 1) if self.log10_transform else df[col]

        if fit:
            # å¢é‡æ›´æ–°ï¼šå¦‚æœå·²æœ‰ scalerï¼ŒåŠ è½½å¹¶æ›´æ–°æå€¼
            scaler_path = os.path.join(self.scaler_root, "scaler_num.pkl")
            if os.path.exists(scaler_path):
                old_scaler = joblib.load(scaler_path)
                old_min = old_scaler.data_min_
                old_max = old_scaler.data_max_
                new_min = df[self.num_cols].min()
                new_max = df[self.num_cols].max()
                # æ›´æ–°ä¸ºå…¨å±€ min/max
                updated_min = np.minimum(old_min, new_min)
                updated_max = np.maximum(old_max, new_max)
                self._num_scaler.data_min_, self._num_scaler.data_max_ = updated_min, updated_max
            else:
                self._num_scaler.fit(df[self.num_cols])
            joblib.dump(self._num_scaler, scaler_path)
        else:
            self._num_scaler = joblib.load(os.path.join(self.scaler_root, "scaler_num.pkl"))

        df[self.num_cols] = self._num_scaler.transform(df[self.num_cols])
        return df
    

#--------------------------------------------------------------
# æ•°æ®å¤„ç†ç±» - å°è£…å…¬å…±åŠŸèƒ½
#--------------------------------------------------------------

class DataProcessor:
    """
    æ•°æ®å¤„ç†ç±» - å°è£…æ•°æ®è®­ç»ƒå’Œäº¤æ˜“å¤„ç†çš„ç›¸å…³æ–¹æ³•

    æä¾›ç»Ÿä¸€çš„æ•°æ®å¤„ç†æ¥å£ï¼Œæ”¯æŒé€šç”¨æ•°æ®å¤„ç†å’Œäº¤æ˜“ç‰¹å®šæ•°æ®å¤„ç†
    """

    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†ç±»"""
        pass

    def _get_usecols_from_model_title(self, model_title,sep='~'):
        """
        æ ¹æ®æ¨¡å‹æ ‡é¢˜è·å–è¦ä½¿ç”¨çš„åˆ—

        Args:
            model_title (str): æ¨¡å‹æ ‡é¢˜ï¼Œ'all'è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰åˆ—ï¼Œå¦åˆ™æŒ‰'~'åˆ†å‰²

        Returns:
            list or None: è¦ä½¿ç”¨çš„åˆ—åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰åˆ—
        """
        if model_title == 'all':
            return None
        else:
            return model_title.split(sep)

    def _create_categorical_function_general(self):
        """
        åˆ›å»ºé€šç”¨çš„åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°

        Returns:
            function: åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°
        """
        def is_categorical(col: str) -> bool:
            return col.lower().startswith(('is_', 'has_', 'with_'))
        return is_categorical

    def _create_categorical_function_tra(self):
        """
        åˆ›å»ºç‰¹å®šäºäº¤æ˜“çš„åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°

        Returns:
            function: åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°
        """
        def is_categorical(col: str) -> bool:
            cls_cols = ['Receiving Currency','Payment Currency', 'Payment Format']
            cls_cols2 = [col.lower() for col in cls_cols]
            return col.lower() in cls_cols2
        return is_categorical

    def _load_and_classify_data(self, 
                data_path, label_name, str_identity, is_train, 
                usecols, drop_columns, 
                is_categorical_func, sep='~',date_type=[],bool_type = []):
        """
        åŠ è½½æ•°æ®å¹¶è¿›è¡Œåˆ†ç±»çš„é€šç”¨æ–¹æ³•

        Args:
            data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
            label_name (str): æ ‡ç­¾åˆ—å
            str_identity (str): æ ‡è¯†åˆ—
            is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒæ•°æ®
            usecols (list): è¦ä½¿ç”¨çš„åˆ—
            drop_columns (list): è¦åˆ é™¤çš„åˆ—
            is_categorical_func (function): åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°
            sep (str): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º'~'

        Returns:
            tuple: (DataFrame, labelåˆ—, åˆ—ç±»å‹å­—å…¸)
        """
        df, y, col_types = DataDeal.getXy(data_path, label_name,
                                    identity_cols=str_identity, sep=sep,
                                    is_train=is_train, usecols=usecols,
                                    drop_columns=drop_columns,
                                    dtype_mapping=None,
                                    is_categorical_func=is_categorical_func,
                                    date_type=date_type,
                                    bool_type=bool_type)
        return df, y, col_types

    def _analyze_numeric_columns(self, df, pc, threshold=100):
        """
        åˆ†ææ•°å€¼åˆ—ï¼Œæ‰¾å‡ºæœ€å¤§å€¼å°äºé˜ˆå€¼çš„åˆ—

        Args:
            df (DataFrame): æ•°æ®æ¡†
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            threshold (int): é˜ˆå€¼ï¼Œé»˜è®¤ä¸º100

        Returns:
            list: å°äºé˜ˆå€¼çš„åˆ—ååˆ—è¡¨
        """
        num_small = DataDeal.columns_by_max_value(df, condition='less', threshold=threshold)
        pc.lg(f"num_small num:{len(num_small)}")
        if len(num_small) > 0:
            DataDeal.num_describe(df[num_small], pc)
            return num_small
        else:
            return []

    def _setup_param_config(self, pc:ParamConfig, str_identity, col_types, num_small, alg_type,
                           model_ai_dir, model_num, file_num, is_train,
                           label_name, drop_columns, date_type=None,classify_type2 = [[]],bool_type = [] ):
        """
        è®¾ç½®å‚æ•°é…ç½®å¯¹è±¡çš„é€šç”¨æ–¹æ³•

        Args:
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            str_identity (str): æ ‡è¯†åˆ—
            col_types (dict): åˆ—ç±»å‹å­—å…¸
            num_small (list): å°æ•°å€¼åˆ—åˆ—è¡¨
            alg_type (str): ç®—æ³•ç±»å‹
            model_ai_dir (str): æ¨¡å‹ä¿å­˜ç›®å½•
            model_num (int): æ¨¡å‹ç¼–å·
            file_num (int): æ–‡ä»¶ç¼–å·
            is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒ
            label_name (str): æ ‡ç­¾åˆ—å
            drop_columns (list): è¦åˆ é™¤çš„åˆ—
            date_type (list, optional): æ—¥æœŸç±»å‹åˆ—åˆ—è¡¨
        """
        # DataDealDL.data_dealéœ€è¦çš„12ä¸ªå‚æ•°
        pc.col_type.identity       = str_identity
        pc.col_type.num_type       = col_types["num_type"]
        pc.col_type.num_small      = num_small
        pc.col_type.classify_type  = col_types["classify_type"]
        pc.col_type.classify_type2 = classify_type2  #ä¸€ç»„ç±»åˆ«ä½¿ç”¨åŒä¸€ä¸ªå­—å…¸
        pc.col_type.date_type      = date_type if date_type is not None else []
        pc.col_type.bool_type      = bool_type
        pc.alg_type                = alg_type
        pc.model_save_dir          = model_ai_dir
        pc.model_num               = model_num
        pc.file_num                = file_num   #ç¬¬å‡ ä¸ªæ–‡ä»¶,é»˜è®¤1
        pc.is_train                = is_train

        #å…¶ä»–å‚æ•°
        pc.label_name              = label_name
        pc.drop_cols               = drop_columns

    def _log_data_info(self, pc:ParamConfig, num_small):
        """
        è®°å½•æ•°æ®ä¿¡æ¯çš„é€šç”¨æ–¹æ³•

        Args:
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            num_small (list): å°æ•°å€¼åˆ—åˆ—è¡¨
        """
        pc.lg(pc.col_type.num_type[:3])
        pc.lg(f"num_small num:{len(num_small)},num type num:{len(pc.col_type.num_type)}")
        pc.lg(pc.col_type.classify_type[:3])
        pc.lg(f"is_merge_identity:{pc.is_merge_identity}")

    def _process_data_with_deal_dl(self, df, pc:ParamConfig):
        """
        ä½¿ç”¨DataDealDLå¤„ç†æ•°æ®çš„é€šç”¨æ–¹æ³•

        Args:
            df (DataFrame): æ•°æ®æ¡†
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡

        Returns:
            DataFrame: å¤„ç†åçš„æ•°æ®æ¡†
        """
        df_processed = DataDealDL.data_deal(df, pc)
        return df_processed

    def _common_data_processing_pipeline(self, 
                data_path, model_title, str_identity,
                alg_type, model_ai_dir, model_num, file_num,
                is_train, label_name, pc:ParamConfig, drop_columns,
                is_categorical_func_type='general', date_type=None, 
                sep='~',classify_type2 = [[]],bool_type = []):
        """
        é€šç”¨æ•°æ®å¤„ç†ç®¡é“

        Args:
            data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
            model_title (str): æ¨¡å‹æ ‡é¢˜
            str_identity (str): æ ‡è¯†åˆ—
            alg_type (str): ç®—æ³•ç±»å‹
            model_ai_dir (str): æ¨¡å‹ä¿å­˜ç›®å½•
            model_num (int): æ¨¡å‹ç¼–å·
            file_num (int): æ–‡ä»¶ç¼–å·
            is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒ
            label_name (str): æ ‡ç­¾åˆ—å
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            drop_columns (list): è¦åˆ é™¤çš„åˆ—
            is_categorical_func_type (str): åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°ç±»å‹ï¼Œ'general'æˆ–'tra'
            date_type (list, optional): æ—¥æœŸç±»å‹åˆ—åˆ—è¡¨
            sep (str): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º'~'

        Returns:
            tuple: (å¤„ç†åçš„DataFrame, æ ‡ç­¾åˆ—, å‚æ•°é…ç½®å¯¹è±¡)
        """
        # 1. è·å–è¦ä½¿ç”¨çš„åˆ—
        usecols = self._get_usecols_from_model_title(model_title)

        # 2. åˆ›å»ºåˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°
        if is_categorical_func_type == 'general':
            is_categorical_func = self._create_categorical_function_general()
        elif is_categorical_func_type == 'tra':
            is_categorical_func = self._create_categorical_function_tra()
        else:
            raise ValueError(f"æœªçŸ¥çš„is_categorical_func_type: {is_categorical_func_type}")

        # 3. åŠ è½½æ•°æ®å¹¶åˆ†ç±»
        # print("data_path:",data_path)
        df, y, col_types = self._load_and_classify_data(
            data_path, label_name, str_identity, is_train,
            usecols, drop_columns, is_categorical_func, sep, date_type, bool_type
        )
        
        self.lg(f"classify_data----------------------")
        self.lg(f"col_types['date_type'] len = {len(col_types['date_type'])}")
        self.lg(f"col_types['num_type'] len = {len(col_types['num_type'])}")
        self.lg(f"col_types['classify_type'] len = {len(col_types['classify_type'])}")
        self.lg(f"df[:3]:\n{df[:3]}")

        # 4. åˆ†ææ•°å€¼åˆ—
        num_small = self._analyze_numeric_columns(df, pc)

        # 5. è®¾ç½®å‚æ•°é…ç½®
        self._setup_param_config(pc, str_identity, col_types, num_small, alg_type,
                               model_ai_dir, model_num, file_num, is_train,
                               label_name, drop_columns, date_type)

        # 6. è®°å½•æ•°æ®ä¿¡æ¯
        self._log_data_info(pc, num_small)

        # 7. å¤„ç†æ•°æ®
        df_processed = self._process_data_with_deal_dl(df, pc)

        return df_processed, y, pc

    def data_deal_train(self, data_path, model_title, str_identity,
                       alg_type, model_ai_dir, model_num, file_num=1,
                       is_train=True, label_name=None, pc:ParamConfig=None,
                       drop_columns=None, date_type=[], sep='~',
                       classify_type2 = [[]],bool_type = []):
        """
        é€šç”¨æ•°æ®è®­ç»ƒå¤„ç†æ–¹æ³• - é‡æ„ç‰ˆæœ¬

        ä½¿ç”¨é€šç”¨æ•°æ®å¤„ç†ç®¡é“æ¥å¤„ç†è®­ç»ƒæ•°æ®ï¼Œç®€åŒ–ä»£ç å¹¶æé«˜å¯ç»´æŠ¤æ€§

        Args:
            data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
            model_title (str): æ¨¡å‹æ ‡é¢˜
            str_identity (str): æ ‡è¯†åˆ—
            alg_type (str): ç®—æ³•ç±»å‹
            model_ai_dir (str): æ¨¡å‹ä¿å­˜ç›®å½•
            model_num (int): æ¨¡å‹ç¼–å·
            file_num (int): æ–‡ä»¶ç¼–å·ï¼Œé»˜è®¤ä¸º1
            is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒï¼Œé»˜è®¤ä¸ºTrue
            label_name (str): æ ‡ç­¾åˆ—å
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            drop_columns (list): è¦åˆ é™¤çš„åˆ—
            date_type (list, optional): æ—¥æœŸç±»å‹åˆ—åˆ—è¡¨
            sep (str): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º'~'

        Returns:
            tuple: (å¤„ç†åçš„DataFrame, æ ‡ç­¾åˆ—, å‚æ•°é…ç½®å¯¹è±¡)
        """
        self.lg = pc.lg
        return self._common_data_processing_pipeline(
            data_path=data_path,
            model_title=model_title,
            str_identity=str_identity,
            alg_type=alg_type,
            model_ai_dir=model_ai_dir,
            model_num=model_num,
            file_num=file_num,
            is_train=is_train,
            label_name=label_name,
            pc=pc,
            drop_columns=drop_columns,
            is_categorical_func_type='general',
            date_type=date_type,
            sep=sep
        )

    def data_deal_train_tra(self, data_path, model_title, str_identity,
                           alg_type, model_ai_dir, model_num, file_num=1,
                           is_train=True, label_name=None, pc:ParamConfig=None,
                           drop_columns=None, date_type=[], sep='~',
                           classify_type2 = [[]],bool_type = []):
        """
        äº¤æ˜“æ•°æ®è®­ç»ƒå¤„ç†æ–¹æ³• - é‡æ„ç‰ˆæœ¬

        ä½¿ç”¨é€šç”¨æ•°æ®å¤„ç†ç®¡é“æ¥å¤„ç†äº¤æ˜“è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨ç‰¹å®šçš„åˆ†ç±»åˆ—åˆ¤æ–­é€»è¾‘

        Args:
            data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
            model_title (str): æ¨¡å‹æ ‡é¢˜
            str_identity (str): æ ‡è¯†åˆ—
            alg_type (str): ç®—æ³•ç±»å‹
            model_ai_dir (str): æ¨¡å‹ä¿å­˜ç›®å½•
            model_num (int): æ¨¡å‹ç¼–å·
            file_num (int): æ–‡ä»¶ç¼–å·ï¼Œé»˜è®¤ä¸º1
            is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒï¼Œé»˜è®¤ä¸ºTrue
            label_name (str): æ ‡ç­¾åˆ—å
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            drop_columns (list): è¦åˆ é™¤çš„åˆ—
            date_type (list, optional): æ—¥æœŸç±»å‹åˆ—åˆ—è¡¨
            sep (str): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º'~'

        Returns:
            tuple: (å¤„ç†åçš„DataFrame, æ ‡ç­¾åˆ—, å‚æ•°é…ç½®å¯¹è±¡)
        """
        self.lg = pc.lg
        return self._common_data_processing_pipeline(
            data_path=data_path,
            model_title=model_title,
            str_identity=str_identity,
            alg_type=alg_type,
            model_ai_dir=model_ai_dir,
            model_num=model_num,
            file_num=file_num,
            is_train=is_train,
            label_name=label_name,
            pc=pc,
            drop_columns=drop_columns,
            is_categorical_func_type='tra',
            date_type=date_type,
            sep=sep
        )


class Data2FeatureBase:
    
    pc = ParamConfig()
    
    def __init__(self):
        """
        ä¸»è¦é€»è¾‘
        1. æ•°æ®è¯»å–  read_csv
        2. æ•°æ®ç±»å‹è½¬æ¢ data_type_change
        3. æ•°æ®è§‚å¯Ÿ  show_*
        4. æ•°å­—åŒ–ï¼Œç±»åˆ«è½¬ç´¢å¼• tonum_*
        5. å½’ä¸€åŒ–  norm_*
        
        
        """

        pass 
        
    @classmethod
    def lg(cls,msg):
        cls.pc.lg(msg)
    
    @classmethod
    def _get_usecols(cls, heads=None, sep='~'):
        """
        æ ¹æ®åˆ—åå¤´éƒ¨å­—ç¬¦ä¸²è·å–è¦ä½¿ç”¨çš„åˆ—

        Args:
            heads (str or None): åˆ—åå¤´éƒ¨å­—ç¬¦ä¸²ï¼Œ'all'è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰åˆ—ï¼Œå…¶ä»–å€¼æŒ‰åˆ†éš”ç¬¦åˆ†å‰²
            sep (str): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º'~'

        Returns:
            list or None: è¦ä½¿ç”¨çš„åˆ—åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰åˆ—
        """
        if heads is None or  heads == 'all':
            return None
        else:
            return heads.split(sep)
        

    @classmethod
    def read_csv(cls,data_path, sep=',', usecols=None, heads=None, heads_sep=None):
        """
        è¯»å–CSVæ–‡ä»¶å¹¶è¿”å›DataFrame

        Args:
            data_path (str): CSVæ–‡ä»¶è·¯å¾„
            sep (str, optional): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º','
            usecols (list, optional): è¦ä½¿ç”¨çš„åˆ—åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰åˆ—
            heads (str, optional): åˆ—åå­—ç¬¦ä¸²ï¼Œç”¨åˆ†éš”ç¬¦åˆ†éš”å¤šä¸ªåˆ—å
            heads_sep (str, optional): åˆ—åçš„åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä½¿ç”¨sepçš„å€¼

        Returns:
            pd.DataFrame: è¯»å–çš„æ•°æ®æ¡†
        """
        if heads_sep is None:
            heads_sep = sep
        if heads is not None:
            usecols = cls._get_usecols(heads,sep=heads_sep)
        df = pd.read_csv(data_path, sep=sep, usecols=usecols)
        return df 
    
    @classmethod
    def data_type_change(cls,df,num_type=None,classify_type=None,date_type=None):
        """
        è½¬æ¢DataFrameä¸­æŒ‡å®šåˆ—çš„æ•°æ®ç±»å‹ï¼›é€šå¸¸æ˜¯æŒ‡å®šnum_typeã€date_type,å°†å‰©ä¸‹çš„åˆ—è½¬æ¢æˆclassify_type

        è¯¥æ–¹æ³•ç”¨äºå°†DataFrameä¸­çš„åˆ—è½¬æ¢ä¸ºæŒ‡å®šçš„æ•°æ®ç±»å‹ï¼Œæ”¯æŒæ•°å€¼å‹ã€åˆ†ç±»å‹å’Œæ—¥æœŸå‹åˆ—çš„è½¬æ¢ã€‚
        ä¸»è¦ç”¨äºæ•°æ®é¢„å¤„ç†é˜¶æ®µï¼Œç¡®ä¿æ•°æ®å…·æœ‰æ­£ç¡®çš„ç±»å‹ä»¥ä¾¿åç»­åˆ†æã€‚

        Args:
            df (pd.DataFrame): è¾“å…¥çš„æ•°æ®è¡¨
            num_type (list): éœ€è¦è½¬æ¢ä¸ºæ•°å€¼å‹çš„åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä¸è½¬æ¢ï¼‰
            classify_type (list): éœ€è¦è½¬æ¢ä¸ºåˆ†ç±»å‹çš„åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä¸è½¬æ¢ï¼‰
            date_type (list): éœ€è¦è½¬æ¢ä¸ºæ—¥æœŸå‹çš„åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä¸è½¬æ¢ï¼‰

        Returns:
            pd.DataFrame: æ•°æ®ç±»å‹è½¬æ¢åçš„æ•°æ®è¡¨

        ä½¿ç”¨ç¤ºä¾‹ï¼š
            # è½¬æ¢æŒ‡å®šåˆ—çš„æ•°æ®ç±»å‹
            df_converted = Data2Feature.data_type_change(
                df,
                num_type=['age', 'salary'],      # è½¬æ¢ä¸ºæ•°å€¼å‹
                classify_type=['gender', 'city'], # è½¬æ¢ä¸ºåˆ†ç±»å‹
                date_type=['create_time', 'update_time']  # è½¬æ¢ä¸ºæ—¥æœŸå‹
            )

        æ³¨æ„äº‹é¡¹ï¼š
        - è½¬æ¢å¤±è´¥çš„åˆ—ä¼šä¿æŒåŸæœ‰æ•°æ®ç±»å‹
        - æ—¥æœŸè½¬æ¢æ”¯æŒå¸¸è§çš„æ—¥æœŸæ ¼å¼
        - æ•°å€¼è½¬æ¢ä¼šå°†æ— æ³•è§£æçš„å€¼è®¾ä¸ºNaN
        
        å¤„ç†é€»è¾‘ï¼š
        1. è‡ªåŠ¨æ¨æ–­æ•°å€¼åˆ—ï¼šå¦‚æœnum_typeä¸ºç©ºæˆ–Noneï¼Œåˆ™è‡ªåŠ¨æ¨æ–­
           - ä½¿ç”¨data.select_dtypes('number')é€‰æ‹©æ‰€æœ‰æ•°å€¼ç±»å‹çš„åˆ—
           - åŒ…æ‹¬int, float, boolç­‰æ•°å€¼ç±»å‹ï¼Œç¡®ä¿æ•°å€¼æ•°æ®ç»Ÿä¸€å¤„ç†

        2. è‡ªåŠ¨æ¨æ–­ç±»åˆ«åˆ—ï¼šå¦‚æœclassify_typeä¸ºç©ºæˆ–Noneï¼Œåˆ™è‡ªåŠ¨æ¨æ–­
           - ä»æ‰€æœ‰åˆ—ä¸­æ’é™¤num_typeå’Œdate_typeåˆ—ï¼Œå‰©ä½™çš„ä½œä¸ºç±»åˆ«åˆ—
           - è¿™ç§æ–¹å¼å¯ä»¥ç®€åŒ–è°ƒç”¨ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®šæ‰€æœ‰ç±»åˆ«åˆ—

        3. ç±»åˆ«åˆ—å¤„ç†ï¼šå°†æŒ‡å®šçš„ç±»åˆ«åˆ—è½¬æ¢ä¸ºpandas stringç±»å‹
           - ä½¿ç”¨astype("string")è€Œä¸æ˜¯astype(str)ï¼Œä»¥è·å¾—æ›´å¥½çš„å†…å­˜æ•ˆç‡
           - é€šè¿‡é›†åˆæ“ä½œç¡®ä¿åªå¤„ç†å®é™…å­˜åœ¨çš„åˆ—

        4. æ•°å€¼åˆ—å¤„ç†ï¼šå°†æŒ‡å®šçš„æ•°å€¼åˆ—è½¬æ¢ä¸ºfloat64ç±»å‹
           - float64æä¾›äº†è¶³å¤Ÿçš„æ•°å€¼ç²¾åº¦ï¼Œé€‚ç”¨äºå¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•
           - ç»Ÿä¸€æ•°å€¼ç±»å‹æœ‰åŠ©äºåç»­çš„å½’ä¸€åŒ–å’Œæ ‡å‡†åŒ–å¤„ç†

        5. æ—¥æœŸåˆ—å¤„ç†ï¼šå°†æŒ‡å®šçš„æ—¥æœŸåˆ—è½¬æ¢ä¸ºdatetimeç±»å‹
           - é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯datetimeç±»å‹ï¼Œé¿å…é‡å¤è½¬æ¢
           - ä½¿ç”¨errors='coerce'å‚æ•°ï¼Œæ— æ•ˆæ—¥æœŸè½¬ä¸ºNaTè€ŒéæŠ¥é”™
           - æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼çš„è‡ªåŠ¨è¯†åˆ«å’Œè½¬æ¢
        """
        df = DataDeal.data_type_change(df,num_type=num_type,classify_type=classify_type,date_type=date_type)
        return df
        
    @classmethod
    def show_col_type(cls, df, numeric_only=False, non_numeric_only=False):
        """
        æ˜¾ç¤ºDataFrameåˆ—çš„æ•°æ®ç±»å‹

        Args:
            df (pd.DataFrame): è¾“å…¥çš„æ•°æ®è¡¨
            numeric_only (bool): æ˜¯å¦åªæ˜¾ç¤ºæ•°å€¼åˆ—ï¼Œé»˜è®¤ä¸ºFalse
            non_numeric_only (bool): æ˜¯å¦åªæ˜¾ç¤ºéæ•°å€¼åˆ—ï¼Œé»˜è®¤ä¸ºFalse

        Note:
            å¦‚æœnumeric_onlyå’Œnon_numeric_onlyéƒ½ä¸ºFalseï¼Œæ˜¾ç¤ºæ‰€æœ‰åˆ—ç±»å‹
            å¦‚æœnumeric_onlyä¸ºTrueï¼Œåªæ˜¾ç¤ºæ•°å€¼åˆ—ç±»å‹
            å¦‚æœnon_numeric_onlyä¸ºTrueï¼Œåªæ˜¾ç¤ºéæ•°å€¼åˆ—ç±»å‹
        """
        if numeric_only and non_numeric_only:
            print("è­¦å‘Šï¼šnumeric_onlyå’Œnon_numeric_onlyä¸èƒ½åŒæ—¶ä¸ºTrueï¼Œå°†æ˜¾ç¤ºæ‰€æœ‰åˆ—ç±»å‹")
            print(df.dtypes)
        elif numeric_only:
            numeric_cols = df.select_dtypes(include=['number']).columns
            if len(numeric_cols) > 0:
                print("æ•°å€¼åˆ—ç±»å‹ï¼š")
                print(df[numeric_cols].dtypes)
            else:
                print("æ²¡æœ‰æ•°å€¼åˆ—")
        elif non_numeric_only:
            non_numeric_cols = df.select_dtypes(exclude=['number']).columns
            if len(non_numeric_cols) > 0:
                print("éæ•°å€¼åˆ—ç±»å‹ï¼š")
                print(df[non_numeric_cols].dtypes)
            else:
                print("æ²¡æœ‰éæ•°å€¼åˆ—")
        else:
            print(df.dtypes)
       
    @classmethod
    def show_date_type(cls, df):
        """
        å±•ç¤ºæ—¥æœŸåˆ—çš„æ•°æ®ç±»å‹

        åŠŸèƒ½:
        1. è¯†åˆ«DataFrameä¸­çš„æ—¥æœŸå’Œæ—¶é—´åˆ—
        2. æ˜¾ç¤ºæ—¥æœŸåˆ—çš„æ•°æ®ç±»å‹
        3. æä¾›æ—¥æœŸåˆ—çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        4. æ˜¾ç¤ºæ—¥æœŸèŒƒå›´å’Œæ ¼å¼ä¿¡æ¯

        å‚æ•°:
        df (pd.DataFrame): è¾“å…¥çš„æ•°æ®è¡¨

        è¿”å›:
        dict: æ—¥æœŸåˆ—ç±»å‹ä¿¡æ¯å­—å…¸ï¼Œkeyä¸ºæ—¥æœŸåˆ—åï¼Œvalueä¸ºæ•°æ®ç±»å‹
              å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—æˆ–DataFrameä¸ºç©ºï¼Œè¿”å›ç©ºå­—å…¸

        è¾“å‡º:
        æ‰“å°æ—¥æœŸåˆ—çš„ç±»å‹ä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®
        """
        if df is None or df.empty:
            print("DataFrameä¸ºç©ºï¼Œæ— æ³•åˆ†ææ—¥æœŸåˆ—")
            return {}

        # è¯†åˆ«æ—¥æœŸåˆ—çš„æ–¹æ³•
        date_columns = []

        # æ–¹æ³•1: é€šè¿‡pandasæ•°æ®ç±»å‹è¯†åˆ«datetime64ç±»å‹
        datetime_cols = df.select_dtypes(include=['datetime64[ns]', 'datetime64']).columns.tolist()
        date_columns.extend(datetime_cols)

        # æ–¹æ³•2: é€šè¿‡åˆ—åæ¨¡å¼è¯†åˆ«å¯èƒ½çš„æ—¥æœŸåˆ—
        potential_date_patterns = [
            'date', 'time', 'dt', 'timestamp', 'created_at', 'updated_at',
            'start_time', 'end_time', 'year', 'month', 'day'
        ]

        for col in df.columns:
            # æ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«æ—¥æœŸç›¸å…³å…³é”®è¯
            if any(pattern in col.lower() for pattern in potential_date_patterns):
                if col not in date_columns:
                    # è¿›ä¸€æ­¥æ£€æŸ¥æ•°æ®æ˜¯å¦çœŸçš„æ˜¯æ—¥æœŸæ ¼å¼
                    try:
                        # å°è¯•è½¬æ¢ä¸ºdatetime
                        pd.to_datetime(df[col].dropna().head(100))
                        date_columns.append(col)
                    except:
                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè¯´æ˜ä¸æ˜¯æ—¥æœŸåˆ—
                        pass

        # æ–¹æ³•3: é€šè¿‡æ•°æ®å†…å®¹è¯†åˆ«å­—ç¬¦ä¸²æ ¼å¼çš„æ—¥æœŸ
        string_cols = df.select_dtypes(include=['object', 'string']).columns
        for col in string_cols:
            if col not in date_columns:
                try:
                    # æ£€æŸ¥å‰å‡ è¡Œæ˜¯å¦å¯ä»¥è§£æä¸ºæ—¥æœŸ
                    sample_data = df[col].dropna().head(50)
                    if len(sample_data) > 0:
                        pd.to_datetime(sample_data)
                        date_columns.append(col)
                except:
                    pass

        # ç§»é™¤é‡å¤é¡¹å¹¶ä¿æŒé¡ºåº
        date_columns = list(dict.fromkeys(date_columns))

        if not date_columns:
            print("æœªå‘ç°æ—¥æœŸåˆ—")
            print("\nå»ºè®®:")
            print("1. æ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«æ—¥æœŸç›¸å…³å…³é”®è¯")
            print("2. ç¡®è®¤æ—¥æœŸåˆ—æ˜¯å¦å·²è½¬æ¢ä¸ºdatetimeç±»å‹")
            print("3. ä½¿ç”¨ pd.to_datetime() æ‰‹åŠ¨è½¬æ¢æ—¥æœŸåˆ—")
            return {}

        print(f"å‘ç° {len(date_columns)} ä¸ªæ—¥æœŸåˆ—:")
        print("-" * 60)

        # æ˜¾ç¤ºæ¯ä¸ªæ—¥æœŸåˆ—çš„è¯¦ç»†ä¿¡æ¯
        for i, col in enumerate(date_columns, 1):
            print(f"\n{i}. åˆ—å: '{col}'")
            print(f"   æ•°æ®ç±»å‹: {df[col].dtype}")

            # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            if df[col].notna().sum() > 0:
                # è·å–éç©ºçš„æ•°æ®
                date_data = df[col].dropna()

                # è½¬æ¢ä¸ºdatetimeè¿›è¡Œç»Ÿè®¡
                try:
                    date_data_dt = pd.to_datetime(date_data)

                    print(f"   éç©ºå€¼æ•°é‡: {len(date_data_dt)}/{len(df)}")
                    print(f"   ç¼ºå¤±å€¼æ•°é‡: {df[col].isna().sum()}")
                    print(f"   æœ€æ—©æ—¥æœŸ: {date_data_dt.min()}")
                    print(f"   æœ€æ™šæ—¥æœŸ: {date_data_dt.max()}")
                    print(f"   æ—¥æœŸèŒƒå›´: {(date_data_dt.max() - date_data_dt.min()).days} å¤©")

                    # æ˜¾ç¤ºæ ·æœ¬æ•°æ®
                    print(f"   æ ·æœ¬æ•°æ®:")
                    for j, sample in enumerate(date_data.head(3)):
                        print(f"     [{j+1}] {sample}")

                except Exception as e:
                    print(f"   è½¬æ¢ä¸ºdatetimeæ—¶å‡ºé”™: {e}")
                    print(f"   æ ·æœ¬æ•°æ®:")
                    for j, sample in enumerate(date_data.head(3)):
                        print(f"     [{j+1}] {sample}")
            else:
                print("   å…¨ä¸ºç©ºå€¼")

        # æ˜¾ç¤ºæ—¥æœŸåˆ—çš„æ€»ä½“ç»Ÿè®¡
        print(f"\n{'='*60}")
        print(f"æ—¥æœŸåˆ—æ±‡æ€»:")
        print(f"  æ€»åˆ—æ•°: {len(date_columns)}")
        print(f"  æ€»æ•°æ®é‡: {len(df)} è¡Œ")

        # ç»Ÿè®¡ä¸åŒç±»å‹çš„æ—¥æœŸåˆ—
        datetime_count = len(df.select_dtypes(include=['datetime64[ns]', 'datetime64']).columns)
        string_date_count = len(date_columns) - datetime_count

        print(f"  datetimeç±»å‹åˆ—: {datetime_count}")
        print(f"  å­—ç¬¦ä¸²æ—¥æœŸåˆ—: {string_date_count}")

        if datetime_count > 0:
            print(f"\nå»ºè®®: å­—ç¬¦ä¸²æ—¥æœŸåˆ—å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç è½¬æ¢ä¸ºdatetime:")
            for col in date_columns:
                if df[col].dtype in ['object', 'string']:
                    print(f"  df['{col}'] = pd.to_datetime(df['{col}'])")

        # åˆ›å»ºå¹¶è¿”å›æ—¥æœŸåˆ—ç±»å‹å­—å…¸
        date_columns_dict = {}
        for col in date_columns:
            date_columns_dict[col] = str(df[col].dtype)
        print(date_columns_dict)
        return date_columns_dict
        
    @classmethod
    def get_col_names(cls, df, col_type='object'):
        """
        æ ¹æ®æŒ‡å®šçš„æ•°æ®ç±»å‹è·å–DataFrameä¸­çš„åˆ—å

        Args:
            df: pandas DataFrame
            col_type: æ•°æ®ç±»å‹è¿‡æ»¤æ¡ä»¶ï¼Œå¯é€‰å€¼ï¼š
                - 'object': objectç±»å‹åˆ—
                - 'cat': åˆ†ç±»ç±»å‹åˆ—ï¼ˆåŒ…æ‹¬str, string, categoryï¼‰
                - 'num': æ•°å€¼ç±»å‹åˆ—ï¼ˆåŒ…æ‹¬æ‰€æœ‰æ•°å­—ç±»å‹ï¼‰
                - 'date': æ—¥æœŸç±»å‹åˆ—ï¼ˆåŒ…æ‹¬datetime64[ns], datetime64ï¼‰
                - 'datetime64[ns]': ç²¾ç¡®çš„datetime64[ns]ç±»å‹
                - 'datetime64': datetime64ç±»å‹
                - 'int': æ•´æ•°ç±»å‹åˆ—
                - 'float': æµ®ç‚¹æ•°ç±»å‹åˆ—
                - 'bool': å¸ƒå°”ç±»å‹åˆ—
                - 'str': å­—ç¬¦ä¸²ç±»å‹åˆ—
                - 'category': åˆ†ç±»ç±»å‹åˆ—
                - 'all': è¿”å›æ‰€æœ‰åˆ—åï¼ˆé»˜è®¤è¡Œä¸ºï¼‰

        Returns:
            list: ç¬¦åˆæŒ‡å®šç±»å‹çš„åˆ—ååˆ—è¡¨
        """
        import pandas as pd
        import numpy as np

        # å¦‚æœè¯·æ±‚æ‰€æœ‰åˆ—ï¼Œç›´æ¥è¿”å›
        if col_type == 'all':
            return list(df.columns)

        # å®šä¹‰ç±»å‹æ˜ å°„å…³ç³»
        type_mapping = {
            # æ•°å€¼ç±»å‹
            'num': ['int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', 'uint32', 'uint64',
                   'float16', 'float32', 'float64', 'number'],
            'int': ['int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', 'uint32', 'uint64', 'integer'],
            'float': ['float16', 'float32', 'float64', 'floating'],

            # å­—ç¬¦ä¸²å’Œåˆ†ç±»ç±»å‹
            'cat': ['object', 'string', 'category', 'str'],
            'str': ['object', 'string'],
            'category': ['category'],

            # æ—¥æœŸæ—¶é—´ç±»å‹
            'date': ['datetime64[ns]', 'datetime64', 'datetime'],
            'datetime64[ns]': ['datetime64[ns]'],
            'datetime64': ['datetime64', 'datetime64[ns]'],

            # å¸ƒå°”ç±»å‹
            'bool': ['bool', 'boolean'],

            # å¯¹è±¡ç±»å‹ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            'object': ['object'],
        }

        # è·å–è¯·æ±‚çš„ç±»å‹åˆ—è¡¨
        target_types = type_mapping.get(col_type, [col_type])

        # æ”¶é›†ç¬¦åˆæ¡ä»¶çš„åˆ—å
        result_columns = []

        for col in df.columns:
            col_dtype = str(df[col].dtype).lower()

            # æ£€æŸ¥åˆ—çš„æ•°æ®ç±»å‹æ˜¯å¦åŒ¹é…ç›®æ ‡ç±»å‹
            for target_type in target_types:
                if target_type.lower() in col_dtype:
                    result_columns.append(col)
                    break

        return result_columns

    @classmethod
    def get_cat_types(cls, df):
        """
        è·å–DataFrameä¸­åˆ†ç±»ç±»å‹çš„åˆ—å

        Args:
            df: pandas DataFrame

        Returns:
            list: åˆ†ç±»ç±»å‹çš„åˆ—ååˆ—è¡¨
        """
        return cls.get_col_names(df, 'cat')

    @classmethod
    def get_col_names_by_pattern(cls, df, pattern='.*'):
        """
        æ ¹æ®æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼è·å–åˆ—å

        Args:
            df: pandas DataFrame
            pattern: æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œé»˜è®¤åŒ¹é…æ‰€æœ‰åˆ—

        Returns:
            list: åŒ¹é…æ¨¡å¼çš„åˆ—ååˆ—è¡¨
        """
        import re
        return [col for col in df.columns if re.match(pattern, col)]

    @classmethod
    def get_col_types_summary(cls, df):
        """
        è·å–DataFrameä¸­å„åˆ—çš„æ•°æ®ç±»å‹æ±‡æ€»

        Args:
            df: pandas DataFrame

        Returns:
            dict: æ•°æ®ç±»å‹åˆ°åˆ—ååˆ—è¡¨çš„æ˜ å°„
        """
        type_summary = {}
        for col in df.columns:
            dtype = str(df[col].dtype)
            if dtype not in type_summary:
                type_summary[dtype] = []
            type_summary[dtype].append(col)
        return type_summary
        
    @classmethod
    def show_null_count(cls,df):
        print(df.isnull().sum())
        
    @classmethod
    def show_one_row(cls, df=None, row_idx=0, n=10, show_all=False, is_print=False):
        """
        æ˜¾ç¤ºDataFrameä¸­æŒ‡å®šè¡Œçš„å‰nä¸ªå­—æ®µ

        å‚æ•°:
        df: DataFrameå¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨å±€çš„df_final_result
        row_idx: è¡Œç´¢å¼•ï¼Œé»˜è®¤ä¸º0ï¼ˆç¬¬ä¸€è¡Œï¼‰
        n: æ˜¾ç¤ºçš„å­—æ®µæ•°é‡ï¼Œé»˜è®¤ä¸º10ä¸ª
        show_all: æ˜¯å¦æ˜¾ç¤ºæ‰€æœ‰å­—æ®µï¼Œå¦‚æœä¸ºTrueåˆ™å¿½ç•¥nå‚æ•°

        åŠŸèƒ½:
        1. æ˜¾ç¤ºæŒ‡å®šè¡Œçš„å‰nä¸ªå­—æ®µçš„é”®å€¼å¯¹
        2. æ”¯æŒæ˜¾ç¤ºDataFrameçš„åŸºæœ¬ä¿¡æ¯
        3. æä¾›å­—æ®µè®¡æ•°å’Œæ€»è§ˆä¿¡æ¯
        4. æ”¯æŒæ˜¾ç¤ºæ‰€æœ‰å­—æ®µæˆ–é™åˆ¶æ˜¾ç¤ºæ•°é‡
        """
        import pandas as pd

        # æ£€æŸ¥DataFrameæ˜¯å¦ä¸ºç©º
        if df is None or df.empty:
            cls.pc.lg("DataFrameä¸ºç©ºï¼Œæ²¡æœ‰æ•°æ®å¯æ˜¾ç¤º")
            return

        # æ£€æŸ¥è¡Œç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
        if row_idx < 0 or row_idx >= len(df):
            cls.pc.lg(f"é”™è¯¯: è¡Œç´¢å¼• {row_idx} è¶…å‡ºèŒƒå›´ [0, {len(df)-1}]")
            return

        # æ˜¾ç¤ºDataFrameåŸºæœ¬ä¿¡æ¯
        cls.pc.lg(f"DataFrameå½¢çŠ¶: {df.shape}")
        cls.pc.lg(f"æ˜¾ç¤ºç¬¬ {row_idx} è¡Œï¼ˆç´¢å¼•: {df.index[row_idx]}ï¼‰")
        cls.pc.lg(f"æ€»å­—æ®µæ•°: {len(df.columns)}")

        if show_all:
            cls.pc.lg(f"æ˜¾ç¤ºæ‰€æœ‰å­—æ®µ:")
            display_count = len(df.columns)
        else:
            cls.pc.lg(f"æ˜¾ç¤ºå‰ {n} ä¸ªå­—æ®µ:")
            display_count = min(n, len(df.columns))

        cls.pc.lg("-" * 60)

        # æ˜¾ç¤ºæŒ‡å®šè¡Œçš„å­—æ®µ
        count = 0
        for k, v in df.iloc[row_idx].items():
            # æ ¼å¼åŒ–æ˜¾ç¤º
            if pd.isna(v):
                value_str = "NaN"
            elif isinstance(v, float):
                if abs(v) < 0.001:
                    value_str = f"{v:.6f}"
                else:
                    value_str = f"{v:.3f}"
            elif isinstance(v, (int, np.integer)):
                value_str = str(v)
            else:
                value_str = str(v)
                # é™åˆ¶å­—ç¬¦ä¸²é•¿åº¦
                if len(value_str) > 50:
                    value_str = value_str[:47] + "..."

            cls.pc.lg(f"{k:30}: {value_str}")
            if is_print:
                print(f"{k:30}: {value_str}")
                
            count += 1

            # å¦‚æœä¸æ˜¾ç¤ºå…¨éƒ¨ä¸”è¾¾åˆ°æŒ‡å®šæ•°é‡ï¼Œåˆ™åœæ­¢
            if not show_all and count >= display_count:
                break

        cls.pc.lg("-" * 60)

        # å¦‚æœè¿˜æœ‰æœªæ˜¾ç¤ºçš„å­—æ®µï¼Œæç¤ºç”¨æˆ·
        if not show_all and len(df.columns) > n:
            remaining = len(df.columns) - n
            cls.pc.lg(f"è¿˜æœ‰ {remaining} ä¸ªå­—æ®µæœªæ˜¾ç¤ºï¼Œä½¿ç”¨ show_all=True å¯æ˜¾ç¤ºå…¨éƒ¨")

            
    @classmethod
    def show_unique_count(cls,df):
        print(df.nunique())
        
        
        
    @classmethod
    def _categorical_not_num_date(cls,df,num_type=[],date_type=[]):
        """
        åˆ›å»ºåˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°ï¼Œåˆ¤æ–­é€»è¾‘ä¸ºæ’é™¤æ•°å€¼åˆ—å’Œæ—¥æœŸåˆ—

        å¦‚æœnum_typeä¸ºç©ºï¼Œåˆ™é€‰æ‹©pandasæ•°è¡¨dfä¸­ç±»å‹ä¸ºnumberçš„åˆ—ï¼Œ
        å¦‚æœdate_typeä¸ºç©ºæˆ–Noneï¼Œåˆ™è‡ªåŠ¨æ¨æ–­dfä¸­æ—¥æœŸç±»å‹çš„åˆ—ä½œä¸ºæ—¥æœŸåˆ—ï¼Œ
        åŒæ—¶æ’é™¤ç±»å‹ä¸ºæ—¥æœŸçš„åˆ—ä»¥åŠdate_typeä¸­æŒ‡å®šçš„åˆ—

        Args:
            df (pd.DataFrame): è¾“å…¥çš„æ•°æ®è¡¨
            num_type (list): æŒ‡å®šçš„æ•°å€¼åˆ—åˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºåˆ™è‡ªåŠ¨æ¨æ–­
            date_type (list): æŒ‡å®šçš„æ—¥æœŸåˆ—åˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºåˆ™è‡ªåŠ¨æ¨æ–­

        Returns:
            function: åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°
        """
        # å¦‚æœnum_typeä¸ºç©ºï¼Œè‡ªåŠ¨æ¨æ–­æ•°å€¼åˆ—
        if num_type is None or len(num_type) == 0:
            num_type = df.select_dtypes('number').columns.tolist()

        # å¦‚æœdate_typeä¸ºç©ºæˆ–Noneï¼Œè‡ªåŠ¨æ¨æ–­æ—¥æœŸåˆ—
        if date_type is None or len(date_type) == 0:
            # ä½¿ç”¨pd.api.types.is_datetime64_any_dtypeè‡ªåŠ¨æ¨æ–­æ—¥æœŸåˆ—
            date_type = [col for col in df.columns
                        if pd.api.types.is_datetime64_any_dtype(df[col])]

        # è·å–æ‰€æœ‰åˆ—å
        col_all = df.columns.tolist()

        # æ’é™¤æ•°å€¼åˆ—å’ŒæŒ‡å®šçš„æ—¥æœŸåˆ—
        exclude_cols = set(num_type) | set(date_type)
        categorical_cols = list(set(col_all) - exclude_cols)
        return categorical_cols
        
        
        
    @classmethod
    def show_col_diff(cls, df1, df2, show_common=False):
        """æ˜¾ç¤ºä¸¤ä¸ªæ•°æ®è¡¨ä¹‹é—´åˆ—çš„å·®å¼‚

        Args:
            df1 (pd.DataFrame): ç¬¬ä¸€ä¸ªæ•°æ®è¡¨
            df2 (pd.DataFrame): ç¬¬äºŒä¸ªæ•°æ®è¡¨

        Returns:
            tuple: (cols1, cols2,common_cols)
                cols1 (list): ç¬¬ä¸€ä¸ªæ•°æ®è¡¨ç¼ºå¤±çš„åˆ—ï¼ˆåœ¨df2ä¸­æœ‰ä½†df1ä¸­æ²¡æœ‰ï¼‰
                cols2 (list): ç¬¬äºŒä¸ªæ•°æ®è¡¨ç¼ºå¤±çš„åˆ—ï¼ˆåœ¨df1ä¸­æœ‰ä½†df2ä¸­æ²¡æœ‰ï¼‰
                common_cols:  å…¬å…±åˆ—

        """
        # è·å–ä¸¤ä¸ªDataFrameçš„åˆ—å
        cols1_set = set(df1.columns)
        cols2_set = set(df2.columns)

        # æ‰¾å‡ºå·®å¼‚
        # df1ç¼ºå¤±çš„åˆ—ï¼šåœ¨df2ä¸­æœ‰ä½†df1ä¸­æ²¡æœ‰
        cols1_missing = list(cols2_set - cols1_set)
        # df2ç¼ºå¤±çš„åˆ—ï¼šåœ¨df1ä¸­æœ‰ä½†df2ä¸­æ²¡æœ‰
        cols2_missing = list(cols1_set - cols2_set)

        # æ‰¾å‡ºå…±åŒçš„åˆ—
        common_cols = list(cols1_set & cols2_set)

        # æ‰“å°å·®å¼‚ä¿¡æ¯
        print(f"\n=== åˆ—å·®å¼‚åˆ†æ ===")
        print(f"DataFrame1 åˆ—æ•°: {len(cols1_set)}")
        print(f"DataFrame2 åˆ—æ•°: {len(cols2_set)}")
        print(f"å…±åŒåˆ—æ•°: {len(common_cols)}")

        if cols1_missing:
            print(f"\nDataFrame1 ç¼ºå¤±çš„åˆ— ({len(cols1_missing)}ä¸ª):")
            for col in sorted(cols1_missing):
                print(f"  - {col}")
        else:
            print("\nDataFrame1 æ²¡æœ‰ç¼ºå¤±çš„åˆ—")

        if cols2_missing:
            print(f"\nDataFrame2 ç¼ºå¤±çš„åˆ— ({len(cols2_missing)}ä¸ª):")
            for col in sorted(cols2_missing):
                print(f"  - {col}")
        else:
            print("\nDataFrame2 æ²¡æœ‰ç¼ºå¤±çš„åˆ—")

        if show_common:
            if common_cols:
                print(f"\nå…±åŒåˆ— ({len(common_cols)}ä¸ª):")
                for col in sorted(common_cols):
                    print(f"  - {col}")
                return cols1_missing, cols2_missing, common_cols
            else:
                print("\næ²¡æœ‰å…±åŒåˆ—")
                return cols1_missing, cols2_missing, []

        return cols1_missing, cols2_missing 
        
    @classmethod
    def tonum_col2index(cls,df, identity=[], classify_type=[], classify_type2=[],
                  dict_file="dict_file.dict", is_pre=False,
                  word2id=None, start_index=1):
        """
        å°†åˆ†ç±»åˆ—è½¬æ¢ä¸ºæ•°å€¼ç´¢å¼•ï¼Œæ”¯æŒè®­ç»ƒå’Œé¢„æµ‹æ¨¡å¼

        è¯¥æ–¹æ³•å°†DataFrameä¸­çš„åˆ†ç±»ç‰¹å¾åˆ—è½¬æ¢ä¸ºæ•°å€¼ç´¢å¼•ï¼Œä¾¿äºæœºå™¨å­¦ä¹ æ¨¡å‹å¤„ç†ã€‚
        æ”¯æŒå•åˆ—åˆ†ç±»å’Œå¤šåˆ—ç»„åˆåˆ†ç±»çš„è½¬æ¢ï¼Œå¹¶èƒ½ä¿å­˜å’ŒåŠ è½½ç¼–ç å­—å…¸ã€‚

        Args:
            df (pd.DataFrame): è¾“å…¥çš„æ•°æ®è¡¨
            identity (list): æ ‡è¯†åˆ—åˆ—è¡¨ï¼Œä¸å‚ä¸ç¼–ç ï¼Œé»˜è®¤ä¸ºç©º
            classify_type (list): éœ€è¦ç¼–ç çš„å•åˆ—åˆ†ç±»åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç©º
            classify_type2 (list): éœ€è¦ç¼–ç çš„å¤šåˆ—ç»„åˆåˆ†ç±»åˆ—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºåˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç©º
            dict_file (str): ç¼–ç å­—å…¸ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸º"dict_file.dict"
            is_pre (bool): æ˜¯å¦ä¸ºé¢„æµ‹æ¨¡å¼ï¼Œé»˜è®¤ä¸ºFalseï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
            word2id (dict): é¢„å®šä¹‰çš„è¯æ±‡åˆ°IDæ˜ å°„å­—å…¸ï¼Œé»˜è®¤ä¸ºNone
            start_index (int): ç´¢å¼•èµ·å§‹å€¼ï¼Œé»˜è®¤ä¸º1

        Returns:
            pd.DataFrame: ç¼–ç åçš„æ•°æ®è¡¨ï¼Œåˆ†ç±»åˆ—å·²è½¬æ¢ä¸ºæ•°å€¼ç´¢å¼•

        Raises:
            ValueError: å½“classify_type2ä¸­çš„å…ƒç´ ä¸æ˜¯åˆ—è¡¨ç±»å‹æ—¶æŠ›å‡º

        å¤„ç†é€»è¾‘ï¼š
        1. å‚æ•°éªŒè¯ï¼šæ£€æŸ¥classify_type2å‚æ•°æ ¼å¼ï¼Œç¡®ä¿æ¯ä¸ªå…ƒç´ éƒ½æ˜¯åˆ—è¡¨
        2. è°ƒç”¨åº•å±‚DataDeal.col2indexæ–¹æ³•è¿›è¡Œå®é™…çš„ç¼–ç è½¬æ¢
        3. æ”¯æŒè®­ç»ƒæ¨¡å¼å’Œé¢„æµ‹æ¨¡å¼çš„ä¸åŒå¤„ç†é€»è¾‘

        å‚æ•°è¯´æ˜ï¼š
        - classify_type: å•åˆ—åˆ†ç±»ï¼Œå¦‚["æ€§åˆ«", "å­¦å†"]
        - classify_type2: å¤šåˆ—ç»„åˆåˆ†ç±»ï¼Œå¦‚ [["çœä»½", "åŸå¸‚"], ["éƒ¨é—¨", "èŒä½"]]
        - is_pre=Trueæ—¶ä¸ºé¢„æµ‹æ¨¡å¼ï¼Œä¼šåŠ è½½å·²æœ‰çš„ç¼–ç å­—å…¸
        - is_pre=Falseæ—¶ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œä¼šåˆ›å»ºæ–°çš„ç¼–ç å­—å…¸å¹¶ä¿å­˜
        """
        # æ£€éªŒclassify_type2å‚æ•°ï¼Œå¦‚æœä¸ä¸ºç©ºæˆ–Noneï¼Œåˆ™å…¶å…ƒç´ å¿…é¡»ä¸ºåˆ—è¡¨
        if classify_type2 is not None and classify_type2:
            for i, item in enumerate(classify_type2):
                if not isinstance(item, list):
                    raise ValueError(f"classify_type2çš„ç¬¬{i+1}ä¸ªå…ƒç´ å¿…é¡»æ˜¯åˆ—è¡¨ï¼Œä½†å¾—åˆ°äº†{type(item)}: {item}")

        # ç¡®ä¿classify_type2ä¸ºåˆ—è¡¨ç±»å‹ï¼ˆé¿å…Noneå€¼ä¼ é€’ç»™åº•å±‚æ–¹æ³•ï¼‰
        classify_type2 = classify_type2 or []

        df = DataDeal.col2index(df,
                identity=identity, classify_type=classify_type, classify_type2=classify_type2,
                dict_file=dict_file, is_pre=is_pre,
                word2id=word2id, start_index=start_index)
        return df  
    
    @classmethod
    def tonum_label_encoding(cls, df, identity=[], classify_type=[], file_path=None,
                             is_pre=False, force_rewrite=False):
        """
        å¯¹åˆ†ç±»åˆ—è¿›è¡ŒLabelEncoderç¼–ç ï¼Œæ”¯æŒè®­ç»ƒå’Œé¢„æµ‹æ¨¡å¼

        Args:
            df (pd.DataFrame): è¾“å…¥çš„æ•°æ®è¡¨
            identity (list): æ ‡è¯†åˆ—åˆ—è¡¨ï¼Œä¸å‚ä¸ç¼–ç ï¼Œé»˜è®¤ä¸ºç©º
            classify_type (list): éœ€è¦ç¼–ç çš„åˆ†ç±»åˆ—ååˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºæˆ–Noneåˆ™è‡ªåŠ¨æ¨æ–­
            file_path (str): ç¼–ç å­—å…¸ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä¿å­˜/åŠ è½½å­—å…¸
            is_pre (bool): æ˜¯å¦ä¸ºé¢„æµ‹æ¨¡å¼ï¼Œé»˜è®¤ä¸ºFalseï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
            force_rewrite (bool): æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒç¼–ç å™¨ï¼Œé»˜è®¤ä¸ºFalse

        Returns:
            pd.DataFrame: ç¼–ç åçš„æ•°æ®è¡¨

        å¤„ç†é€»è¾‘ï¼š
        1. è®­ç»ƒæ¨¡å¼(is_pre=False)ï¼š
           - å¦‚æœç¼–ç å­—å…¸æ–‡ä»¶å­˜åœ¨ä¸”force_rewrite=Falseï¼ŒåŠ è½½å­—å…¸å¹¶åº”ç”¨
           - å¦åˆ™é‡æ–°è®­ç»ƒç¼–ç å™¨ï¼Œä¿å­˜å­—å…¸å¹¶åº”ç”¨

        2. é¢„æµ‹æ¨¡å¼(is_pre=True)ï¼š
           - å¦‚æœforce_rewrite=Trueï¼Œåˆ™é‡æ–°è®­ç»ƒç¼–ç å™¨ï¼ˆå¿½ç•¥is_preï¼‰
           - å¦åˆ™åªåŠ è½½å¹¶åº”ç”¨ç°æœ‰ç¼–ç å™¨ï¼Œä¸é‡æ–°è®­ç»ƒ

        3. å¦‚æœfile_pathä¸ºNoneï¼Œå§‹ç»ˆè¿›è¡Œè®­ç»ƒä½†ä¸ä¿å­˜å­—å…¸

        4. å¦‚æœclassify_typeä¸ºç©ºæˆ–Noneï¼Œè‡ªåŠ¨æ¨æ–­ç±»åˆ«åˆ—ï¼š
           - é€‰æ‹©dfä¸­æ‰€æœ‰éæ•°å­—åˆ—ä½œä¸ºç±»åˆ«åˆ—
           - æ’é™¤identityä¸­æŒ‡å®šçš„æ ‡è¯†åˆ—
        """
        # è‡ªåŠ¨æ¨æ–­ç±»åˆ«åˆ—ï¼ˆå¦‚æœclassify_typeä¸ºç©ºï¼‰
        if not classify_type:
            # è·å–æ‰€æœ‰éæ•°å­—åˆ—
            non_numeric_cols = df.select_dtypes(exclude=['number']).columns.tolist()

            # æ’é™¤identityä¸­çš„åˆ—
            identity_set = set(identity) if identity else set()
            classify_type = [col for col in non_numeric_cols if col not in identity_set]

            if classify_type:
                print(f"è‡ªåŠ¨æ¨æ–­ç±»åˆ«åˆ—ï¼š{classify_type}")
            else:
                print("æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç±»åˆ«åˆ—è¿›è¡Œç¼–ç ")
                return df

        # æ£€æŸ¥åˆ†ç±»åˆ—æ˜¯å¦å­˜åœ¨äºDataFrameä¸­
        valid_cols = [col for col in classify_type if col in df.columns]
        if len(valid_cols) != len(classify_type):
            missing_cols = set(classify_type) - set(valid_cols)
            print(f"è­¦å‘Šï¼šä»¥ä¸‹åˆ—ä¸å­˜åœ¨äºDataFrameä¸­ï¼š{missing_cols}")
            classify_type = valid_cols
            if not classify_type:
                return df

        # æ ¹æ®æ¨¡å¼å†³å®šå¤„ç†é€»è¾‘
        should_load = (file_path and os.path.exists(file_path) and
                      not force_rewrite and is_pre)

        if should_load:
            # é¢„æµ‹æ¨¡å¼ï¼šåŠ è½½ç°æœ‰ç¼–ç å­—å…¸
            try:
                label_encoding_dict = pkl_load(file_path)
                cls._apply_existing_encoders(df, classify_type, label_encoding_dict)
                print("é¢„æµ‹æ¨¡å¼ï¼šå·²åŠ è½½ç°æœ‰ç¼–ç å­—å…¸")
            except Exception as e:
                print(f"åŠ è½½ç¼–ç å­—å…¸å¤±è´¥ï¼Œé‡æ–°è®­ç»ƒç¼–ç å™¨ï¼š{e}")
                force_rewrite = True  # è®¾ç½®ä¸ºé‡æ–°è®­ç»ƒ

        if force_rewrite or not should_load:
            # è®­ç»ƒæ¨¡å¼æˆ–å¼ºåˆ¶é‡å†™ï¼šè®­ç»ƒæ–°çš„ç¼–ç å™¨
            mode = "å¼ºåˆ¶é‡å†™" if force_rewrite else ("é¢„æµ‹æ¨¡å¼ï¼ˆé‡è®­ç»ƒï¼‰" if is_pre else "è®­ç»ƒæ¨¡å¼")
            print(f"{mode}ï¼šè®­ç»ƒæ–°çš„ç¼–ç å™¨")

            label_encoding_dict = cls._train_new_encoders(df, classify_type)

            # ä¿å­˜ç¼–ç å­—å…¸ï¼ˆå¦‚æœæŒ‡å®šäº†æ–‡ä»¶è·¯å¾„ï¼‰
            if file_path:
                try:
                    pkl_save(label_encoding_dict, file_path=file_path)
                    print(f"ç¼–ç å­—å…¸å·²ä¿å­˜è‡³ï¼š{file_path}")
                except Exception as e:
                    print(f"ä¿å­˜ç¼–ç å­—å…¸å¤±è´¥ï¼š{e}")

        return df

    @classmethod
    def _apply_existing_encoders(cls, df, classify_type, label_encoding_dict):
        """åº”ç”¨ç°æœ‰çš„ç¼–ç å™¨ï¼Œå°†æœªçŸ¥ç±»åˆ«æ˜ å°„ä¸º<UNK>ï¼ˆç¼–ç ä¸º0ï¼‰"""
        for col in classify_type:
            if col in label_encoding_dict:
                try:
                    le = preprocessing.LabelEncoder()
                    le.classes_ = label_encoding_dict[col]

                    # å¤„ç†æ–°å‡ºç°çš„ç±»åˆ«ï¼ˆå°†å…¶æ˜ å°„ä¸º0ï¼Œå³<UNK>ï¼‰
                    unique_values = set(df[col].astype(str).unique())
                    known_values = set(le.classes_)
                    unknown_values = unique_values - known_values

                    if unknown_values:
                        print(f"åˆ— '{col}' å‘ç°æœªçŸ¥ç±»åˆ«ï¼š{unknown_values}ï¼Œå°†æ˜ å°„ä¸º<UNK>ï¼ˆç¼–ç ä¸º0ï¼‰")
                        df[col] = df[col].astype(str).apply(
                            lambda x: le.transform([x])[0] if x in known_values else 0
                        )
                    else:
                        df[col] = le.transform(df[col].astype(str))

                except Exception as e:
                    print(f"åº”ç”¨åˆ— '{col}' çš„ç¼–ç å™¨å¤±è´¥ï¼š{e}")
                    # ä½¿ç”¨é»˜è®¤å€¼0å¡«å……ï¼ˆ<UNK>ï¼‰
                    df[col] = 0
            else:
                print(f"è­¦å‘Šï¼šç¼–ç å­—å…¸ä¸­ä¸å­˜åœ¨åˆ— '{col}'ï¼Œä½¿ç”¨é»˜è®¤å€¼0ï¼ˆ<UNK>ï¼‰")
                df[col] = 0

    @classmethod
    def _train_new_encoders(cls, df, classify_type):
        """è®­ç»ƒæ–°çš„ç¼–ç å™¨ï¼Œä¸ºæ¯ä¸ªåˆ†ç±»åˆ—æ·»åŠ <UNK>æ ‡è®°ï¼ˆç¼–ç ä¸º0ï¼‰"""
        label_encoding_dict = {}
        for col in classify_type:
            try:
                # è·å–è¯¥åˆ—çš„å”¯ä¸€å€¼ï¼Œè¿‡æ»¤æ‰ç©ºå€¼
                unique_values = df[col].astype(str).unique()

                # æ·»åŠ <UNK>æ ‡è®°åˆ°ç±»åˆ«åˆ—è¡¨ä¸­ï¼Œå¹¶å°†å…¶æ”¾åœ¨ç¬¬ä¸€ä½ï¼ˆç¼–ç ä¸º0ï¼‰
                all_classes = ['<UNK>'] + list(unique_values)

                le = preprocessing.LabelEncoder()
                le.fit(all_classes)

                # å¯¹æ•°æ®è¿›è¡Œç¼–ç ï¼Œå°†æœªçŸ¥å€¼æ˜ å°„ä¸º<UNK>ï¼ˆç¼–ç ä¸º0ï¼‰
                # ç”±äºè®­ç»ƒæ•°æ®ä¸­çš„å€¼éƒ½æ˜¯å·²çŸ¥å€¼ï¼Œæ‰€ä»¥ç›´æ¥ç¼–ç å³å¯
                df[col] = le.transform(df[col].astype(str))

                label_encoding_dict[col] = le.classes_
                print(f"åˆ— '{col}' ç¼–ç å®Œæˆï¼Œç±»åˆ«æ•°é‡ï¼š{len(le.classes_)}ï¼ˆåŒ…å«<UNK>æ ‡è®°ï¼‰")
            except Exception as e:
                print(f"è®­ç»ƒåˆ— '{col}' çš„ç¼–ç å™¨å¤±è´¥ï¼š{e}")
                # ä½¿ç”¨ç®€å•æ˜ å°„ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆï¼ŒåŒæ ·æ·»åŠ <UNK>æ ‡è®°
                unique_values = df[col].astype(str).unique()
                all_classes = ['<UNK>'] + list(unique_values)
                value_to_id = {cls: idx for idx, cls in enumerate(all_classes)}

                # å¯¹æ•°æ®è¿›è¡Œç¼–ç ï¼Œå·²çŸ¥å€¼ä»1å¼€å§‹ï¼Œ<UNK>ä¸º0
                df[col] = df[col].astype(str).map(value_to_id)
                label_encoding_dict[col] = all_classes
                print(f"åˆ— '{col}' ä½¿ç”¨ç®€å•æ˜ å°„ï¼Œç±»åˆ«æ•°é‡ï¼š{len(all_classes)}ï¼ˆåŒ…å«<UNK>æ ‡è®°ï¼‰")

        return label_encoding_dict
 
    def _analyze_numeric_columns(self, df, pc, threshold=100):
        """
        åˆ†ææ•°å€¼åˆ—ï¼Œæ‰¾å‡ºæœ€å¤§å€¼å°äºé˜ˆå€¼çš„åˆ—

        Args:
            df (DataFrame): æ•°æ®æ¡†
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            threshold (int): é˜ˆå€¼ï¼Œé»˜è®¤ä¸º100

        Returns:
            list: å°äºé˜ˆå€¼çš„åˆ—ååˆ—è¡¨
        """
        num_small = DataDeal.columns_by_max_value(df, condition='less', threshold=threshold)
        pc.lg(f"num_small num:{len(num_small)}")
        if len(num_small) > 0:
            DataDeal.num_describe(df[num_small], pc)
            return num_small
        else:
            return []
 
    @classmethod
    def _setup_param_config(cls, pc:ParamConfig, str_identity, col_types, num_small, alg_type,
                           model_ai_dir, model_num, file_num, is_train,
                           label_name, drop_columns, date_type=None,classify_type2 = [[]],bool_type = [] ):
        """
        è®¾ç½®å‚æ•°é…ç½®å¯¹è±¡çš„é€šç”¨æ–¹æ³•

        Args:
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            str_identity (str): æ ‡è¯†åˆ—
            col_types (dict): åˆ—ç±»å‹å­—å…¸
            num_small (list): å°æ•°å€¼åˆ—åˆ—è¡¨
            alg_type (str): ç®—æ³•ç±»å‹
            model_ai_dir (str): æ¨¡å‹ä¿å­˜ç›®å½•
            model_num (int): æ¨¡å‹ç¼–å·
            file_num (int): æ–‡ä»¶ç¼–å·
            is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒ
            label_name (str): æ ‡ç­¾åˆ—å
            drop_columns (list): è¦åˆ é™¤çš„åˆ—
            date_type (list, optional): æ—¥æœŸç±»å‹åˆ—åˆ—è¡¨
        """
        # DataDealDL.data_dealéœ€è¦çš„12ä¸ªå‚æ•°
        pc.col_type.identity       = str_identity
        pc.col_type.num_type       = col_types["num_type"]
        pc.col_type.num_small      = num_small
        pc.col_type.classify_type  = col_types["classify_type"]
        pc.col_type.classify_type2 = classify_type2  #ä¸€ç»„ç±»åˆ«ä½¿ç”¨åŒä¸€ä¸ªå­—å…¸
        pc.col_type.date_type      = date_type if date_type is not None else []
        pc.col_type.bool_type      = bool_type
        pc.alg_type                = alg_type
        pc.model_save_dir          = model_ai_dir
        pc.model_num               = model_num
        pc.file_num                = file_num   #ç¬¬å‡ ä¸ªæ–‡ä»¶,é»˜è®¤1
        pc.is_train                = is_train

        #å…¶ä»–å‚æ•°
        pc.label_name              = label_name
        pc.drop_cols               = drop_columns

    @classmethod
    def _log_data_info(cls, pc:ParamConfig, num_small):
        """
        è®°å½•æ•°æ®ä¿¡æ¯çš„é€šç”¨æ–¹æ³•

        Args:
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            num_small (list): å°æ•°å€¼åˆ—åˆ—è¡¨
        """
        pc.lg(pc.col_type.num_type[:3])
        pc.lg(f"num_small num:{len(num_small)},num type num:{len(pc.col_type.num_type)}")
        pc.lg(pc.col_type.classify_type[:3])
        pc.lg(f"is_merge_identity:{pc.is_merge_identity}")

 
 
 
    @classmethod
    def norm_min_max_scaler(cls, X, num_type=[], 
                            model_path=f"min_max_scaler.pkl", 
                            is_train=True):
        if is_train:
            df = DataDeal.min_max_scaler(X, 
                            num_type=num_type, 
                            model_path=model_path, 
                            reuse=True, 
                            col_sort=True, 
                            force_rewrite=True)
        else:
            df = DataDeal.min_max_scaler(X, 
                            num_type=num_type, 
                            model_path=model_path, 
                            reuse=True, 
                            col_sort=True)
        return df  
    
    def _process_data_with_deal_dl(self, df, pc:ParamConfig):
        """
        ä½¿ç”¨DataDealDLå¤„ç†æ•°æ®çš„é€šç”¨æ–¹æ³•

        Args:
            df (DataFrame): æ•°æ®æ¡†
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡

        Returns:
            DataFrame: å¤„ç†åçš„æ•°æ®æ¡†
        """
        df_processed = DataDealDL.data_deal(df, pc)
        return df_processed
    
    @classmethod
    def getXy(cls,
            data_path, heads, str_identity,
            alg_type, model_ai_dir, model_num, file_num,
            is_train, label_name, pc:ParamConfig, drop_columns,
            is_categorical_func_type=None, date_type=None, 
            sep='~',classify_type2 = [[]],bool_type = []):
        df = cls.read_csv(data_path, sep=sep, heads=heads)
        
        pass 
    
    @classmethod
    def processing(cls, 
                df, str_identity,
                alg_type, model_ai_dir, model_num, file_num,
                is_train, label_name, pc:ParamConfig, drop_columns,
                date_type, col_types):
        """
        é€šç”¨æ•°æ®å¤„ç†ç®¡é“

        Args:
            data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
            model_title (str): æ¨¡å‹æ ‡é¢˜
            str_identity (str): æ ‡è¯†åˆ—
            alg_type (str): ç®—æ³•ç±»å‹
            model_ai_dir (str): æ¨¡å‹ä¿å­˜ç›®å½•
            model_num (int): æ¨¡å‹ç¼–å·
            file_num (int): æ–‡ä»¶ç¼–å·
            is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒ
            label_name (str): æ ‡ç­¾åˆ—å
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            drop_columns (list): è¦åˆ é™¤çš„åˆ—
            is_categorical_func_type (str): åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°ç±»å‹ï¼Œ'general'æˆ–'tra'
            date_type (list, optional): æ—¥æœŸç±»å‹åˆ—åˆ—è¡¨
            sep (str): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º'~'

        Returns:
            tuple: (å¤„ç†åçš„DataFrame, æ ‡ç­¾åˆ—, å‚æ•°é…ç½®å¯¹è±¡)
        """
  
        # 4. åˆ†ææ•°å€¼åˆ—
        num_small = cls._analyze_numeric_columns(df, pc)

        # 5. è®¾ç½®å‚æ•°é…ç½®
        cls._setup_param_config(pc, str_identity, col_types, num_small, alg_type,
                               model_ai_dir, model_num, file_num, is_train,
                               label_name, drop_columns, date_type)

        # 6. è®°å½•æ•°æ®ä¿¡æ¯
        cls._log_data_info(pc, num_small)

        # 7. å¤„ç†æ•°æ®
        df_processed = cls._process_data_with_deal_dl(df, pc)

        return df_processed, pc
    
    @classmethod
    def deal(cls, 
                data_path, model_title, str_identity,
                alg_type, model_ai_dir, model_num, file_num,
                is_train, label_name, pc:ParamConfig, drop_columns,
                is_categorical_func_type=None, date_type=None, 
                sep='~',classify_type2 = [[]],bool_type = []):
        pass 
        
   

class Data2Feature(Data2FeatureBase):
    
    
    def __init__(self):
        """
        ä¸»è¦é€»è¾‘
        1. æ•°æ®è¯»å–  read_csv
        2. æ•°æ®ç±»å‹è½¬æ¢ data_type_change
        3. æ•°æ®è§‚å¯Ÿ  show_*
        4. æ•°å­—åŒ–ï¼Œç±»åˆ«è½¬ç´¢å¼• tonum_*
        5. å½’ä¸€åŒ–  norm_*
        
        
        """
        super().__init__()

        pass 
        


    @classmethod
    def read_csv(cls,data_path, sep=',', usecols=None, heads=None, heads_sep=None):
        """
        è¯»å–CSVæ–‡ä»¶å¹¶è¿”å›DataFrame

        Args:
            data_path (str): CSVæ–‡ä»¶è·¯å¾„
            sep (str, optional): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º','
            usecols (list, optional): è¦ä½¿ç”¨çš„åˆ—åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰åˆ—
            heads (str, optional): åˆ—åå­—ç¬¦ä¸²ï¼Œç”¨åˆ†éš”ç¬¦åˆ†éš”å¤šä¸ªåˆ—å
            heads_sep (str, optional): åˆ—åçš„åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä½¿ç”¨sepçš„å€¼

        Returns:
            pd.DataFrame: è¯»å–çš„æ•°æ®æ¡†
        """
        if heads_sep is None:
            heads_sep = sep
        if heads is not None:
            usecols = cls._get_usecols(heads,sep=heads_sep)
        df = pd.read_csv(data_path, sep=sep, usecols=usecols)
        return df 
    
        
    @classmethod
    def data_agg(cls, df,identifys=[['From','time8'],['To','time8']],
                num_type=['Amount'],
                classify_type=['Payment Format', 'Currency'],
                stat_lable=['count','sum','mean','std','min','max','median','q25','q75','skew','kurtosis','cv','iqr','range','se']):
        """
        é“¶è¡Œäº¤æ˜“æµæ°´æ•°æ®èšåˆç»Ÿè®¡æ–¹æ³•

        å‚æ•°:
        df: è¾“å…¥çš„äº¤æ˜“æ•°æ®DataFrame
        identifys: åˆ†ç»„æ ‡è¯†åˆ—åˆ—è¡¨ï¼Œé»˜è®¤ä¸º[['From','time8'],['To','time8']]
        num_type: æ•°å€¼ç±»å‹åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸º['Amount']
        classify_type: åˆ†ç±»ç±»å‹åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸º['Payment Format', 'Currency']
        stat_lable: éœ€è¦è®¡ç®—çš„ç»Ÿè®¡æŒ‡æ ‡åˆ—è¡¨ï¼Œæ”¯æŒçš„æŒ‡æ ‡åŒ…æ‹¬:
                - count: è®¡æ•°
                - sum: æ±‚å’Œ
                - mean: å‡å€¼
                - std: æ ‡å‡†å·®
                - min: æœ€å°å€¼
                - max: æœ€å¤§å€¼
                - median: ä¸­ä½æ•°
                - q25: 25%åˆ†ä½æ•°
                - q75: 75%åˆ†ä½æ•°
                - skew: ååº¦
                - kurtosis: å³°åº¦
                - cv: å˜å¼‚ç³»æ•°
                - iqr: å››åˆ†ä½è·
                - range: æå·®
                - se: æ ‡å‡†è¯¯å·®

        èƒŒæ™¯:
        1. é“¶è¡Œäº¤æ˜“æµæ°´æ•°æ®é›†ï¼ŒåŒ…å«From,To,time8,time14,Amount,Payment Format,Currency
        2. Fromä¸ºä»˜æ¬¾è´¦æˆ·ï¼ŒToä¸ºæ”¶æ¬¾è´¦æˆ·
        3. time8ä¸º8ä½æŒ‰å¤©çš„æ—¶é—´ï¼Œ['From','time8']æ„å‘³ç€æŒ‰å¤©å¯¹ä»˜æ¬¾è´¦æˆ·åˆ†ç±»
        4. ['To','time8']æ„å‘³ç€å°†æ¥ä¼šæŒ‰å¤©å¯¹æ”¶æ¬¾è´¦æˆ·åˆ†ç±»

        ä¸»è¦é€»è¾‘ï¼š
        1. å¯¹äºæ¯ä¸ªidentifys[i]ï¼š
        - å½¢æˆä¸´æ—¶df_tmp = num_type + classify_type + identifys[i]çš„åˆ—ç»„åˆ
        - æŒ‰identifys[i]åˆ†ç»„èšåˆæ•°æ®
        - æ ¹æ®stat_lableå‚æ•°ç”Ÿæˆå¯¹åº”çš„ç»Ÿè®¡ç»“æœ
        2. å°†æ‰€æœ‰df_tmpåˆå¹¶æˆæ–°çš„DataFrameè¿”å›
        3. åªè®¡ç®—stat_lableä¸­æŒ‡å®šçš„ç»Ÿè®¡æŒ‡æ ‡ï¼Œæé«˜è®¡ç®—æ•ˆç‡

        ç¤ºä¾‹:
        # åªè®¡ç®—åŸºç¡€ç»Ÿè®¡æŒ‡æ ‡
        df_basic = data_agg(df, stat_lable=['count','sum','mean','std'])

        # è®¡ç®—å®Œæ•´çš„æ³¢åŠ¨æ€§æŒ‡æ ‡
        df_full = data_agg(df, stat_lable=['count','sum','mean','std','q25','q75','skew','kurtosis','cv','iqr'])
        """
        import pandas as pd
        import numpy as np

        all_results = []

        # å¯¹identifysä¸­çš„æ¯ä¸ªåˆ†ç»„é”®è¿›è¡Œå¤„ç†
        # for i, group_cols in enumerate(identifys):
        #     # print(f"å¤„ç†åˆ†ç»„é”® {i+1}/{len(identifys)}: {group_cols}")

        #     # æ„å»ºä¸´æ—¶df_tmpçš„åˆ—ï¼šnum_type + classify_type + group_cols
        #     tmp_cols = []
        #     tmp_cols.extend(group_cols)  # æ·»åŠ åˆ†ç»„é”®åˆ—

        #     # æ£€æŸ¥å¹¶æ·»åŠ æ•°å€¼ç±»å‹åˆ—
        #     available_num_cols = [col for col in num_type if col in df.columns]
        #     tmp_cols.extend(available_num_cols)

        #     # æ£€æŸ¥å¹¶æ·»åŠ åˆ†ç±»ç±»å‹åˆ—
        #     available_cat_cols = [col for col in classify_type if col in df.columns]
        #     tmp_cols.extend(available_cat_cols)

        #     # åˆ›å»ºä¸´æ—¶DataFrame
        #     df_tmp = df[tmp_cols].copy()
        #     # print(f"ä¸´æ—¶DataFrameåˆ—: {df_tmp.columns.tolist()}")
        #     # print(f"ä¸´æ—¶DataFrameå½¢çŠ¶: {df_tmp.shape}")

        #     # æŒ‰å½“å‰åˆ†ç»„é”®è¿›è¡Œèšåˆ
        #     grouped = df_tmp.groupby(group_cols)

        #     # ä¸ºå½“å‰åˆ†ç»„åˆ›å»ºç»Ÿè®¡ç»“æœ
        #     group_results = []

            # print(f"å¼€å§‹æ•°æ®èšåˆï¼Œç»Ÿè®¡æŒ‡æ ‡: {stat_lable}")
        # print(f"åˆ†ç»„æ ‡è¯†: {identifys}, æ•°å€¼åˆ—: {num_type}, åˆ†ç±»åˆ—: {classify_type}")

        # å¯¹identifysä¸­çš„æ¯ä¸ªåˆ†ç»„é”®è¿›è¡Œå¤„ç†
        for i, group_cols in enumerate(identifys):
            # print(f"å¤„ç†åˆ†ç»„é”® {i+1}/{len(identifys)}: {group_cols}")

            # æ„å»ºä¸´æ—¶df_tmpçš„åˆ—ï¼šnum_type + classify_type + group_cols
            tmp_cols = []
            tmp_cols.extend(group_cols)  # æ·»åŠ åˆ†ç»„é”®åˆ—

            # æ£€æŸ¥å¹¶æ·»åŠ æ•°å€¼ç±»å‹åˆ—
            available_num_cols = [col for col in num_type if col in df.columns]
            tmp_cols.extend(available_num_cols)

            # æ£€æŸ¥å¹¶æ·»åŠ åˆ†ç±»ç±»å‹åˆ—
            available_cat_cols = [col for col in classify_type if col in df.columns]
            tmp_cols.extend(available_cat_cols)

            # åˆ›å»ºä¸´æ—¶DataFrame
            df_tmp = df[tmp_cols].copy()
            # print(f"ä¸´æ—¶DataFrameåˆ—: {df_tmp.columns.tolist()}")
            # print(f"ä¸´æ—¶DataFrameå½¢çŠ¶: {df_tmp.shape}")

            # æŒ‰å½“å‰åˆ†ç»„é”®è¿›è¡Œèšåˆ
            grouped = df_tmp.groupby(group_cols)

            # ä¸ºå½“å‰åˆ†ç»„åˆ›å»ºç»Ÿè®¡ç»“æœ
            group_results = []

            # 1. å¯¹æ•°å€¼åˆ—è¿›è¡Œç»Ÿè®¡
            for num_col in available_num_cols:
                # print(f"  å¯¹æ•°å€¼åˆ— {num_col} è¿›è¡Œç»Ÿè®¡...")

                # æ ¹æ®stat_lableå‚æ•°åŠ¨æ€ç”Ÿæˆç»Ÿè®¡åˆ—å
                stat_columns = []
                for stat in stat_lable:
                    stat_columns.append(f'{num_col}_{stat}')

                # print(f"  å°†è®¡ç®—ç»Ÿè®¡åˆ—: {stat_columns}")

                # è·å–æ‰€æœ‰å”¯ä¸€çš„åˆ†ç»„ç»„åˆ
                all_groups = df_tmp[group_cols].drop_duplicates()
                # print(f"  å‘ç° {len(all_groups)} ä¸ªå”¯ä¸€åˆ†ç»„")

                # åˆ›å»ºç»“æœDataFrameï¼ŒåŒ…å«æ‰€æœ‰åˆ†ç»„å’Œç»Ÿè®¡åˆ—
                num_result = all_groups.copy()
                for col in stat_columns:
                    num_result[col] = 0.0  # åˆå§‹åŒ–æ‰€æœ‰ç»Ÿè®¡åˆ—ä¸º0

                # åˆ›å»ºå­—å…¸æ¥å¿«é€ŸæŸ¥æ‰¾åˆ†ç»„å¯¹åº”çš„è¡Œç´¢å¼•
                group_to_index = {}
                for idx, row in all_groups.iterrows():
                    key = tuple(row[group_cols])
                    group_to_index[key] = idx

                # è®¡ç®—æ¯ä¸ªåˆ†ç»„çš„ç»Ÿè®¡æŒ‡æ ‡
                for group_key, group_data in grouped:
                    # print(f"    å¤„ç†åˆ†ç»„: {group_key}, æ•°æ®é‡: {len(group_data)}")

                    try:
                        values = group_data[num_col].dropna()  # ç§»é™¤NaNå€¼

                        if len(values) == 0:
                            # print(f"      è­¦å‘Š: åˆ†ç»„ {group_key} æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                            continue

                        # åŸºç¡€ç»Ÿè®¡
                        count = len(values)
                        sum_val = values.sum()
                        mean_val = values.mean()
                        std_val = values.std(ddof=0) if count > 1 else 0.0
                        min_val = values.min()
                        max_val = values.max()
                        median_val = values.median()

                        # åˆ†ä½æ•°
                        q25_val = values.quantile(0.25)
                        q75_val = values.quantile(0.75)

                        # è¡ç”Ÿç»Ÿè®¡
                        skew_val = values.skew() if count > 2 else 0.0
                        kurt_val = values.kurtosis() if count > 3 else 0.0
                        cv_val = std_val / mean_val if mean_val != 0 else 0.0
                        iqr_val = q75_val - q25_val
                        range_val = max_val - min_val
                        se_val = std_val / np.sqrt(count) if count > 0 else 0.0

                        # è·å–è¯¥åˆ†ç»„åœ¨ç»“æœDataFrameä¸­çš„è¡Œç´¢å¼•
                        if group_key in group_to_index:
                            row_idx = group_to_index[group_key]

                            # æ›´æ–°ç»Ÿè®¡å€¼
                            num_result.at[row_idx, f'{num_col}_count'] = count
                            num_result.at[row_idx, f'{num_col}_sum'] = sum_val
                            num_result.at[row_idx, f'{num_col}_mean'] = mean_val
                            num_result.at[row_idx, f'{num_col}_std'] = std_val
                            num_result.at[row_idx, f'{num_col}_min'] = min_val
                            num_result.at[row_idx, f'{num_col}_max'] = max_val
                            num_result.at[row_idx, f'{num_col}_median'] = median_val
                            num_result.at[row_idx, f'{num_col}_q25'] = q25_val
                            num_result.at[row_idx, f'{num_col}_q75'] = q75_val
                            num_result.at[row_idx, f'{num_col}_skew'] = skew_val
                            num_result.at[row_idx, f'{num_col}_kurtosis'] = kurt_val
                            num_result.at[row_idx, f'{num_col}_cv'] = cv_val
                            num_result.at[row_idx, f'{num_col}_iqr'] = iqr_val
                            num_result.at[row_idx, f'{num_col}_range'] = range_val
                            num_result.at[row_idx, f'{num_col}_se'] = se_val

                        # print(f"      å®Œæˆ {count} ä¸ªæ•°æ®ç‚¹çš„ç»Ÿè®¡")

                    except Exception as e:
                        print(f"      è®¡ç®—åˆ†ç»„ {group_key} çš„ç»Ÿè®¡æ—¶å‡ºé”™: {e}")
                        continue

                # ç¡®ä¿æ‰€æœ‰ç»Ÿè®¡åˆ—éƒ½å­˜åœ¨ä¸”ä¸ºæ•°å€¼ç±»å‹
                for col in stat_columns:
                    if col not in num_result.columns:
                        num_result[col] = 0.0
                    else:
                        num_result[col] = pd.to_numeric(num_result[col], errors='coerce').fillna(0.0)

                # print(f"  æ•°å€¼åˆ— {num_col} ç»Ÿè®¡å®Œæˆï¼Œç»“æœå½¢çŠ¶: {num_result.shape}")
                group_results.append(num_result)

            # 2. å¯¹åˆ†ç±»åˆ—è¿›è¡Œäº¤å‰ç»Ÿè®¡
            for cat_col in available_cat_cols:
                # print(f"  å¯¹åˆ†ç±»åˆ— {cat_col} è¿›è¡Œäº¤å‰ç»Ÿè®¡...")

                # è·å–å”¯ä¸€å€¼ï¼ˆè¿‡æ»¤æ‰NaNï¼‰
                unique_values = df_tmp[cat_col].dropna().unique()

                for num_col in available_num_cols:
                    # print(f"    å¤„ç†åˆ†ç±»åˆ— {cat_col} ä¸æ•°å€¼åˆ— {num_col} çš„äº¤å‰ç»Ÿè®¡")

                    # é¢„å®šä¹‰æ‰€æœ‰åˆ†ç±»ç»Ÿè®¡åˆ—
                    cat_stat_columns = []
                    for cat_value in unique_values:
                        cat_stat_columns.extend([
                            f'{cat_col}_{cat_value}_{num_col}_count',
                            f'{cat_col}_{cat_value}_{num_col}_sum',
                            f'{cat_col}_{cat_value}_{num_col}_mean',
                            f'{cat_col}_{cat_value}_{num_col}_std'
                        ])

                    # è·å–æ‰€æœ‰å”¯ä¸€çš„åˆ†ç»„ç»„åˆ
                    all_groups = df_tmp[group_cols].drop_duplicates()

                    # åˆ›å»ºåˆ†ç±»ç»Ÿè®¡ç»“æœDataFrame
                    cat_result = all_groups.copy()
                    for col in cat_stat_columns:
                        cat_result[col] = 0.0  # åˆå§‹åŒ–æ‰€æœ‰åˆ†ç±»ç»Ÿè®¡åˆ—ä¸º0

                    # åˆ›å»ºå­—å…¸æ¥å¿«é€ŸæŸ¥æ‰¾åˆ†ç»„å¯¹åº”çš„è¡Œç´¢å¼•
                    group_to_index = {}
                    for idx, row in all_groups.iterrows():
                        key = tuple(row[group_cols])
                        group_to_index[key] = idx

                    # è®¡ç®—æ¯ä¸ªåˆ†ç±»å€¼çš„ç»Ÿè®¡
                    for cat_value in unique_values:
                        filtered_data = df_tmp[df_tmp[cat_col] == cat_value]
                        if len(filtered_data) == 0:
                            continue

                        # print(f"      å¤„ç†åˆ†ç±»å€¼ {cat_value}, æ•°æ®é‡: {len(filtered_data)}")

                        # æŒ‰åˆ†ç»„é”®å’Œåˆ†ç±»å€¼è¿›è¡Œåˆ†ç»„
                        cat_grouped = filtered_data.groupby(group_cols)

                        for group_key, group_data in cat_grouped:
                            try:
                                values = group_data[num_col].dropna()
                                if len(values) == 0:
                                    continue

                                count = len(values)
                                sum_val = values.sum()
                                mean_val = values.mean()
                                std_val = values.std(ddof=0) if count > 1 else 0.0

                                # æ›´æ–°å¯¹åº”çš„ç»Ÿè®¡å€¼
                                if group_key in group_to_index:
                                    row_idx = group_to_index[group_key]
                                    cat_result.at[row_idx, f'{cat_col}_{cat_value}_{num_col}_count'] = count
                                    cat_result.at[row_idx, f'{cat_col}_{cat_value}_{num_col}_sum'] = sum_val
                                    cat_result.at[row_idx, f'{cat_col}_{cat_value}_{num_col}_mean'] = mean_val
                                    cat_result.at[row_idx, f'{cat_col}_{cat_value}_{num_col}_std'] = std_val

                            except Exception as e:
                                print(f"        è®¡ç®—åˆ†ç»„ {group_key} åˆ†ç±»ç»Ÿè®¡æ—¶å‡ºé”™: {e}")
                                continue

                    # ç¡®ä¿æ‰€æœ‰åˆ†ç±»ç»Ÿè®¡åˆ—éƒ½å­˜åœ¨ä¸”ä¸ºæ•°å€¼ç±»å‹
                    for col in cat_stat_columns:
                        if col not in cat_result.columns:
                            cat_result[col] = 0.0
                        else:
                            cat_result[col] = pd.to_numeric(cat_result[col], errors='coerce').fillna(0.0)

                    # print(f"      åˆ†ç±»åˆ— {cat_col} ä¸æ•°å€¼åˆ— {num_col} äº¤å‰ç»Ÿè®¡å®Œæˆï¼Œç»“æœå½¢çŠ¶: {cat_result.shape}")
                    group_results.append(cat_result)

            # 3. åˆå¹¶å½“å‰åˆ†ç»„çš„æ‰€æœ‰ç»Ÿè®¡ç»“æœ
            if group_results:
                # print(f"  åˆå¹¶ {len(group_results)} ä¸ªç»Ÿè®¡ç»“æœ...")

                # è·å–æ‰€æœ‰å”¯ä¸€çš„åˆ†ç»„ç»„åˆï¼ˆç¡®ä¿åŒ…å«æ‰€æœ‰å¯èƒ½çš„åˆ†ç»„ï¼‰
                all_groups = df_tmp[group_cols].drop_duplicates()

                # åˆ›å»ºåŒ…å«æ‰€æœ‰åˆ†ç»„çš„åŸºå‡†DataFrame
                group_final = all_groups.copy()

                # åˆå¹¶æ‰€æœ‰ç»Ÿè®¡ç»“æœåˆ°åŸºå‡†DataFrame
                for i, result_df in enumerate(group_results):
                    # print(f"    åˆå¹¶ç¬¬ {i+1} ä¸ªç»“æœï¼Œå½¢çŠ¶: {result_df.shape}")

                    # ä½¿ç”¨å¤–è¿æ¥ç¡®ä¿æ‰€æœ‰åˆ†ç»„éƒ½è¢«ä¿ç•™
                    group_final = group_final.merge(
                        result_df,
                        on=group_cols,
                        how='outer'
                    )

                # æ·»åŠ åˆ†ç»„æ ‡è¯†
                group_final['group_key'] = '_'.join(group_cols)

                # æœ€ç»ˆå¤„ç†æ‰€æœ‰NaNå€¼ï¼šå°†ç»Ÿè®¡åˆ—çš„NaNè½¬æ¢ä¸º0
                for col in group_final.columns:
                    if col not in group_cols + ['group_key']:
                        group_final[col] = pd.to_numeric(group_final[col], errors='coerce').fillna(0.0)

                # print(f"  åˆ†ç»„ {group_cols} åˆå¹¶å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: {group_final.shape}")
                # print(f"  NaNå€¼æ•°é‡: {group_final.isnull().sum().sum()}")
                all_results.append(group_final)
        

        # 4. åˆå¹¶æ‰€æœ‰åˆ†ç»„çš„æœ€ç»ˆç»“æœ
        if all_results:
            final_result = pd.concat(all_results, ignore_index=True)

            # æœ€ç»ˆNaNå€¼å¤„ç†ï¼šç¡®ä¿æ‰€æœ‰ç»Ÿè®¡åˆ—éƒ½æ²¡æœ‰NaN
            stat_cols = [col for col in final_result.columns if col != 'group_key']
            for col in stat_cols:
                if final_result[col].dtype in ['float64', 'int64']:
                    final_result[col] = final_result[col].fillna(0)

            # å°†group_keyåˆ—ç§»åŠ¨åˆ°ç¬¬ä¸€åˆ—ä½ç½®
            if 'group_key' in final_result.columns:
                cols = ['group_key'] + [col for col in final_result.columns if col != 'group_key']
                final_result = final_result[cols]

            return final_result
        else:
            return pd.DataFrame()


    @classmethod
    def cols_more2one(cls, df, cols=['From','To'], new_col_name='key'):
        """å¤šåˆ—äº’æ–¥åˆå¹¶ä¸ºä¸€åˆ—
        colsä¸­çš„åˆ—æ˜¯äº’æ–¥çš„ï¼ŒåŒä¸€è¡Œåªèƒ½æœ‰ä¸€ä¸ªåˆ—æœ‰å€¼ï¼Œå…¶ä½™åˆ—ä¸ºNaN,ç°åœ¨å°†è¿™äº›åˆ—åˆå¹¶ä¸ºä¸€ä¸ªåˆ—,æ–°åˆ—åä¸ºnew_col_name
        """

        # éªŒè¯è¾“å…¥åˆ—æ˜¯å¦å­˜åœ¨
        missing_cols = [col for col in cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"ä»¥ä¸‹åˆ—åœ¨DataFrameä¸­ä¸å­˜åœ¨: {missing_cols}")

        # åˆ›å»ºæ–°åˆ—ï¼Œä½¿ç”¨bfillæˆ–ffillæ¥å¡«å……éNaNå€¼
        # æ–¹æ³•1: ä½¿ç”¨combine_firstæ–¹æ³•
        result_df = df.copy()

        # åˆå§‹åŒ–æ–°åˆ—ä¸ºNaN
        result_df[new_col_name] = np.nan

        # æŒ‰é¡ºåºåˆå¹¶åˆ—ï¼Œåé¢çš„åˆ—ä¼šå¡«å……å‰é¢åˆ—çš„NaNä½ç½®
        # ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•é¿å…StringDtypeé—®é¢˜
        for col in cols:
            # æ‰¾å‡ºæ–°åˆ—ä¸­ä¸ºNaNä½†åœ¨å½“å‰åˆ—ä¸­ä¸ä¸ºNaNçš„ä½ç½®
            mask = result_df[new_col_name].isna() & result_df[col].notna()
            # åœ¨è¿™äº›ä½ç½®ä¸Šç”¨å½“å‰åˆ—çš„å€¼å¡«å……
            result_df.loc[mask, new_col_name] = result_df.loc[mask, col]

        # éªŒè¯åˆå¹¶ç»“æœï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨å†²çªï¼ˆå³åŸæ•°æ®ä¸­åŒä¸€è¡Œæœ‰å¤šä¸ªéNaNå€¼ï¼‰
        # è®¡ç®—æ¯è¡ŒéNaNå€¼çš„æ•°é‡
        non_nan_count = df[cols].notna().sum(axis=1)
        conflicts = non_nan_count > 1

        if conflicts.any():
            print(f"è­¦å‘Š: å‘ç° {conflicts.sum()} è¡Œæ•°æ®å­˜åœ¨å†²çªï¼ˆå¤šåˆ—åŒæ—¶æœ‰å€¼ï¼‰")
            print("å†²çªè¡Œç¤ºä¾‹:")
            print(df[conflicts][cols].head())

            # å¯¹äºå†²çªè¡Œï¼Œä¼˜å…ˆä½¿ç”¨ç¬¬ä¸€ä¸ªéNaNå€¼
            for idx in df[conflicts].index:
                for col in cols:
                    if pd.notna(df.loc[idx, col]):
                        result_df.loc[idx, new_col_name] = df.loc[idx, col]
                        break

        # åˆ é™¤åŸå§‹åˆ—
        result_df = result_df.drop(columns=cols)

        # å°†æ–°åˆ—ç§»åŠ¨åˆ°ç¬¬ä¸€åˆ—ä½ç½®
        cols = [new_col_name] + [col for col in result_df.columns if col != new_col_name]
        result_df = result_df[cols]

        # print(f"æˆåŠŸå°† {len(cols)-1} åˆ—åˆå¹¶ä¸º '{new_col_name}' åˆ—")
        # print(f"åˆå¹¶åçš„éNaNå€¼æ•°é‡: {result_df[new_col_name].notna().sum()}")

        return result_df 

    @classmethod
    def data_agg_byday(cls, df,
                col_time='time8',
                interval=1,
                win_len=1,
                identifys=[['From','time8'],['To','time8']],
                num_type =['Amount'],
                classify_type=['Payment Format', 'Currency'],
                merge_del_cols=['From','To'],
                new_col_name='key',
                stat_lable=['count','sum','mean','std','min','max','median','q25','q75','skew','kurtosis','cv','iqr','range','se']):
        """
        æŒ‰å¤©æ»šåŠ¨çª—å£èšåˆäº¤æ˜“æ•°æ®

        å‚æ•°:
        df: è¾“å…¥çš„äº¤æ˜“æ•°æ®DataFrame
        col_time: æ—¶é—´åˆ—åï¼Œé»˜è®¤ä¸º'time8'
        interval: æ»šåŠ¨é—´éš”ï¼Œé»˜è®¤ä¸º1å¤©
        win_len: çª—å£é•¿åº¦ï¼Œé»˜è®¤ä¸º1å¤©
        identifys: åˆ†ç»„æ ‡è¯†åˆ—åˆ—è¡¨ï¼Œé»˜è®¤ä¸º[['From','time8'],['To','time8']]
        num_type: æ•°å€¼ç±»å‹åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸º['Amount']
        classify_type: åˆ†ç±»ç±»å‹åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸º['Payment Format', 'Currency']
        merge_del_cols: éœ€è¦åˆå¹¶çš„åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸º['From','To']
        new_col_name: åˆå¹¶åçš„æ–°åˆ—åï¼Œé»˜è®¤ä¸º'key'

        è¿”å›:
        df_final: åˆå¹¶æ‰€æœ‰çª—å£ç»“æœçš„DataFrame

        åŠŸèƒ½è¯´æ˜:
        1. ä½¿ç”¨æ»šåŠ¨çª—å£æŒ‰å¤©å¤„ç†äº¤æ˜“æ•°æ®
        2. å¯¹æ¯ä¸ªçª—å£çš„æ•°æ®è¿›è¡Œèšåˆç»Ÿè®¡ï¼ˆè°ƒç”¨data_aggæ–¹æ³•ï¼‰
        3. å°†å¤šä¸ªæ ‡è¯†åˆ—åˆå¹¶ä¸ºä¸€ä¸ªç»Ÿä¸€çš„å…³é”®åˆ—ï¼ˆè°ƒç”¨cols_more2oneæ–¹æ³•ï¼‰
        4. å°†æ‰€æœ‰çª—å£çš„ç»“æœåˆå¹¶ä¸ºä¸€ä¸ªæœ€ç»ˆDataFrameè¿”å›
        """

        # åˆ›å»ºç©ºçš„DataFrameç”¨äºå­˜å‚¨æ‰€æœ‰çª—å£çš„ç»“æœ
        df_final = pd.DataFrame()

        print(f"å¼€å§‹æŒ‰å¤©æ»šåŠ¨çª—å£èšåˆï¼Œæ—¶é—´åˆ—: {col_time}, é—´éš”: {interval}, çª—å£é•¿åº¦: {win_len}")
        # print(f"åˆ†ç»„æ ‡è¯†: {identifys}, æ•°å€¼åˆ—: {num_type}, åˆ†ç±»åˆ—: {classify_type}")

        window_count = 0

        # ä¸€æ¬¡æå–ä¸€å¤©çš„æ•°æ®ï¼Œæ»šåŠ¨çª—å£å¤„ç†
        for s, e, df_sub in DataDeal.rolling_windows(
            df=df,
            col_time=col_time,
            interval=interval,
            win_len=win_len):

            window_count += 1
            print(f'\nå¤„ç†ç¬¬ {window_count} ä¸ªçª—å£: {s} ~ {e}ï¼Œè®°å½•æ•° {len(df_sub)}')

            if len(df_sub) == 0:
                # print(f"  çª—å£ {s} ~ {e} æ²¡æœ‰æ•°æ®ï¼Œè·³è¿‡")
                continue

            # 1. å¯¹å½“å‰çª—å£æ•°æ®è¿›è¡Œèšåˆç»Ÿè®¡
            # print(f"  å¼€å§‹èšåˆç»Ÿè®¡...")
            df_agg_by_day = cls.data_agg(df_sub,
                    identifys=identifys,
                    num_type=num_type,
                    classify_type=classify_type,
                    stat_lable=stat_lable)

            # print(f"  èšåˆå®Œæˆï¼Œç»“æœå½¢çŠ¶: {df_agg_by_day.shape}")

            # 2. å°†å¤šä¸ªæ ‡è¯†åˆ—åˆå¹¶ä¸ºä¸€ä¸ªå…³é”®åˆ—
            if merge_del_cols and all(col in df_agg_by_day.columns for col in merge_del_cols):
                # print(f"  åˆå¹¶åˆ— {merge_del_cols} ä¸ºæ–°åˆ— '{new_col_name}'...")
                df_agg_by_day = cls.cols_more2one(df_agg_by_day,
                                        cols=merge_del_cols,
                                        new_col_name=new_col_name)
                # print(f"  åˆ—åˆå¹¶å®Œæˆï¼Œç»“æœå½¢çŠ¶: {df_agg_by_day.shape}")
            else:
                print(f"  è·³è¿‡åˆ—åˆå¹¶ï¼Œæ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨: {merge_del_cols}")
                print(f"  DataFrameåˆ—: {df_agg_by_day.columns.tolist()}")

            # 3. æ·»åŠ çª—å£æ—¶é—´ä¿¡æ¯
            df_agg_by_day['window_start'] = s
            df_agg_by_day['window_end'] = e
            df_agg_by_day['window_seq'] = window_count

            # 4. å°†å½“å‰çª—å£ç»“æœåˆå¹¶åˆ°æœ€ç»ˆç»“æœä¸­
            if df_final.empty:
                df_final = df_agg_by_day.copy()
                # print(f"  åˆå§‹åŒ–æœ€ç»ˆç»“æœDataFrameï¼Œå½¢çŠ¶: {df_final.shape}")
            else:
                # ä½¿ç”¨concatåˆå¹¶ï¼Œä¿æŒåˆ—å¯¹é½
                df_final = pd.concat([df_final, df_agg_by_day], ignore_index=True)
                # print(f"  åˆå¹¶å½“å‰çª—å£ç»“æœï¼Œæœ€ç»ˆå½¢çŠ¶: {df_final.shape}")

            # å¯é€‰ï¼šè®°å½•è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦è°ƒè¯•ï¼‰
            # pc.lg(f"çª—å£ {s} ~ {e} èšåˆå®Œæˆï¼Œç»“æœå½¢çŠ¶: {df_agg_by_day.shape}")
            # pc.lg(f"çª—å£ {s} ~ {e} èšåˆç»“æœç¤ºä¾‹:\n{df_agg_by_day[:3]}")

        print(f"\næ‰€æœ‰çª—å£å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {window_count} ä¸ªçª—å£")
        print(f"æœ€ç»ˆç»“æœå½¢çŠ¶: {df_final.shape}")

        if not df_final.empty:
            print(f"æœ€ç»ˆç»“æœåˆ—: {df_final.columns.tolist()}")
            print(f"çª—å£åºåˆ—èŒƒå›´: {df_final['window_seq'].min()} ~ {df_final['window_seq'].max()}")

            # å°†çª—å£ä¿¡æ¯åˆ—ç§»åˆ°æœ€å
            info_cols = ['window_start', 'window_end', 'window_seq']
            other_cols = [col for col in df_final.columns if col not in info_cols]
            df_final = df_final[other_cols]

            # å°†df_finalä¸­çš„NaNå€¼æ›¿æ¢ä¸º0
            print(f"å¼€å§‹å¤„ç†df_finalä¸­çš„NaNå€¼...")
            nan_before = df_final.isnull().sum().sum()
            print(f"å¤„ç†å‰NaNå€¼æ€»æ•°: {nan_before}")

            if nan_before > 0:
                # æ˜¾ç¤ºæ¯åˆ—çš„NaNå€¼æ•°é‡
                nan_by_col = df_final.isnull().sum()
                cols_with_nan = nan_by_col[nan_by_col > 0]
                if len(cols_with_nan) > 0:
                    # print("å„åˆ—NaNå€¼æ•°é‡:")
                    for col, count in cols_with_nan.items():
                        print(f"  {col}: {count}")

                # æ›¿æ¢NaNå€¼ä¸º0
                df_final = df_final.fillna(0)

                nan_after = df_final.isnull().sum().sum()
                print(f"å¤„ç†åNaNå€¼æ€»æ•°: {nan_after}")
                print("âœ“ æ‰€æœ‰NaNå€¼å·²æ›¿æ¢ä¸º0")
            else:
                print("âœ“ df_finalä¸­æ²¡æœ‰NaNå€¼")

        return df_final



        
        
        @classmethod
        def data_type_change(cls,df,num_type=None,classify_type=None,date_type=None):
            """
            è½¬æ¢DataFrameä¸­æŒ‡å®šåˆ—çš„æ•°æ®ç±»å‹ï¼›é€šå¸¸æ˜¯æŒ‡å®šnum_typeã€date_type,å°†å‰©ä¸‹çš„åˆ—è½¬æ¢æˆclassify_type

            è¯¥æ–¹æ³•ç”¨äºå°†DataFrameä¸­çš„åˆ—è½¬æ¢ä¸ºæŒ‡å®šçš„æ•°æ®ç±»å‹ï¼Œæ”¯æŒæ•°å€¼å‹ã€åˆ†ç±»å‹å’Œæ—¥æœŸå‹åˆ—çš„è½¬æ¢ã€‚
            ä¸»è¦ç”¨äºæ•°æ®é¢„å¤„ç†é˜¶æ®µï¼Œç¡®ä¿æ•°æ®å…·æœ‰æ­£ç¡®çš„ç±»å‹ä»¥ä¾¿åç»­åˆ†æã€‚

            Args:
                df (pd.DataFrame): è¾“å…¥çš„æ•°æ®è¡¨
                num_type (list): éœ€è¦è½¬æ¢ä¸ºæ•°å€¼å‹çš„åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä¸è½¬æ¢ï¼‰
                classify_type (list): éœ€è¦è½¬æ¢ä¸ºåˆ†ç±»å‹çš„åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä¸è½¬æ¢ï¼‰
                date_type (list): éœ€è¦è½¬æ¢ä¸ºæ—¥æœŸå‹çš„åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä¸è½¬æ¢ï¼‰

            Returns:
                pd.DataFrame: æ•°æ®ç±»å‹è½¬æ¢åçš„æ•°æ®è¡¨

            ä½¿ç”¨ç¤ºä¾‹ï¼š
                # è½¬æ¢æŒ‡å®šåˆ—çš„æ•°æ®ç±»å‹
                df_converted = Data2Feature.data_type_change(
                    df,
                    num_type=['age', 'salary'],      # è½¬æ¢ä¸ºæ•°å€¼å‹
                    classify_type=['gender', 'city'], # è½¬æ¢ä¸ºåˆ†ç±»å‹
                    date_type=['create_time', 'update_time']  # è½¬æ¢ä¸ºæ—¥æœŸå‹
                )

            æ³¨æ„äº‹é¡¹ï¼š
            - è½¬æ¢å¤±è´¥çš„åˆ—ä¼šä¿æŒåŸæœ‰æ•°æ®ç±»å‹
            - æ—¥æœŸè½¬æ¢æ”¯æŒå¸¸è§çš„æ—¥æœŸæ ¼å¼
            - æ•°å€¼è½¬æ¢ä¼šå°†æ— æ³•è§£æçš„å€¼è®¾ä¸ºNaN
            """
            df = DataDeal.data_type_change(df,num_type=num_type,classify_type=classify_type,date_type=date_type)
            return df
            
        @classmethod
        def data_filter(cls, df, data_dict={}, type='remove'):
            """
            æ ¹æ®æŒ‡å®šæ¡ä»¶è¿‡æ»¤DataFrameæ•°æ®

            å‚æ•°:
            df (pd.DataFrame): è¦è¿‡æ»¤çš„DataFrame
            data_dict (dict): è¿‡æ»¤æ¡ä»¶å­—å…¸ï¼Œkeyä¸ºåˆ—åï¼Œvalueä¸ºåˆ—çš„å€¼åˆ—è¡¨(listç±»å‹)
            type (str): è¿‡æ»¤ç±»å‹ï¼Œå¯é€‰å€¼ä¸º'remove'æˆ–'in'
                    - remove: åˆ é™¤åˆ—ä¸­å€¼ä¸ºvalueçš„è¡Œ
                    - in: ä¿ç•™åˆ—ä¸­å€¼å­˜åœ¨äºvalueåˆ—è¡¨ä¸­çš„è¡Œ

            è¿”å›:
            pd.DataFrame: è¿‡æ»¤åçš„DataFrame

            ç¤ºä¾‹:
            # åˆ é™¤coloråˆ—ä¸­å€¼ä¸º'red'æˆ–'blue'çš„è¡Œ
            df_filtered = Data2Feature.data_filter(df, {'color': ['red', 'blue']}, 'remove')

            # åªä¿ç•™nameåˆ—ä¸­å€¼ä¸º'Alice'æˆ–'Bob'çš„è¡Œ
            df_filtered = Data2Feature.data_filter(df, {'name': ['Alice', 'Bob']}, 'in')
            """
            import pandas as pd

            # å‚æ•°éªŒè¯
            if df is None or df.empty:
                cls.pc.lg("è­¦å‘Š: è¾“å…¥DataFrameä¸ºç©º")
                return df.copy()

            if not data_dict:
                cls.pc.lg("è­¦å‘Š: data_dictä¸ºç©ºï¼Œè¿”å›åŸå§‹DataFrame")
                return df.copy()

            if type not in ['remove', 'in']:
                raise ValueError("typeå‚æ•°å¿…é¡»æ˜¯'remove'æˆ–'in'")

            # åˆ›å»ºç»“æœDataFrameçš„å‰¯æœ¬
            result_df = df.copy()
            original_count = len(result_df)

            cls.pc.lg(f"å¼€å§‹æ•°æ®è¿‡æ»¤ï¼ŒåŸå§‹è¡Œæ•°: {original_count}")
            cls.pc.lg(f"è¿‡æ»¤ç±»å‹: {type}")
            cls.pc.lg(f"è¿‡æ»¤æ¡ä»¶: {data_dict}")

            # å¯¹æ¯ä¸ªåˆ—åº”ç”¨è¿‡æ»¤æ¡ä»¶
            for column, values in data_dict.items():
                # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                if column not in result_df.columns:
                    cls.pc.lg(f"è­¦å‘Š: åˆ— '{column}' ä¸å­˜åœ¨äºDataFrameä¸­ï¼Œè·³è¿‡æ­¤æ¡ä»¶")
                    continue

                # æ£€æŸ¥valuesæ˜¯å¦ä¸ºåˆ—è¡¨
                if not isinstance(values, (list, tuple, set)):
                    cls.pc.lg(f"è­¦å‘Š: åˆ— '{column}' çš„å€¼ä¸æ˜¯åˆ—è¡¨ç±»å‹ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨")
                    values = [values]

                # è®°å½•è¿‡æ»¤å‰çš„è¡Œæ•°
                before_count = len(result_df)

                # æ ¹æ®è¿‡æ»¤ç±»å‹åº”ç”¨æ¡ä»¶
                if type == 'remove':
                    # åˆ é™¤åˆ—ä¸­å€¼åœ¨valuesåˆ—è¡¨ä¸­çš„è¡Œ
                    mask = ~result_df[column].isin(values)
                    result_df = result_df[mask]
                    removed_count = before_count - len(result_df)
                    cls.pc.lg(f"åˆ— '{column}': åˆ é™¤äº† {removed_count} è¡Œ (å€¼åœ¨ {values} ä¸­)")

                elif type == 'in':
                    # åªä¿ç•™åˆ—ä¸­å€¼åœ¨valuesåˆ—è¡¨ä¸­çš„è¡Œ
                    mask = result_df[column].isin(values)
                    result_df = result_df[mask]
                    kept_count = len(result_df)
                    removed_count = before_count - kept_count
                    cls.pc.lg(f"åˆ— '{column}': ä¿ç•™äº† {kept_count} è¡Œ (å€¼åœ¨ {values} ä¸­)ï¼Œåˆ é™¤äº† {removed_count} è¡Œ")

            # ç»Ÿè®¡æœ€ç»ˆç»“æœ
            final_count = len(result_df)
            total_removed = original_count - final_count

            cls.pc.lg(f"è¿‡æ»¤å®Œæˆ:")
            cls.pc.lg(f"  åŸå§‹è¡Œæ•°: {original_count}")
            cls.pc.lg(f"  æœ€ç»ˆè¡Œæ•°: {final_count}")
            cls.pc.lg(f"  æ€»è®¡åˆ é™¤: {total_removed} è¡Œ")
            cls.pc.lg(f"  åˆ é™¤æ¯”ä¾‹: {total_removed/original_count*100:.2f}%" if original_count > 0 else "  åˆ é™¤æ¯”ä¾‹: 0%")

            return result_df 
            
        @classmethod
        def data_make(cls,
                    data_type={'numf':'float32','num':'int32', "date":"yyyy-mm-dd",'classify':'string'},
                    num_rows=100):
            """
            æ ¹æ®data_typeéšæœºç”Ÿæˆæ•°æ®

            å‚æ•°:
            data_type (dict): æ•°æ®ç±»å‹é…ç½®å­—å…¸
                - numf: æµ®ç‚¹æ•°ç±»å‹ï¼Œé»˜è®¤'float32'
                - num: æ•´æ•°ç±»å‹ï¼Œé»˜è®¤'int32'
                - date: æ—¥æœŸç±»å‹ï¼Œé»˜è®¤'yyyy-mm-dd'
                - classify: åˆ†ç±»ç±»å‹ï¼Œé»˜è®¤'string'
            num_rows (int): ç”Ÿæˆçš„è¡Œæ•°ï¼Œé»˜è®¤100è¡Œ

            è¿”å›:
            pd.DataFrame: ç”Ÿæˆçš„éšæœºæ•°æ®DataFrame

            ç¤ºä¾‹:
            # ç”Ÿæˆé»˜è®¤é…ç½®çš„æµ‹è¯•æ•°æ®
            df = Data2Feature.data_make()

            # ç”ŸæˆæŒ‡å®šè¡Œæ•°çš„æ•°æ®
            df = Data2Feature.data_make(num_rows=1000)

            # è‡ªå®šä¹‰æ•°æ®ç±»å‹é…ç½®
            df = Data2Feature.data_make({
                'numf': 'float64',
                'num': 'int16',
                'date': 'dd/mm/yyyy',
                'classify': 'category'
            }, num_rows=500)
            """
            import pandas as pd
            import numpy as np
            import random
            from datetime import datetime, timedelta
            import string

            cls.pc.lg(f"å¼€å§‹ç”Ÿæˆéšæœºæ•°æ®ï¼Œè¡Œæ•°: {num_rows}")
            cls.pc.lg(f"æ•°æ®ç±»å‹é…ç½®: {data_type}")

            # ç”Ÿæˆæ•°æ®
            data = {}

            # ç”Ÿæˆæµ®ç‚¹æ•°åˆ—
            if 'numf' in data_type:
                float_col = np.random.normal(0, 1, num_rows).astype(data_type['numf'])
                data['float_col'] = float_col
                cls.pc.lg(f"ç”Ÿæˆæµ®ç‚¹æ•°åˆ— 'float_col'ï¼Œç±»å‹: {data_type['numf']}")

            # ç”Ÿæˆæ•´æ•°åˆ—
            if 'num' in data_type:
                int_col = np.random.randint(0, 1000, num_rows).astype(data_type['num'])
                data['int_col'] = int_col
                cls.pc.lg(f"ç”Ÿæˆæ•´æ•°åˆ— 'int_col'ï¼Œç±»å‹: {data_type['num']}")

            # ç”Ÿæˆæ—¥æœŸåˆ—
            if 'date' in data_type:
                date_format = data_type['date']
                start_date = datetime(2020, 1, 1)
                end_date = datetime(2024, 12, 31)

                if date_format == 'yyyy-mm-dd':
                    date_col = [start_date + timedelta(days=random.randint(0, (end_date - start_date).days))
                            for _ in range(num_rows)]
                    date_col = [date.strftime('%Y-%m-%d') for date in date_col]
                elif date_format == 'dd/mm/yyyy':
                    date_col = [start_date + timedelta(days=random.randint(0, (end_date - start_date).days))
                            for _ in range(num_rows)]
                    date_col = [date.strftime('%d/%m/%Y') for date in date_col]
                elif date_format == 'mm-dd-yyyy':
                    date_col = [start_date + timedelta(days=random.randint(0, (end_date - start_date).days))
                            for _ in range(num_rows)]
                    date_col = [date.strftime('%m-%d-%Y') for date in date_col]
                else:
                    # é»˜è®¤æ ¼å¼
                    date_col = [start_date + timedelta(days=random.randint(0, (end_date - start_date).days))
                            for _ in range(num_rows)]
                    date_col = [date.strftime('%Y-%m-%d') for date in date_col]

                data['date_col'] = date_col
                cls.pc.lg(f"ç”Ÿæˆæ—¥æœŸåˆ— 'date_col'ï¼Œæ ¼å¼: {date_format}")

            # ç”Ÿæˆåˆ†ç±»åˆ—
            if 'classify' in data_type:
                classify_type = data_type['classify']

                # ç”Ÿæˆä¸€äº›éšæœºçš„åˆ†ç±»å€¼
                categories = [
                    ['Category_A', 'Category_B', 'Category_C', 'Category_D', 'Category_E'],
                    ['Red', 'Green', 'Blue', 'Yellow', 'Black'],
                    ['Active', 'Inactive', 'Pending', 'Completed'],
                    ['Type_1', 'Type_2', 'Type_3']
                ]

                for i, category_list in enumerate(categories):
                    if i >= 2:  # æœ€å¤šç”Ÿæˆ4ä¸ªåˆ†ç±»åˆ—
                        break

                    col_name = f'category_col_{i+1}'
                    classify_data = [random.choice(category_list) for _ in range(num_rows)]

                    if classify_type == 'category':
                        # è½¬æ¢ä¸ºpandas categoryç±»å‹
                        data[col_name] = pd.Categorical(classify_data)
                    else:
                        # ä¿æŒä¸ºå­—ç¬¦ä¸²ç±»å‹
                        data[col_name] = classify_data

                    cls.pc.lg(f"ç”Ÿæˆåˆ†ç±»åˆ— '{col_name}'ï¼Œç±»å‹: {classify_type}")

            # ç”Ÿæˆé¢å¤–çš„æ··åˆæ•°æ®åˆ—
            # IDåˆ—
            data['id'] = range(1, num_rows + 1)
            cls.pc.lg("ç”ŸæˆIDåˆ— 'id'")

            # éšæœºæ–‡æœ¬åˆ—
            text_data = []
            for _ in range(num_rows):
                word_length = random.randint(3, 10)
                word = ''.join(random.choices(string.ascii_lowercase, k=word_length))
                text_data.append(word)
            data['text_col'] = text_data
            cls.pc.lg("ç”Ÿæˆæ–‡æœ¬åˆ— 'text_col'")

            # å¸ƒå°”åˆ—
            bool_data = [random.choice([True, False]) for _ in range(num_rows)]
            data['bool_col'] = bool_data
            cls.pc.lg("ç”Ÿæˆå¸ƒå°”åˆ— 'bool_col'")

            # åˆ›å»ºDataFrame
            df = pd.DataFrame(data)

            cls.pc.lg(f"æ•°æ®ç”Ÿæˆå®Œæˆï¼ŒDataFrameå½¢çŠ¶: {df.shape}")
            cls.pc.lg(f"ç”Ÿæˆçš„åˆ—: {list(df.columns)}")

            # æ˜¾ç¤ºæ•°æ®ç±»å‹ä¿¡æ¯
            cls.pc.lg("æ•°æ®ç±»å‹:")
            for col, dtype in df.dtypes.items():
                cls.pc.lg(f"  {col}: {dtype}")

            # æ˜¾ç¤ºå‰å‡ è¡Œæ ·æœ¬
            cls.pc.lg("æ•°æ®æ ·æœ¬ (å‰3è¡Œ):")
            cls.pc.lg(df.head(3).to_string())

            return df 
            
        @classmethod
        def data_sample_cat(cls, df, y=None, n=10, indetify=[], cat_cols=[], time_col='time8'):
            """
            è¿ç»­æ—¶é—´é‡‡æ ·ï¼ŒæŒ‰ç±»åˆ«é‡‡æ ·

            å‚æ•°:
            df (pd.DataFrame): è¾“å…¥çš„DataFrame
            y: ç›®æ ‡å˜é‡ï¼ˆæš‚æœªä½¿ç”¨ï¼Œä¿ç•™å‚æ•°ï¼‰
            n (int): æ¯ä¸ªç±»åˆ«é‡‡æ ·çš„æ•°æ®è¡Œæ•°ï¼Œé»˜è®¤ä¸º10
            indetify (list): æ ‡è¯†åˆ—åˆ—è¡¨ï¼Œç”¨äºå»é‡
            cat_cols (list): ç±»åˆ«åˆ—åˆ—è¡¨ï¼Œéœ€è¦æŒ‰è¿™äº›åˆ—çš„ç±»åˆ«è¿›è¡Œé‡‡æ ·
            time_col (str): æ—¶é—´åˆ—åï¼Œé»˜è®¤ä¸º'time8'

            è¿”å›:
            pd.DataFrame: é‡‡æ ·åçš„æ•°æ®

            ä¸»è¦é€»è¾‘:
            1. å¾ªç¯cat_colsç±»åˆ«åˆ—ï¼Œæ¯ä¸ªç±»åˆ«åˆ—è¿›è¡Œé‡‡æ ·ï¼š
            1.1 è·å–è¯¥ç±»åˆ«åˆ—çš„æ‰€æœ‰æ•°æ®ä¸é‡å¤çš„ç±»åˆ«
            1.2 å¾ªç¯ä¸€ä¸ªåˆ—ä¸­æ‰€æœ‰ä¸é‡å¤çš„ç±»åˆ«ï¼Œé’ˆå¯¹æ¯ä¸ªä¸é‡å¤çš„ç±»åˆ«ï¼š
                1.2.1 å–è¯¥ç±»åˆ«æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ—¶é—´é™åºæ’åºï¼Œå–å‰nè¡Œæ•°æ®
            2. åˆå¹¶å¾ªç¯çš„æ•°æ®ï¼Œç„¶åæŒ‰indetify+time_colä½œä¸ºä¸»é”®è¿›è¡Œå»é‡ï¼Œè¿”å›åˆå¹¶åçš„æ•°æ®

            ç¤ºä¾‹:
            # æŒ‰Payment Formatåˆ—è¿›è¡Œç±»åˆ«é‡‡æ ·ï¼Œæ¯ä¸ªç±»åˆ«å–æœ€æ–°çš„10æ¡è®°å½•
            df_sampled = Data2Feature.data_sample_cat(
                df=df,
                y=None,
                n=10,
                indetify=['From'],
                cat_cols=['Payment Format'],
                time_col='time8'
            )
            """
            import pandas as pd

            # å‚æ•°éªŒè¯
            if df is None or df.empty:
                cls.pc.lg("è­¦å‘Š: è¾“å…¥DataFrameä¸ºç©º")
                return pd.DataFrame()

            if not cat_cols:
                cls.pc.lg("è­¦å‘Š: cat_colsä¸ºç©ºï¼Œè¿”å›ç©ºDataFrame")
                return pd.DataFrame()

            if time_col not in df.columns:
                cls.pc.lg(f"è­¦å‘Š: æ—¶é—´åˆ— '{time_col}' ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºDataFrame")
                return pd.DataFrame()

            cls.pc.lg(f"å¼€å§‹æŒ‰ç±»åˆ«é‡‡æ ·æ•°æ®")
            cls.pc.lg(f"åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
            cls.pc.lg(f"é‡‡æ ·æ•°é‡: {n} è¡Œ/ç±»åˆ«")
            cls.pc.lg(f"ç±»åˆ«åˆ—: {cat_cols}")
            cls.pc.lg(f"æ ‡è¯†åˆ—: {indetify}")
            cls.pc.lg(f"æ—¶é—´åˆ—: {time_col}")

            # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹ä»¥ä¾¿æ’åº
            df_sample = df.copy()
            if not pd.api.types.is_datetime64_any_dtype(df_sample[time_col]):
                try:
                    df_sample[time_col] = pd.to_datetime(df_sample[time_col])
                    cls.pc.lg(f"å°†æ—¶é—´åˆ— '{time_col}' è½¬æ¢ä¸ºdatetimeç±»å‹")
                except Exception as e:
                    cls.pc.lg(f"è­¦å‘Š: æ— æ³•è½¬æ¢æ—¶é—´åˆ— '{time_col}' ä¸ºdatetimeç±»å‹: {e}")
                    cls.pc.lg("å°è¯•ä½¿ç”¨å­—ç¬¦ä¸²æ’åº")

            # å­˜å‚¨æ‰€æœ‰é‡‡æ ·ç»“æœ
            all_samples = []

            # 1. å¾ªç¯cat_colsç±»åˆ«åˆ—ï¼Œæ¯ä¸ªç±»åˆ«åˆ—è¿›è¡Œé‡‡æ ·
            for cat_col in cat_cols:
                if cat_col not in df_sample.columns:
                    cls.pc.lg(f"è­¦å‘Š: ç±»åˆ«åˆ— '{cat_col}' ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                    continue

                cls.pc.lg(f"\nå¤„ç†ç±»åˆ«åˆ—: {cat_col}")

                # 1.1 è·å–è¯¥ç±»åˆ«åˆ—çš„æ‰€æœ‰æ•°æ®ä¸é‡å¤çš„ç±»åˆ«
                unique_categories = df_sample[cat_col].dropna().unique()
                cls.pc.lg(f"å‘ç° {len(unique_categories)} ä¸ªä¸é‡å¤ç±»åˆ«: {unique_categories}")

                # 1.2 å¾ªç¯ä¸€ä¸ªåˆ—ä¸­æ‰€æœ‰ä¸é‡å¤çš„ç±»åˆ«ï¼Œé’ˆå¯¹æ¯ä¸ªä¸é‡å¤çš„ç±»åˆ«è¿›è¡Œé‡‡æ ·
                for category in unique_categories:
                    cls.pc.lg(f"  å¤„ç†ç±»åˆ« '{category}':")

                    # ç­›é€‰è¯¥ç±»åˆ«çš„æ‰€æœ‰æ•°æ®
                    category_data = df_sample[df_sample[cat_col] == category].copy()
                    cls.pc.lg(f"    ç±»åˆ« '{category}' å…±æœ‰ {len(category_data)} æ¡è®°å½•")

                    if len(category_data) == 0:
                        cls.pc.lg(f"    è·³è¿‡ç©ºç±»åˆ«")
                        continue

                    # 1.2.1 å–è¯¥ç±»åˆ«æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ—¶é—´é™åºæ’åºï¼Œå–å‰nè¡Œæ•°æ®
                    try:
                        # æŒ‰æ—¶é—´é™åºæ’åº
                        category_sorted = category_data.sort_values(by=time_col, ascending=False)

                        # å–å‰nè¡Œ
                        sample_data = category_sorted.head(n)
                        cls.pc.lg(f"    é‡‡æ ·äº† {len(sample_data)} æ¡è®°å½•")

                        # æ·»åŠ é‡‡æ ·ä¿¡æ¯
                        sample_data = sample_data.copy()
                        sample_data['_sample_cat_col'] = cat_col
                        sample_data['_sample_category'] = category
                        sample_data['_sample_count'] = len(sample_data)

                        all_samples.append(sample_data)

                    except Exception as e:
                        cls.pc.lg(f"    é”™è¯¯: å¤„ç†ç±»åˆ« '{category}' æ—¶å‡ºé”™: {e}")
                        continue

            # 2. åˆå¹¶å¾ªç¯çš„æ•°æ®
            if not all_samples:
                cls.pc.lg("è­¦å‘Š: æ²¡æœ‰é‡‡æ ·åˆ°ä»»ä½•æ•°æ®")
                return pd.DataFrame()

            cls.pc.lg(f"\nåˆå¹¶ {len(all_samples)} ä¸ªé‡‡æ ·ç»“æœ")
            merged_data = pd.concat(all_samples, ignore_index=True)
            cls.pc.lg(f"åˆå¹¶åæ•°æ®å½¢çŠ¶: {merged_data.shape}")

            # æŒ‰indetify+time_colä½œä¸ºä¸»é”®è¿›è¡Œå»é‡
            if indetify:
                dedup_cols = indetify + [time_col]
                # æ£€æŸ¥å»é‡åˆ—æ˜¯å¦å­˜åœ¨
                missing_cols = [col for col in dedup_cols if col not in merged_data.columns]
                if missing_cols:
                    cls.pc.lg(f"è­¦å‘Š: å»é‡åˆ—ä¸­ç¼ºå°‘ä»¥ä¸‹åˆ—: {missing_cols}")
                    cls.pc.lg("è·³è¿‡å»é‡æ­¥éª¤")
                else:
                    # å»é‡å‰è®°å½•æ•°é‡
                    before_dedup = len(merged_data)

                    # æŒ‰æŒ‡å®šåˆ—å»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è®°å½•ï¼ˆå³æœ€æ–°çš„è®°å½•ï¼‰
                    dedup_data = merged_data.drop_duplicates(subset=dedup_cols, keep='first')

                    # å»é‡åè®°å½•æ•°é‡
                    after_dedup = len(dedup_data)
                    dedup_count = before_dedup - after_dedup

                    cls.pc.lg(f"æŒ‰åˆ— {dedup_cols} å»é‡:")
                    cls.pc.lg(f"  å»é‡å‰: {before_dedup} æ¡è®°å½•")
                    cls.pc.lg(f"  å»é‡å: {after_dedup} æ¡è®°å½•")
                    cls.pc.lg(f"  åˆ é™¤é‡å¤: {dedup_count} æ¡è®°å½•")

                    merged_data = dedup_data
            else:
                cls.pc.lg("æœªæŒ‡å®šæ ‡è¯†åˆ—ï¼Œè·³è¿‡å»é‡æ­¥éª¤")

            # ç§»é™¤ä¸´æ—¶æ·»åŠ çš„é‡‡æ ·ä¿¡æ¯åˆ—
            temp_cols = ['_sample_cat_col', '_sample_category', '_sample_count']
            existing_temp_cols = [col for col in temp_cols if col in merged_data.columns]
            if existing_temp_cols:
                merged_data = merged_data.drop(columns=existing_temp_cols)
                cls.pc.lg(f"ç§»é™¤ä¸´æ—¶åˆ—: {existing_temp_cols}")

            # æœ€ç»ˆç»Ÿè®¡
            cls.pc.lg(f"\né‡‡æ ·å®Œæˆ:")
            cls.pc.lg(f"  æœ€ç»ˆæ•°æ®å½¢çŠ¶: {merged_data.shape}")
            cls.pc.lg(f"  å¤„ç†çš„ç±»åˆ«åˆ—: {[col for col in cat_cols if col in df.columns]}")

            # æ˜¾ç¤ºæ¯ä¸ªåŸå§‹ç±»åˆ«åˆ—çš„é‡‡æ ·ç»Ÿè®¡
            for cat_col in cat_cols:
                if cat_col in merged_data.columns:
                    unique_cats = merged_data[cat_col].nunique()
                    cls.pc.lg(f"  {cat_col}: {unique_cats} ä¸ªç±»åˆ«")

            return merged_data
            
        @classmethod
        def show_col_type(cls, df, numeric_only=False, non_numeric_only=False):
            """
            æ˜¾ç¤ºDataFrameåˆ—çš„æ•°æ®ç±»å‹

            Args:
                df (pd.DataFrame): è¾“å…¥çš„æ•°æ®è¡¨
                numeric_only (bool): æ˜¯å¦åªæ˜¾ç¤ºæ•°å€¼åˆ—ï¼Œé»˜è®¤ä¸ºFalse
                non_numeric_only (bool): æ˜¯å¦åªæ˜¾ç¤ºéæ•°å€¼åˆ—ï¼Œé»˜è®¤ä¸ºFalse

            Note:
                å¦‚æœnumeric_onlyå’Œnon_numeric_onlyéƒ½ä¸ºFalseï¼Œæ˜¾ç¤ºæ‰€æœ‰åˆ—ç±»å‹
                å¦‚æœnumeric_onlyä¸ºTrueï¼Œåªæ˜¾ç¤ºæ•°å€¼åˆ—ç±»å‹
                å¦‚æœnon_numeric_onlyä¸ºTrueï¼Œåªæ˜¾ç¤ºéæ•°å€¼åˆ—ç±»å‹
            """
            if numeric_only and non_numeric_only:
                print("è­¦å‘Šï¼šnumeric_onlyå’Œnon_numeric_onlyä¸èƒ½åŒæ—¶ä¸ºTrueï¼Œå°†æ˜¾ç¤ºæ‰€æœ‰åˆ—ç±»å‹")
                print(df.dtypes)
            elif numeric_only:
                numeric_cols = df.select_dtypes(include=['number']).columns
                if len(numeric_cols) > 0:
                    print("æ•°å€¼åˆ—ç±»å‹ï¼š")
                    print(df[numeric_cols].dtypes)
                else:
                    print("æ²¡æœ‰æ•°å€¼åˆ—")
            elif non_numeric_only:
                non_numeric_cols = df.select_dtypes(exclude=['number']).columns
                if len(non_numeric_cols) > 0:
                    print("éæ•°å€¼åˆ—ç±»å‹ï¼š")
                    print(df[non_numeric_cols].dtypes)
                else:
                    print("æ²¡æœ‰éæ•°å€¼åˆ—")
            else:
                print(df.dtypes)
            
        @classmethod
        def show_null_count(cls,df):
            print(df.isnull().sum())
            
        @classmethod
        def show_one_row(cls, df=None, row_idx=0, n=10, show_all=False):
            """
            æ˜¾ç¤ºDataFrameä¸­æŒ‡å®šè¡Œçš„å‰nä¸ªå­—æ®µ

            å‚æ•°:
            df: DataFrameå¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨å±€çš„df_final_result
            row_idx: è¡Œç´¢å¼•ï¼Œé»˜è®¤ä¸º0ï¼ˆç¬¬ä¸€è¡Œï¼‰
            n: æ˜¾ç¤ºçš„å­—æ®µæ•°é‡ï¼Œé»˜è®¤ä¸º10ä¸ª
            show_all: æ˜¯å¦æ˜¾ç¤ºæ‰€æœ‰å­—æ®µï¼Œå¦‚æœä¸ºTrueåˆ™å¿½ç•¥nå‚æ•°

            åŠŸèƒ½:
            1. æ˜¾ç¤ºæŒ‡å®šè¡Œçš„å‰nä¸ªå­—æ®µçš„é”®å€¼å¯¹
            2. æ”¯æŒæ˜¾ç¤ºDataFrameçš„åŸºæœ¬ä¿¡æ¯
            3. æä¾›å­—æ®µè®¡æ•°å’Œæ€»è§ˆä¿¡æ¯
            4. æ”¯æŒæ˜¾ç¤ºæ‰€æœ‰å­—æ®µæˆ–é™åˆ¶æ˜¾ç¤ºæ•°é‡
            """
            import pandas as pd

            # æ£€æŸ¥DataFrameæ˜¯å¦ä¸ºç©º
            if df is None or df.empty:
                cls.pc.lg("DataFrameä¸ºç©ºï¼Œæ²¡æœ‰æ•°æ®å¯æ˜¾ç¤º")
                return

            # æ£€æŸ¥è¡Œç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
            if row_idx < 0 or row_idx >= len(df):
                cls.pc.lg(f"é”™è¯¯: è¡Œç´¢å¼• {row_idx} è¶…å‡ºèŒƒå›´ [0, {len(df)-1}]")
                return

            # æ˜¾ç¤ºDataFrameåŸºæœ¬ä¿¡æ¯
            cls.pc.lg(f"DataFrameå½¢çŠ¶: {df.shape}")
            cls.pc.lg(f"æ˜¾ç¤ºç¬¬ {row_idx} è¡Œï¼ˆç´¢å¼•: {df.index[row_idx]}ï¼‰")
            cls.pc.lg(f"æ€»å­—æ®µæ•°: {len(df.columns)}")

            if show_all:
                cls.pc.lg(f"æ˜¾ç¤ºæ‰€æœ‰å­—æ®µ:")
                display_count = len(df.columns)
            else:
                cls.pc.lg(f"æ˜¾ç¤ºå‰ {n} ä¸ªå­—æ®µ:")
                display_count = min(n, len(df.columns))

            cls.pc.lg("-" * 60)

            # æ˜¾ç¤ºæŒ‡å®šè¡Œçš„å­—æ®µ
            count = 0
            for k, v in df.iloc[row_idx].items():
                # æ ¼å¼åŒ–æ˜¾ç¤º
                if pd.isna(v):
                    value_str = "NaN"
                elif isinstance(v, float):
                    if abs(v) < 0.001:
                        value_str = f"{v:.6f}"
                    else:
                        value_str = f"{v:.3f}"
                elif isinstance(v, (int, np.integer)):
                    value_str = str(v)
                else:
                    value_str = str(v)
                    # é™åˆ¶å­—ç¬¦ä¸²é•¿åº¦
                    if len(value_str) > 50:
                        value_str = value_str[:47] + "..."

                cls.pc.lg(f"{k:30}: {value_str}")
                count += 1

                # å¦‚æœä¸æ˜¾ç¤ºå…¨éƒ¨ä¸”è¾¾åˆ°æŒ‡å®šæ•°é‡ï¼Œåˆ™åœæ­¢
                if not show_all and count >= display_count:
                    break

            cls.pc.lg("-" * 60)

            # å¦‚æœè¿˜æœ‰æœªæ˜¾ç¤ºçš„å­—æ®µï¼Œæç¤ºç”¨æˆ·
            if not show_all and len(df.columns) > n:
                remaining = len(df.columns) - n
                cls.pc.lg(f"è¿˜æœ‰ {remaining} ä¸ªå­—æ®µæœªæ˜¾ç¤ºï¼Œä½¿ç”¨ show_all=True å¯æ˜¾ç¤ºå…¨éƒ¨")

                
        @classmethod
        def show_unique_count(cls,df):
            print(df.nunique())
            

        @classmethod
        def show_describe(cls,df,cols=[],show_category=False):
            """
            æ˜¾ç¤ºæ•°æ®çš„æè¿°ç»Ÿè®¡ä¿¡æ¯

            Args:
                df (pd.DataFrame): è¾“å…¥çš„æ•°æ®è¡¨
                cols (list): æŒ‡å®šè¦æ˜¾ç¤ºçš„åˆ—ï¼Œå¦‚æœä¸ºç©ºæˆ–Noneåˆ™æ˜¾ç¤ºæ‰€æœ‰åˆ—
                show_category (bool): æ˜¾ç¤ºç±»åˆ«ç±»å‹çš„ä¿¡æ¯è¿˜æ˜¯æ•°å­—ç±»å‹çš„ä¿¡æ¯ï¼Œé»˜è®¤ä¸ºFalse(æ˜¾ç¤ºæ•°å­—ç±»å‹)
            """
            # å¦‚æœcolsä¸ä¸ºç©ºæˆ–Noneï¼Œåˆ™åªæ˜¾ç¤ºæŒ‡å®šåˆ—
            if cols and len(cols) > 0:
                df_display = df[cols]
            else:
                df_display = df

            if show_category:
                # æ˜¾ç¤ºç±»åˆ«åˆ—çš„æè¿°ç»Ÿè®¡
                print("=== ç±»åˆ«åˆ—æè¿°ç»Ÿè®¡ ===")
                try:
                    # ä½¿ç”¨_categorical_function_not_num_dateæ–¹æ³•è¯†åˆ«ç±»åˆ«åˆ—
                    categorical_cols = cls._categorical_not_num_date(df_display)

                    if len(categorical_cols) > 0:
                        # å¯¹æ¯ä¸ªç±»åˆ«åˆ—æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                        for col in categorical_cols:
                            if col in df_display.columns:
                                print(f"\nåˆ— '{col}' çš„ç»Ÿè®¡ä¿¡æ¯:")
                                print(f"  å”¯ä¸€å€¼æ•°é‡: {df_display[col].nunique()}")
                                print(f"  ç¼ºå¤±å€¼æ•°é‡: {df_display[col].isnull().sum()}")
                                print(f"  å‰10ä¸ªæœ€é¢‘ç¹çš„å€¼:")
                                print(df_display[col].value_counts().head(10))
                    else:
                        print("æ²¡æœ‰ç±»åˆ«åˆ—")
                except Exception as e:
                    print(f"æ˜¾ç¤ºç±»åˆ«åˆ—ç»Ÿè®¡æ—¶å‡ºé”™: {e}")
            else:
                # æ˜¾ç¤ºæ•°å€¼åˆ—çš„æè¿°ç»Ÿè®¡
                print("=== æ•°å€¼åˆ—æè¿°ç»Ÿè®¡ ===")
                try:
                    # æ˜¾ç¤ºæ•°å€¼åˆ—çš„ç»Ÿè®¡ä¿¡æ¯
                    numeric_cols = df_display.select_dtypes(include=['number']).columns
                    if len(numeric_cols) > 0:
                        print(df_display[numeric_cols].describe())
                    else:
                        print("æ²¡æœ‰æ•°å€¼åˆ—")
                except Exception as e:
                    print(f"æ˜¾ç¤ºæ•°å€¼åˆ—ç»Ÿè®¡æ—¶å‡ºé”™: {e}")

            # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
            print(f"\n=== æ•°æ®åŸºæœ¬ä¿¡æ¯ ===")
            print(f"æ€»è¡Œæ•°: {len(df_display)}")
            print(f"æ€»åˆ—æ•°: {len(df_display.columns)}")
            print(f"ç¼ºå¤±å€¼æ€»æ•°: {df_display.isnull().sum().sum()}")
            print(f"å†…å­˜ä½¿ç”¨: {df_display.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB") 
            
        @classmethod
        def tonum_col2index(cls,df, identity=[], classify_type=[], classify_type2=[],
                    dict_file="dict_file.dict", is_pre=False,
                    word2id=None, start_index=1):
            """
            å°†åˆ†ç±»åˆ—è½¬æ¢ä¸ºæ•°å€¼ç´¢å¼•ï¼Œæ”¯æŒè®­ç»ƒå’Œé¢„æµ‹æ¨¡å¼

            è¯¥æ–¹æ³•å°†DataFrameä¸­çš„åˆ†ç±»ç‰¹å¾åˆ—è½¬æ¢ä¸ºæ•°å€¼ç´¢å¼•ï¼Œä¾¿äºæœºå™¨å­¦ä¹ æ¨¡å‹å¤„ç†ã€‚
            æ”¯æŒå•åˆ—åˆ†ç±»å’Œå¤šåˆ—ç»„åˆåˆ†ç±»çš„è½¬æ¢ï¼Œå¹¶èƒ½ä¿å­˜å’ŒåŠ è½½ç¼–ç å­—å…¸ã€‚

            Args:
                df (pd.DataFrame): è¾“å…¥çš„æ•°æ®è¡¨
                identity (list): æ ‡è¯†åˆ—åˆ—è¡¨ï¼Œä¸å‚ä¸ç¼–ç ï¼Œé»˜è®¤ä¸ºç©º
                classify_type (list): éœ€è¦ç¼–ç çš„å•åˆ—åˆ†ç±»åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç©º
                classify_type2 (list): éœ€è¦ç¼–ç çš„å¤šåˆ—ç»„åˆåˆ†ç±»åˆ—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºåˆ—ååˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç©º
                dict_file (str): ç¼–ç å­—å…¸ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸º"dict_file.dict"
                is_pre (bool): æ˜¯å¦ä¸ºé¢„æµ‹æ¨¡å¼ï¼Œé»˜è®¤ä¸ºFalseï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
                word2id (dict): é¢„å®šä¹‰çš„è¯æ±‡åˆ°IDæ˜ å°„å­—å…¸ï¼Œé»˜è®¤ä¸ºNone
                start_index (int): ç´¢å¼•èµ·å§‹å€¼ï¼Œé»˜è®¤ä¸º1

            Returns:
                pd.DataFrame: ç¼–ç åçš„æ•°æ®è¡¨ï¼Œåˆ†ç±»åˆ—å·²è½¬æ¢ä¸ºæ•°å€¼ç´¢å¼•

            Raises:
                ValueError: å½“classify_type2ä¸­çš„å…ƒç´ ä¸æ˜¯åˆ—è¡¨ç±»å‹æ—¶æŠ›å‡º

            å¤„ç†é€»è¾‘ï¼š
            1. å‚æ•°éªŒè¯ï¼šæ£€æŸ¥classify_type2å‚æ•°æ ¼å¼ï¼Œç¡®ä¿æ¯ä¸ªå…ƒç´ éƒ½æ˜¯åˆ—è¡¨
            2. è°ƒç”¨åº•å±‚DataDeal.col2indexæ–¹æ³•è¿›è¡Œå®é™…çš„ç¼–ç è½¬æ¢
            3. æ”¯æŒè®­ç»ƒæ¨¡å¼å’Œé¢„æµ‹æ¨¡å¼çš„ä¸åŒå¤„ç†é€»è¾‘

            å‚æ•°è¯´æ˜ï¼š
            - classify_type: å•åˆ—åˆ†ç±»ï¼Œå¦‚["æ€§åˆ«", "å­¦å†"]
            - classify_type2: å¤šåˆ—ç»„åˆåˆ†ç±»ï¼Œå¦‚ [["çœä»½", "åŸå¸‚"], ["éƒ¨é—¨", "èŒä½"]]
            - is_pre=Trueæ—¶ä¸ºé¢„æµ‹æ¨¡å¼ï¼Œä¼šåŠ è½½å·²æœ‰çš„ç¼–ç å­—å…¸
            - is_pre=Falseæ—¶ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œä¼šåˆ›å»ºæ–°çš„ç¼–ç å­—å…¸å¹¶ä¿å­˜
            """
            # æ£€éªŒclassify_type2å‚æ•°ï¼Œå¦‚æœä¸ä¸ºç©ºæˆ–Noneï¼Œåˆ™å…¶å…ƒç´ å¿…é¡»ä¸ºåˆ—è¡¨
            if classify_type2 is not None and classify_type2:
                for i, item in enumerate(classify_type2):
                    if not isinstance(item, list):
                        raise ValueError(f"classify_type2çš„ç¬¬{i+1}ä¸ªå…ƒç´ å¿…é¡»æ˜¯åˆ—è¡¨ï¼Œä½†å¾—åˆ°äº†{type(item)}: {item}")

            # ç¡®ä¿classify_type2ä¸ºåˆ—è¡¨ç±»å‹ï¼ˆé¿å…Noneå€¼ä¼ é€’ç»™åº•å±‚æ–¹æ³•ï¼‰
            classify_type2 = classify_type2 or []

            df = DataDeal.col2index(df,
                    identity=identity, classify_type=classify_type, classify_type2=classify_type2,
                    dict_file=dict_file, is_pre=is_pre,
                    word2id=word2id, start_index=start_index)
            return df  
        
        @classmethod
        def tonum_label_encoding(cls, df, identity=[], classify_type=[], file_path=None,
                                is_pre=False, force_rewrite=False):
            """
            å¯¹åˆ†ç±»åˆ—è¿›è¡ŒLabelEncoderç¼–ç ï¼Œæ”¯æŒè®­ç»ƒå’Œé¢„æµ‹æ¨¡å¼

            Args:
                df (pd.DataFrame): è¾“å…¥çš„æ•°æ®è¡¨
                identity (list): æ ‡è¯†åˆ—åˆ—è¡¨ï¼Œä¸å‚ä¸ç¼–ç ï¼Œé»˜è®¤ä¸ºç©º
                classify_type (list): éœ€è¦ç¼–ç çš„åˆ†ç±»åˆ—ååˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºæˆ–Noneåˆ™è‡ªåŠ¨æ¨æ–­
                file_path (str): ç¼–ç å­—å…¸ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä¿å­˜/åŠ è½½å­—å…¸
                is_pre (bool): æ˜¯å¦ä¸ºé¢„æµ‹æ¨¡å¼ï¼Œé»˜è®¤ä¸ºFalseï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
                force_rewrite (bool): æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒç¼–ç å™¨ï¼Œé»˜è®¤ä¸ºFalse

            Returns:
                pd.DataFrame: ç¼–ç åçš„æ•°æ®è¡¨

            å¤„ç†é€»è¾‘ï¼š
            1. è®­ç»ƒæ¨¡å¼(is_pre=False)ï¼š
            - å¦‚æœç¼–ç å­—å…¸æ–‡ä»¶å­˜åœ¨ä¸”force_rewrite=Falseï¼ŒåŠ è½½å­—å…¸å¹¶åº”ç”¨
            - å¦åˆ™é‡æ–°è®­ç»ƒç¼–ç å™¨ï¼Œä¿å­˜å­—å…¸å¹¶åº”ç”¨

            2. é¢„æµ‹æ¨¡å¼(is_pre=True)ï¼š
            - å¦‚æœforce_rewrite=Trueï¼Œåˆ™é‡æ–°è®­ç»ƒç¼–ç å™¨ï¼ˆå¿½ç•¥is_preï¼‰
            - å¦åˆ™åªåŠ è½½å¹¶åº”ç”¨ç°æœ‰ç¼–ç å™¨ï¼Œä¸é‡æ–°è®­ç»ƒ

            3. å¦‚æœfile_pathä¸ºNoneï¼Œå§‹ç»ˆè¿›è¡Œè®­ç»ƒä½†ä¸ä¿å­˜å­—å…¸

            4. å¦‚æœclassify_typeä¸ºç©ºæˆ–Noneï¼Œè‡ªåŠ¨æ¨æ–­ç±»åˆ«åˆ—ï¼š
            - é€‰æ‹©dfä¸­æ‰€æœ‰éæ•°å­—åˆ—ä½œä¸ºç±»åˆ«åˆ—
            - æ’é™¤identityä¸­æŒ‡å®šçš„æ ‡è¯†åˆ—
            """
            # è‡ªåŠ¨æ¨æ–­ç±»åˆ«åˆ—ï¼ˆå¦‚æœclassify_typeä¸ºç©ºï¼‰
            if not classify_type:
                # è·å–æ‰€æœ‰éæ•°å­—åˆ—
                non_numeric_cols = df.select_dtypes(exclude=['number']).columns.tolist()

                # æ’é™¤identityä¸­çš„åˆ—
                identity_set = set(identity) if identity else set()
                classify_type = [col for col in non_numeric_cols if col not in identity_set]

                if classify_type:
                    print(f"è‡ªåŠ¨æ¨æ–­ç±»åˆ«åˆ—ï¼š{classify_type}")
                else:
                    print("æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç±»åˆ«åˆ—è¿›è¡Œç¼–ç ")
                    return df

            # æ£€æŸ¥åˆ†ç±»åˆ—æ˜¯å¦å­˜åœ¨äºDataFrameä¸­
            valid_cols = [col for col in classify_type if col in df.columns]
            if len(valid_cols) != len(classify_type):
                missing_cols = set(classify_type) - set(valid_cols)
                print(f"è­¦å‘Šï¼šä»¥ä¸‹åˆ—ä¸å­˜åœ¨äºDataFrameä¸­ï¼š{missing_cols}")
                classify_type = valid_cols
                if not classify_type:
                    return df

            # æ ¹æ®æ¨¡å¼å†³å®šå¤„ç†é€»è¾‘
            should_load = (file_path and os.path.exists(file_path) and
                        not force_rewrite and is_pre)

            if should_load:
                # é¢„æµ‹æ¨¡å¼ï¼šåŠ è½½ç°æœ‰ç¼–ç å­—å…¸
                try:
                    label_encoding_dict = pkl_load(file_path)
                    cls._apply_existing_encoders(df, classify_type, label_encoding_dict)
                    print("é¢„æµ‹æ¨¡å¼ï¼šå·²åŠ è½½ç°æœ‰ç¼–ç å­—å…¸")
                except Exception as e:
                    print(f"åŠ è½½ç¼–ç å­—å…¸å¤±è´¥ï¼Œé‡æ–°è®­ç»ƒç¼–ç å™¨ï¼š{e}")
                    force_rewrite = True  # è®¾ç½®ä¸ºé‡æ–°è®­ç»ƒ

            if force_rewrite or not should_load:
                # è®­ç»ƒæ¨¡å¼æˆ–å¼ºåˆ¶é‡å†™ï¼šè®­ç»ƒæ–°çš„ç¼–ç å™¨
                mode = "å¼ºåˆ¶é‡å†™" if force_rewrite else ("é¢„æµ‹æ¨¡å¼ï¼ˆé‡è®­ç»ƒï¼‰" if is_pre else "è®­ç»ƒæ¨¡å¼")
                print(f"{mode}ï¼šè®­ç»ƒæ–°çš„ç¼–ç å™¨")

                label_encoding_dict = cls._train_new_encoders(df, classify_type)

                # ä¿å­˜ç¼–ç å­—å…¸ï¼ˆå¦‚æœæŒ‡å®šäº†æ–‡ä»¶è·¯å¾„ï¼‰
                if file_path:
                    try:
                        pkl_save(label_encoding_dict, file_path=file_path)
                        print(f"ç¼–ç å­—å…¸å·²ä¿å­˜è‡³ï¼š{file_path}")
                    except Exception as e:
                        print(f"ä¿å­˜ç¼–ç å­—å…¸å¤±è´¥ï¼š{e}")

            return df



    
    
        @classmethod
        def norm_min_max_scaler(cls, X, num_type=[], 
                                model_path=f"min_max_scaler.pkl", 
                                is_train=True):
            if is_train:
                df = DataDeal.min_max_scaler(X, 
                                num_type=num_type, 
                                model_path=model_path, 
                                reuse=True, 
                                col_sort=True, 
                                force_rewrite=True)
            else:
                df = DataDeal.min_max_scaler(X, 
                                num_type=num_type, 
                                model_path=model_path, 
                                reuse=True, 
                                col_sort=True)
            return df  

        @classmethod
        def getXy(cls,
                data_path, heads, str_identity,
                alg_type, model_ai_dir, model_num, file_num,
                is_train, label_name, pc:ParamConfig, drop_columns,
                is_categorical_func_type=None, date_type=None, 
                sep='~',classify_type2 = [[]],bool_type = []):
            df = cls.read_csv(data_path, sep=sep, heads=heads)
            
            pass 
        
        @classmethod
        def processing(cls, 
                    df, str_identity,
                    alg_type, model_ai_dir, model_num, file_num,
                    is_train, label_name, pc:ParamConfig, drop_columns,
                    date_type, col_types):
            """
            é€šç”¨æ•°æ®å¤„ç†ç®¡é“

            Args:
                data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
                model_title (str): æ¨¡å‹æ ‡é¢˜
                str_identity (str): æ ‡è¯†åˆ—
                alg_type (str): ç®—æ³•ç±»å‹
                model_ai_dir (str): æ¨¡å‹ä¿å­˜ç›®å½•
                model_num (int): æ¨¡å‹ç¼–å·
                file_num (int): æ–‡ä»¶ç¼–å·
                is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒ
                label_name (str): æ ‡ç­¾åˆ—å
                pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
                drop_columns (list): è¦åˆ é™¤çš„åˆ—
                is_categorical_func_type (str): åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°ç±»å‹ï¼Œ'general'æˆ–'tra'
                date_type (list, optional): æ—¥æœŸç±»å‹åˆ—åˆ—è¡¨
                sep (str): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º'~'

            Returns:
                tuple: (å¤„ç†åçš„DataFrame, æ ‡ç­¾åˆ—, å‚æ•°é…ç½®å¯¹è±¡)
            """
    
            # 4. åˆ†ææ•°å€¼åˆ—
            num_small = cls._analyze_numeric_columns(df, pc)

            # 5. è®¾ç½®å‚æ•°é…ç½®
            cls._setup_param_config(pc, str_identity, col_types, num_small, alg_type,
                                model_ai_dir, model_num, file_num, is_train,
                                label_name, drop_columns, date_type)

            # 6. è®°å½•æ•°æ®ä¿¡æ¯
            cls._log_data_info(pc, num_small)

            # 7. å¤„ç†æ•°æ®
            df_processed = cls._process_data_with_deal_dl(df, pc)

            return df_processed, pc
        
        @classmethod
        def deal(cls, 
                    data_path, model_title, str_identity,
                    alg_type, model_ai_dir, model_num, file_num,
                    is_train, label_name, pc:ParamConfig, drop_columns,
                    is_categorical_func_type=None, date_type=None, 
                    sep='~',classify_type2 = [[]],bool_type = []):
            pass 
            
    
class DataToFeature:
    """
    æ•°æ®å¤„ç†ç±» - å°è£…æ•°æ®è®­ç»ƒå’Œäº¤æ˜“å¤„ç†çš„ç›¸å…³æ–¹æ³•

    æä¾›ç»Ÿä¸€çš„æ•°æ®å¤„ç†æ¥å£ï¼Œæ”¯æŒé€šç”¨æ•°æ®å¤„ç†å’Œäº¤æ˜“ç‰¹å®šæ•°æ®å¤„ç†
    """

    def __init__(self,df = None,y=None,col_types=None):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†ç±»"""
        self.df = df 
        self.y = y
        self.col_types = col_types
        self.cat_func = None 

    def _get_usecols(self, model_title=None, sep='~'):
        """
        æ ¹æ®æ¨¡å‹æ ‡é¢˜è·å–è¦ä½¿ç”¨çš„åˆ—

        Args:
            model_title (str): æ¨¡å‹æ ‡é¢˜ï¼Œ'all'è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰åˆ—ï¼Œå¦åˆ™æŒ‰'~'åˆ†å‰²

        Returns:
            list or None: è¦ä½¿ç”¨çš„åˆ—åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰åˆ—
        """
        if model_title == 'all':
            return None
        else:
            return model_title.split(sep)

    def _categorical_function_reg(self):
        """
        åˆ›å»ºé€šç”¨çš„åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°,é€šè¿‡æ­£åˆ™åŒ¹é…

        Returns:
            function: åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°
        """
        def is_categorical(col: str) -> bool:
            return col.lower().startswith(('is_', 'has_', 'with_'))
        return is_categorical
    
    def set_categorical_function(self,cat_func):
        """
        åˆ›å»ºé€šç”¨çš„åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°,é€šè¿‡æ­£åˆ™åŒ¹é…

        Returns:
            function: åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°
        """
        self.cat_func = cat_func
 

    def _categorical_function_in(self,cls_cols=[]):
        """
        æŒ‡å®šåˆ—åˆ›å»ºåˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°,ä¸åˆ†åŒºå¤§å°å†™

        Returns:
            function: åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°
        """
        def is_categorical(col: str) -> bool:
            cls_cols2 = [col.lower() for col in cls_cols]
            return col.lower() in cls_cols2
        return is_categorical
    
    def _categorical_function_not_num_date(self,df,num_type=[],date_type=[]):
        """
        åˆ›å»ºåˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°ï¼Œåˆ¤æ–­é€»è¾‘ä¸ºæ’é™¤æ•°å€¼åˆ—å’Œæ—¥æœŸåˆ—

        å¦‚æœnum_typeä¸ºç©ºï¼Œåˆ™é€‰æ‹©pandasæ•°è¡¨dfä¸­ç±»å‹ä¸ºnumberçš„åˆ—ï¼Œ
        å¦‚æœdate_typeä¸ºç©ºæˆ–Noneï¼Œåˆ™è‡ªåŠ¨æ¨æ–­dfä¸­æ—¥æœŸç±»å‹çš„åˆ—ä½œä¸ºæ—¥æœŸåˆ—ï¼Œ
        åŒæ—¶æ’é™¤ç±»å‹ä¸ºæ—¥æœŸçš„åˆ—ä»¥åŠdate_typeä¸­æŒ‡å®šçš„åˆ—

        Args:
            df (pd.DataFrame): è¾“å…¥çš„æ•°æ®è¡¨
            num_type (list): æŒ‡å®šçš„æ•°å€¼åˆ—åˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºåˆ™è‡ªåŠ¨æ¨æ–­
            date_type (list): æŒ‡å®šçš„æ—¥æœŸåˆ—åˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºåˆ™è‡ªåŠ¨æ¨æ–­

        Returns:
            function: åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°
        """
        # å¦‚æœnum_typeä¸ºç©ºï¼Œè‡ªåŠ¨æ¨æ–­æ•°å€¼åˆ—
        if num_type is None or len(num_type) == 0:
            num_type = df.select_dtypes('number').columns.tolist()

        # å¦‚æœdate_typeä¸ºç©ºæˆ–Noneï¼Œè‡ªåŠ¨æ¨æ–­æ—¥æœŸåˆ—
        if date_type is None or len(date_type) == 0:
            # ä½¿ç”¨pd.api.types.is_datetime64_any_dtypeè‡ªåŠ¨æ¨æ–­æ—¥æœŸåˆ—
            date_type = [col for col in df.columns
                        if pd.api.types.is_datetime64_any_dtype(df[col])]

        # è·å–æ‰€æœ‰åˆ—å
        col_all = df.columns.tolist()

        # æ’é™¤æ•°å€¼åˆ—å’ŒæŒ‡å®šçš„æ—¥æœŸåˆ—
        exclude_cols = set(num_type) | set(date_type)
        categorical_cols = list(set(col_all) - exclude_cols)

        # åˆ›å»ºåˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°
        def is_categorical(col: str) -> bool:
            return col in categorical_cols

        return is_categorical
    

    def _load_and_classify_data(self, 
                data_path, label_name, str_identity, is_train, 
                usecols, drop_columns, 
                is_categorical_func, sep='~',date_type=[],bool_type = []):
        """
        åŠ è½½æ•°æ®å¹¶è¿›è¡Œåˆ†ç±»çš„é€šç”¨æ–¹æ³•

        Args:
            data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
            label_name (str): æ ‡ç­¾åˆ—å
            str_identity (str): æ ‡è¯†åˆ—
            is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒæ•°æ®
            usecols (list): è¦ä½¿ç”¨çš„åˆ—
            drop_columns (list): è¦åˆ é™¤çš„åˆ—
            is_categorical_func (function): åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°
            sep (str): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º'~'

        Returns:
            tuple: (DataFrame, labelåˆ—, åˆ—ç±»å‹å­—å…¸)
        """
        df, y, col_types = DataDeal.getXy(data_path, label_name,
                                    identity_cols=str_identity, sep=sep,
                                    is_train=is_train, usecols=usecols,
                                    drop_columns=drop_columns,
                                    dtype_mapping=None,
                                    is_categorical_func=is_categorical_func,
                                    date_type=date_type,
                                    bool_type=bool_type)
        return df, y, col_types

    def _analyze_numeric_columns(self, df, pc, threshold=100):
        """
        åˆ†ææ•°å€¼åˆ—ï¼Œæ‰¾å‡ºæœ€å¤§å€¼å°äºé˜ˆå€¼çš„åˆ—

        Args:
            df (DataFrame): æ•°æ®æ¡†
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            threshold (int): é˜ˆå€¼ï¼Œé»˜è®¤ä¸º100

        Returns:
            list: å°äºé˜ˆå€¼çš„åˆ—ååˆ—è¡¨
        """
        num_small = DataDeal.columns_by_max_value(df, condition='less', threshold=threshold)
        pc.lg(f"num_small num:{len(num_small)}")
        if len(num_small) > 0:
            DataDeal.num_describe(df[num_small], pc)
            return num_small
        else:
            return []

    def _setup_param_config(self, pc:ParamConfig, str_identity, col_types, num_small, alg_type,
                           model_ai_dir, model_num, file_num, is_train,
                           label_name, drop_columns, date_type=None,classify_type2 = [[]],bool_type = [] ):
        """
        è®¾ç½®å‚æ•°é…ç½®å¯¹è±¡çš„é€šç”¨æ–¹æ³•

        Args:
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            str_identity (str): æ ‡è¯†åˆ—
            col_types (dict): åˆ—ç±»å‹å­—å…¸
            num_small (list): å°æ•°å€¼åˆ—åˆ—è¡¨
            alg_type (str): ç®—æ³•ç±»å‹
            model_ai_dir (str): æ¨¡å‹ä¿å­˜ç›®å½•
            model_num (int): æ¨¡å‹ç¼–å·
            file_num (int): æ–‡ä»¶ç¼–å·
            is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒ
            label_name (str): æ ‡ç­¾åˆ—å
            drop_columns (list): è¦åˆ é™¤çš„åˆ—
            date_type (list, optional): æ—¥æœŸç±»å‹åˆ—åˆ—è¡¨
        """
        # DataDealDL.data_dealéœ€è¦çš„12ä¸ªå‚æ•°
        pc.col_type.identity       = str_identity
        pc.col_type.num_type       = col_types["num_type"]
        pc.col_type.num_small      = num_small
        pc.col_type.classify_type  = col_types["classify_type"]
        pc.col_type.classify_type2 = classify_type2  #ä¸€ç»„ç±»åˆ«ä½¿ç”¨åŒä¸€ä¸ªå­—å…¸
        pc.col_type.date_type      = date_type if date_type is not None else []
        pc.col_type.bool_type      = bool_type
        pc.alg_type                = alg_type
        pc.model_save_dir          = model_ai_dir
        pc.model_num               = model_num
        pc.file_num                = file_num   #ç¬¬å‡ ä¸ªæ–‡ä»¶,é»˜è®¤1
        pc.is_train                = is_train

        #å…¶ä»–å‚æ•°
        pc.label_name              = label_name
        pc.drop_cols               = drop_columns

    def _log_data_info(self, pc:ParamConfig, num_small):
        """
        è®°å½•æ•°æ®ä¿¡æ¯çš„é€šç”¨æ–¹æ³•

        Args:
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            num_small (list): å°æ•°å€¼åˆ—åˆ—è¡¨
        """
        pc.lg(pc.col_type.num_type[:3])
        pc.lg(f"num_small num:{len(num_small)},num type num:{len(pc.col_type.num_type)}")
        pc.lg(pc.col_type.classify_type[:3])
        pc.lg(f"is_merge_identity:{pc.is_merge_identity}")

    def _process_data_with_deal_dl(self, df, pc:ParamConfig):
        """
        ä½¿ç”¨DataDealDLå¤„ç†æ•°æ®çš„é€šç”¨æ–¹æ³•

        Args:
            df (DataFrame): æ•°æ®æ¡†
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡

        Returns:
            DataFrame: å¤„ç†åçš„æ•°æ®æ¡†
        """
        df_processed = DataDealDL.data_deal(df, pc)
        return df_processed
    
    

    def _processing_pipeline(self, 
                data_path, model_title, str_identity,
                alg_type, model_ai_dir, model_num, file_num,
                is_train, label_name, pc:ParamConfig, drop_columns,
                is_categorical_func_type=None, date_type=None, 
                sep='~',classify_type2 = [[]],bool_type = []):
        """
        é€šç”¨æ•°æ®å¤„ç†ç®¡é“

        Args:
            data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
            model_title (str): æ¨¡å‹æ ‡é¢˜
            str_identity (str): æ ‡è¯†åˆ—
            alg_type (str): ç®—æ³•ç±»å‹
            model_ai_dir (str): æ¨¡å‹ä¿å­˜ç›®å½•
            model_num (int): æ¨¡å‹ç¼–å·
            file_num (int): æ–‡ä»¶ç¼–å·
            is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒ
            label_name (str): æ ‡ç­¾åˆ—å
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            drop_columns (list): è¦åˆ é™¤çš„åˆ—
            is_categorical_func_type (str): åˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°ç±»å‹ï¼Œ'general'æˆ–'tra'
            date_type (list, optional): æ—¥æœŸç±»å‹åˆ—åˆ—è¡¨
            sep (str): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º'~'

        Returns:
            tuple: (å¤„ç†åçš„DataFrame, æ ‡ç­¾åˆ—, å‚æ•°é…ç½®å¯¹è±¡)
        """
        if self.df is None:
            # 1. è·å–è¦ä½¿ç”¨çš„åˆ—
            usecols = self._get_usecols(model_title)

            # 2. åˆ›å»ºåˆ†ç±»åˆ—åˆ¤æ–­å‡½æ•°
            if is_categorical_func_type is None:
                is_categorical_func = self.cat_func
            elif is_categorical_func_type == 'general':
                is_categorical_func = self._categorical_function_reg()
            elif is_categorical_func_type == 'tra':
                is_categorical_func = self._categorical_function_in()
            else:
                raise ValueError(f"æœªçŸ¥çš„is_categorical_func_type: {is_categorical_func_type}")

            # 3. åŠ è½½æ•°æ®å¹¶åˆ†ç±»
            # print("data_path:",data_path)
            df, y, col_types = self._load_and_classify_data(
                data_path, label_name, str_identity, is_train,
                usecols, drop_columns, is_categorical_func, sep, date_type, bool_type
            )
            self.df = df 
            self.y  = y 
            self.col_types = col_types
            
            self.lg(f"classify_data----------------------")
            self.lg(f"col_types['date_type'] len = {len(col_types['date_type'])}")
            self.lg(f"col_types['num_type'] len = {len(col_types['num_type'])}")
            self.lg(f"col_types['classify_type'] len = {len(col_types['classify_type'])}")
            self.lg(f"df[:3]:\n{df[:3]}")

        # 4. åˆ†ææ•°å€¼åˆ—
        num_small = self._analyze_numeric_columns(self.df, pc)

        # 5. è®¾ç½®å‚æ•°é…ç½®
        self._setup_param_config(pc, str_identity, col_types, num_small, alg_type,
                               model_ai_dir, model_num, file_num, is_train,
                               label_name, drop_columns, date_type)

        # 6. è®°å½•æ•°æ®ä¿¡æ¯
        self._log_data_info(pc, num_small)

        # 7. å¤„ç†æ•°æ®
        df_processed = self._process_data_with_deal_dl(df, pc)

        return df_processed, y, pc

    def deal(self, data_path, model_title, str_identity,
                       alg_type, model_ai_dir, model_num, file_num=1,
                       is_train=True, label_name=None, pc:ParamConfig=None,
                       drop_columns=None, date_type=[], sep='~',
                       classify_type2 = [[]],bool_type = [], is_categorical_func_type=None):
        """
        é€šç”¨æ•°æ®è®­ç»ƒå¤„ç†æ–¹æ³• - é‡æ„ç‰ˆæœ¬

        ä½¿ç”¨é€šç”¨æ•°æ®å¤„ç†ç®¡é“æ¥å¤„ç†è®­ç»ƒæ•°æ®ï¼Œç®€åŒ–ä»£ç å¹¶æé«˜å¯ç»´æŠ¤æ€§

        Args:
            data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
            model_title (str): æ¨¡å‹æ ‡é¢˜
            str_identity (str): æ ‡è¯†åˆ—
            alg_type (str): ç®—æ³•ç±»å‹
            model_ai_dir (str): æ¨¡å‹ä¿å­˜ç›®å½•
            model_num (int): æ¨¡å‹ç¼–å·
            file_num (int): æ–‡ä»¶ç¼–å·ï¼Œé»˜è®¤ä¸º1
            is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒï¼Œé»˜è®¤ä¸ºTrue
            label_name (str): æ ‡ç­¾åˆ—å
            pc (ParamConfig): å‚æ•°é…ç½®å¯¹è±¡
            drop_columns (list): è¦åˆ é™¤çš„åˆ—
            date_type (list, optional): æ—¥æœŸç±»å‹åˆ—åˆ—è¡¨
            sep (str): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸º'~'

        Returns:
            tuple: (å¤„ç†åçš„DataFrame, æ ‡ç­¾åˆ—, å‚æ•°é…ç½®å¯¹è±¡)
        """
        self.lg = pc.lg
        return self._processing_pipeline(
            data_path=data_path,
            model_title=model_title,
            str_identity=str_identity,
            alg_type=alg_type,
            model_ai_dir=model_ai_dir,
            model_num=model_num,
            file_num=file_num,
            is_train=is_train,
            label_name=label_name,
            pc=pc,
            drop_columns=drop_columns,
            is_categorical_func_type=is_categorical_func_type,
            date_type=date_type,
            sep=sep
        )

    
#------------------------------------------------------------------
# æ–‡æœ¬å¤„ç†
#------------------------------------------------------------------
import numpy as np 
import pandas as pd 
from tpf import read,write
# from tpf.data.deal import DataDeal as dtl

class TextDeal:
    
    def __init__(self, data) -> None:
        """æ–‡æœ¬å¤„ç†æ–¹æ³•é›† 
        - data: pandasæ•°è¡¨ 
        """
        self.data = data 
        
    def log(self,msg, print_level=1):
        if self.print_level >= print_level:
            print(msg)
        
        
    def get_data(self):
        return self.data 
    
    def update_data(self,data):
        self.data = data 
        
    def head(self,num):
        return self.data.head(num)
        

    def word2id(self, c_names, word2id=None, start_index=1):
        """æ–‡æœ¬è½¬æ¢æˆç´¢å¼•
        - c_names:åˆ—å
        - word2id:ç¼–ç å­—å…¸ï¼Œkeyä¸ºå…³é”®å­—ï¼Œvalueä¸ºè¿ç»­çš„æ•´æ•°ç´¢å¼•ï¼›è‹¥éNoneï¼Œåˆ™åœ¨è¯¥å­—å…¸åŸºæœ¬ä¸Šæ·»åŠ æ–°çš„keyä¸index
        - start_index:å¼€å§‹ç´¢å¼•ç¼–ç ï¼Œé»˜è®¤ä¸º1ï¼Œå› ä¸º0ç»™äº†æœªçŸ¥ç±»åˆ«'<UNK>'

        return
        -----------------------------
        æ¯ä¸ªåˆ—çš„ç¼–ç å­—å…¸,'<UNK>':0ï¼Œå³æ¯ä¸€åˆ—çš„ç´¢å¼•0ä»£è¡¨æœªè®°å½•çš„è¯

        """
    
        cls_dict = {'<UNK>': 0}
        global_word2id = {'<UNK>': 0} if word2id is None or len(word2id)==0 else word2id.copy()
        next_index = start_index if len(global_word2id) == 1 else max(global_word2id.values()) + 1
        
        # é¦–å…ˆæ”¶é›†æ‰€æœ‰åˆ—çš„æ‰€æœ‰å”¯ä¸€è¯æ±‡
        all_words = set()
        for cname in c_names:
            all_words.update(set(self.data[cname]))
        
        # ä¸ºæ‰€æœ‰è¯æ±‡åˆ›å»ºå…¨å±€æ˜ å°„
        for word in all_words:
            if word not in global_word2id:
                global_word2id[word] = next_index
                next_index += 1
        
        # ä¸ºæ¯åˆ—åˆ›å»ºå•ç‹¬çš„æ˜ å°„å­—å…¸ï¼ˆåŸºäºå…¨å±€æ˜ å°„ï¼‰
        for cname in c_names:
            cls_dict[cname] = global_word2id
        
        # åº”ç”¨æ˜ å°„åˆ°æ¯åˆ—
        for col in c_names:
            self.data[col] = (
                self.data[col]
                    .map(cls_dict[col])          # å·²çŸ¥ç±»åˆ« â†’ ç´¢å¼•ï¼ŒæœªçŸ¥ â†’ NaN
                    .fillna(0)                   # NaN â†’ 0
                    .astype(np.int64)            # è½¬æ¢ä¸ºæ•´æ•°
            )
            
        return self.data, cls_dict

    
    
    def word2id_pre(self, c_names, word2id=None):
        """
        é¢„æµ‹æ—¶å°†æŒ‡å®šåˆ—ä¸­çš„ç±»åˆ«è½¬æˆç´¢å¼•ã€‚
        æœªçŸ¥ç±»åˆ«ç»Ÿä¸€æ˜ å°„ä¸º 0ã€‚
        
        Parameters
        ----------
        c_names : list[str]
            éœ€è¦è½¬æ¢çš„åˆ—ååˆ—è¡¨ã€‚
        word2id : dict, optional
            ç±»åˆ«åˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸ã€‚è‹¥æœªæä¾›ï¼Œåˆ™æ‰€æœ‰å€¼è§†ä¸ºæœªçŸ¥ï¼Œå…¨éƒ¨å¡« 0ã€‚
        """
        if word2id is None:
            word2id = {}

        # ç”¨ 0 ä½œä¸ºé»˜è®¤å€¼ï¼Œä¸€æ¬¡æ€§å®Œæˆæ˜ å°„
        for col in c_names:
            self.data[col] = (
                self.data[col]
                    .map(word2id[col])          # å·²çŸ¥ç±»åˆ« â†’ ç´¢å¼•ï¼ŒæœªçŸ¥ â†’ NaN
                    .fillna(0)             # NaN â†’ 0
                    .astype("int32")       # æˆ– Int64 ä»¥ä¿ç•™ç¼ºå¤±ï¼Œä½†è¿™é‡Œç»Ÿä¸€ç”¨ 0
            )
            
    
    def col_filter(self,regex):
        """
        é€‰æ‹©æŒ‡å®šçš„åˆ—,ä¸åŒçš„åˆ—ä»¥|åˆ†éš”,"name|age",
        "ä¸€å…ƒ.*" åŒ¹é… "ä¸€å…ƒä¸€æ¬¡","ä¸€å…ƒäºŒæ¬¡"ç­‰æ‰€æœ‰ä»¥"ä¸€å…ƒ"å¼€å¤´çš„å­—ç¬¦ä¸² 
        """
        self.data = self.data.filter(regex=regex)
        self.log("æ•°æ®è¿‡æ»¤ä¹‹åçš„åˆ—-------------------------:",2)
        self.log(self.data.info(),2)

    def empty_num(self,col_name):
        self.data.loc[(self.data[col_name].isnull()), col_name] = np.mean(self.data[col_name])

    def empty_str(self,col_name,char_null="N"):
        self.data.loc[(self.data[col_name].isnull()), col_name] = char_null

    def error_max_7mean(self,col_name):
        """
        è¶…è¿‡å‡å€¼7å€çš„æ•°æ®è½¬ä¸ºå‡å€¼7å€
        """
        col_mean = np.mean(self.data[col_name])
        self.data[col_name][self.data[col_name]>7*col_mean] = 7*col_mean
    
    
    def onehot_encoding(self,c_names):
        """pandas onehotç¼–ç ï¼Œæ¯ä¸ªç±»åˆ«ä¸€ä¸ªæ–°åˆ—
        """
        for cname in c_names:
            c_new_1 = pd.get_dummies(self.data[cname], prefix=cname)
            self.data = pd.concat([self.data,c_new_1],axis=1)
            self.data.drop([cname], axis=1, inplace=True)

    def col_drop(self,c_names):
        self.data.drop(c_names,axis=1,inplace=True)

    def replace_blank(self,to_float=True):
        """
        å»é™¤ç©ºæ ¼ï¼Œå¹¶å°†NILç½®0
        """
        for col in self.columns():
            index = 0
            for val in self.data[col]:
                # print("data type :",type(val))
                if isinstance(val,str):
                    matchObj = re.search( r'\s+', val)

                    if to_float:
                        # print("---col:{},val--{}==".format(col,val))
                        if val == "NIL":
                            val = "0"
                        if matchObj:
                            self.data[col].iloc[index] = float(val.replace('\s+','',regex=True,inplace=True))
                        else:
                            self.data[col].iloc[index] = float(val)
                    else:
                        if matchObj:
                            self.data[col].iloc[index] = val.replace('\s+','',regex=True,inplace=True)
                else:
                    continue
                index +=1



    def min_max_scaler(self,feature_range=(0, 1)):
        """
        return
        ---------------------
        <class 'numpy.ndarray'>,MinMaxScalerè‡ªåŠ¨å°†pandas.core.frame.DataFrameè½¬ä¸ºäº†numpy.ndarray
        
        """
        self.scaler = MinMaxScaler(feature_range=feature_range)
        self.replace_blank()
        data = self.scaler.fit_transform(self.data)
        return data 

    def min_max_scaler_inverse(self, data):
        data = self.scaler.inverse_transform(data)
        return data 


class TextEmbedding:
    cls_dict = {}
    def __init__(self):
        pass
    
    @classmethod
    def cls2index(cls,df, classify_type=[],word2id=None,start_index=1):
        """ç±»åˆ«è½¬ç´¢å¼•"""
        DataDeal.str_pd(df,classify_type)
        tt = TextDeal(data=df)
        for cc in classify_type:
            df,cls_dict = tt.word2id([cc],word2id=word2id,start_index=start_index)
            cls.cls_dict.update(cls_dict)
        return  df 
    
    @classmethod
    def cls2index2(cls,df, classify_type=[],word2id=None,start_index=1):
        """ç±»åˆ«è½¬ç´¢å¼•"""
        DataDeal.str_pd(df,classify_type)
        tt = TextDeal(data=df)

        df,cls_dict = tt.word2id(classify_type,word2id=word2id,start_index=start_index)
        cls.cls_dict.update(cls_dict)
        return  df 
    
    @classmethod
    def cls2index_pre(cls,df, classify_type, word2id):
        """ç±»åˆ«è½¬ç´¢å¼•é¢„æµ‹"""
        DataDeal.str_pd(df,classify_type)
        tt = TextDeal(data=df)
        tt.word2id_pre(classify_type,word2id=word2id)


    @classmethod
    def col2index(cls,df,classify_type,classify_type2=[],
                  dict_file="dict_file.dict",
                  is_pre=False,word2id=None,start_index=1):
        """
        ç±»åˆ«åˆ—ç´¢å¼•ç¼–ç ï¼šå°†æ–‡æœ¬ç±»åˆ«è½¬æ¢ä¸ºæ•°å€¼ç´¢å¼•ï¼Œæ”¯æŒç‹¬ç«‹ç¼–ç å’Œå…±äº«ç¼–ç 

        ä¸»è¦è®¡ç®—é€»è¾‘ï¼š
        1. è®­ç»ƒ/æ¨ç†æ¨¡å¼åˆ†æ”¯
           - è®­ç»ƒæ¨¡å¼ (is_pre=False)ï¼šåˆ›å»ºæ–°çš„ç¼–ç å­—å…¸å¹¶ä¿å­˜
           - æ¨ç†æ¨¡å¼ (is_pre=True)ï¼šåŠ è½½å·²æœ‰ç¼–ç å­—å…¸è¿›è¡Œè½¬æ¢

        2. æ¨ç†æ¨¡å¼å¤„ç†é€»è¾‘
           - åŠ è½½ç¼–ç å­—å…¸ï¼šä»dict_fileæ–‡ä»¶ä¸­è¯»å–å·²ä¿å­˜çš„word2idæ˜ å°„
           - åˆå¹¶ç¼–ç åˆ—ï¼šå°†classify_type2ä¸­çš„åˆ—åˆå¹¶åˆ°classify_typeä¸­ç»Ÿä¸€å¤„ç†
           - åº”ç”¨å·²æœ‰ç¼–ç ï¼šä½¿ç”¨cls.cls2index_preæ–¹æ³•ï¼ŒåŸºäºç°æœ‰å­—å…¸è¿›è¡Œè½¬æ¢
           - æœªçŸ¥ç±»åˆ«å¤„ç†ï¼šæœªåœ¨å­—å…¸ä¸­çš„ç±»åˆ«ç»Ÿä¸€æ˜ å°„ä¸º0ï¼ˆ'<UNK>'æ ‡è®°ï¼‰

        3. è®­ç»ƒæ¨¡å¼å¤„ç†é€»è¾‘
           - å…±äº«ç¼–ç ç»„å¤„ç†ï¼š
             * éå†classify_type2ä¸­çš„æ¯ä¸ªå…±äº«ç»„ï¼ˆå¦‚[['From', 'To']]ï¼‰
             * è°ƒç”¨cls.cls2index2ä¸ºæ¯ä¸ªå…±äº«ç»„åˆ›å»ºç»Ÿä¸€çš„ç¼–ç ç©ºé—´
             * å¤šåˆ—å…±äº«åŒä¸€å­—å…¸ï¼Œç¡®ä¿ç›¸åŒçš„å€¼åœ¨ä¸åŒåˆ—ä¸­è·å¾—ç›¸åŒç´¢å¼•
             * ä½¿ç”¨ç©ºå­—å…¸{}åˆå§‹åŒ–ï¼Œåˆ›å»ºæ–°çš„ç¼–ç æ˜ å°„

           - ç‹¬ç«‹ç¼–ç å¤„ç†ï¼š
             * è°ƒç”¨cls.cls2indexä¸ºclassify_typeä¸­çš„ç‹¬ç«‹åˆ—åˆ›å»ºç¼–ç 
             * æ¯åˆ—ç‹¬ç«‹çš„ç¼–ç ç©ºé—´ï¼Œä¸åŒåˆ—çš„ç›¸åŒå€¼å¯èƒ½æœ‰ä¸åŒç´¢å¼•
             * è‡ªåŠ¨å¤„ç†'<UNK>'æ ‡è®°ï¼Œç´¢å¼•ä¸º0

        4. ç¼–ç å­—å…¸ä¿å­˜
           - å°†ç”Ÿæˆçš„ç¼–ç å­—å…¸ä¿å­˜åˆ°dict_fileæ–‡ä»¶
           - å­—å…¸æ ¼å¼ï¼š{åˆ—å: {ç±»åˆ«å€¼: ç´¢å¼•}} æˆ– {å…±äº«ç»„å: {ç±»åˆ«å€¼: ç´¢å¼•}}
           - æ”¯æŒå¢é‡æ›´æ–°ï¼šæ–°æ•°æ®ä¼šè¢«æ·»åŠ åˆ°ç°æœ‰å­—å…¸ä¸­

        5. å†…å­˜ç®¡ç†
           - ä½¿ç”¨TextDealç±»è¿›è¡Œå®é™…çš„ç¼–ç è½¬æ¢
           - è‡ªåŠ¨å¤„ç†å­—ç¬¦ä¸²ç±»å‹è½¬æ¢
           - æ”¯æŒå¤§é‡ç±»åˆ«çš„å†…å­˜é«˜æ•ˆå¤„ç†

        Args:
            df: è¾“å…¥çš„pandas DataFrame
            classify_type: ç‹¬ç«‹ç¼–ç çš„ç±»åˆ«åˆ—åˆ—è¡¨
            classify_type2: å…±äº«ç¼–ç ç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåˆ—åˆ—è¡¨
            dict_file: ç¼–ç å­—å…¸æ–‡ä»¶è·¯å¾„
            is_pre: æ˜¯å¦ä¸ºé¢„å¤„ç†æ¨¡å¼ï¼ˆTrue=æ¨ç†ï¼ŒFalse=è®­ç»ƒï¼‰
            word2id: é¢„åŠ è½½çš„ç¼–ç å­—å…¸ï¼Œæ¨ç†æ¨¡å¼ä½¿ç”¨
            start_index: å¼€å§‹ç´¢å¼•ç¼–ç ï¼Œé»˜è®¤ä¸º1ï¼Œå› ä¸º0ç»™äº†æœªçŸ¥ç±»åˆ«'<UNK>'

        Returns:
            DataFrame: ç±»åˆ«åˆ—è¢«æ›¿æ¢ä¸ºæ•°å€¼ç´¢å¼•åçš„æ•°æ®è¡¨

        Algorithm:
            è®­ç»ƒæ¨¡å¼:
                1. for each shared_group in classify_type2:
                   df = cls2index2(df, shared_group, word2id={})
                2. df = cls2index(df, classify_type)
                3. save encoding dictionary to file

            æ¨ç†æ¨¡å¼:
                1. load encoding dictionary from file
                2. merge classify_type2 into classify_type
                3. df = cls2index_pre(df, classify_type, word2id)

        Example:
            # ç‹¬ç«‹ç¼–ç 
            df_encoded = TextEmbedding.col2index(
                df,
                classify_type=['currency', 'payment_type'],
                dict_file='encoding.dict',
                is_pre=False
            )

            # å…±äº«ç¼–ç ï¼ˆFromå’ŒToåˆ—ä½¿ç”¨åŒä¸€ç¼–ç ç©ºé—´ï¼‰
            df_encoded = TextEmbedding.col2index(
                df,
                classify_type=['transaction_type'],
                classify_type2=[['From', 'To']],
                dict_file='shared_encoding.dict',
                is_pre=False
            )
        """
        if is_pre:
            if word2id is None:
                word2id = read(dict_file)
            for cc in classify_type2:
                classify_type.extend(cc)
            cls.cls2index_pre(df, classify_type=classify_type, word2id=word2id) 
        else: #é‡æ–°ç¼–ç 
            for c in classify_type2:
            ## ç±»åˆ«ç¼–ç æ‰©å……,æœºæ„ä½œä¸ºè´¦æˆ·ç‰¹å¾,pc.col_type.classify_typeä¸èƒ½å†åŒ…å«bankäº†ï¼Œå¦åˆ™ä¼šé‡å¤ç¼–ç 
                df = cls.cls2index2(df, classify_type=c,word2id={},start_index=start_index)

            ## ç±»åˆ«ç´¢å¼•ç¼–ç 
            df = cls.cls2index(df, classify_type=classify_type,start_index=start_index)
            write(cls.cls_dict,dict_file)

# ================== æµ‹è¯•å‡½æ•° ==================

def test_get_col_names_optimization():
    """
    æµ‹è¯•ä¼˜åŒ–åçš„get_col_namesæ–¹æ³•
    """
    import pandas as pd
    import numpy as np
    from datetime import datetime

    print("æµ‹è¯•ä¼˜åŒ–åçš„get_col_namesæ–¹æ³•")
    print("=" * 50)

    # åˆ›å»ºæµ‹è¯•DataFrame
    test_data = {
        'id': [1, 2, 3, 4, 5],
        'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],
        'age': [25, 30, 35, 40, 45],
        'salary': [50000.5, 60000.0, 70000.5, 80000.0, 90000.5],
        'is_active': [True, False, True, True, False],
        'category': pd.Categorical(['A', 'B', 'A', 'C', 'B']),
        'join_date': pd.to_datetime(['2020-01-01', '2019-05-15', '2021-03-10', '2018-11-20', '2022-07-05']),
        'score': np.array([85.5, 92.0, 78.5, 88.0, 95.5], dtype=np.float32)
    }

    df = pd.DataFrame(test_data)

    print("æµ‹è¯•DataFrameä¿¡æ¯:")
    print(f"å½¢çŠ¶: {df.shape}")
    print(f"åˆ—å: {list(df.columns)}")
    print(f"æ•°æ®ç±»å‹:\n{df.dtypes}")
    print()

    # æµ‹è¯•å„ç§col_typeå‚æ•°
    test_cases = [
        ('all', 'æ‰€æœ‰åˆ—'),
        ('num', 'æ•°å€¼ç±»å‹åˆ—'),
        ('int', 'æ•´æ•°ç±»å‹åˆ—'),
        ('float', 'æµ®ç‚¹æ•°ç±»å‹åˆ—'),
        ('cat', 'åˆ†ç±»ç±»å‹åˆ—'),
        ('str', 'å­—ç¬¦ä¸²ç±»å‹åˆ—'),
        ('bool', 'å¸ƒå°”ç±»å‹åˆ—'),
        ('date', 'æ—¥æœŸç±»å‹åˆ—'),
        ('datetime64[ns]', 'datetime64[ns]ç±»å‹åˆ—'),
        ('object', 'objectç±»å‹åˆ—'),
    ]

    for col_type, description in test_cases:
        try:
            result = DataDeal.get_col_names(df, col_type)
            print(f"{description} ({col_type}): {result}")
        except Exception as e:
            print(f"{description} ({col_type}): é”™è¯¯ - {e}")

    print()

    # æµ‹è¯•æ–°å¢çš„è¾…åŠ©æ–¹æ³•
    print("æµ‹è¯•æ–°å¢çš„è¾…åŠ©æ–¹æ³•:")
    print("-" * 30)

    # æµ‹è¯•æŒ‰æ¨¡å¼è·å–åˆ—å
    pattern_result = DataDeal.get_col_names_by_pattern(df, r'.*date.*')
    print(f"åŒ…å«'date'çš„åˆ—: {pattern_result}")

    # æµ‹è¯•ç±»å‹æ±‡æ€»
    type_summary = DataDeal.get_col_types_summary(df)
    print("æ•°æ®ç±»å‹æ±‡æ€»:")
    for dtype, cols in type_summary.items():
        print(f"  {dtype}: {cols}")

    print("\næµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_get_col_names_optimization()
             
