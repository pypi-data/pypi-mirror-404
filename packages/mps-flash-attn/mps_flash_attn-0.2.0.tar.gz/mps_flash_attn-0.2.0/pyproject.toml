[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "mps-flash-attn"
version = "0.2.0"
description = "Flash Attention for PyTorch on Apple Silicon (M1/M2/M3/M4)"
readme = "README.md"
license = "MIT"
authors = [
    {name = "imperatormk"}
]
keywords = ["flash-attention", "apple-silicon", "pytorch", "mps", "metal", "transformer", "attention"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "Operating System :: MacOS :: MacOS X",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
requires-python = ">=3.10"
dependencies = [
    "torch>=2.0.0",
]

[project.urls]
Homepage = "https://github.com/mpsops/mps-flash-attention"
Repository = "https://github.com/mpsops/mps-flash-attention"
Issues = "https://github.com/mpsops/mps-flash-attention/issues"

[tool.setuptools]
packages = ["mps_flash_attn"]
include-package-data = true

[tool.setuptools.package-data]
mps_flash_attn = [
    "lib/*.dylib",
    "kernels/*.metallib",
    "kernels/*.bin",
    "kernels/*.json",
]
