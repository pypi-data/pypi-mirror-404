Metadata-Version: 2.4
Name: hallu-assess
Version: 0.1.0
Summary: Measure how much an LLM’s answers drift to detect hallucination risk and improve prompts for more reliable results.
Author: Ashwin B
License-Expression: MIT
Project-URL: Homepage, https://github.com/ashley1902/hallu-assess
Project-URL: Repository, https://github.com/ashley1902/hallu-assess
Project-URL: Issues, https://github.com/ashley1902/hallu-assess/issues
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy
Requires-Dist: sentence-transformers
Requires-Dist: langchain-core
Requires-Dist: langchain-openai
Requires-Dist: langchain-google-genai
Requires-Dist: langchain-anthropic
Dynamic: license-file

# hallu-assess

Measure how much an LLM’s answers drift to detect hallucination risk and improve prompts for more reliable outputs.

## What it does

`hallu-assess` evaluates hallucination risk by:
- running the same prompt multiple times
- embedding each response
- measuring semantic deviation across outputs

High deviation → higher hallucination risk.

## Installation

```bash
pip install hallu-assess
```
## Detailed Documentation - WIP
