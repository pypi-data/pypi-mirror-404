from PIL import Image
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest
from vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor

from kalorda.utils.logger import logger

from .config import (
    MAX_CONCURRENCY,
    PROMPT,
)
from .process.image_process import DeepseekOCRProcessor


class DeepseekOCRInfer:
    max_tokens = 8192
    max_model_len = 8192
    max_num_batched_tokens = 20480
    gpu_memory_utilization = 0.8

    def __init__(self, model_weights_dir: str, lora_weights_dir: str = None):
        self.engine = LLM(
            model=model_weights_dir,
            trust_remote_code=True,
            block_size=256,
            enforce_eager=False,
            max_model_len=self.max_model_len,
            swap_space=0,
            max_num_seqs=MAX_CONCURRENCY,
            tensor_parallel_size=1,
            gpu_memory_utilization=self.gpu_memory_utilization,
            logits_processors=[NGramPerReqLogitsProcessor],
            enable_lora=lora_weights_dir is not None,
        )

        self.sampling_params = SamplingParams(
            temperature=0.0,
            max_tokens=self.max_tokens,
            extra_args=dict(
                ngram_size=30,
                window_size=90,
                # whitelist: <td>, </td>
                whitelist_token_ids={128821, 128822},
            ),
            skip_special_tokens=False,
            # ignore_eos=False,
        )

        tokenizer = AutoTokenizer.from_pretrained(model_weights_dir, trust_remote_code=True)
        self.processor = DeepseekOCRProcessor(tokenizer=tokenizer)

        self.prompt = PROMPT
        self.lora_request = (
            LoRARequest("deepseek_ocr", 1, lora_local_path=lora_weights_dir) if lora_weights_dir is not None else None
        )

    def generate(self, image_file: str):
        image = self.load_image(image_file)
        if image is None:
            return None

        inputs = [
            {
                "prompt": self.prompt,
                "multi_modal_data": {"image": image},
            }
        ]

        response = self.engine.generate(inputs, self.sampling_params, lora_request=self.lora_request)
        ocr_result = response[0].outputs[0].text
        tokens_count = len(response[0].outputs[0].token_ids)
        return ocr_result, tokens_count

    def load_image(self, image_file: str):
        try:
            image = Image.open(image_file).convert("RGB")
            return image
        except Exception as e:
            return None
