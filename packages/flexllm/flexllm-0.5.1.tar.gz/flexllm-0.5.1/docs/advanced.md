# é«˜çº§ç”¨æ³•

## å¤šæ¨¡æ€å¤„ç†

### MllmClient

å¤„ç†å›¾æ–‡æ··åˆå†…å®¹ï¼š

```python
from flexllm import MllmClient

client = MllmClient(
    base_url="https://api.openai.com/v1",
    api_key="your-key",
    model="gpt-4o",
)

# æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡"},
            {"type": "image_url", "image_url": {"url": "/path/to/image.jpg"}}
        ]
    }
]

# å•æ¡è°ƒç”¨ï¼ˆcall_llm è¿”å›åˆ—è¡¨ï¼‰
results = await client.call_llm([messages])
result = results[0]

# æ‰¹é‡è°ƒç”¨
messages_list = [[msg1], [msg2], ...]  # æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ç»„æ¶ˆæ¯
results = await client.call_llm(messages_list)
```

**æ”¯æŒçš„å›¾åƒæºï¼š**
- æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆè‡ªåŠ¨è½¬ base64ï¼‰
- HTTP/HTTPS URLï¼ˆè‡ªåŠ¨ä¸‹è½½è½¬ base64ï¼‰
- base64 ç¼–ç å­—ç¬¦ä¸²
- PIL Image å¯¹è±¡

### å›¾åƒå¤„ç†å™¨

```python
from flexllm.msg_processors import (
    encode_image_to_base64,
    ImageCacheConfig,
    unified_batch_process_messages,
)

# å•å¼ å›¾ç‰‡ç¼–ç 
base64_data = await encode_image_to_base64("/path/to/image.jpg")

# æ‰¹é‡æ¶ˆæ¯é¢„å¤„ç†ï¼ˆé«˜æ€§èƒ½ï¼‰
processed = await unified_batch_process_messages(
    messages_list,
    show_progress=True,
)
```

---

## è¡¨æ ¼å’Œæ–‡ä»¶å¤¹å¤„ç†

### MllmTableProcessor

å¤„ç† CSV/Excel è¡¨æ ¼æ•°æ®ï¼š

```python
from flexllm import MllmClient, MllmTableProcessor

client = MllmClient(base_url="...", api_key="...", model="gpt-4o")
processor = MllmTableProcessor(client)

# åŠ è½½æ•°æ®
df = processor.load_dataframe("data.xlsx", sheet_name=0, max_num=100)

# æ–¹å¼1ï¼šç›´æ¥å¤„ç†è¡¨æ ¼æ–‡ä»¶ï¼ˆæ¨èï¼‰
results = await processor.call_table(
    table_path="data.xlsx",
    text_col="question",      # æ–‡æœ¬åˆ—å
    image_col="image_path",   # å›¾åƒåˆ—åï¼ˆå¯é€‰ï¼ŒNone è¡¨ç¤ºçº¯æ–‡æœ¬ï¼‰
)

# æ–¹å¼2ï¼šå¤„ç† DataFrame
results = await processor.call_dataframe(
    df,
    text_col="question",
    image_col=None,  # çº¯æ–‡æœ¬æ¨¡å¼
)

# æ–¹å¼3ï¼šæ‰¹é‡å¤„ç†è¡¨æ ¼ä¸­çš„å›¾åƒ
results = await processor.call_table_images(
    table_path="images.xlsx",
    image_col="image_path",
    text_prompt="æè¿°è¿™å¼ å›¾ç‰‡",
)
```

### MllmFolderProcessor

æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„å›¾åƒï¼š

```python
from flexllm import MllmClient, MllmFolderProcessor

client = MllmClient(base_url="...", api_key="...", model="gpt-4o")
processor = MllmFolderProcessor(client)

# æ‰«æå›¾åƒ
images = processor.scan_folder_images(
    "/path/to/images",
    recursive=True,
    max_num=100,
    extensions={'.jpg', '.png'},
)

# æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„å›¾åƒ
results = await processor.call_folder_images(
    "/path/to/images",
    text_prompt="æè¿°è¿™å¼ å›¾ç‰‡",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªå›¾åƒåˆ†æåŠ©æ‰‹",
    recursive=True,
)

# æˆ–å¤„ç†æŒ‡å®šçš„å›¾åƒæ–‡ä»¶åˆ—è¡¨
results = await processor.call_image_files(
    image_files=["/path/to/img1.jpg", "/path/to/img2.png"],
    text_prompt="è¿™å¼ å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ",
)
```

---

## é“¾å¼æ¨ç†

### ChainOfThoughtClient

å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ï¼š

```python
from flexllm import OpenAIClient
from flexllm.clients.chain_of_thought import ChainOfThoughtClient, Step

# åˆ›å»ºåº•å±‚å®¢æˆ·ç«¯
base_client = OpenAIClient(base_url="...", api_key="...", model="gpt-4")

# åˆ›å»ºé“¾å¼æ¨ç†å®¢æˆ·ç«¯
client = ChainOfThoughtClient(openai_client=base_client)

# å®šä¹‰æ¨ç†æ­¥éª¤
steps = [
    Step(
        name="åˆ†æé—®é¢˜",
        prepare_messages_fn=lambda ctx: [
            {"role": "user", "content": f"åˆ†æé—®é¢˜: {ctx.query}"}
        ],
        get_next_step_fn=lambda response, ctx: "ç»¼åˆ" if "éœ€è¦" in response else None,
    ),
    Step(
        name="ç»¼åˆ",
        prepare_messages_fn=lambda ctx: [
            {"role": "user", "content": f"åŸºäºåˆ†æç»™å‡ºç­”æ¡ˆ: {ctx.get('analysis')}"}
        ],
        get_next_step_fn=lambda response, ctx: None,  # è¿”å› None è¡¨ç¤ºç»“æŸ
    ),
]

# æ³¨å†Œæ­¥éª¤
client.add_steps(steps)

# æ‰§è¡Œæ¨ç†é“¾
context = await client.execute_chain(
    initial_step_name="åˆ†æé—®é¢˜",
    initial_context={"query": "å¤æ‚é—®é¢˜"},
)
print(context.final_response)
```

---

## è´Ÿè½½å‡è¡¡ç­–ç•¥

### å¤š Endpoint é…ç½®

```python
from flexllm import LLMClientPool

pool = LLMClientPool(
    endpoints=[
        {
            "base_url": "http://fast-host:8000/v1",
            "api_key": "key1",
            "model": "qwen",
            "concurrency_limit": 50,  # endpoint çº§åˆ«å¹¶å‘ï¼ˆå¯é€‰ï¼‰
            "max_qps": 500,           # endpoint çº§åˆ« QPSï¼ˆå¯é€‰ï¼‰
        },
        {
            "base_url": "http://slow-host:8000/v1",
            "api_key": "key2",
            "model": "qwen",
            "concurrency_limit": 5,   # è¾ƒæ…¢æœåŠ¡ä½¿ç”¨æ›´ä½çš„å¹¶å‘
            "max_qps": 50,
        },
    ],
    fallback=True,
    failure_threshold=3,   # è¿ç»­å¤±è´¥ 3 æ¬¡æ ‡è®°ä¸ºä¸å¥åº·
    recovery_time=60.0,    # 60 ç§’åå°è¯•æ¢å¤
    concurrency_limit=10,  # å…¨å±€é»˜è®¤å€¼ï¼ˆæœªæŒ‡å®š endpoint çº§åˆ«é…ç½®æ—¶ä½¿ç”¨ï¼‰
    max_qps=100,           # å…¨å±€é»˜è®¤å€¼
)
```

å¤š endpoint æ¨¡å¼ä½¿ç”¨è½®è¯¢ï¼ˆround_robinï¼‰ç­–ç•¥åˆ†é…è¯·æ±‚ï¼Œé…åˆå…±äº«é˜Ÿåˆ—å®ç°åŠ¨æ€è´Ÿè½½å‡è¡¡ã€‚

### Endpoint çº§åˆ« Rate Limit

æ¯ä¸ª endpoint å¯ä»¥ç‹¬ç«‹é…ç½® `concurrency_limit` å’Œ `max_qps`ï¼Œä»¥é€‚åº”å¼‚æ„ endpoint åœºæ™¯ï¼ˆä¸åŒæœåŠ¡æ€§èƒ½å·®å¼‚å¤§ï¼‰ï¼š

```python
from flexllm import LLMClientPool, EndpointConfig

# æ–¹å¼1ï¼šä½¿ç”¨ EndpointConfigï¼ˆæ¨èï¼‰
pool = LLMClientPool(
    endpoints=[
        EndpointConfig(
            base_url="http://fast-api.com/v1",
            api_key="key1",
            model="qwen",
            concurrency_limit=50,  # é«˜æ€§èƒ½æœåŠ¡
            max_qps=500,
        ),
        EndpointConfig(
            base_url="http://slow-api.com/v1",
            api_key="key2",
            model="qwen",
            concurrency_limit=5,   # ä½æ€§èƒ½æœåŠ¡
            max_qps=50,
        ),
    ],
)

# æ–¹å¼2ï¼šä½¿ç”¨ dict é…ç½®
pool = LLMClientPool(
    endpoints=[
        {"base_url": "http://fast.com/v1", "concurrency_limit": 50, "max_qps": 500},
        {"base_url": "http://slow.com/v1", "concurrency_limit": 5, "max_qps": 50},
    ],
    concurrency_limit=10,  # å…¨å±€é»˜è®¤å€¼
    max_qps=100,           # å…¨å±€é»˜è®¤å€¼
)
```

**é…ç½®ä¼˜å…ˆçº§**ï¼šendpoint çº§åˆ«é…ç½® > å…¨å±€é…ç½® > é»˜è®¤å€¼

**CLI é…ç½®æ–¹å¼**ï¼ˆ`~/.flexllm/config.yaml`ï¼‰ï¼š

```yaml
batch:
  concurrency: 10       # å…¨å±€é»˜è®¤å¹¶å‘
  max_qps: 100          # å…¨å±€é»˜è®¤ QPS
  endpoints:
    - base_url: http://fast-api.com/v1
      api_key: key1
      model: qwen
      concurrency_limit: 50
      max_qps: 500
    - base_url: http://slow-api.com/v1
      api_key: key2
      model: qwen
      concurrency_limit: 5
      max_qps: 50
  fallback: true
```

**CLI ä¼˜å…ˆçº§**ï¼š`-m å‚æ•°` > `batch.endpoints` > é»˜è®¤æ¨¡å‹

- æŒ‡å®š `-m model`ï¼šä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹é…ç½®
- æœªæŒ‡å®š `-m` ä¸”é…ç½®äº† `batch.endpoints`ï¼šè‡ªåŠ¨ä½¿ç”¨ `LLMClientPool`
- éƒ½æ²¡æœ‰ï¼šä½¿ç”¨é»˜è®¤æ¨¡å‹

**ä½¿ç”¨åœºæ™¯**ï¼š
- æ··åˆéƒ¨ç½²ï¼šæœ¬åœ° GPU æœåŠ¡ï¼ˆé«˜å¹¶å‘ï¼‰+ äº‘ APIï¼ˆå—é™ï¼‰
- æˆæœ¬ä¼˜åŒ–ï¼šä»˜è´¹ APIï¼ˆä½å¹¶å‘ï¼‰+ å…è´¹ APIï¼ˆé«˜å¹¶å‘ï¼‰
- æ€§èƒ½é€‚é…ï¼šå¿«é€ŸæœåŠ¡å¤„ç†æ›´å¤šè¯·æ±‚ï¼Œæ…¢é€ŸæœåŠ¡ä¸è¢«å‹å®

### Fallback é‡è¯•æœºåˆ¶

å½“å¯ç”¨ `fallback=True` æ—¶ï¼Œé‡è¯•æ¬¡æ•°ä¼šåœ¨å¤šä¸ª endpoint é—´åˆ†é…ï¼Œé¿å…å•ä¸ª endpoint è¶…æ—¶å¯¼è‡´çš„é•¿æ—¶é—´ç­‰å¾…ï¼š

```python
pool = LLMClientPool(
    endpoints=[...],  # å‡è®¾ 3 ä¸ª endpoint
    fallback=True,
    retry_times=6,    # æ€»é‡è¯•æ¬¡æ•°
)
# æ¯ä¸ª endpoint å®é™…é‡è¯• 6 // 3 = 2 æ¬¡
# å•ä¸ªè¯·æ±‚æœ€å¤šå°è¯• 3 ä¸ª endpoint Ã— 2 æ¬¡ = 6 æ¬¡

# ä¸æŒ‡å®š retry_times æ—¶ï¼Œfallback æ¨¡å¼é»˜è®¤ä¸º 0ï¼ˆå¿«é€Ÿåˆ‡æ¢ï¼‰
pool = LLMClientPool(endpoints=[...], fallback=True)
# æ¯ä¸ª endpoint å°è¯• 1 æ¬¡å³åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª
```

### åˆ†å¸ƒå¼æ‰¹é‡è¯·æ±‚

```python
# å°†è¯·æ±‚åˆ†æ•£åˆ°å¤šä¸ª endpoint å¹¶è¡Œå¤„ç†
results = await pool.chat_completions_batch(
    messages_list,
    distribute=True,  # å¯ç”¨åˆ†å¸ƒå¼
)
```

---

## æ€§èƒ½ä¼˜åŒ–

### å¹¶å‘æ§åˆ¶

```python
client = LLMClient(
    concurrency_limit=100,  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    max_qps=50,             # æ¯ç§’æœ€å¤§è¯·æ±‚æ•°
    timeout=120,            # å•è¯·æ±‚è¶…æ—¶
)
```

### ç¼“å­˜ä¼˜åŒ–

```python
from flexllm import ResponseCacheConfig

# IPC æ¨¡å¼ï¼ˆå¤šè¿›ç¨‹å…±äº«ï¼Œæ¨èï¼‰
cache = ResponseCacheConfig(
    enabled=True,
    ttl=3600,
    use_ipc=True,
)

# æœ¬åœ°æ¨¡å¼ï¼ˆå•è¿›ç¨‹ï¼Œæ›´å¿«ï¼‰
cache = ResponseCacheConfig(
    enabled=True,
    ttl=3600,
    use_ipc=False,
)
```

### æ‰¹é‡å¤„ç†æœ€ä½³å®è·µ

```python
# 1. ä½¿ç”¨è¾“å‡ºæ–‡ä»¶ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
results = await client.chat_completions_batch(
    messages_list,
    output_jsonl="results.jsonl",
)

# 2. ä½¿ç”¨ metadata_list ä¿å­˜é¢å¤–ä¿¡æ¯
# é€‚åˆéœ€è¦è¿½è¸ªæ•°æ®æ¥æºçš„åœºæ™¯
metadata_list = [
    {"id": "001", "source": "data.jsonl", "line": 1},
    {"id": "002", "source": "data.jsonl", "line": 2},
]
results = await client.chat_completions_batch(
    messages_list,
    metadata_list=metadata_list,  # å…ƒæ•°æ®ä¼šä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶
    output_jsonl="results.jsonl",
)
# è¾“å‡ºæ–‡ä»¶æ ¼å¼ï¼š{"index": 0, "output": "...", "status": "success", "input": [...], "metadata": {"id": "001", ...}}

# 3. é…åˆç¼“å­˜ä½¿ç”¨
client = LLMClient(
    cache=ResponseCacheConfig(enabled=True),
)

# 4. è¿­ä»£å¼å¤„ç†ï¼ˆå†…å­˜å‹å¥½ï¼‰
async for batch_result in client.iter_chat_completions_batch(
    messages_list,
    batch_size=100,
):
    process(batch_result)
```

---

## Thinking æ¨¡å¼

### OpenAI å…¼å®¹ï¼ˆDeepSeek ç­‰ï¼‰

```python
from flexllm import OpenAIClient

client = OpenAIClient(
    base_url="https://api.deepseek.com/v1",
    api_key="your-key",
    model="deepseek-reasoner",
)

# å¯ç”¨æ€è€ƒ
result = await client.chat_completions(
    messages,
    thinking=True,
    return_raw=True,
)

# è§£ææ€è€ƒå†…å®¹
parsed = OpenAIClient.parse_thoughts(result.data)
print("æ€è€ƒè¿‡ç¨‹:", parsed["thought"])
print("æœ€ç»ˆç­”æ¡ˆ:", parsed["answer"])
```

### Claude

```python
from flexllm import ClaudeClient

client = ClaudeClient(
    api_key="your-key",
    model="claude-sonnet-4-20250514",
)

# å¯ç”¨æ‰©å±•æ€è€ƒ
result = await client.chat_completions(
    messages,
    thinking=True,       # æˆ– thinking=15000 æŒ‡å®š budget_tokens
    return_raw=True,
)

# è§£ææ€è€ƒå†…å®¹
parsed = ClaudeClient.parse_thoughts(result.data)
print("æ€è€ƒè¿‡ç¨‹:", parsed["thought"])
print("æœ€ç»ˆç­”æ¡ˆ:", parsed["answer"])
```

### Gemini

```python
from flexllm import GeminiClient

client = GeminiClient(
    api_key="your-key",
    model="gemini-2.5-flash",
)

# æ€è€ƒçº§åˆ«æ§åˆ¶
result = await client.chat_completions(
    messages,
    thinking="high",  # "minimal", "low", "medium", "high"
)
```

---

## é”™è¯¯å¤„ç†

### è‡ªåŠ¨é‡è¯•

```python
client = LLMClient(
    retry_times=3,      # é‡è¯•æ¬¡æ•°
    retry_delay=1.0,    # é‡è¯•é—´éš”
)
```

### è¿›åº¦æ¡çŠ¶æ€æ˜¾ç¤º

æ‰¹é‡å¤„ç†æ—¶ï¼Œè¿›åº¦æ¡ä¼šå®æ—¶æ˜¾ç¤ºé‡è¯•å’Œå¤±è´¥ä¿¡æ¯ï¼š

```
[â–‰â–‰â–‰â–‰â–‰â–‰â–‰â–‰â–‰â–‰          ] 50.0% (500/1000) âš¡ 25.3 req/s avg: 0.04s ğŸ’° $0.0012 â†»12 âœ—2
```

| æ ‡è®° | è¯´æ˜ |
|------|------|
| `â†»N` | æ€»é‡è¯•æ¬¡æ•°ï¼ˆåŒ…æ‹¬å†…éƒ¨é‡è¯•å’Œ fallback é‡è¯•ï¼‰ |
| `âœ—N` | æœ€ç»ˆå¤±è´¥çš„è¯·æ±‚æ•° |

**é”™è¯¯è­¦å‘Š**ï¼šé¦–æ¬¡é‡åˆ°æ–°é”™è¯¯ç±»å‹æ—¶ï¼Œä¼šæ‰“å°ä¸€æ¬¡è­¦å‘Šï¼š
```
âš ï¸  æ–°é”™è¯¯ç±»å‹: timeout: Request timed out after 120s
```
ç›¸åŒé”™è¯¯ç±»å‹åç»­å‡ºç°ä¸ä¼šé‡å¤æ‰“å°ã€‚

### æ‰¹é‡å¤„ç†é”™è¯¯

```python
results, summary = await client.chat_completions_batch(
    messages_list,
    return_summary=True,
)

print(f"æˆåŠŸ: {summary['success']}")
print(f"å¤±è´¥: {summary['failed']}")
print(f"ç¼“å­˜å‘½ä¸­: {summary['cached']}")
```

### æ‰‹åŠ¨é”™è¯¯å¤„ç†

```python
from flexllm import BatchResultItem

results = await client.chat_completions_batch(
    messages_list,
    return_raw=True,
)

for item in results:
    if item.status == "success":
        print(item.content)
    elif item.status == "error":
        print(f"é”™è¯¯: {item.error}")
    elif item.status == "cached":
        print(f"ç¼“å­˜: {item.content}")
```

---

## ä¸Šä¸‹æ–‡ç®¡ç†

```python
# æ¨èï¼šä½¿ç”¨ async with è‡ªåŠ¨æ¸…ç†èµ„æº
async with LLMClient(...) as client:
    result = await client.chat_completions(messages)

# åŒæ­¥ç‰ˆæœ¬ä½¿ç”¨ with
with LLMClient(...) as client:
    result = client.chat_completions_sync(messages)

# æ‰‹åŠ¨æ¸…ç†ï¼ˆå¼‚æ­¥ï¼‰
client = LLMClient(...)
try:
    result = await client.chat_completions(messages)
finally:
    await client.aclose()

# æ‰‹åŠ¨æ¸…ç†ï¼ˆåŒæ­¥ï¼‰
client = LLMClient(...)
try:
    result = client.chat_completions_sync(messages)
finally:
    client.close()
```

---

## æˆæœ¬è¿½è¸ª

### åŸºæœ¬ç”¨æ³•

æ‰¹é‡å¤„ç†æ—¶è¿½è¸ªæˆæœ¬ï¼š

```python
from flexllm import LLMClient

client = LLMClient(...)

# æ–¹å¼1ï¼šè·å–æˆæœ¬æŠ¥å‘Š
results, cost_report = await client.chat_completions_batch(
    messages_list,
    return_cost_report=True,
)
print(f"æ€»æˆæœ¬: ${cost_report.total_cost:.4f}")
print(f"æ€» tokens: {cost_report.total_tokens:,}")
print(f"å¹³å‡æˆæœ¬/è¯·æ±‚: ${cost_report.avg_cost_per_request:.6f}")

# æ–¹å¼2ï¼šè¿›åº¦æ¡å®æ—¶æ˜¾ç¤ºæˆæœ¬
results = await client.chat_completions_batch(
    messages_list,
    track_cost=True,  # è¿›åº¦æ¡æ˜¾ç¤º ğŸ’° $0.0012
)
```

### CostReport å±æ€§

| å±æ€§ | è¯´æ˜ |
|------|------|
| `total_cost` | æ€»æˆæœ¬ï¼ˆç¾å…ƒï¼‰ |
| `total_input_tokens` | æ€»è¾“å…¥ tokens |
| `total_output_tokens` | æ€»è¾“å‡º tokens |
| `total_tokens` | æ€» tokens |
| `request_count` | è¯·æ±‚æ•° |
| `avg_cost_per_request` | å¹³å‡æˆæœ¬/è¯·æ±‚ |
| `avg_input_tokens` | å¹³å‡è¾“å…¥ tokens |
| `avg_output_tokens` | å¹³å‡è¾“å‡º tokens |

### é¢„ç®—æ§åˆ¶

ä½¿ç”¨ `CostTrackerConfig` è®¾ç½®é¢„ç®—é™åˆ¶ï¼š

```python
from flexllm import LLMClient, CostTrackerConfig

# å¸¦é¢„ç®—æ§åˆ¶çš„å®¢æˆ·ç«¯
client = LLMClient(
    ...,
    cost_tracker=CostTrackerConfig.with_budget(
        limit=5.0,        # ç¡¬é™åˆ¶ï¼šè¶…è¿‡ $5 è‡ªåŠ¨åœæ­¢
        warning=4.0,      # è½¯é™åˆ¶ï¼šè¶…è¿‡ $4 è§¦å‘è­¦å‘Š
        on_warning=lambda current, total: print(f"âš ï¸ é¢„ç®—è­¦å‘Š: ${current:.2f}/{total:.2f}")
    )
)

try:
    results = await client.chat_completions_batch(messages_list)
except BudgetExceededError as e:
    print(f"é¢„ç®—è¶…é™: {e}")
```

### é…ç½®æ–¹å¼

```python
from flexllm import CostTrackerConfig

# æ–¹å¼1ï¼šä»…è¿½è¸ªï¼ˆä¸é™åˆ¶é¢„ç®—ï¼‰
config = CostTrackerConfig.tracking_only()

# æ–¹å¼2ï¼šå¸¦é¢„ç®—æ§åˆ¶
config = CostTrackerConfig.with_budget(
    limit=10.0,
    warning=8.0,
    on_warning=my_warning_handler,
)

# æ–¹å¼3ï¼šç¦ç”¨
config = CostTrackerConfig.disabled()

# åº”ç”¨åˆ°å®¢æˆ·ç«¯
client = LLMClient(..., cost_tracker=config)
```

### CLI ç”¨æ³•

```bash
# è¿›åº¦æ¡é»˜è®¤æ˜¾ç¤ºå®æ—¶æˆæœ¬ï¼ˆtrack_cost=Trueï¼‰
flexllm batch input.jsonl -o output.jsonl

# è¾“å‡ºç¤ºä¾‹ï¼š
# [â–‰â–‰â–‰â–‰â–‰â–‰â–‰â–‰â–‰â–‰          ] 50.0% (50/100) âš¡ 2.5 req/s avg: 0.8s ğŸ’° $0.0012
```

### æˆæœ¬ä¼°ç®—

æˆæœ¬åŸºäº `flexllm/pricing.py` ä¸­çš„æ¨¡å‹å®šä»·è¡¨ä¼°ç®—ã€‚æ”¯æŒçš„æ¨¡å‹åŒ…æ‹¬ï¼š

- OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo ç­‰
- Anthropic: claude-3.5-sonnet, claude-3-opus ç­‰
- Google: gemini-2.0-flash, gemini-1.5-pro ç­‰
- DeepSeek: deepseek-chat, deepseek-reasoner ç­‰
- å…¶ä»–: qwen, yi, llama ç­‰ä¸»æµæ¨¡å‹

æœªåœ¨å®šä»·è¡¨ä¸­çš„æ¨¡å‹ä¼šä½¿ç”¨é»˜è®¤ä¼°ç®—ä»·æ ¼ã€‚
