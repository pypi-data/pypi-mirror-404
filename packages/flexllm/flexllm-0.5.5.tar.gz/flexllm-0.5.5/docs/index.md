# flexllm æ–‡æ¡£

é«˜æ€§èƒ½ LLM å®¢æˆ·ç«¯åº“ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†ã€å“åº”ç¼“å­˜å’Œæ–­ç‚¹ç»­ä¼ ã€‚

## æ–‡æ¡£ç›®å½•

```
docs/
â”œâ”€â”€ index.md              # æœ¬æ–‡æ¡£ï¼ˆä¸»å…¥å£ï¼‰
â”œâ”€â”€ api.md                # API è¯¦ç»†å‚è€ƒ
â”œâ”€â”€ advanced.md           # é«˜çº§ç”¨æ³•
â””â”€â”€ roadmap.md            # å¼€å‘è·¯çº¿å›¾
```

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install flexllm[all]
```

### åŸºæœ¬ä½¿ç”¨

```python
from flexllm import LLMClient

client = LLMClient(
    model="gpt-4",
    base_url="https://api.openai.com/v1",
    api_key="your-api-key"
)

# åŒæ­¥è°ƒç”¨
result = client.chat_completions_sync([
    {"role": "user", "content": "Hello!"}
])
print(result)
```

## æ ¸å¿ƒæ¦‚å¿µ

### 1. å®¢æˆ·ç«¯å±‚æ¬¡

```
LLMClient (æ¨èï¼Œç»Ÿä¸€å…¥å£)
    â”œâ”€â”€ OpenAIClient (OpenAI å…¼å®¹ API)
    â”œâ”€â”€ GeminiClient (Google Gemini)
    â””â”€â”€ ClaudeClient (Anthropic Claude)

LLMClientPool (å¤š Endpoint è´Ÿè½½å‡è¡¡)
    â””â”€â”€ å†…éƒ¨ç®¡ç†å¤šä¸ª LLMClient
```

### 2. è¯·æ±‚æ¨¡å¼

| æ¨¡å¼ | æ–¹æ³• | è¯´æ˜ |
|------|------|------|
| å•æ¡åŒæ­¥ | `chat_completions_sync()` | ç®€å•åœºæ™¯ |
| å•æ¡å¼‚æ­¥ | `chat_completions()` | é«˜æ€§èƒ½åœºæ™¯ |
| æ‰¹é‡å¼‚æ­¥ | `chat_completions_batch()` | å¤§è§„æ¨¡å¤„ç† |
| æµå¼è¾“å‡º | `chat_completions_stream()` | å®æ—¶æ˜¾ç¤º |

### 3. ç¼“å­˜æœºåˆ¶

```python
from flexllm import ResponseCacheConfig

# å¯ç”¨ç¼“å­˜ï¼ˆ1å°æ—¶ TTLï¼‰
cache = ResponseCacheConfig(enabled=True, ttl=3600)

# æ°¸ä¹…ç¼“å­˜
cache = ResponseCacheConfig(enabled=True, ttl=0)
```

ç¼“å­˜åŸºäºæ¶ˆæ¯å†…å®¹çš„ hashï¼Œç›¸åŒè¯·æ±‚è‡ªåŠ¨å‘½ä¸­ç¼“å­˜ã€‚

### 4. æˆæœ¬è¿½è¸ª

æ”¯æŒå®æ—¶æˆæœ¬è¿½è¸ªå’Œé¢„ç®—æ§åˆ¶ï¼š

```python
# ç®€å•å¯ç”¨æˆæœ¬è¿½è¸ª
results, cost_report = await client.chat_completions_batch(
    messages_list,
    return_cost_report=True,
)
print(f"æ€»æˆæœ¬: ${cost_report.total_cost:.4f}")

# è¿›åº¦æ¡å®æ—¶æ˜¾ç¤ºæˆæœ¬
results = await client.chat_completions_batch(
    messages_list,
    track_cost=True,  # è¿›åº¦æ¡ä¸­æ˜¾ç¤º ğŸ’° $0.0012
)
```

è¯¦è§ [é«˜çº§ç”¨æ³• - æˆæœ¬è¿½è¸ª](advanced.md#æˆæœ¬è¿½è¸ª)ã€‚

### 5. æ–­ç‚¹ç»­ä¼ 

æ‰¹é‡å¤„ç†æ”¯æŒè‡ªåŠ¨æ–­ç‚¹ç»­ä¼ ï¼š

```python
results = await client.chat_completions_batch(
    messages_list,
    output_file="results.jsonl",  # å…³é”®ï¼šæŒ‡å®šè¾“å‡ºæ–‡ä»¶
)
```

- ç»“æœå¢é‡å†™å…¥æ–‡ä»¶
- ç¨‹åºä¸­æ–­åï¼Œé‡æ–°è¿è¡Œè‡ªåŠ¨è·³è¿‡å·²å®Œæˆçš„è¯·æ±‚
- é…åˆç¼“å­˜ä½¿ç”¨æ•ˆæœæ›´å¥½

## æ”¯æŒçš„ Provider

| Provider | å®¢æˆ·ç«¯ | è¯´æ˜ |
|----------|--------|------|
| OpenAI | LLMClient/OpenAIClient | GPT ç³»åˆ— |
| DeepSeek | LLMClient/OpenAIClient | æ”¯æŒ thinking æ¨¡å¼ |
| Qwen | LLMClient/OpenAIClient | é€šä¹‰åƒé—® |
| vLLM | LLMClient/OpenAIClient | æœ¬åœ°éƒ¨ç½² |
| Ollama | LLMClient/OpenAIClient | æœ¬åœ°éƒ¨ç½² |
| Gemini | LLMClient/GeminiClient | Google AI |
| Vertex AI | GeminiClient | GCP æ‰˜ç®¡ |
| Claude | LLMClient/ClaudeClient | Anthropic Claude |

## CLI å·¥å…·

flexllm æä¾›å‘½ä»¤è¡Œå·¥å…·ï¼ˆåˆ«å `xllm`ï¼‰ï¼Œæ”¯æŒ Tab è‡ªåŠ¨è¡¥å…¨ã€‚

```bash
# å®‰è£…è‡ªåŠ¨è¡¥å…¨ï¼ˆå¯é€‰ï¼‰
flexllm --install-completion
```

### å‘½ä»¤ä¸€è§ˆ

| å‘½ä»¤ | è¯´æ˜ |
|------|------|
| `ask` | å¿«é€Ÿé—®ç­”ï¼ˆæ”¯æŒç®¡é“è¾“å…¥ï¼‰ |
| `chat` | äº¤äº’å¼å¯¹è¯ |
| `batch` | æ‰¹é‡å¤„ç† JSONLï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰ |
| `list` | åˆ—å‡ºæœ¬åœ°é…ç½®çš„æ¨¡å‹ |
| `set-model` | è®¾ç½®é»˜è®¤æ¨¡å‹ |
| `models` | åˆ—å‡ºè¿œç¨‹æœåŠ¡å™¨çš„å¯ç”¨æ¨¡å‹ |
| `credits` | æŸ¥è¯¢ API Key ä½™é¢ |
| `pricing` | æŸ¥è¯¢æ¨¡å‹å®šä»·ä¿¡æ¯ |
| `test` | æµ‹è¯• LLM æœåŠ¡è¿æ¥ |
| `init` | åˆå§‹åŒ–é…ç½®æ–‡ä»¶ |
| `mock` | å¯åŠ¨ Mock LLM æœåŠ¡å™¨ï¼ˆæµ‹è¯•ç”¨ï¼‰ |
| `version` | æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯ |

### å¿«é€Ÿç¤ºä¾‹

```bash
# å¿«é€Ÿé—®ç­”
flexllm ask "ä»€ä¹ˆæ˜¯ Python?"
flexllm ask "è§£é‡Šä»£ç " -s "ä½ æ˜¯ä»£ç ä¸“å®¶"
echo "é•¿æ–‡æœ¬" | flexllm ask "æ€»ç»“ä¸€ä¸‹"

# äº¤äº’å¯¹è¯
flexllm chat
flexllm chat "ä½ å¥½" -m gpt-4

# æ¨¡å‹ç®¡ç†
flexllm list                      # æŸ¥çœ‹å·²é…ç½®æ¨¡å‹
flexllm set-model gpt-4           # è®¾ç½®é»˜è®¤æ¨¡å‹

# æ‰¹é‡å¤„ç†ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
flexllm batch input.jsonl -o output.jsonl

# æŸ¥è¯¢ API Key ä½™é¢
flexllm credits                   # æŸ¥è¯¢é»˜è®¤æ¨¡å‹çš„ key ä½™é¢
flexllm credits -m grok-4         # æŸ¥è¯¢æŒ‡å®šæ¨¡å‹çš„ key ä½™é¢

# æŸ¥è¯¢æ¨¡å‹å®šä»·
flexllm pricing gpt-4o            # æŸ¥è¯¢æŒ‡å®šæ¨¡å‹å®šä»·

# æµ‹è¯•è¿æ¥
flexllm test

# åˆå§‹åŒ–é…ç½®
flexllm init
```

### credits å‘½ä»¤

æŸ¥è¯¢ API Key ä½™é¢ï¼Œè‡ªåŠ¨æ ¹æ® base_url è¯†åˆ« providerã€‚

**æ”¯æŒçš„ Providerï¼š**

| Provider | è¯†åˆ«æ–¹å¼ |
|----------|---------|
| OpenRouter | `openrouter.ai` in base_url |
| SiliconFlow | `siliconflow.cn` in base_url |
| DeepSeek | `deepseek.com` in base_url |
| AI/ML API | `aimlapi.com` in base_url |
| OpenAI | `api.openai.com` in base_urlï¼ˆéå®˜æ–¹ APIï¼‰ |

**ä¸æ”¯æŒçš„ Providerï¼š**
- Anthropic: éœ€è¦ Admin API key
- xAI: éœ€è¦å•ç‹¬çš„ Management API key
- Together AI/Groq/Mistral: æ— å…¬å¼€ä½™é¢æŸ¥è¯¢ API

```bash
$ flexllm credits
OpenRouter è´¦æˆ·ä½™é¢
æ¨¡å‹é…ç½®: grok-4.1-fast
API Key: sk-or-v1-cef5a7...8559
----------------------------------------
  å‰©ä½™é¢åº¦: $96.63
  æ€»é¢åº¦ä¸Šé™: $150.00
  å·²ä½¿ç”¨: $53.37
  ä»Šæ—¥æ¶ˆè´¹: $3.3637
  æœ¬æœˆæ¶ˆè´¹: $53.36
```

### é…ç½®æ–‡ä»¶

é…ç½®æ–‡ä»¶ä½ç½®ï¼š`~/.flexllm/config.yaml`ï¼ˆè¿è¡Œ `flexllm init` åˆ›å»ºï¼‰

```yaml
# é»˜è®¤æ¨¡å‹
default: "gpt-4"

# æ¨¡å‹åˆ—è¡¨
models:
  - id: gpt-4
    name: gpt-4
    provider: openai
    base_url: https://api.openai.com/v1
    api_key: your-api-key

  - id: local-ollama
    name: local-ollama
    provider: openai
    base_url: http://localhost:11434/v1
    api_key: EMPTY

# batch å‘½ä»¤é…ç½®ï¼ˆå¯é€‰ï¼‰
batch:
  concurrency: 10       # å¹¶å‘æ•°ï¼ˆå…¨å±€é»˜è®¤ï¼Œå¯è¢« endpoint çº§åˆ«é…ç½®è¦†ç›–ï¼‰
  max_qps: 100          # æ¯ç§’æœ€å¤§è¯·æ±‚æ•°ï¼ˆå…¨å±€é»˜è®¤ï¼‰
  timeout: 120          # è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
  cache: false          # å¯ç”¨å“åº”ç¼“å­˜
  return_usage: true    # è¾“å‡º token ç»Ÿè®¡
  track_cost: true      # è¿›åº¦æ¡æ˜¾ç¤ºå®æ—¶æˆæœ¬

  # å¤š endpoint é…ç½®ï¼ˆé…ç½®å batch å‘½ä»¤è‡ªåŠ¨ä½¿ç”¨ LLMClientPoolï¼‰
  endpoints:
    - base_url: http://fast-api.com/v1
      api_key: key1
      model: qwen
      concurrency_limit: 50   # endpoint çº§åˆ«å¹¶å‘ï¼ˆå¯é€‰ï¼‰
      max_qps: 500            # endpoint çº§åˆ« QPSï¼ˆå¯é€‰ï¼‰
    - base_url: http://slow-api.com/v1
      api_key: key2
      model: qwen
      concurrency_limit: 5    # è¾ƒæ…¢æœåŠ¡ä½¿ç”¨æ›´ä½çš„å¹¶å‘
      max_qps: 50
  fallback: true              # å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å…¶ä»– endpoint
```

ä¹Ÿæ”¯æŒç¯å¢ƒå˜é‡é…ç½®ï¼š`FLEXLLM_BASE_URL`ã€`FLEXLLM_API_KEY`ã€`FLEXLLM_MODEL`

### batch å‘½ä»¤

æ‰¹é‡å¤„ç† JSONL æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹è¾“å…¥æ ¼å¼ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€‚

**æ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š**

| æ ¼å¼ | æ£€æµ‹è§„åˆ™ | ç¤ºä¾‹ |
|------|----------|------|
| openai_chat | å­˜åœ¨ `messages` å­—æ®µ | `{"messages": [{"role": "user", "content": "..."}]}` |
| alpaca | å­˜åœ¨ `instruction` å­—æ®µ | `{"instruction": "ç¿»è¯‘", "input": "Hello"}` |
| simple | å­˜åœ¨ `q`/`question`/`prompt` å­—æ®µ | `{"q": "ä»€ä¹ˆæ˜¯AI?", "system": "ä½ æ˜¯ä¸“å®¶"}` |

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```bash
# åŸºæœ¬ç”¨æ³•
flexllm batch input.jsonl -o output.jsonl

# æŒ‡å®šæ¨¡å‹å’Œå¹¶å‘æ•°
flexllm batch input.jsonl -o output.jsonl -m gpt-4 -c 20

# ä¸ dtflow é…åˆï¼ˆç®¡é“è¾“å…¥ï¼‰
dt transform qa.jsonl --preset=openai_chat | flexllm batch -o output.jsonl

# å…¨å±€ system prompt
flexllm batch input.jsonl -o output.jsonl --system "ä½ æ˜¯ç¿»è¯‘ä¸“å®¶"

# æ–­ç‚¹ç»­ä¼ ï¼ˆé»˜è®¤è¡Œä¸ºï¼Œä¸­æ–­åé‡æ–°è¿è¡Œå³å¯ç»§ç»­ï¼‰
# é»˜è®¤æ˜¾ç¤ºå®æ—¶æˆæœ¬å’Œ token ç»Ÿè®¡
flexllm batch input.jsonl -o output.jsonl
```

**è¾“å‡ºæ ¼å¼ï¼š**

```jsonl
{"index": 0, "output": "LLMå“åº”", "status": "success", "input": [...], "metadata": {"id": "001"}}
```

- `output`: LLM å“åº”å†…å®¹
- `input`: è½¬æ¢åçš„ messages æ ¼å¼
- `metadata`: è¾“å…¥æ–‡ä»¶ä¸­é™¤ messages å¤–çš„å…¶ä»–å­—æ®µ

## ä¸‹ä¸€æ­¥

- [API è¯¦ç»†å‚è€ƒ](api.md) - å®Œæ•´çš„ API æ–‡æ¡£
- [é«˜çº§ç”¨æ³•](advanced.md) - è´Ÿè½½å‡è¡¡ã€å¤šæ¨¡æ€ã€é“¾å¼æ¨ç†ç­‰
