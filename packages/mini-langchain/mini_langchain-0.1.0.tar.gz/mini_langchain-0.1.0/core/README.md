# Mini LangChain âš¡
> **Next-Gen LLM Framework.** Built in Rust. Bindings for Python & Node.js.
> *High Performance. Low Token Overhead. Type Safe.*

[![Rust](https://img.shields.io/badge/Core-Rust_ğŸ¦€-orange.svg)]()
[![Python](https://img.shields.io/badge/Bindings-Python_ğŸ-blue.svg)]()
[![Node.js](https://img.shields.io/badge/Bindings-Node.js_ğŸ’š-green.svg)]()
[![License](https://img.shields.io/badge/License-MIT-purple.svg)]()

---

## ğŸ”® Why Mini LangChain?

Standard frameworks are bloated. **Mini LangChain** is stripped down to the bare metal.

*   **ğŸš€ Blazing Fast**: Core logic runs in native Rust. No GIL bottlenecks for heavy lifting.
*   **ğŸ’° Token Efficient**: Native token counting and automatic prompt minification.
*   **ğŸŒ Cross-Language**: Write your logic in Python or Node.js; let Rust handle the heavy lifting.
*   **ğŸ§  Smart Memory**: Thread-safe `ConversationBufferMemory` shared across chains.

---

## âš¡ Features (Ready)

| Module | Status | Description |
| :--- | :---: | :--- |
| **ğŸ§  Memory** | âœ… | `ConversationBufferMemory` (Context preservation) |
| **ğŸ“‚ Loaders** | âœ… | `TextLoader` & `Document` Schema |
| **ğŸ” RAG** | âœ… | `InMemoryVectorStore` & `Embeddings` (Cosine Sim) |
| **ğŸ¤– Agents** | âœ… | `AgentExecutor` (Zero-shot Tool Use) |
| **â›“ï¸ Chains** | âœ… | `LLMChain` with Prompt Templates |

---

## ğŸ› ï¸ Installation

### Python ğŸ
```bash
pip install mini_langchain
```

### Node.js ğŸ’š
```bash
npm install mini-langchain-node
```

---

## ğŸ’» Usage

### 1. RAG & Vector Search ğŸ”
*Embed documents and search by semantic similarity.*

**Rust Core ğŸ¦€**
```rust
use mini_langchain_core::{
    vectorstore::{VectorStore, InMemoryVectorStore},
    embedding::MockEmbeddings,
    schema::Document
};
use std::sync::Arc;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // 1. Initialize
    let store = InMemoryVectorStore::new(Arc::new(MockEmbeddings));

    // 2. Add Documents
    let docs = vec![
        Document::new("Rust is memory safe ğŸ¦€".to_string())
            .with_metadata("tag", "tech"),
        Document::new("Node.js is async ğŸ’š".to_string())
            .with_metadata("tag", "tech"),
    ];
    store.add_documents(&docs).await?;

    // 3. Search
    let results = store.similarity_search("memory", 1).await?;
    println!("{}", results[0].page_content); // "Rust is memory safe ğŸ¦€"
    
    Ok(())
}
```

**Python**
```python
from mini_langchain import InMemoryVectorStore, MockEmbeddings, Document

# 1. Initialize
store = InMemoryVectorStore(MockEmbeddings())

# 2. Add Documents
store.add_documents([
    Document("Rust is memory safe ğŸ¦€", {"tag": "tech"}),
    Document("Node.js is async ğŸ’š", {"tag": "tech"})
])

# 3. Search
docs = store.similarity_search("memory", 1)
print(docs[0].page_content) # "Rust is memory safe ğŸ¦€"
```

**Node.js**
```javascript
const { InMemoryVectorStore, MockEmbeddings, Document } = require('mini-langchain-node');

// 1. Initialize
const store = new InMemoryVectorStore(new MockEmbeddings());

// 2. Add Documents
await store.addDocuments([
    new Document("Rust is memory safe ğŸ¦€", { tag: "tech" }),
    new Document("Node.js is async ğŸ’š", { tag: "tech" })
]);

// 3. Search
const docs = await store.similaritySearch("memory", 1);
console.log(docs[0].pageContent);
```

### 2. Conversational Chains ğŸ’¬
*Maintain context effortlessly.*

```python
from mini_langchain import Chain, PromptTemplate, SambaNovaLLM, ConversationBufferMemory

# 1. Setup
llm = SambaNovaLLM("Meta-Llama-3.1-8B-Instruct", "your-api-key")
memory = ConversationBufferMemory()
prompt = PromptTemplate("History: {history} \nUser: {input}", ["history", "input"])

# 2. Run
chain = Chain(prompt, llm, memory)
print(chain.invoke({"input": "Hello!"}))
print(chain.invoke({"input": "My name is User."})) # Memory remembers!
```

---

## ğŸ—ºï¸ Roadmap
- [ ] **Embeddings**: Integration with SambaNova/OpenAI embeddings.
- [ ] **Persistent Storage**: SQLite/PGVector support.
- [ ] **Tools**: Search & Calculator implementations.

---

<center>
<i>Built with â¤ï¸ by GrandpaEJ</i>
</center>
