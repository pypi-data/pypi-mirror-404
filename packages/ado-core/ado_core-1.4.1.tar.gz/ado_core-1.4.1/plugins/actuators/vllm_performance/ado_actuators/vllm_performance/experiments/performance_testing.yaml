# Copyright (c) IBM Corporation
# SPDX-License-Identifier: MIT
# The input to an experiment is an Entity. For the Entity to be a valid input
# it's properties which  match what is defined here
test-deployment-v1:
  identifier: test-deployment-v1
  actuatorIdentifier: "vllm_performance"
  requiredProperties: # Any entity passed to this experiment must have constitutive properties with these values
    - identifier: 'model'
      metadata:
        description: 'model to use for testing. Must be available through the huggingface hub'
      propertyDomain:
        variableType: "OPEN_CATEGORICAL_VARIABLE_TYPE"
        values: ["meta-llama/Llama-3.1-8B-Instruct", "ibm-granite/granite-3.3-8b-instruct", "openai/gpt-oss-20b"]
    - identifier: 'request_rate'
      metadata:
        description: "(benchmark) The number of requests to send per second"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [-1, 1000]
        interval: 1 # -1 means send all requests at time 0
  optionalProperties:
    - identifier: 'num_prompts'
      metadata:
        description: "(benchmark) The number of prompts to send (total number of requests)"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 10001]
        interval: 1
    - identifier: 'max_concurrency'
      metadata:
        description: "(benchmark) The maximum number of concurrent requests to send"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [-1, 500] # -1 means no concurrency control
        interval: 1
    - identifier: 'burstiness'
      metadata:
        description: "(benchmark) The burstiness of the requests - 1.0 is a Poisson distribution with rate = request_rate. Others are gamma distributions with lambda = request_rate and shape = burstiness."
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [0, 10]
        interval: 1
    - identifier: 'number_input_tokens'
      metadata:
        description: "(benchmark) The number of input tokens to send per request"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 128000]
        interval: 1
    - identifier: 'max_output_tokens'
      metadata:
        description: "(benchmark) The maximum number of output tokens to generate per request"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 10000]
        interval: 1
    - identifier: image
      metadata:
        description: "(deployment) Docker image to use to create vllm deployments"
      propertyDomain:
        variableType: "OPEN_CATEGORICAL_VARIABLE_TYPE"
        values: ["vllm/vllm-openai:v0.14.0"]
    - identifier: n_cpus
      metadata:
        description: "(deployment) the number of CPUs to use"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 17]
        interval: 1
    - identifier: memory
      metadata:
        description: "(deployment) the amount of memory to allocate to vLLM pod"
      propertyDomain:
        variableType: "CATEGORICAL_VARIABLE_TYPE"
        values: ["64Gi", "128Gi", "256Gi"]
    - identifier: dtype
      metadata:
        description: "(deployment) data type for model weights and activations. “auto” will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models."
      propertyDomain:
        variableType: "CATEGORICAL_VARIABLE_TYPE"
        values: ["auto", "half", "float16", "bfloat16", "float", "float32"]
    - identifier: 'gpu_memory_utilization'
      metadata:
        description: "(deployment) The fraction of GPU memory to be used for the model executor,"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        values: [.5, .75, .9]
    - identifier: 'cpu_offload'
      metadata:
        description: "(deployment) The amount of model weights in GB to offload to the CPU per GPU. 0 means all weights are on GPU,"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        values: [0, 8, 16, 24, 32]
    - identifier: 'max_batch_tokens'
      metadata:
        description: "(deployment) maximum number of batched tokens per iteration"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [8192, 32769]
        interval: 1024
    - identifier: 'max_num_seq'
      metadata:
        description: "(deployment) Maximum number of sequences per iteration"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [32, 2049]
        interval: 32
    - identifier: 'n_gpus'
      metadata:
        description: "(deployment) Number of GPUs to use"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 9]
        interval: 1
    - identifier: 'gpu_type'
      metadata:
        description: "(deployment) The GPU type to use"
      propertyDomain:
        variableType: "OPEN_CATEGORICAL_VARIABLE_TYPE"
        values: ['NVIDIA-A100-80GB-PCIe', 'NVIDIA-A100-SXM4-80GB']
    - identifier: 'dataset'
      metadata:
        description: "(benchmark) The dataset to use for the benchmark"
      propertyDomain:
        variableType: 'CATEGORICAL_VARIABLE_TYPE'
        values: ['random']
  defaultParameterization:
    - property:
        identifier: 'image'
      value: "vllm/vllm-openai:v0.14.0"
    - property:
        identifier: n_cpus
      value: 8
    - property:
        identifier: memory
      value: "128Gi"
    - property:
        identifier: dtype
      value: "auto"
    - property:
        identifier: 'num_prompts'
      value: 500
    - property:
        identifier: 'max_concurrency'
      value: -1
    - property:
        identifier: 'burstiness'
      value: 1.0
    - property:
        identifier: 'number_input_tokens'
      value: 1024
    - property:
        identifier: 'max_output_tokens'
      value: 128
    - property:
        identifier: 'dataset'
      value: 'random'
    - property:
        identifier: 'gpu_memory_utilization'
      value: .9
    - property:
        identifier: 'cpu_offload'
      value: 0
    - property:
        identifier: 'max_batch_tokens'
      value: 16384
    - property:
        identifier: 'max_num_seq'
      value: 256
    - property:
        identifier: 'n_gpus'
      value: 1
    - property:
        identifier: 'gpu_type'
      value: 'NVIDIA-A100-80GB-PCIe'
  # measurements
  targetProperties:
    - identifier: "duration"
    - identifier: "completed"
    - identifier: "total_input_tokens"
    - identifier: "total_output_tokens"
    - identifier: "request_throughput"
    - identifier: "output_throughput"
    - identifier: "total_token_throughput"
    - identifier: "mean_ttft_ms"
    - identifier: "median_ttft_ms"
    - identifier: "std_ttft_ms"
    - identifier: "p25_ttft_ms"
    - identifier: "p50_ttft_ms"
    - identifier: "p75_ttft_ms"
    - identifier: "p99_ttft_ms"
    - identifier: "mean_tpot_ms"
    - identifier: "median_tpot_ms"
    - identifier: "std_tpot_ms"
    - identifier: "p25_tpot_ms"
    - identifier: "p50_tpot_ms"
    - identifier: "p75_tpot_ms"
    - identifier: "p99_tpot_ms"
    - identifier: "mean_itl_ms"
    - identifier: "median_itl_ms"
    - identifier: "std_itl_ms"
    - identifier: "p25_itl_ms"
    - identifier: "p50_itl_ms"
    - identifier: "p75_itl_ms"
    - identifier: "p99_itl_ms"
    - identifier: "mean_e2el_ms"
    - identifier: "median_e2el_ms"
    - identifier: "std_e2el_ms"
    - identifier: "p25_e2el_ms"
    - identifier: "p50_e2el_ms"
    - identifier: "p75_e2el_ms"
    - identifier: "p99_e2el_ms"
  metadata:
    description: 'VLLM performance testing across compute resource and workload configuration'
test-endpoint-v1:
  identifier: test-endpoint-v1
  actuatorIdentifier: "vllm_performance"
  requiredProperties: # Any entity passed to this experiment must have constitutive properties with these values
    - identifier: 'model'
      metadata:
        description: 'model to use for testing. Assumed to be served by all endpoints tested. Required to obtain correct tokenizer for benchmarking metrics calculation'
      propertyDomain:
        variableType: "OPEN_CATEGORICAL_VARIABLE_TYPE"
        values: ["meta-llama/Llama-3.1-8B-Instruct", "ibm-granite/granite-3.3-8b-instruct", "openai/gpt-oss-20b"]
    - identifier: 'endpoint'
      metadata:
        description: 'The endpoint(s) to test'
      propertyDomain:
        variableType: "OPEN_CATEGORICAL_VARIABLE_TYPE"
        values: ["http://localhost:8000"]
    - identifier: 'request_rate'
      metadata:
        description: "The number of requests to send per second"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [-1, 1000]
        interval: 1 # -1 means send all requests at time 0
  optionalProperties:
    - identifier: 'num_prompts'
      metadata:
        description: "The number of prompts to send (total number of requests)"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 10001]
        interval: 1
    - identifier: 'number_input_tokens'
      metadata:
        description: "The number of input tokens to send per request"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 128000]
        interval: 1 # -1 means send all requests at time 0
    - identifier: 'max_output_tokens'
      metadata:
        description: "The maximum number of output tokens to generate per request"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 10000]
        interval: 1 # -1 means send all requests at time 0
    - identifier: 'burstiness'
      metadata:
        description: "The burstiness of the requests - 1.0 is a Poisson distribution with rate = request_rate. Others are gamma distributions with lambda = request_rate and shape = burstiness."
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [0, 10]
        interval: 1
    - identifier: 'max_concurrency'
      metadata:
        description: "The maximum number of concurrent requests to send"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [-1, 500] # -1 means no concurrency control
        interval: 1
    - identifier: 'dataset'
      metadata:
        description: "The dataset to use for the benchmark"
      propertyDomain:
        variableType: 'CATEGORICAL_VARIABLE_TYPE'
        values: ['random']
  defaultParameterization:
    - value: 1000
      property:
        identifier: 'num_prompts'
    - value: -1
      property:
        identifier: 'max_concurrency'
    - value: 1.0
      property:
        identifier: 'burstiness'
    - value: 1024
      property:
        identifier: 'number_input_tokens'
    - value: 128
      property:
        identifier: 'max_output_tokens'
    - value: 'random'
      property:
        identifier: "dataset"
  # measurements
  targetProperties:
    - identifier: "duration"
    - identifier: "completed"
    - identifier: "total_input_tokens"
    - identifier: "total_output_tokens"
    - identifier: "request_throughput"
    - identifier: "output_throughput"
    - identifier: "total_token_throughput"
    - identifier: "mean_ttft_ms"
    - identifier: "median_ttft_ms"
    - identifier: "std_ttft_ms"
    - identifier: "p25_ttft_ms"
    - identifier: "p50_ttft_ms"
    - identifier: "p75_ttft_ms"
    - identifier: "p99_ttft_ms"
    - identifier: "mean_tpot_ms"
    - identifier: "median_tpot_ms"
    - identifier: "std_tpot_ms"
    - identifier: "p25_tpot_ms"
    - identifier: "p50_tpot_ms"
    - identifier: "p75_tpot_ms"
    - identifier: "p99_tpot_ms"
    - identifier: "mean_itl_ms"
    - identifier: "median_itl_ms"
    - identifier: "std_itl_ms"
    - identifier: "p25_itl_ms"
    - identifier: "p50_itl_ms"
    - identifier: "p75_itl_ms"
    - identifier: "p99_itl_ms"
    - identifier: "mean_e2el_ms"
    - identifier: "median_e2el_ms"
    - identifier: "std_e2el_ms"
    - identifier: "p25_e2el_ms"
    - identifier: "p50_e2el_ms"
    - identifier: "p75_e2el_ms"
    - identifier: "p99_e2el_ms"
  metadata:
    description: 'Test inference performance of a model served by vLLM endpoint across inference workload configurations'
test-deployment-guidellm-v1:
  identifier: test-deployment-guidellm-v1
  actuatorIdentifier: "vllm_performance"
  requiredProperties:
    - identifier: 'model'
      metadata:
        description: 'model to use for testing. Must be available through the huggingface hub'
      propertyDomain:
        variableType: "OPEN_CATEGORICAL_VARIABLE_TYPE"
        values: ["meta-llama/Llama-3.1-8B-Instruct", "ibm-granite/granite-3.3-8b-instruct", "openai/gpt-oss-20b"]
    - identifier: 'request_rate'
      metadata:
        description: "(benchmark) The number of requests to send per second"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [-1, 1000]
        interval: 1 # -1 means send all requests at time 0
  optionalProperties:
    - identifier: 'num_prompts'
      metadata:
        description: "(benchmark) The number of prompts to send (total number of requests)"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 10001]
        interval: 1
    - identifier: 'max_concurrency'
      metadata:
        description: "(benchmark) The maximum number of concurrent requests to send"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [-1, 500] # -1 means no concurrency control
        interval: 1
    - identifier: 'number_input_tokens'
      metadata:
        description: "(benchmark) The number of input tokens to send per request"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 128000]
        interval: 1
    - identifier: 'max_output_tokens'
      metadata:
        description: "(benchmark) The maximum number of output tokens to generate per request"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 10000]
        interval: 1
    - identifier: 'burstiness'
      metadata:
        description: "(benchmark) The burstiness of the requests - forced to 1.0 to match the meaning of burstiness=1 in vLLM experiments (Poisson distribution). GuideLLM uses poisson profile for this behavior."
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        values: [1.0]
    - identifier: image
      metadata:
        description: "(deployment) Docker image to use to create vllm deployments"
      propertyDomain:
        variableType: "OPEN_CATEGORICAL_VARIABLE_TYPE"
        values: ["vllm/vllm-openai:v0.14.0"]
    - identifier: n_cpus
      metadata:
        description: "(deployment) the number of CPUs to use"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 17]
        interval: 1
    - identifier: memory
      metadata:
        description: "(deployment) the amount of memory to allocate to vLLM pod"
      propertyDomain:
        variableType: "CATEGORICAL_VARIABLE_TYPE"
        values: ["64Gi", "128Gi", "256Gi"]
    - identifier: dtype
      metadata:
        description: '(deployment) data type for model weights and activations. "auto" will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models.'
      propertyDomain:
        variableType: "CATEGORICAL_VARIABLE_TYPE"
        values: ["auto", "half", "float16", "bfloat16", "float", "float32"]
    - identifier: 'gpu_memory_utilization'
      metadata:
        description: "(deployment) The fraction of GPU memory to be used for the model executor,"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        values: [.5, .75, .9]
    - identifier: 'cpu_offload'
      metadata:
        description: "(deployment) The amount of model weights in GB to offload to the CPU per GPU. 0 means all weights are on GPU,"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        values: [0, 8, 16, 24, 32]
    - identifier: 'max_batch_tokens'
      metadata:
        description: "(deployment) maximum number of batched tokens per iteration"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [8192, 32769]
        interval: 1024
    - identifier: 'max_num_seq'
      metadata:
        description: "(deployment) Maximum number of sequences per iteration"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [32, 2049]
        interval: 32
    - identifier: 'n_gpus'
      metadata:
        description: "(deployment) Number of GPUs to use"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 9]
        interval: 1
    - identifier: 'gpu_type'
      metadata:
        description: "(deployment) The GPU type to use"
      propertyDomain:
        variableType: "OPEN_CATEGORICAL_VARIABLE_TYPE"
        values: ['NVIDIA-A100-80GB-PCIe', 'NVIDIA-A100-SXM4-80GB']
    - identifier: 'dataset'
      metadata:
        description: "(benchmark) The dataset to use for the benchmark"
      propertyDomain:
        variableType: 'CATEGORICAL_VARIABLE_TYPE'
        values: ['random']
  defaultParameterization:
    - property:
        identifier: 'image'
      value: "vllm/vllm-openai:v0.14.0"
    - property:
        identifier: n_cpus
      value: 8
    - property:
        identifier: memory
      value: "128Gi"
    - property:
        identifier: dtype
      value: "auto"
    - property:
        identifier: 'num_prompts'
      value: 500
    - property:
        identifier: 'max_concurrency'
      value: -1
    - property:
        identifier: 'number_input_tokens'
      value: 1024
    - property:
        identifier: 'max_output_tokens'
      value: 128
    - property:
        identifier: 'burstiness'
      value: 1.0
    - property:
        identifier: 'dataset'
      value: 'random'
    - property:
        identifier: 'gpu_memory_utilization'
      value: .9
    - property:
        identifier: 'cpu_offload'
      value: 0
    - property:
        identifier: 'max_batch_tokens'
      value: 16384
    - property:
        identifier: 'max_num_seq'
      value: 256
    - property:
        identifier: 'n_gpus'
      value: 1
    - property:
        identifier: 'gpu_type'
      value: 'NVIDIA-A100-80GB-PCIe'
  targetProperties:
    - identifier: "duration"
    - identifier: "completed"
    - identifier: "total_input_tokens"
    - identifier: "total_output_tokens"
    - identifier: "request_throughput"
    - identifier: "output_throughput"
    - identifier: "total_token_throughput"
    - identifier: "mean_ttft_ms"
    - identifier: "median_ttft_ms"
    - identifier: "std_ttft_ms"
    - identifier: "p25_ttft_ms"
    - identifier: "p50_ttft_ms"
    - identifier: "p75_ttft_ms"
    - identifier: "p99_ttft_ms"
    - identifier: "mean_tpot_ms"
    - identifier: "median_tpot_ms"
    - identifier: "std_tpot_ms"
    - identifier: "p25_tpot_ms"
    - identifier: "p50_tpot_ms"
    - identifier: "p75_tpot_ms"
    - identifier: "p99_tpot_ms"
    - identifier: "mean_itl_ms"
    - identifier: "median_itl_ms"
    - identifier: "std_itl_ms"
    - identifier: "p25_itl_ms"
    - identifier: "p50_itl_ms"
    - identifier: "p75_itl_ms"
    - identifier: "p99_itl_ms"
    - identifier: "mean_e2el_ms"
    - identifier: "median_e2el_ms"
    - identifier: "std_e2el_ms"
    - identifier: "p25_e2el_ms"
    - identifier: "p50_e2el_ms"
    - identifier: "p75_e2el_ms"
    - identifier: "p99_e2el_ms"
  metadata:
    description: 'VLLM performance testing using GuideLLM benchmark suite across compute resource and workload configuration'
test-endpoint-guidellm-v1:
  identifier: test-endpoint-guidellm-v1
  actuatorIdentifier: "vllm_performance"
  requiredProperties:
    - identifier: 'model'
      metadata:
        description: 'model to use for testing. Assumed to be served by all endpoints tested. Required to obtain correct tokenizer for benchmarking metrics calculation'
      propertyDomain:
        variableType: "OPEN_CATEGORICAL_VARIABLE_TYPE"
        values: ["meta-llama/Llama-3.1-8B-Instruct", "ibm-granite/granite-3.3-8b-instruct", "openai/gpt-oss-20b"]
    - identifier: 'endpoint'
      metadata:
        description: 'The endpoint(s) to test'
      propertyDomain:
        variableType: "OPEN_CATEGORICAL_VARIABLE_TYPE"
        values: ["http://localhost:8000"]
    - identifier: 'request_rate'
      metadata:
        description: "The number of requests to send per second"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [-1, 1000]
        interval: 1 # -1 means send all requests at time 0
  optionalProperties:
    - identifier: 'num_prompts'
      metadata:
        description: "The number of prompts to send (total number of requests)"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 10001]
        interval: 1
    - identifier: 'number_input_tokens'
      metadata:
        description: "The number of input tokens to send per request"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 128000]
        interval: 1
    - identifier: 'max_output_tokens'
      metadata:
        description: "The maximum number of output tokens to generate per request"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [1, 10000]
        interval: 1
    - identifier: 'burstiness'
      metadata:
        description: "(benchmark) The burstiness of the requests - forced to 1.0 to match the meaning of burstiness=1 in vLLM experiments (Poisson distribution). GuideLLM uses poisson profile for this behavior."
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        values: [1.0]
    - identifier: 'max_concurrency'
      metadata:
        description: "The maximum number of concurrent requests to send"
      propertyDomain:
        variableType: 'DISCRETE_VARIABLE_TYPE'
        domainRange: [-1, 500] # -1 means no concurrency control
        interval: 1
    - identifier: 'dataset'
      metadata:
        description: "(benchmark) The dataset to use for the benchmark"
      propertyDomain:
        variableType: 'CATEGORICAL_VARIABLE_TYPE'
        values: ['random']
  defaultParameterization:
    - value: 1000
      property:
        identifier: 'num_prompts'
    - value: -1
      property:
        identifier: 'max_concurrency'
    - value: 1024
      property:
        identifier: 'number_input_tokens'
    - value: 128
      property:
        identifier: 'max_output_tokens'
    - value: 1.0
      property:
        identifier: 'burstiness'
    - value: 'random'
      property:
        identifier: 'dataset'
  targetProperties:
    - identifier: "duration"
    - identifier: "completed"
    - identifier: "total_input_tokens"
    - identifier: "total_output_tokens"
    - identifier: "request_throughput"
    - identifier: "output_throughput"
    - identifier: "total_token_throughput"
    - identifier: "mean_ttft_ms"
    - identifier: "median_ttft_ms"
    - identifier: "std_ttft_ms"
    - identifier: "p25_ttft_ms"
    - identifier: "p50_ttft_ms"
    - identifier: "p75_ttft_ms"
    - identifier: "p99_ttft_ms"
    - identifier: "mean_tpot_ms"
    - identifier: "median_tpot_ms"
    - identifier: "std_tpot_ms"
    - identifier: "p25_tpot_ms"
    - identifier: "p50_tpot_ms"
    - identifier: "p75_tpot_ms"
    - identifier: "p99_tpot_ms"
    - identifier: "mean_itl_ms"
    - identifier: "median_itl_ms"
    - identifier: "std_itl_ms"
    - identifier: "p25_itl_ms"
    - identifier: "p50_itl_ms"
    - identifier: "p75_itl_ms"
    - identifier: "p99_itl_ms"
    - identifier: "mean_e2el_ms"
    - identifier: "median_e2el_ms"
    - identifier: "std_e2el_ms"
    - identifier: "p25_e2el_ms"
    - identifier: "p50_e2el_ms"
    - identifier: "p75_e2el_ms"
    - identifier: "p99_e2el_ms"
  metadata:
    description: 'Test inference performance of a model served by vLLM endpoint using GuideLLM benchmark suite across inference workload configurations'
