---
name: analyze
description: "Perform a full 3-stage analysis following the Reality Check methodology. Use for manual analysis without automatic database registration."
---

<!-- GENERATED FILE - DO NOT EDIT DIRECTLY -->
<!-- Source: integrations/_templates/ + _config/skills.yaml -->
<!-- Regenerate: make assemble-skills -->

# Manual 3-Stage Analysis (Codex)

Perform a full 3-stage analysis following the Reality Check methodology. Use for manual analysis without automatic database registration.

## Invocation

```
$analyze <url>
```

Note: Codex reserves `/...` for built-in commands. Use `$analyze` instead.

Perform a full 3-stage analysis following the Reality Check methodology. Use this for manual analysis when you want to review claims before registering them.

## Prerequisites

### Environment

Set `REALITYCHECK_DATA` to point to your data repository:

```bash
export REALITYCHECK_DATA=/path/to/realitycheck-data/data/realitycheck.lance
```

The `PROJECT_ROOT` is derived from this path - all analysis files go there.

### CLI Commands

Reality Check provides CLI tools (`rc-db`, `rc-validate`, `rc-export`, `rc-embed`).

**Check availability:**
```bash
which rc-db  # Should show path if pip-installed
```

**If commands are not found**, either:
1. Install: `pip install realitycheck` (recommended)
2. Use `uv run` from framework directory: `uv run python scripts/db.py ...`
3. Add framework as submodule and use: `.framework/scripts/db.py ...`

### Red Flags: Wrong Repository

**IMPORTANT**: Always write to the DATA repository, never to the framework repository.

If you see these directories, you're in the **framework** repo (wrong place for data):
- `scripts/`
- `tests/`
- `integrations/`
- `methodology/`

Stop and verify `REALITYCHECK_DATA` is set correctly.

### Data Source of Truth

**LanceDB is the source of truth**, not YAML files.

- Query sources: `rc-db source get <id>` or `rc-db source list`
- Query claims: `rc-db claim get <id>` or `rc-db claim list`
- Search: `rc-db search "query"`

**Ignore YAML files** like `claims/registry.yaml` or `reference/sources.yaml` - these are exports/legacy format.

## Fetching Content

To retrieve and parse source content:
- Primary: `WebFetch` for most URLs
- Alternative: `curl -L -sS "URL" | rc-html-extract - --format json`
- `rc-html-extract` returns structured `{title, published, text, headings, word_count}`

## Methodology

### Stage 1: Descriptive Analysis
- Summarize the source neutrally
- Extract key claims, predictions, and assumptions
- Identify theoretical lineage (what traditions/thinkers it builds on)
- Map argument structure (chain arguments, logical flow)
- Note scope and domain (what does this attempt to explain?)
- Define key terms with operational proxies

### Stage 2: Evaluative Analysis
- Assess internal coherence and logical consistency
- Evaluate evidence quality using Evidence Hierarchy (E1-E6)
- Verify key factual claims (especially crux claims)
- Search for disconfirming evidence (5 min minimum)
- Check for internal tensions/self-contradictions
- Note persuasion techniques and rhetorical devices
- Identify unstated assumptions and dependencies
- Flag unfalsifiable claims

### Stage 3: Dialectical Analysis
- Steelman the strongest version of the argument
- Identify strongest counterarguments and counterfactuals
- Consider base rates and historical analogs
- Map relationships to other theories (supporting/contradicting)
- Synthesize: where does this fit in the broader landscape?

---

## Evidence Hierarchy

Use this hierarchy to rate **strength of evidential support** for claims.

| Level | Strength | Description | Credence Range |
|-------|----------|-------------|----------------|
| **E1** | Strong Empirical | Systematic review, meta-analysis, replicated experiments | 0.9-1.0 |
| **E2** | Moderate Empirical | Single peer-reviewed study, official statistics | 0.6-0.8 |
| **E3** | Strong Theoretical | Expert consensus, working papers, preprints | 0.5-0.7 |
| **E4** | Weak Theoretical | Industry reports, credible journalism | 0.3-0.5 |
| **E5** | Opinion/Forecast | Personal observation, anecdote, expert opinion | 0.2-0.4 |
| **E6** | Unsupported | Pure speculation, unfalsifiable claims | 0.0-0.2 |

## Claim Types

| Type | Symbol | Definition |
|------|--------|------------|
| Fact | `[F]` | Empirically verified, consensus reality |
| Theory | `[T]` | Coherent explanatory framework with empirical support |
| Hypothesis | `[H]` | Testable proposition, awaiting evidence |
| Prediction | `[P]` | Future-oriented claim with specified conditions |
| Assumption | `[A]` | Underlying premise (stated or unstated) |
| Counterfactual | `[C]` | Alternative scenario for comparison |
| Speculation | `[S]` | Unfalsifiable or untestable claim |
| Contradiction | `[X]` | Identified logical inconsistency |

---

## Output

Generate:
1. Analysis document following the 3-stage methodology
2. Extracted claims formatted for database registration

**Rigor contract (v1)**:
- Claim tables should include `Layer`, `Actor`, `Scope`, and `Quantifier` fields (see `docs/WORKFLOWS.md` → “Analysis Rigor Contract (v1)”).
- Include a `Corrections & Updates` section (including capture failures and recency checks) for auditability.

After analysis, register manually:
```bash
rc-db source add --id "..." --title "..." --type "..." --author "..." --year YYYY --url "..."
rc-db claim add --id "..." --text "..." --type "[F]" --domain "..." --evidence-level "E2" --credence 0.XX --source-ids "..."
```

Or use `$check` for fully automated analysis + registration.

---

## Related Skills

- `$check`
- `$extract`
