Metadata-Version: 2.4
Name: whisper-s2t-reborn
Version: 1.4.3
Summary: A streamlined Speech-to-Text pipeline for Whisper using CTranslate2
Author: Blair Chintella
License: MIT
Project-URL: Homepage, https://github.com/BBC-Esq/WhisperS2T-reborn
Requires-Python: <3.14,>=3.11
Description-Content-Type: text/markdown
Requires-Dist: ctranslate2==4.6.2
Requires-Dist: huggingface-hub
Requires-Dist: numpy<3.0.0,>=1.26.4
Requires-Dist: platformdirs
Requires-Dist: requests
Requires-Dist: rich
Requires-Dist: tokenizers
Requires-Dist: torch<=2.9.1,>=2.8.0
Requires-Dist: tqdm
Provides-Extra: gpu
Requires-Dist: nvidia-cuda-runtime-cu12==12.8.90; extra == "gpu"
Requires-Dist: nvidia-cublas-cu12==12.8.4.1; extra == "gpu"
Requires-Dist: nvidia-cudnn-cu12==9.10.2.21; extra == "gpu"

<h1 align="center">WhisperS2T-Reborn âš¡</h1>
<p align="center"><b>A Streamlined Speech-to-Text Pipeline for Whisper Models using CTranslate2</b></p>

<hr><br>

WhisperS2T-Reborn is a streamlined fork of the original [WhisperS2T](https://github.com/shashikg/WhisperS2T) project, focused exclusively on the CTranslate2 backend for fast and efficient speech transcription.

## What's Different from the Original?

This fork simplifies the original WhisperS2T by:

- **Single Backend Focus**: Removed TensorRT-LLM, HuggingFace, and OpenAI backendsâ€”CTranslate2 only
- **Curated Model Selection**: Uses optimized CTranslate2 whisper models from [ctranslate2-4you](https://huggingface.co/ctranslate2-4you) on HuggingFace
- **Cleaner Codebase**: Streamlined architecture with reduced dependencies
- **Simplified Setup**: Easier installation without complex backend configurations

## Features

- ðŸš€ **Fast Inference**: CTranslate2 backend provides excellent speed/accuracy tradeoff
- ðŸŽ™ï¸ **Built-in VAD**: Integrated Voice Activity Detection using NeMo's Marblenet models
- ðŸŽ§ **Flexible Audio Input**: Handles both small and large audio files efficiently
- ðŸŒ **Multi-language Support**: Transcription and translation for 99+ languages
- â±ï¸ **Word-level Timestamps**: Optional word alignment for precise timing
- ðŸ“ **Multiple Export Formats**: Export to TXT, JSON, TSV, SRT, and VTT

## Supported Models

| Model | English-only | Multilingual |
|-------|--------------|--------------|
| tiny | âœ… tiny.en | âœ… tiny |
| base | âœ… base.en | âœ… base |
| small | âœ… small.en | âœ… small |
| medium | âœ… medium.en | âœ… medium |
| large-v3 | â€” | âœ… large-v3 |
| distil-small.en | âœ… | â€” |
| distil-medium.en | âœ… | â€” |
| distil-large-v3 | â€” | âœ… |

## Installation

### Prerequisites

Install FFmpeg for audio processing:

**Ubuntu/Debian:**
```sh
apt-get install -y libsndfile1 ffmpeg
```

**macOS:**
```sh
brew install ffmpeg
```

**Conda (any platform):**
```sh
conda install conda-forge::ffmpeg
```

### Install WhisperS2T-Reborn

**CPU only:**
```sh
pip install whisper-s2t-reborn
```

**With GPU support (recommended for faster inference):**
```sh
pip install whisper-s2t-reborn[gpu]
```

> **Note:** The `[gpu]` extra installs NVIDIA CUDA libraries required for GPU acceleration with CTranslate2. Requires an NVIDIA GPU with compatible drivers.

## Quick Start

### Basic Transcription
```python
import whisper_s2t

# Load model (downloads automatically on first use)
model = whisper_s2t.load_model(model_identifier="large-v3")

# Transcribe with VAD
files = ['audio/sample.wav']
out = model.transcribe_with_vad(files,
                                lang_codes=['en'],
                                tasks=['transcribe'],
                                initial_prompts=[None],
                                batch_size=32)

print(out[0][0])
# {'text': 'Your transcribed text here...',
#  'avg_logprob': -0.25,
#  'no_speech_prob': 0.0001,
#  'start_time': 0.0,
#  'end_time': 24.8}
```

### With Word Timestamps
```python
model = whisper_s2t.load_model("large-v3", asr_options={'word_timestamps': True})

out = model.transcribe_with_vad(files,
                                lang_codes=['en'],
                                tasks=['transcribe'],
                                initial_prompts=[None],
                                batch_size=32)
```

### Export Transcripts
```python
from whisper_s2t import write_outputs

# Export to various formats
write_outputs(out, format='srt', save_dir='./output/')
write_outputs(out, format='vtt', save_dir='./output/')
write_outputs(out, format='json', save_dir='./output/')
```

### Translation
```python
# Translate non-English audio to English
out = model.transcribe_with_vad(files,
                                lang_codes=['fr'],  # Source language
                                tasks=['translate'],  # Translate to English
                                initial_prompts=[None],
                                batch_size=32)
```

## Configuration Options

### Model Loading Options
```python
model = whisper_s2t.load_model(
    model_identifier="large-v3",  # Model name or path
    device="cuda",                 # "cuda" or "cpu"
    compute_type="float16",        # "float16", "float32", or "bfloat16"
    asr_options={
        'beam_size': 5,
        'word_timestamps': False,
        'repetition_penalty': 1.01,
    }
)
```

### Transcription Options
```python
out = model.transcribe_with_vad(
    files,
    lang_codes=['en'],           # Language codes for each file
    tasks=['transcribe'],        # 'transcribe' or 'translate'
    initial_prompts=[None],      # Optional prompts for each file
    batch_size=32                # Batch size for inference
)
```

## Acknowledgements

- [**Original WhisperS2T**](https://github.com/shashikg/WhisperS2T): This project is a fork of WhisperS2T by Shashi Kant Gupta
- [**OpenAI Whisper**](https://github.com/openai/whisper): The foundational Whisper model
- [**CTranslate2**](https://github.com/OpenNMT/CTranslate2/): Fast inference engine for Transformer models
- [**NVIDIA NeMo**](https://github.com/NVIDIA/NeMo): VAD models used in this pipeline
