# LLM Benchmark Output Example

This is an example of what the output looks like when running `make benchmark-llm`.

## Example Run

```bash
$ make benchmark-llm
Running LLM model benchmarks...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           LLM Model Benchmark Results                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Testing query: where is similarity_threshold configured?

  â†’ Testing anthropic/claude-3.5-sonnet... âœ“ 2.3s, $0.0115
  â†’ Testing anthropic/claude-3-haiku... âœ“ 0.9s, $0.0008
  â†’ Testing openai/gpt-4o... âœ“ 2.5s, $0.0081
  â†’ Testing openai/gpt-4o-mini... âœ“ 1.1s, $0.0004
  â†’ Testing google/gemini-flash-1.5... âœ“ 0.7s, $0.0002
  â†’ Testing meta-llama/llama-3.1-70b-instruct... âœ“ 1.8s, $0.0011
  â†’ Testing mistralai/mistral-large... âœ“ 1.5s, $0.0063


Query: "where is similarity_threshold configured?"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model                           â”‚ Time(s) â”‚ Input  â”‚ Output â”‚ Cost($)  â”‚ Quality â”‚ Status          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gemini-flash-1.5                â”‚    0.7s â”‚   1245 â”‚    356 â”‚  $0.0002 â”‚ â˜…â˜…â˜…â˜†â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ claude-3-haiku                  â”‚    0.9s â”‚   1245 â”‚    412 â”‚  $0.0008 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ gpt-4o-mini                     â”‚    1.1s â”‚   1245 â”‚    389 â”‚  $0.0004 â”‚ â˜…â˜…â˜…â˜†â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ mistral-large                   â”‚    1.5s â”‚   1245 â”‚    445 â”‚  $0.0063 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ llama-3.1-70b-instruct          â”‚    1.8s â”‚   1245 â”‚    423 â”‚  $0.0011 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ claude-3.5-sonnet               â”‚    2.3s â”‚   1245 â”‚    523 â”‚  $0.0115 â”‚ â˜…â˜…â˜…â˜…â˜…  â”‚ âœ“ 5 results     â”‚
â”‚ gpt-4o                          â”‚    2.5s â”‚   1245 â”‚    498 â”‚  $0.0081 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Testing query: how does the indexer handle TypeScript files?

  â†’ Testing anthropic/claude-3.5-sonnet... âœ“ 2.1s, $0.0109
  â†’ Testing anthropic/claude-3-haiku... âœ“ 0.8s, $0.0007
  â†’ Testing openai/gpt-4o... âœ“ 2.3s, $0.0075
  â†’ Testing openai/gpt-4o-mini... âœ“ 1.0s, $0.0003
  â†’ Testing google/gemini-flash-1.5... âœ“ 0.6s, $0.0001
  â†’ Testing meta-llama/llama-3.1-70b-instruct... âœ“ 1.7s, $0.0010
  â†’ Testing mistralai/mistral-large... âœ“ 1.4s, $0.0058


Query: "how does the indexer handle TypeScript files?"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model                           â”‚ Time(s) â”‚ Input  â”‚ Output â”‚ Cost($)  â”‚ Quality â”‚ Status          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gemini-flash-1.5                â”‚    0.6s â”‚   1312 â”‚    334 â”‚  $0.0001 â”‚ â˜…â˜…â˜…â˜†â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ claude-3-haiku                  â”‚    0.8s â”‚   1312 â”‚    398 â”‚  $0.0007 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ gpt-4o-mini                     â”‚    1.0s â”‚   1312 â”‚    371 â”‚  $0.0003 â”‚ â˜…â˜…â˜…â˜†â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ mistral-large                   â”‚    1.4s â”‚   1312 â”‚    429 â”‚  $0.0058 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ llama-3.1-70b-instruct          â”‚    1.7s â”‚   1312 â”‚    407 â”‚  $0.0010 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ claude-3.5-sonnet               â”‚    2.1s â”‚   1312 â”‚    509 â”‚  $0.0109 â”‚ â˜…â˜…â˜…â˜…â˜…  â”‚ âœ“ 5 results     â”‚
â”‚ gpt-4o                          â”‚    2.3s â”‚   1312 â”‚    482 â”‚  $0.0075 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Testing query: show me examples of error handling in the search module

  â†’ Testing anthropic/claude-3.5-sonnet... âœ“ 2.4s, $0.0122
  â†’ Testing anthropic/claude-3-haiku... âœ“ 0.9s, $0.0009
  â†’ Testing openai/gpt-4o... âœ“ 2.6s, $0.0087
  â†’ Testing openai/gpt-4o-mini... âœ“ 1.2s, $0.0005
  â†’ Testing google/gemini-flash-1.5... âœ“ 0.8s, $0.0002
  â†’ Testing meta-llama/llama-3.1-70b-instruct... âœ“ 1.9s, $0.0012
  â†’ Testing mistralai/mistral-large... âœ“ 1.6s, $0.0067


Query: "show me examples of error handling in the search module"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model                           â”‚ Time(s) â”‚ Input  â”‚ Output â”‚ Cost($)  â”‚ Quality â”‚ Status          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gemini-flash-1.5                â”‚    0.8s â”‚   1389 â”‚    367 â”‚  $0.0002 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ claude-3-haiku                  â”‚    0.9s â”‚   1389 â”‚    428 â”‚  $0.0009 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ gpt-4o-mini                     â”‚    1.2s â”‚   1389 â”‚    395 â”‚  $0.0005 â”‚ â˜…â˜…â˜…â˜†â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ mistral-large                   â”‚    1.6s â”‚   1389 â”‚    458 â”‚  $0.0067 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ llama-3.1-70b-instruct          â”‚    1.9s â”‚   1389 â”‚    437 â”‚  $0.0012 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â”‚ claude-3.5-sonnet               â”‚    2.4s â”‚   1389 â”‚    541 â”‚  $0.0122 â”‚ â˜…â˜…â˜…â˜…â˜…  â”‚ âœ“ 5 results     â”‚
â”‚ gpt-4o                          â”‚    2.6s â”‚   1389 â”‚    512 â”‚  $0.0087 â”‚ â˜…â˜…â˜…â˜…â˜†  â”‚ âœ“ 5 results     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â•â•â• Benchmark Summary â•â•â•

Performance by Model:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model                   â”‚ Avg Time â”‚ Avg Cost â”‚ Avg Quality â”‚ Success Rate â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gemini-flash-1.5        â”‚     0.7s â”‚ $0.0002  â”‚ â˜…â˜…â˜…â˜†â˜†      â”‚         100% â”‚
â”‚ claude-3-haiku          â”‚     0.9s â”‚ $0.0008  â”‚ â˜…â˜…â˜…â˜…â˜†      â”‚         100% â”‚
â”‚ gpt-4o-mini             â”‚     1.1s â”‚ $0.0004  â”‚ â˜…â˜…â˜…â˜†â˜†      â”‚         100% â”‚
â”‚ mistral-large           â”‚     1.5s â”‚ $0.0063  â”‚ â˜…â˜…â˜…â˜…â˜†      â”‚         100% â”‚
â”‚ llama-3.1-70b-instruct  â”‚     1.8s â”‚ $0.0011  â”‚ â˜…â˜…â˜…â˜…â˜†      â”‚         100% â”‚
â”‚ claude-3.5-sonnet       â”‚     2.3s â”‚ $0.0115  â”‚ â˜…â˜…â˜…â˜…â˜…      â”‚         100% â”‚
â”‚ gpt-4o                  â”‚     2.5s â”‚ $0.0081  â”‚ â˜…â˜…â˜…â˜…â˜†      â”‚         100% â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ Recommendations:

  ğŸƒ Fastest: google/gemini-flash-1.5 (0.7s avg)
  ğŸ’° Cheapest: google/gemini-flash-1.5 ($0.0002 avg)
  â­ Best Quality: anthropic/claude-3.5-sonnet

ğŸ¯ Overall Recommendation:
  For speed: Use google/gemini-flash-1.5 (~0.7s per query)
  For cost: Use google/gemini-flash-1.5 (~$0.0002 per query)
  For quality: Use anthropic/claude-3.5-sonnet (best result relevance)

Benchmark completed!
```

## Key Insights from Example

### Speed Leaders
1. **Google Gemini Flash 1.5**: 0.7s average - 3x faster than premium models
2. **Claude 3 Haiku**: 0.9s average - Best balance of speed and quality
3. **GPT-4o Mini**: 1.1s average - Good OpenAI option for speed

### Cost Leaders
1. **Google Gemini Flash 1.5**: $0.0002/query - 57x cheaper than Claude 3.5 Sonnet
2. **GPT-4o Mini**: $0.0004/query - Good budget option
3. **Claude 3 Haiku**: $0.0008/query - Best value for quality

### Quality Leaders
1. **Claude 3.5 Sonnet**: â˜…â˜…â˜…â˜…â˜… - Best result relevance
2. **Claude 3 Haiku**: â˜…â˜…â˜…â˜…â˜† - Very good accuracy
3. **GPT-4o**: â˜…â˜…â˜…â˜…â˜† - Strong performance

### Cost vs Performance Analysis

**Budget Tier (<$0.001/query)**:
- Gemini Flash ($0.0002): Best overall value - fast, cheap, decent quality
- GPT-4o Mini ($0.0004): Slightly better quality than Gemini
- Claude Haiku ($0.0008): Best accuracy in budget tier

**Premium Tier (>$0.007/query)**:
- Claude 3.5 Sonnet ($0.0115): Best quality, slower
- GPT-4o ($0.0081): Good quality, slower
- Mistral Large ($0.0063): Mid-tier option

### Use Case Recommendations

| Scenario | Recommended Model | Rationale |
|----------|------------------|-----------|
| **Development/Testing** | `claude-3-haiku` | Fast (0.9s), cheap ($0.0008), accurate (â˜…â˜…â˜…â˜…â˜†) |
| **Production Chat** | `claude-3.5-sonnet` | Best quality (â˜…â˜…â˜…â˜…â˜…), worth the cost |
| **High-Volume Queries** | `gemini-flash-1.5` | Cheapest ($0.0002), still good quality |
| **Cost-Conscious Production** | `claude-3-haiku` | Best balance of all factors |
| **Complex/Nuanced Queries** | `claude-3.5-sonnet` | Best understanding of subtlety |
| **Quick Lookups** | `gemini-flash-1.5` | Fastest (0.7s), nearly free |

## How to Read Quality Ratings

The quality rating (â˜…â˜…â˜…â˜…â˜…) is based on:

- **â˜…â˜…â˜…â˜†â˜†** (3 stars): Returned ranked results successfully
- **â˜…â˜…â˜…â˜…â˜†** (4 stars): Returned results + found good coverage (â‰¥5 results)
- **â˜…â˜…â˜…â˜…â˜…** (5 stars): Perfect - results + coverage + multiple search queries

In the example above:
- Claude 3.5 Sonnet: â˜…â˜…â˜…â˜…â˜… (perfect execution)
- Claude Haiku, Mistral, Llama: â˜…â˜…â˜…â˜…â˜† (very good)
- Gemini Flash, GPT-4o Mini: â˜…â˜…â˜…â˜†â˜† (good, found results but limited coverage)

## Interpreting Success Rate

All models showed 100% success rate in this example, meaning:
- No API errors (401, 429, 500)
- All queries generated successfully
- All searches completed
- All results ranked by LLM

If success rate < 100%, possible causes:
- Rate limiting (429 errors)
- API key issues (401 errors)
- Timeout errors
- Model unavailability

## Running Your Own Benchmark

To reproduce these results:

```bash
# Set API key
export OPENROUTER_API_KEY='your-key-here'

# Ensure project is indexed
mcp-vector-search index

# Run full benchmark
make benchmark-llm

# Or test specific models
make benchmark-llm-fast  # Just cheap models

# Or custom query
make benchmark-llm-query QUERY="your custom query"
```

Expected runtime: 5-10 minutes (with rate limiting delays)

## See Also

- [LLM Benchmarking Guide](../guides/llm-benchmarking.md) - Full documentation
- [Chat Command Documentation](../reference/chat-command.md) - Using the chat command
- [OpenRouter Models](https://openrouter.ai/models) - Model details and pricing
