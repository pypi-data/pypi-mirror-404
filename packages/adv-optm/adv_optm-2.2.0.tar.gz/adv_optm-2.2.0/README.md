# Advanced Optimizers (AIO)

A comprehensive, all-in-one collection of optimization algorithms for deep learning, designed for **maximum efficiency**, **minimal memory footprint**, and **superior performance** across diverse model architectures and training scenarios.

[![PyPI](https://img.shields.io/pypi/v/adv_optm)](https://pypi.org/project/adv_optm/)

## üî• What's New

### in 2.1.x

- Added Signum (SignSGD with momentum): A new optimizer in the family (SignSGD_adv)
- More info coming soon.

### in 2.0.x

* Implemented torch.compile for all advanced optimizers. Enabled via (compiled_optimizer=True) to fuse and optimize the optimizer step path.
* Better and improved 1-bit factored mode via (nnmf_factor=True).
* Various improvements across the optimizers.

### in 1.2.x
* Added **advanced variants** of [Muon optimizer](https://kellerjordan.github.io/posts/muon/) with **features** and **settings** from recent papers.

| Optimizer | Description |
|---|---|
| `Muon_adv` | Advanced Muon implementation with CANS, NorMuon, Low-Rank ortho, etc. features. |
| `AdaMuon_adv` | Advanced AdaMuon implementation, which combines Muon's geometry with Adam-like adaptive scaling and sign-based orthogonalization. |

> *Documentation coming soon.*

* Implemented [Cautious Weight Decay](https://arxiv.org/abs/2510.12402) for all advanced optimizers.

* Improved parameter update and weight decay for **BF16** with **stochastic rounding**. The updates are now accumulated in **float32** and rounded once at the end.

* Use fused and in-place operations whenever possible for all advanced optimizers.

* **Prodigy variants** are now **50% faster** by [avoiding CUDA syncs](https://github.com/Koratahiu/Advanced_Optimizers/pull/5). Thanks to **@dxqb**!

---

## üì¶ Installation

```bash
pip install adv_optm
```

---

## üß† Core Innovations

This library integrates multiple state-of-the-art optimization techniques validated through extensive research and practical training, with **1-bit compression for optimizer states**:

### **Memory-Efficient Optimization (SMMF-inspired)**
- **Paper**: [SMMF: Square-Matricized Momentum Factorization](https://arxiv.org/abs/2412.08894)
- **Approach**: Uses rank-1 non-negative matrix factorization with reconstruction cycle (factor ‚Üí reconstruct ‚Üí update ‚Üí factor)
- **Innovation**:
  - First moment split into **1-bit sign + absolute value**
  - Final storage: **four factored vectors + one 1-bit sign state**
  - Preserves Adam-like update quality with drastically reduced memory

---

## ‚ö° Performance Characteristics

### Memory Efficiency (SDXL Model ‚Äì 6.5GB)
| Optimizer | Memory Usage | Description |
|-----------|--------------|-------------|
| `Adopt_Factored` | 328 MB | 4 small vectors + 1-bit state |
| `Adopt_Factored + AdEMAMix` | 625 MB | 6 small vectors + two 1-bit states |
| `Simplified_AdEMAMix` | 328 MB | Same as standard factored (no extra state) |

### Speed Comparison (SDXL, Batch Size 4)
| Optimizer | Speed | Notes |
|-----------|-------|-------|
| `Adafactor` | ~8.5s/it | Baseline |
| `Adopt_Factored` | ~10s/it | +18% overhead from compression |
| `Adopt_Factored + AdEMAMix` | ~12s/it | +41% overhead (3 factored states) |

---

## üß™ Available Optimizers

### Standard Optimizers (All support `factored=True/False`)
| Optimizer | Description | Best For |
|-----------|-------------|----------|
| `Adam_Adv` | Advanced Adam implementation | General purpose |
| `Adopt_Adv` | Adam-variant with independent beta2 | Stable training for small batch size regimes |
| `Prodigy_Adv` | Prodigy with D-Adaptation | Adam with automatic LR tuning |
| `Simplified_AdEMAMix` | Adam variant with accumulator momentum | Small/large batch training when tuned correctly |
| `Lion_Adv` | Advanced Lion implementation | Memory-constrained environments |
| `Prodigy_Lion_Adv` | Prodigy + Lion combination | Lion with automatic LR tuning |

---

## ‚öôÔ∏è Feature Matrix

| Feature | Adam_Adv | Adopt_Adv | Prodigy_Adv | Simplified_AdEMAMix | Lion_Adv |
|---------|----------|-----------|-------------|---------------------|----------|
| Factored | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì |
| AdEMAMix | ‚úì | ‚úì | ‚úì | ‚úó | ‚úó |
| Simplified_AdEMAMix | ‚úó | ‚úì | ‚úì | ‚úì | ‚úó |
| OrthoGrad | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì |
| Grams | ‚úì | ‚úì | ‚úì | ‚úó | ‚úó |
| Cautious | ‚úì | ‚úì | ‚úì | ‚úó | ‚úì |
| atan2 | ‚úì | ‚úì | ‚úì | ‚úó | ‚úó |
| Stochastic Rounding | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì |
| Fused Backward Pass | ‚úì | ‚úì | ‚úì | ‚úì | ‚úì |
| **Kourkoutas-Œ≤** | ‚úì | ‚úì | ‚úì | ‚úì | ‚úó |

---

## üõ†Ô∏è Comprehensive Feature Guide

### A. Universal Safe Features
*These features work with all optimizers and are generally safe to enable.*

| Feature | Description | Recommended Usage | Performance Impact | Theoretical Basis | Compatibility |
|--------|-------------|-------------------|--------------------|-------------------|--------------|
| **Fused Back Pass** | Fuses backward pass; gradients used immediately and memory freed on-the-fly | Memory-constrained environments | Reduces peak memory | Memory optimization | All optimizers |
| **Stochastic Rounding** | Replaces nearest rounding with stochastic rounding to preserve small gradient updates in BF16 | BF16 training | Minimal overhead (<5%) | [Revisiting BFloat16 Training](https://arxiv.org/abs/2010.06192) | All optimizers |
| **OrthoGrad** | Removes gradient component parallel to weights to reduce overfitting | Full fine-tuning without weight decay | +33% time overhead (BS=4); less at larger BS | [Grokking at Edge](https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability) | All optimizers |
| **Factored** | Memory-efficient optimization via rank-1 1-bit factorization of optimizer states | Large models / memory-limited hardware | Adds compression overhead | [SMMF](https://arxiv.org/abs/2412.08894) | All optimizers |

### B. Individual Features

| Feature | Description | Recommended Usage | Performance Impact | Theoretical Basis | Compatibility |
|--------|-------------|-------------------|--------------------|-------------------|--------------|
| **Cautious** | Only applies update if gradient direction aligns with momentum direction | Accelerating convergence | No overhead | [C-Optim](https://github.com/kyleliang919/C-Optim) | Adam/Adopt/Prodigy/Lion |
| **Grams** | Update direction derived purely from current gradient | When Cautious is insufficient | No overhead | [Grams](https://github.com/Gunale0926/Grams) | Adam/Adopt/Prodigy |
| **AdEMAMix** | Dual EMA system that retains relevance of gradients over tens of thousands of steps | Long training runs, especially where model forgetting is a concern | +1 state memory | [AdEMAMix](https://arxiv.org/abs/2409.03137) | Adam/Adopt/Prodigy |
| **Simplified_AdEMAMix** | Accumulator-based momentum, single EMA variant of AdEMAMix | All scenarios when tuned correctly | No overhead | [Connections](https://arxiv.org/abs/2502.02431) | Adam/Adopt/Prodigy |
| **atan2** | Robust epsilon replacement with built-in gradient clipping | Use for stable bounded updates (or for Adopt as it needs that) | No overhead | [Adam-atan2](https://github.com/lucidrains/adam-atan2-pytorch) | Adam/Adopt/Prodigy |
| **Kourkoutas-Œ≤** | Layer-wise adaptive Œ≤‚ÇÇ based on gradient ‚Äúsunspike‚Äù ratio | Noisy/small/large-batch/high-LR training | No overhead | [Kourkoutas-Œ≤]() | Adam/Adopt/Prodigy/Simplified_AdEMAMix |

> **Note**: If both **Cautious** and **Grams** are enabled, **Grams takes precedence** and Cautious is disabled.

---

## üîç Feature Deep Dives

### AdEMAMix

- Adds a **slow-decaying second EMA** (`beta3`) that retains gradient memory over tens of thousands of steps.
- Particularly effective for **small batch sizes**, where Adam‚Äôs standard first moment is nearly useless.

#### Tunable Hyperparameters
| Parameter | Default | Tuning Guide |
|-----------|---------|--------------|
| `beta3` | 0.9999 | ‚Ä¢ Runs >120k steps: **0.9999**<br>‚Ä¢ Runs ‚â§120k steps: **0.999** |
| `alpha` | 5 | ‚Ä¢ Reduce to **2‚Äì3** if diverging<br>‚Ä¢ Increase to strengthen long-term memory |

> ‚úÖ **Pro Tip**: Set `beta1=0` in Adam/Adopt/Prodigy to skip standard EMA entirely and rely solely on AdEMAMix‚Äôs slow EMA, ideal for small-batch regimes.

---

### Simplified_AdEMAMix

- Introduced in [Connections between Schedule-Free Optimizers, AdEMAMix, and Accelerated SGD Variants (arXiv:2502.02431)](https://arxiv.org/abs/2502.02431).
- Replaces Adam‚Äôs first moment with a **theory-based momentum** with emphasize on raw gradient, combining the stability of long memory with responsiveness to recent gradients.
- **Key insight**: Classical momentum **does not accelerate** in noisy (small-batch) regimes; this accumulator do.

#### Tunable Hyperparameters
| Parameter | Default | Tuning Guide |
|----------|---------|--------------|
| `beta1` | 0.99 | Controls accumulator memory length:<br>‚Ä¢ Small BS: **0.99‚Äì0.9999**<br>‚Ä¢ Large BS: **0.9** |
| `Grad Œ±` | 100 | Most critical parameter:<br>‚Ä¢ Inversely scales with batch size<br>‚Ä¢ **100‚Äì10** for small BS (‚â§32)<br>‚Ä¢ **1‚Äì0.1** for large BS (‚â•512) |

> ‚ö†Ô∏è **Critical**: Requires **~100x smaller learning rate** than AdamW (e.g., 1e-6 vs 1e-4).
> For `Prodigy_Adv`, set `initial_d` to:
> - **LoRA**: `1e-8`
> - **Full FT**: `1e-10`
> - **Embedding**: `1e-7`

> ‚ö†Ô∏è **Incompatible** with: **Cautious**, **Grams**, **atan2**, and standard update clipping.

---

### atan2

- Replaces `eps` in Adam-family optimizers with a **scale-invariant**, bounded update rule.
- Automatically clips updates to **[-2, 2]**, preventing destabilizing jumps.
- **Highly recommended** for `Adopt_Adv`, which is prone to instability without clipping.

> üìö **Reference**:
> - Paper: https://arxiv.org/abs/2407.05872
> - Code: https://github.com/lucidrains/adam-atan2-pytorch

---

### **Kourkoutas-Œ≤**

**Kourkoutas-Œ≤** introduces a **sunspike-driven, layer-wise adaptive second-moment decay (Œ≤‚ÇÇ)** as an optional enhancement for `Adam_Adv`, `Adopt_Adv`, `Prodigy_Adv`, and `Simplified_AdEMAMix`.

Instead of using a fixed Œ≤‚ÇÇ (e.g., 0.999 or 0.95), it **dynamically modulates Œ≤‚ÇÇ per layer** based on a bounded *sunspike ratio*:

- **During gradient bursts** ‚Üí Œ≤‚ÇÇ ‚Üì toward `Lower Œ≤‚ÇÇ` ‚Üí faster reaction
- **During calm phases** ‚Üí Œ≤‚ÇÇ ‚Üë toward `The Selected Œ≤‚ÇÇ` ‚Üí stronger smoothing

This is especially effective for **noisy training, small batch sizes, and high learning rates**, where gradient norms shift abruptly due to noise or aggressive LR schedules.

#### Pros/Cons

| **Category** | **Details** |
|--------------|-------------|
| ‚úÖ **Pros** | ‚Ä¢ **Layer-wise adaptation** blends benefits of high Œ≤‚ÇÇ (strong smoothing) and low Œ≤‚ÇÇ (fast reaction).<br>‚Ä¢ **Robust to sudden loss landscape shifts**, reacts quickly during gradient bursts, smooths during calm phases.<br>‚Ä¢ **High tolerance to aggressive learning rates**. |
| ‚ö†Ô∏è **Cons** | ‚Ä¢ **Potentially unstable at the start of training** due to unreliable early gradient norms; mitigated by using `K-Œ≤ Warmup Steps`. |

> üí° **Best Practice**: Set `K_warmup_steps` equal to your standard LR warmup steps. During warmup, the optimizer uses the static `beta2`; adaptation begins only after warmup ends.

> üìö **Reference**:
> - Paper: [Kourkoutas-Œ≤: A Sunspike-Driven Adam Optimizer with Desert Flair](https://arxiv.org/abs/2508.12996)
> - Code: [kbeta](https://github.com/sck-at-ucy/kbeta)

---

## üìö References

1. [Revisiting BFloat16 Training](https://arxiv.org/abs/2010.06192)
2. [SMMF: Square-Matricized Momentum Factorization](https://arxiv.org/abs/2412.08894)
3. [The AdEMAMix Optimizer](https://arxiv.org/abs/2409.03137)
4. [Connections between Schedule-Free Optimizers, AdEMAMix, and Accelerated SGD](https://arxiv.org/abs/2502.02431)
6. [Kourkoutas-Œ≤: A Sunspike-Driven Adam Optimizer with Desert Flair](https://arxiv.org/abs/2508.12996)
7. [Scaling Exponents Across Parameterizations and Optimizers](https://arxiv.org/abs/2407.05872)
