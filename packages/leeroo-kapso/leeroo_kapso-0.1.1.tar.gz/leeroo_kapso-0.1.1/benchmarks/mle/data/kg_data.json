{
  "nodes": {
    "0": {
      "id": "0",
      "name": "Kaggle competitions",
      "type": "specialization",
      "content": "This component of KG is specifiaclly built for solving problems in kaggle competitions. Below you can find some helpful tips for solving problems in kaggle competitions: \n- Always consider early stopping in your codes.\n- Always avoid OOF, cross validation and heavy folding as they are time consuming and doesn't help to the performance. Ensembling different models, is more helpful than using heavy folding. Also, split train and validation in an startified manner and be very carefull with any leakage when training. \n- Find the most valuable part of data and focus more on that. For example in competitiosn with more than one data type in training (e.g. Image + Text), make sure to consider all types and train at least one model for each type (some types may have ensemble of models). focus more on the one with more value and contribution (e.g. lower loss). \n - If ensembling and using a meta-model make 100% sure it has a higher performance than other models."
    },
    "1": {
      "id": "1",
      "name": "Text Classification Best Approaches",
      "type": "workflow",
      "content": "The best ways to solve text classification problems are: \n 1. Fine-tuning a pretrained transformer for example \"microsoft/deberta-v3-base\" or \"answerdotai/ModernBERT-base\", or even ensemble them. Also adding tf-idf is a must if applicable to the problem.  \n 2. Using pretrained transformer embeddings plus adding extra features with a separate classifier. \n 3. Extracting TF-IDF features with a simple classifier. \n\n Fine-tuning advanced transformers like deberta v3 ensembled with tf-idf and LR always gives the best results, but when the dataset is very large (e.g., over 500,000 samples), using pretrained embeddings plus a classifier is more efficient. TF-IDF with a lightweight model works reliably across all dataset sizes. Ensembling the output probabilities of these methods with a simple meta-classifier like LR always achieves the strongest performance. In contrast, older approaches such as LSTMs, CNNs, and RNNs generally no longer perform competitively and you should never use them. Finally, adding spaCy, NLTK, fasttext, handcrafted, domain-specific, or problem-specific features almost always increases accuracy and MUST be considered. \n - Make sure to always consider the max token size and not just simple truncating. \n - Tf-idf + LR is light weight it is best to always be considered in ensemblings."
    },
    "2": {
      "id": "2",
      "name": "Fine-tuning Transformers for text classification",
      "type": "concept",
      "content": "Best model for fine-tuning are most of the time \"microsoft/deberta-v3-base\" with chunking and sliding window for training and and \"answerdotai/ModernBERT-base\" for long contexts. However if training set is big (larger than 100000 rows and instances) you must use  more lightweight model. distil RoBERTa, and deberta-small are the best options in this case. For datasets with long text (e.g. higher than 256 tokens) it is crucial to use sliding window or chunking strategies in both training and testing and never truncate, since truncate results in loss of information. Apply gradual unfreezing instead of training all layers at once; freezing all layers reduces performance. Use a warmup schedule to improve training stability; typically 5-10% of total steps. Learning rate should be small (e.g., 2e-5); increase slightly if training loss reduction is slow. Fine-tune multiple models and combine their outputs with a meta-classifier; ensembles usually outperform single-model training; CRITICAL: ** Never use folding and cross validation with transformers, since training them for example 5 times is very time consuming **"
    },
    "3": {
      "id": "3",
      "name": "Fine-tuning RoBERTa/DistilBERT code Example",
      "type": "code",
      "content": "from transformers import RobertaForSequenceClassification, RobertaTokenizerFast, Trainer, TrainingArguments\n\ntokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n\ntrainer = Trainer(\n    model=model,\n    args=TrainingArguments(output_dir='./results', learning_rate=2e-5, per_device_train_batch_size=16, num_train_epochs=3),\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer\n)\ntrainer.train()"
    },
    "4": {
      "id": "4",
      "name": "Code Example of Fine-tuning DeBERTa-v3",
      "type": "code",
      "content": "from datasets import load_dataset\nfrom transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\n\ndataset = load_dataset('imdb')\nmodel_name = 'microsoft/deberta-v3-base'\ntokenizer = DebertaV2Tokenizer.from_pretrained(model_name)\nmodel = DebertaV2ForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=256)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_strategy='epoch',\n    logging_dir='./logs',\n    logging_steps=50,\n    fp16=True\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'].shuffle(seed=42).select(range(2000)),\n    eval_dataset=tokenized_datasets['test'].shuffle(seed=42).select(range(500)),\n    tokenizer=tokenizer\n)\ntrainer.train()"
    },
    "5": {
      "id": "5",
      "name": "TF-IDF + Classifier",
      "type": "concept",
      "content": "Merge word-level and character-level TF-IDF vectors. Naive bayes and Logistic Regression are generally the best classifiers to be on top of tf-idf. CRITICAL: ** Any tree-based classifier (Random Forest, XGBoost, etc.) must always apply dimensionality reduction to for example 100 or 500 features before training, this is usefull when the unique word count is very high (>500,000). **"
    },
    "7": {
      "id": "7",
      "name": "Code example of TF-IDF + Classifier",
      "type": "code",
      "content": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\ntfidf_features = FeatureUnion([\n  ('word', TfidfVectorizer(max_features=50000, ngram_range=(1,2), analyzer='word')),\n  ('char', TfidfVectorizer(max_features=50000, ngram_range=(2,5), analyzer='char'))\n])\nclf = Pipeline([\n    ('features', tfidf_features),\n    ('lr', LogisticRegression())\n])\nclf.fit(X_train, y_train)"
    },
    "8": {
      "id": "8",
      "name": "Code example of Pretrained Embeddings + MLP for classification",
      "type": "code",
      "content": "import torch\nfrom torch import nn, optim\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = AutoModel.from_pretrained('distilbert-base-uncased')\n\nbatch_size = 32\nmlp = nn.Sequential(\n    nn.Linear(768, 128),\n    nn.ReLU(),\n    nn.Linear(128, num_classes)\n)\noptimizer = optim.Adam(mlp.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\nfor i in range(0, len(X_train), batch_size):\n    batch_texts = X_train[i:i+batch_size]\n    batch_labels = torch.tensor(y_train[i:i+batch_size])\n    inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt')\n    with torch.no_grad():\n        embeddings = model(**inputs).last_hidden_state[:,0,:]  # CLS token\n    optimizer.zero_grad()\n    outputs = mlp(embeddings)\n    loss = criterion(outputs, batch_labels)\n    loss.backward()\n    optimizer.step()"
    },
    "6": {
      "id": "6",
      "name": "Pretrained Embeddings + MLP for classification",
      "type": "concept",
      "content": "Use pretrained transformer embeddings (CLS token or mean-pooled) directly. Feed embeddings into an MLP in PyTorch for classification. Batch processing avoids memory issues on large datasets."
    },
    "9": {
      "id": "9",
      "name": "Tabular and General Data Best Approaches",
      "type": "workflow",
      "content": "There are two critical factors in solving tabular problems:\n\n1. Having lots of features that capture different kinds of information from the problem and domain, including handcrafted, generated, made by other python libraries, processed and aggregated, features that introduce new perspectives that are collectively exhaustive and cover all impactful information.\n\n2. Ensembling as many models as possible, especially those that process information in different ways. \n\n- For example for consider an ensemble of all catboost, xgboost, random forest, KNN, SVM, LR, with a meta classifier.  \n\n- Gradient Boosted Trees, such as catboost and XGBoost are the strongest default choice and almost always achieve state-of-the-art results on pure tabular datasets, however it is critically important to consider the context of features and whether the test may have a different distribution for example, position, time and average based features that may differ in test time, One way to handle this is to use a deeper version that represents the information in position or time, a naive example is if you want to predict X, use average of X per time instead of time. \n - Never use Light GBM unless working with sparse matrix or dataframes. \n - Tree classifiers can't handle too many features (more than 200 features) when not working with sparse matrix. So in these cases make sure to reduce the number of features.   \n - Pure deep learning models without embeddings or domain-specific tricks rarely outperform boosting methods and must never be the first choice. However, Neural Networks become competitive when combining tabular data with other modalities (e.g., text, images), in which case they are the preferred choice. \n - To ensemble or blend the result of multiple models LR and Ridge are the best options. never use simple averaging of their output. \n - If the final metric is loss and your accuracy is already high, make sure to round final predictions probabilities (e.g. 0.98 -> 1). \n - In cases where there are too many features and the number of data instaces are low, it is almost critical to use dimension reduction algorithms like PCA to avoid overfitting."
    },
    "10": {
      "id": "10",
      "name": "Image Data Best Approaches",
      "type": "workflow",
      "content": "There are three critical factors in solving image problems:\n\n1. Choosing the right backbone architecture: Use strong pretrained convolutional or transformer-based models (e.g., Vision Transformers, EfficientNet) and fine-tune them on your dataset. The choice depends on dataset size, image resolution, and computational budget.\n\n2. Data diversity and augmentation: Providing varied and realistic input through augmentation. \n\n 3. With a strong baseline for model base and rich data by augmentation, adding handcrafted features and ensembling pretrained models with different information and training sources can improve the performance even more.\n\n Convolutional and transformer-based models pretrained on large datasets are the default best choice and almost always outperform training from scratch. Purely custom deep networks without transfer learning or domain-specific augmentation rarely reach competitive accuracy and should not be the first choice. For image datasets with large number of labels and classes (for example more than 1000), It is Highly CRITICAL to consider methods that scale regardless of the number of classes like ARC Face + KNN and XGboost instead of dense head classifier, softmax and anything similar to these."
    },
    "11": {
      "id": "11",
      "name": "Image Data Augmentation",
      "type": "concept",
      "content": "Generally, augmentation improves performance and must be used when dataset is small, especially task-aware augmentations. Also it is very crucial and of utmost importance to augment data after train validation spliting to avoid leakage. Common augmentations include flips, rotation, crop & resize, color jitter, Gaussian noise, Cutout, Mixup, CutMix, and RandAugment/AutoAugment. "
    },
    "12": {
      "id": "12",
      "name": "Image Models and Architectures",
      "type": "concept",
      "content": "Generally ensembling 2 small pretrained models EfficientNet-B0 (torchvision), wide_resnet50_2 (torchvision) after fine tuning must be the base. Depending on the task adding more models to the ensemble must always be considered (so solution always always have at least 3 models) as below:\n1- When the resolution and image size is low ( less than 128 * 128) ensemble of small pretrained CNNs is typically the best choice. In these cases the ensemble of EfficientNet-B0 and wide_resnet50_2 is enough in most cases but adding mobilenet_v3_small (torchvision) can be helpful too.\n2- In cases with very small (less than 500 images and less than 2 GB volume) dataset adding pretrained \"convnext_tiny.fb_in22k\" (timm library) to ensemble along the EfficientNet-B0 and wide_resnet50_2 is helpful.\n3- When the global context is important and the training data is small (less than 5000 sampels) adding EfficientNet-B3, EfficientNet-B5, or \"facebook/dinov2-base\" is a good option to ensemble with EfficientNet-B0 and wide_resnet50_2.\n4- For very high resolution and image sizes (>= 384 * 384) the best option is ensembling different versions of EfficientNet (torchvision) for different downsampled images: A solid option is EfficientNet-B0 (torchvision) after downsampling image size to 224 * 224, EfficientNet-B6 (torchvision) after downsampling image size to 512 * 512 and EfficientNet-B7 (torchvision) after downsampling image size to  640 * 640. Note that wide_resnet50_2 (torchvision) has an internal mechanism to handle high resolutions (even 1024 * 1024) so adding it to the ensemble is helpful. All in all a strong valid option for high resolution images (greather than 348) is ensemble of EfficientNets-B0 (torchvision) for 224 * 224, EfficientNet-B6 (torchvision) for 512 * 512 and wide_resnet50_2 (torchvision) 348 * 348 .\n\n This is highly critical to use pretrained models so never fallback to training with no pretrained weights so always raise an error if the pretrained weights of a model didn't load. some libraries may not contain the pretrained weights. Below you can find the list of models with available weights in a library so make sure to mention the exact name and library as mentioned here: EfficientNets-B0 (torchvision), \"EfficientNets-B6\" (torchvision), \"EfficientNets-B7\" (torchvision), \"wide_resnet50_2\" (torchvision), \"mobilenet_v3_small\" (torchvision), \"convnext_tiny.fb_in22k\" (timm), \"facebook/dinov2-base\" (transformers). \n\n - Best option for ensembling are Linear Regression and Ridge, since they are really fast to train always try both and choose the better one based on validation performance. Do not use simple averaging \n\n - Never train a single model on different image sizes. \n - The epoch number should never be more than 50, in those cases it is better to increase the learning rate slightly or implement a planner for it. \n some useful pretrained image models: \"convnextv2_base-fcmae_ft_in22k_in1k\",  \"microsoft/swin-base-patch4-window7-224\", \"convnextv2_large\", \"swinv2-small-roi\", "
    },
    "13": {
      "id": "13",
      "name": "Image to Image Models",
      "type": "concept",
      "content": "Generally for the image to image problems, an option is using an ensemble of models from the U-Net family and always with a pretrained encoder as below:\n1- Best option mostly is ensembling the result of a U-Net with pretrained wide_resnet50_2 (torchvision) and EfficientNet-B0 (torchvision). .\n2- For medical images and high-resolution images (higher than 512 px), ensembling U-Net with wide_resnet50_2 and \"microsoft/swin-base-patch4-window7-224\" (transformers) as encoder of unet is a better option.\n\n- When using a pretrained encoder, at first steps of training the decoder, freeze the encoder and then unfreeze it.\n- Always use a pretrained encoder as training from scratch almost never performs better.\n - It is very important to mention the exact name and library of each model. \n - Since generating images or modifying images needs lots of training, always start with a very high learning rate and then decrease it gradually. \n - Depending on the task applying domain specific augmentation and preprocesses on the source image can improve the performance significantly and must be considered. \n - In denoising or segmentation problems use simple linear regression for ensembling. \n - Make sure to use very small models (less than 200 parameters) and freeze most parameters of a pretrained model for highly small datasets (less than 200 train instances)"
    },
    "14": {
      "id": "14",
      "name": "Audio Data Best Approaches",
      "type": "workflow",
      "content": "To solve audio problems there are 2 considerations: 1-Choosing the right backbone architecture: Using strong pretrained models. 2- Feature engineering for audio data: Adding domain specific features, data augmentions.  \n\n Convolutional and transformer-based models pretrained on large datasets are the default best choice and almost always outperform training from scratch. Purely custom deep networks without transfer learning or domain-specific augmentation rarely reach competitive accuracy and should not be the first choice. \n Add lots of task specific audi TTA. Cropping,"
    },
    "15": {
      "id": "15",
      "name": "Audio Models and Architectures",
      "type": "concept",
      "content": "For audio problems model you can follow the below conditions for model selection: \n 1- Generally ensembling \"MIT/ast-finetuned-audioset-10-10-0.4593\" and \"mispeech/ced-mini\" from transformers library and resnet18 from torchvision library on Log Mel spectrograms is the best option. \n 2- You can also considerresnet18 from torchvision library on Log Mel spectrograms too \n\n - make sure to mention the exact name and library of each model.\n - Make sure to use a pretrained model and not training from scratch as training from scratch rarely performs better. However only for resnet18 in audio problems training from scratch can be considered. \n  - Note that the input shape of both \"MIT/ast-finetuned-audioset-10-10-0.4593\" and \"mispeech/ced-mini\" in transformers library is a 1D tensor of raw audio waveform with sampling rate 16000. \n - It is critical to always use Ridge or LR (other methods if task is not classification or regression) as meta-classifier in ensembling and never use simple averaging."
    },
    "16": {
      "id": "16",
      "name": "Audio Feature Engineering and Data Augmentation",
      "type": "concept",
      "content": "- Data augmentation: Generally, augmentation improves performance and must be used, especially task-aware augmentations. Some can hurt if misaligned with the problem (e.g. mixup in classification). Common augmentations include: Time-domain augmentations, Noise adding, SpecAugment and Mixup data augmentations \n - Feature engineering: Adding domain specific features can significantly improve performance. Below you can find some example of these features: \n MFCCs, log Mel-spectrogram, Spectral centroid, Spectral bandwidth, Spectral roll-off, Zero-crossing rate, RMS energy, Duration, Delta MFCCs, Delta energy, Mean/variance/min/max of MFCCs, Mean/variance/min/max of energy, Mean/variance/min/max of spectral centroid. "
    },
    "17": {
      "id": "17",
      "name": "Token classification - Text Normalization",
      "type": "workflow",
      "content": "Generally to solve token classification it is best to use the same models as text classification but for tokens like deberta. However, when processing tokens the dataset may be very large, so in those cases use a very small and simple models like catboost or xgboost. \n - A few Neighborhood tokens from both side and features are mostly helpful for semantic understanding. \n "
    },
    "18": {
      "id": "18",
      "name": "A simple Implementation of XGboost",
      "type": "code",
      "content": "import xgboost as xgb\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nX, y = load_iris(return_X_y=True)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndval = xgb.DMatrix(X_val, label=y_val)\n\nparams = {\n    \"objective\": \"multi:softprob\",\n    \"num_class\": 3,\n    \"eval_metric\": \"mlogloss\"\n}\n\nbst = xgb.train(\n    params=params,\n    dtrain=dtrain,\n    num_boost_round=500,\n    evals=[(dval, \"val\")],\n    early_stopping_rounds=20,\n    verbose_eval=10\n)\n\npred = np.argmax(bst.predict(dval), axis=1)\nprint(\"Accuracy:\", (pred == y_val).mean())"
    },
    "19": {
      "id": "19",
      "name": "A simple Implementation of Catboost",
      "type": "code",
      "content": "import numpy as np\nimport pandas as pd\nfrom catboost import CatBoostClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris(as_frame=True)\nX = iris.frame.drop(columns=[\"target\"])\ny = iris.frame[\"target\"]\n\nrng = np.random.RandomState(0)\nX[\"cat_col\"] = rng.choice([\"A\", \"B\", \"C\"], size=len(X))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n\nmodel = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=20, random_state=0)\nmodel.fit(X_train, y_train, cat_features=[\"cat_col\"])\n\npreds = model.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, preds))"
    },
    "20": {
      "id": "20",
      "name": "Video Data Processing",
      "type": "workflow",
      "content": "1- One way of handling video datasets is using 2.5D representations plus EfficientNet-B0 to B6 (torchvision), wide_resnet50_2 (torchvision) 2- In cases that dataset size is small enough and time budget allows, use pretrained models like: \"facebook/vjepa2-vitl-fpc64-256\", \"microsoft/xclip-base-patch32\" "
    },
    "21": {
      "id": "21",
      "name": "3D Data Processing",
      "type": "workflow",
      "content": "1- One way of handling 3d datasets is using 2.5D representations plus EfficientNet-B0 to 6 (torchvision), wide_resnet50_2 (torchvision). 2- In cases that dataset size is small enough and time budget allows, use pretraind models like:  pip install timm-3d  and import timm_3d, torch; print(timm_3d.create_model('tf_efficientnet_b0.in1k', pretrained=True, num_classes=0, global_pool='')(torch.randn(2,3,128,128,128)).shape)"
    },
    "22": {
      "id": "22",
      "name": "Object Detection",
      "type": "workflow",
      "content": "Some pretrained models for object detection: \"yolo11l\", \"hustvl/yolos-small\", \"facebook/detr-resnet-50\", \"facebook/detr-resnet-101\". You may even use these models for ensembling if dataset size allows."
    },
    "2022": {
      "id": "2022",
      "name": "Object Detection codes",
      "type": "code",
      "content": "#newer versions:\nfrom ultralytics import YOLO# Load a pretrained YOLO11n model \n model = YOLO(\"yolo11l.pt\") # \"yolo11m.pt\". \n\n #old version is available too but newer is better:\n from ultralytics import YOLOvv8\nmodel = YOLOvv8.from_pretrained(\"Ultralytics/YOLOv8\")\nsource = 'path/000000039769.jpg'\nmodel.predict(source=source, save=True) \n"
    },
    "1000": {
        "id": "1000",
        "name": "Text-to-Speech",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Text-to-Speech:\nfacebook/mms-tts-eng : facebook/mms-tts-eng is an English VITS-based text-to-speech model from Meta's Massively Multilingual Speech project that converts English text into speech waveforms. It produces non-deterministic, natural-sounding audio conditioned on input text.\nfacebook/mms-tts-swh : Swahili text-to-speech model (VITS-based) from Meta's Massively Multilingual Speech project that converts written Swahili into natural-sounding speech waveforms.\nfacebook/mms-tts-vie : facebook/mms-tts-vie is a Vietnamese VITS-based text-to-speech model from Meta's Massively Multilingual Speech project that converts Vietnamese text into natural-sounding speech waveforms."
    },
    "1002": {
        "id": "1002",
        "name": "Text-to-Audio",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Text-to-Audio:\nchuubjak/vits-tts-thai : A transformers-based VITS text-to-speech model that generates audio from Thai text input.\nsil-ai/senga-nt-asr-inferred-force-aligned-speecht5-MAT : This model is a fine-tuned version of microsoft/speecht5_tts for text-to-speech audio generation, trained with force-aligned data inferred from ASR. It produces speech waveforms from text inputs using the SpeechT5 architecture.\nsil-ai/wsg-chapter-audio-dataset-force-aligned-speecht5 : This is a fine-tuned SpeechT5 text-to-speech model adapted on the wsg chapter audio dataset with force-aligned data, for generating speech audio from text.\nsil-ai/senga-nt-asr-inferred-force-aligned-speecht5-MAT-ACT : This model is a fine-tuned version of microsoft/speecht5_tts for text-to-speech generation, likely adapted using ASR-inferred, force-aligned data for more accurate timing and prosody. It outputs speech audio from input text using the SpeechT5 architecture.\nShabdobhedi/speecht5_finetuned_Bengali_Shabdobhedii : This is a Bengali text-to-speech model fine-tuned from microsoft/speecht5_tts, generating speech audio from Bengali text input. It is suitable for Bengali TTS applications where a neural vocoder is available.\nMarvis-AI/marvis-tts-100m-v0.2-MLX-6bit : A 6-bit quantized MLX-format text-to-speech model that generates English, French, and German audio from text using the mlx-audio toolkit.\nOmarrran/turkish_finetuned_speecht5_tts : This model is a Turkish text-to-speech system fine-tuned from microsoft/speecht5_tts, generating natural-sounding Turkish speech from input text. It targets applications like accessibility tools, virtual assistants, and educational platforms."
    },
    "1004": {
        "id": "1004",
        "name": "Automatic Speech Recognition",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Automatic Speech Recognition:\nMahmoudAshraf/mms-300m-1130-forced-aligner : This model is a converted MMS-300M CTC checkpoint for multilingual forced alignment, mapping given transcripts to precise time spans in corresponding audio across many languages. It is optimized for lower memory usage compared to the standard TorchAudio forced alignment API.\nfacebook/wav2vec2-base-960h : facebook/wav2vec2-base-960h is an English automatic speech recognition model that transcribes 16kHz raw audio into text, fine-tuned on 960 hours of LibriSpeech. It achieves low word error rates on both clean and noisy speech.\nopenai/whisper-small : Whisper Small is a multilingual encoder-decoder ASR and speech-translation model that transcribes or translates up to 30s audio segments, with strong performance especially on English speech. It supports many languages and can be extended to long-form audio via chunking or further improved via fine-tuning.\n \"openai/whisper-large-v3-turbo\", \"facebook/wav2vec2-base-960h\" "
    },
    "1006": {
        "id": "1006",
        "name": "Audio-to-Audio",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Audio-to-Audio:\nJacobLinCool/MP-SENet-DNS : This is a speech enhancement / denoising audio-to-audio model based on MP-SENet, intended to clean noisy speech signals."
    },
    "1008": {
        "id": "1008",
        "name": "Voice Activity Detection",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Voice Activity Detection:\npipecat-ai/smart-turn-v2 : Smart Turn v2 is a multilingual semantic voice activity detection model that predicts whether a speaker has finished their turn from raw audio waveforms. It outputs a single probability indicating turn completion, enabling better timing for voice agents, transcription, and TTS systems.\noverji/ji-VAD : A Chinese voice activity detection model based on wav2vec2 that identifies speech segments in audio for tasks like diarization, ASR pre-processing, or silence trimming.\nkamilakesbi/speaker-segmentation-fine-tuned-callhome-jpn : This pyannote-based segmentation model performs voice activity detection and speaker segmentation/diarization, fine-tuned on the CALLHOME Japanese dataset."
    },
    "1010": {
        "id": "1010",
        "name": "Time Series Forecasting",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Time Series Forecasting:\nautogluon/chronos-bolt-small : Chronos-Bolt Small is a 48M-parameter T5-based time series foundation model for fast, zero-shot probabilistic and point forecasting over multiple future steps. It encodes historical data in patches and directly generates multi-step quantile forecasts, offering substantial speed and memory gains over earlier Chronos models.\nautogluon/chronos-bolt-base : Chronos-Bolt Base is a pretrained T5-based time series foundation model for fast, zero-shot probabilistic and point forecasting over multiple future steps. It chunks historical data into patches and directly generates multi-step quantile forecasts, offering higher accuracy and much lower latency/memory use than the original Chronos models.\namazon/chronos-2 : Chronos-2 is a 120M-parameter encoder-only time series foundation model for zero-shot probabilistic forecasting, supporting univariate, multivariate, and covariate-informed tasks. It produces multi-step-ahead quantile forecasts and is optimized for efficient in-context learning across many related series on CPU or GPU.\nibm-granite/granite-timeseries-ttm-r1 : TinyTimeMixers (TTM) are compact, pre-trained multivariate time-series forecasting models (\u22641M parameters) for minutely-to-hourly point forecasts, supporting both zero-shot and finetuned forecasting. Each variant targets a specific context/forecast length pair (e.g., 512\u219296, 1024\u219296) for accurate, resource-efficient deployment on GPUs or CPUs.\namazon/chronos-bolt-base : amazon/chronos-bolt-base is a pretrained T5-based time series foundation model that performs fast, zero-shot direct multi-step probabilistic forecasting from historical sequences. It generates multi-quantile forecasts over future horizons and is optimized for high throughput and low memory usage compared to earlier Chronos models.\namazon/chronos-bolt-small : amazon/chronos-bolt-small is a 48M-parameter T5-based time series foundation model that performs fast, zero-shot direct multi-step forecasting with quantile outputs. It chunks historical context into patches and generates multi-horizon probabilistic forecasts, offering substantial speed and memory gains over the original Chronos models.\namazon/chronos-t5-tiny : amazon/chronos-t5-tiny is an 8M-parameter T5-based pretrained time series forecasting model that tokenizes scaled and quantized time series to generate probabilistic future trajectories. It is designed for efficient, general-purpose forecasting across diverse time series domains.\namazon/chronos-bolt-tiny : amazon/chronos-bolt-tiny is a tiny (9M parameter) T5-based time series foundation model for fast zero-shot probabilistic and point forecasting over multiple future steps. It chunks historical context into patches and directly generates multi-step quantile forecasts, offering large speed and memory gains over the original Chronos models.\namazon/chronos-t5-small : amazon/chronos-t5-small is a 46M-parameter T5-based pretrained time series forecasting model that tokenizes scaled and quantized time series and generates probabilistic multi-step forecasts via autoregressive sampling. It is suitable for general-purpose univariate or batched time series forecasting tasks.\nautogluon/chronos-bolt-tiny : Chronos-Bolt Tiny is a 9M-parameter T5-based time series foundation model for fast, zero-shot probabilistic and point forecasting over multiple future steps. It encodes patched historical context and directly generates multi-step quantile forecasts, offering high accuracy with low latency and memory usage."
    },
    "1012": {
        "id": "1012",
        "name": "Graph Machine Learning",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Graph Machine Learning:\nmgalkin/ultra_50g : ULTRA 50g is a foundation model for knowledge graph reasoning that performs link prediction (KG completion) in a zero-shot or fine-tuned manner on arbitrary multi-relational graphs. It uses GNN-based, transferable relation representations instead of graph-specific entity/relation embeddings.\nvector-institute/atomformer-base : AtomFormer-base is a transformer model for atomistic graph data that uses Gaussian pair-wise positional embeddings and atomic metadata to learn representations for 3D molecular/atomic systems, primarily for pretraining and downstream fine-tuning. It predicts per-atom forces and per-system energies and can be used as a feature extractor for graph-based tasks.\nmgalkin/ultra_3g : ULTRA 3g is a foundation model for knowledge graph reasoning that performs link prediction on arbitrary multi-relational graphs in zero-shot or fine-tuned settings, using transferable GNN-based relation representations instead of graph-specific embeddings.\nDaizeDong/GraphsGPT-1W : DaizeDong/GraphsGPT-1W is a graph machine learning checkpoint from the ICML 2024 paper \"A Graph is Worth K Words,\" designed to Euclideanize graphs using a pure Transformer architecture for applications in biology, medicine, and chemistry.\nhrishivish23/giorom-3d-t-sand-3d : GIOROM-3D-T-Sand-3D is a reduced-order neural-operator transformer for Lagrangian dynamics that predicts particle accelerations from sparse, time-windowed velocity inputs on 3D sand simulations. It learns on highly sparse graphs and supports hybrid Eulerian\u2013Lagrangian representations for efficient physics-based simulation.\nDaizeDong/GraphsGPT-4W : This model is a graph machine learning checkpoint from the ICML 2024 paper \"A Graph is Worth K Words: Euclideanizing Graph using Pure Transformer\", designed to process graph-structured data (especially in biology, medical, and chemistry domains) using transformer-based architectures. It maps graphs into Euclidean representations suitable for downstream tasks.\nhrishivish23/giorom-3d-t-plasticine-3d : GIOROM-3D-T-Plasticine-3D is a reduced-order neural-operator transformer for simulating 3D plasticine-like Lagrangian dynamics on sparse particle graphs, predicting accelerations from past velocity windows. It learns from reduced-order particle sets while remaining compatible with higher-resolution simulations via an external integral transform model.\nhrishivish23/giorom-3d-t-elasticity-3d : GIOROM-3D-T-Elasticity-3D is a reduced-order neural-operator transformer for Lagrangian dynamics that predicts accelerations on sparse particle graphs for 3D elasticity simulations, enabling efficient graph-based physics modeling. It learns from reduced-order particle sets while remaining compatible with higher-resolution full-order simulations via an external integral transform model.\nmgalkin/ultra_4g : ULTRA 4g is a foundation model for knowledge graph reasoning that performs link prediction on arbitrary multi-relational graphs in a zero-shot or fine-tuned setting. It uses graph neural networks and relation-interaction-based representations to provide unified, transferable embeddings across many KGs."
    },
    "1014": {
        "id": "1014",
        "name": "Visual Question Answering",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Visual Question Answering:\nSalesforce/blip-vqa-base : This BLIP-based model answers natural-language questions about images, performing visual question answering using a ViT-base backbone. It generates short textual answers conditioned on both the image and the question.\ngoogle/deplot : DePlot is a visual question answering model that converts charts and plots into linearized tables, enabling downstream reasoning (e.g., chart QA) by large language models. It performs plot-to-text (table) translation from an input image and prompt.\ngoogle/pix2struct-docvqa-base : google/pix2struct-docvqa-base is a Pix2Struct image-to-text model fine-tuned for visual question answering on scanned documents, taking a document image and a rendered question as input and generating an answer in text form. It supports multilingual usage and is suitable for document understanding tasks like DocVQA.\ngoogle/pix2struct-ai2d-base : google/pix2struct-ai2d-base is an image-encoder/text-decoder model fine-tuned for visual question answering on scientific diagrams (AI2D), taking an image plus a textual question and generating a short textual answer.\nmicrosoft/git-large-vqav2 : microsoft/git-large-vqav2 is a large GenerativeImage2Text Transformer decoder conditioned on CLIP image tokens and text, fine-tuned on VQAv2 for visual question answering and related image-to-text tasks.\nmicrosoft/git-base-vqav2 : GIT-base-vqav2 is a generative image-to-text Transformer decoder conditioned on CLIP image tokens, fine-tuned on VQAv2 for visual question answering over images.\nmicrosoft/git-base-textvqa : GIT-base-textvqa is a generative image-to-text Transformer decoder conditioned on CLIP image tokens, fine-tuned for answering text-centric questions about images (TextVQA).\nyanka9/vilt_finetuned_deepfashionVQA_v2 : A ViLT-based vision-and-language model fine-tuned for fashion-related visual question answering, answering questions about clothing attributes such as fabric, color, shape, and presence of accessories from images."
    },
    "1016": {
        "id": "1016",
        "name": "Document Question Answering",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Document Question Answering:\nimpira/layoutlm-document-qa : A fine-tuned multi-modal LayoutLM model for answering natural-language questions about the content of documents (e.g., invoices, contracts, PDFs) using both text and layout information.\nmagorshunov/layoutlm-invoices : A fine-tuned LayoutLM model for document question answering on invoices and other documents, capable of extracting both consecutive and non-consecutive text spans from PDFs and images. It is trained on invoices plus SQuAD2.0 and DocVQA for broader document comprehension.\nimpira/layoutlm-invoices : A fine-tuned LayoutLM model for answering natural-language questions about invoices and other documents, including non-consecutive text spans (e.g., multi-line fields like addresses). It supports document question answering on PDFs and images such as invoices and contracts.\n2KKLabs/Kaleidoscope_large_v1 : Kaleidoscope_large_v1 is a multilingual (Russian-focused) document question answering model fine-tuned from ruBert-large to extract span answers from a given document context. It is suited for interactive QA over text documents in Russian and partially in English.\nVesleAnne/VesleAnne : VesleAnne is a fine-tuned LayoutLMv2 model for document question answering, leveraging both text and layout information from documents to predict answers. It is based on microsoft/layoutlmv2-base-uncased and trained on an unspecified dataset.\n2KKLabs/Kaleidoscope_small_v1 : This model is a fine-tuned ruBert-based document question answering system that extracts short answers from a given document context in response to user questions, primarily in Russian with some English capability. It is suited for interactive QA over textual documents such as customer support materials or knowledge bases.\nTusharGoel/LayoutLMv2-finetuned-docvqa : LayoutLMv2 model fine-tuned on the DocVQA dataset to answer natural-language questions about document images using both text and layout information.\naslessor/layoutlm-invoices : A fine-tuned multi-modal LayoutLM model for answering questions on invoices and other documents, including non-consecutive text spans. It supports document question answering such as extracting invoice numbers, amounts, and addresses from PDFs or images.\nQuantumhash/Quantum_Doc_QA : A fine-tuned multi-modal LayoutLM model for answering natural-language questions about document images such as invoices, contracts, and financial statements.\nTusharGoel/LiLT-Document-QA : A LiLT-based transformer model fine-tuned on English DocVQA for answering questions about the content of documents using both text and layout information."
    },
    "1018": {
        "id": "1018",
        "name": "Depth Estimation",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Depth Estimation:\ndepth-anything/Depth-Anything-V2-Base-hf : Monocular depth estimation model based on DPT with a DINOv2 backbone, providing fine-grained, robust relative depth maps from single RGB images using the Transformers library.\ndepth-anything/Depth-Anything-V2-Large-hf : Monocular depth estimation model based on DPT with a DINOv2 backbone, providing fine-grained, robust relative/absolute depth maps from single RGB images. Optimized for efficient, zero-shot depth estimation using the Transformers library.\nIntel/zoedepth-nyu-kitti : ZoeDepth is a monocular depth estimation model that predicts metric (absolute) depth from a single RGB image, fine-tuned on NYU and KITTI. It extends the DPT framework to provide state-of-the-art zero-shot depth estimation performance.\nLiheYoung/depth-anything-large-hf : Depth Anything large is a Transformer-based depth-estimation model using a DPT architecture with a DINOv2 backbone, trained on ~62M images for zero-shot relative and absolute depth prediction from single RGB images.\ndepth-anything/Depth-Anything-V2-Small-hf : Depth Anything V2 Small is a monocular depth estimation model that predicts fine-grained relative (and optionally absolute) depth maps from single RGB images using a DPT+DINOv2 architecture. It is optimized for robustness and efficiency compared to prior SD-based depth models.\nIntel/dpt-large : Intel/dpt-large is a ViT-based Dense Prediction Transformer for zero-shot monocular depth estimation from a single RGB image, producing dense depth maps. It is trained on the MIX 6 dataset with ~1.4M images for robust cross-dataset transfer.\nLiheYoung/depth-anything-base-hf : Depth Anything is a DPT-based model with a DINOv2 backbone for zero-shot monocular depth estimation from single RGB images, trained on ~62M images for state-of-the-art relative and absolute depth prediction.\nLiheYoung/depth-anything-small-hf : Depth Anything Small is a DPT-based monocular depth estimation model with a DINOv2 backbone, trained on ~62M images for zero-shot relative and absolute depth prediction from a single RGB image.\ndepth-anything/Depth-Anything-V2-Metric-Outdoor-Large-hf : This model is a Transformers-based Depth Anything V2 checkpoint fine-tuned for metric (absolute) outdoor depth estimation, producing dense depth maps from single RGB images. It uses a DPT architecture with a DINOv2 backbone trained on large-scale synthetic and real data.\ndepth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf : This is a Transformers-compatible Depth Anything V2 model fine-tuned for indoor metric depth estimation, producing absolute depth maps from single RGB images. It uses a DPT architecture with a DINOv2 backbone and is optimized for indoor scenes (Hypersim)."
    },
    "1020": {
        "id": "1020",
        "name": "Object Detection",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Object Detection:\nmicrosoft/table-transformer-structure-recognition : Transformer-based DETR model fine-tuned on PubTables1M to detect table structure elements (e.g., rows, columns) in document images.\nmicrosoft/table-transformer-detection : Transformer-based DETR-style object detection model fine-tuned to detect tables in document images, especially PDFs and scanned pages.\nfacebook/detr-resnet-50 : DETR ResNet-50 is a transformer-based end-to-end object detection model trained on COCO 2017, predicting object classes and bounding boxes directly from images. It uses object queries and bipartite matching loss to detect up to 100 objects per image.\nhustvl/yolos-small : YOLOS-small is a Vision Transformer-based object detection model fine-tuned on COCO 2017, predicting bounding boxes and class labels for objects in images. It uses a DETR-style bipartite matching loss to learn one-to-one assignments between object queries and ground-truth annotations.\nmicrosoft/table-transformer-structure-recognition-v1.1-all : Transformer-based DETR-style object detection model specialized for recognizing table structures (rows, columns, cells) in document images, trained on PubTables1M and FinTabNet.c.\nPekingU/rtdetr_r101vd_coco_o365 : RT-DETR R101vd is a real-time, end-to-end Transformer-based object detector trained on COCO and pre-trained on Objects365, providing high-accuracy bounding boxes and class labels without NMS. It is suitable for fast, high-quality object detection in images using the Transformers library.\nhustvl/yolos-tiny : YOLOS-tiny is a compact Vision Transformer-based object detection model fine-tuned on COCO 2017, predicting object classes and bounding boxes in images. It uses a DETR-style bipartite matching loss to learn one-to-one assignments between queries and ground-truth objects.\nPekingU/rtdetr_r50vd_coco_o365 : RT-DETR is a real-time end-to-end Transformer-based object detector that predicts object classes and bounding boxes without NMS, trained on COCO and pre-trained with Objects365 for higher accuracy. This variant uses a ResNet-50vd backbone and is optimized for fast, accurate object detection on natural images.\nTahaDouaji/detr-doc-table-detection : detr-doc-table-detection is an object detection model based on facebook/detr-resnet-50, trained to detect both bordered and borderless tables in document images. It outputs bounding boxes and class labels for tables, suitable for document analysis and table extraction pipelines.\nustc-community/dfine-small-obj365 : D-FINE is a real-time object detection model that predicts precise bounding boxes and categories for objects in images, trained on Objects365. It uses fine-grained distribution refinement and self-distillation to improve localization accuracy within a DETR-style architecture."
    },
    "1022": {
        "id": "1022",
        "name": "Image Segmentation",
        "type": "workflow",
        "content": "One solid and small choice for segmentation is \"yolo11n-seg\". SegFormerâ€‘B0, and Mask2Former are good choices too. Also here are a list of available pre-trained models for Image Segmentation:\nCIDAS/clipseg-rd64-refined : CLIPSeg is a vision model for zero-shot and one-shot image segmentation using text and/or image prompts, with a refined architecture using reduced 64-dimensional features.\nZhengPeng7/BiRefNet : BiRefNet is a high-resolution image segmentation model for dichotomous image segmentation, background removal, salient and camouflaged object detection, producing precise foreground masks from input images. It provides official weights and code for SOTA performance on DIS, HRSOD, and COD tasks.\njonathandinu/face-parsing : Semantic segmentation model fine-tuned for detailed face parsing, producing pixel-wise labels for facial regions (skin, eyes, nose, hair, etc.) from input images. Suitable for applications needing structured facial part masks such as AR effects, editing, or analysis.\nnvidia/segformer-b0-finetuned-ade-512-512 : SegFormer-b0 is a transformer-based semantic segmentation model fine-tuned on ADE20K at 512x512 resolution, producing per-pixel class labels for scenes. It uses a hierarchical Transformer encoder with a lightweight MLP decoder for efficient, accurate segmentation.\nfacebook/mask2former-swin-large-cityscapes-semantic : Mask2Former Swin Large for Cityscapes performs high-quality semantic segmentation by predicting sets of masks and labels for urban scene images. It uses a multi-scale deformable attention Transformer and masked attention decoder for efficient, accurate segmentation.\nbriaai/RMBG-1.4 : BRIA RMBG-1.4 is a saliency-based background removal model that segments foreground objects from the background across diverse image types. It produces high-quality masks suitable for applications like e-commerce, advertising, and general content creation.\nbriaai/RMBG-2.0 : BRIA RMBG-2.0 is a dichotomous image segmentation model for high-quality background removal, producing a single-channel grayscale alpha matte to separate foreground from background. It is optimized for diverse content types like stock, e-commerce, gaming, and advertising, and is released for non-commercial use under CC BY-NC 4.0.\nZhengPeng7/BiRefNet_HR : BiRefNet_HR is a high-resolution dichotomous image segmentation model for tasks like background removal, salient object detection, camouflaged object detection, and general mask generation, optimized for 2048x2048 inputs.\nmattmdjaga/segformer_b2_clothes : SegFormer B2 model fine-tuned for semantic segmentation of human clothes and body parts, producing per-pixel labels for 18 classes (e.g., upper-clothes, pants, face, arms, bag). It can be used for both detailed clothing parsing and general human segmentation in images.\nfacebook/mask2former-swin-tiny-coco-instance : Mask2Former is an instance segmentation model that predicts a set of object masks and corresponding labels for images, using a Swin-Tiny backbone trained on COCO. It outputs per-instance segmentation maps suitable for downstream vision tasks."
    },
    "21022": {
      "id": "21022",
      "name": "Image Segmentation Yolo code example",
      "type": "code",
      "content": "from ultralytics import YOLO \nmodel = YOLO(\"yolo11m-seg.pt\")  # load a pretrained model (recommended for training) also YOLO(\"yolo11l-seg.pt\") and YOLO(\"yolo11s-seg.pt\") available too."
  },
    "1024": {
        "id": "1024",
        "name": "Image-to-Text",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Image-to-Text:\nSalesforce/blip-image-captioning-large : This model generates natural language captions for images, supporting both conditional (prompt-guided) and unconditional image captioning in English. It is based on the BLIP framework with a ViT-large backbone and is pretrained on COCO.\nmicrosoft/trocr-base-printed : Transformer-based OCR model that converts images of single-line printed text into machine-readable text, fine-tuned on the SROIE dataset.\nU4R/StructTable-base : StructEqTable-base converts table images into LaTeX code and supports downstream table-related reasoning tasks such as structural extraction and question answering, with efficient TensorRT-accelerated inference.\ntrl-internal-testing/tiny-Qwen2_5_VLForConditionalGeneration : A tiny Qwen2.5 vision-language model for conditional image-to-text generation, primarily intended for TRL unit testing rather than real-world use.\nvikp/texify : vikp/texify converts images containing equations or text into LaTeX markup using OCR specialized for mathematical content.\nmicrosoft/trocr-base-handwritten : Transformer-based OCR model that converts single-line handwritten text images into machine-readable text, using a BEiT encoder and RoBERTa decoder.\ntrl-internal-testing/tiny-LlavaForConditionalGeneration : A minimal Llava-based image-to-text model intended for TRL unit tests and experimentation, not for production use.\nRiksarkivet/trocr-base-handwritten-hist-swe-2 : This model is a TrOCR-based handwritten text recognition system for converting single-line images of historical Swedish handwriting (ca. 1600\u20131900) into machine-readable text. It is optimized for running-text manuscripts and performs best within its trained historical domain.\nmicrosoft/trocr-small-printed : Transformer-based OCR model that converts single-line images of printed text into machine-readable text, fine-tuned on the SROIE dataset."
    },
    "1026": {
        "id": "1026",
        "name": "Image-to-Image",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Image-to-Image:\ncaidas/swin2SR-classical-sr-x2-64 : This Swin2SR model performs 2\u00d7 image super-resolution, enhancing low-resolution images into higher-resolution outputs using a SwinV2 Transformer architecture."
    },
    "1030": {
        "id": "1030",
        "name": "Text-to-Video",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Text-to-Video:\nSearchium-ai/clip4clip-webvid150k : A CLIP4Clip-based video\u2013text retrieval model trained on 150k WebVid video\u2013caption pairs, producing aligned embeddings for text queries and video clips. It is intended for large-scale video search and retrieval applications."
    },
    "1034": {
        "id": "1034",
        "name": "Mask Generation",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Mask Generation:\nfacebook/sam-vit-base : The Segment Anything ViT-B model generates high-quality segmentation masks for objects in images, using prompts such as points or bounding boxes, or fully automatically over a grid of points. It is trained on a large-scale dataset and works well in zero-shot settings for diverse segmentation tasks.\nfacebook/sam2-hiera-large : SAM 2 (facebook/sam2-hiera-large) is a promptable foundation model for generating segmentation masks in images and videos, supporting clicks, boxes, masks, and automatic mask generation. It can track and refine multiple objects across frames and is integrated with the \ud83e\udd17 Transformers and the official FAIR SAM2 codebase.\nfacebook/sam2.1-hiera-large : SAM 2.1 Hiera Large is a foundation model for promptable visual segmentation that generates high-quality masks for objects in images and videos, supporting points, boxes, masks, and batched inputs. It can perform automatic mask generation, interactive refinement, and multi-object tracking across video frames.\nfacebook/sam-vit-large : The Segment Anything ViT-L model generates high-quality segmentation masks for objects in images, either from prompts (points/boxes) or automatically over the whole image, with strong zero-shot performance.\nfacebook/sam2.1-hiera-tiny : SAM 2.1 Hiera Tiny is a promptable visual segmentation model that generates masks for objects in images and videos from points, boxes, or masks, and can automatically segment all objects in an image. It supports interactive refinement, multi-object tracking, and streaming video inference.\nfacebook/sam2.1-hiera-base-plus : SAM 2.1 Hiera Base Plus is a promptable visual segmentation model that generates high-quality masks for objects in images and videos, supporting points, boxes, and masks as prompts. It can perform automatic mask generation, interactive refinement, multi-object segmentation, and long-term video tracking.\nyonigozlan/EdgeTAM-hf : EdgeTAM is an on-device, promptable segmentation and tracking model for images and videos, providing fast automatic and interactive mask generation, including multi-object and batched scenarios. It is an efficient variant of SAM 2 designed to run in real time on mobile hardware.\nsyscv-community/sam-hq-vit-base : SAM-HQ is an enhanced Segment Anything Model that generates high-quality segmentation masks from prompts (points, boxes, masks) or automatically via dense point grids, especially improving fine structures and boundaries over the original SAM."
    },
    "1038": {
        "id": "1038",
        "name": "Image-to-3D",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Image-to-3D:\nzxhezexin/openlrm-mix-base-1.1 : OpenLRM Mix Base 1.1 is an image-to-3D model that reconstructs 3D objects from single images using a DINOv2-based encoder and triplane decoder, trained on Objaverse and MVImgNet. It targets research, non-commercial use cases for fast 3D asset generation.\ndepth-anything/DA3-BASE : Depth Anything 3 (DA3-BASE) is an image-to-3D vision transformer that predicts spatially consistent multi-view geometry, including relative depth and camera poses, from one or more input images. It outputs depth maps, confidence maps, and camera intrinsics/extrinsics, and can export 3D content in formats like GLB and PLY."
    },
    "1040": {
        "id": "1040",
        "name": "Keypoint Detection",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Keypoint Detection:\nusyd-community/vitpose-plus-base : usyd-community/vitpose-plus-base is a ViTPose+ vision transformer model for generic human body keypoint detection, estimating joint locations from person crops in images. It supports multi-dataset pose estimation via a Mixture-of-Experts architecture.\nusyd-community/vitpose-base-simple : usyd-community/vitpose-base-simple is a ViT-based human pose estimation model that predicts 2D keypoints for people in images, given person bounding boxes. It is suitable for applications like human pose tracking, action recognition, and behavior analysis.\nmagic-leap-community/superpoint : magic-leap-community/superpoint is a self-supervised fully-convolutional model for detecting repeatable keypoints and computing 256-D descriptors, useful as a feature extractor for tasks like homography estimation and image matching.\nusyd-community/vitpose-plus-large : usyd-community/vitpose-plus-large is a ViT-based pose estimation model that predicts human body keypoints from images given person bounding boxes. It is suitable for tasks like human pose estimation, action understanding, and downstream applications in fitness, surveillance, and animation.\nETH-CVG/lightglue_superpoint : LightGlue SuperPoint matches local keypoints between pairs of images, enabling robust feature matching, pose estimation, and related multi-view geometry tasks. It adaptively prunes non-matchable points and stops early on easy pairs for efficient, real-time performance.\nusyd-community/vitpose-plus-small : usyd-community/vitpose-plus-small is a ViT-based keypoint detection model for human pose estimation, predicting body joint locations from person crops in images. It is suitable for applications like action recognition, fitness tracking, and animation where 2D human keypoints are needed.\nETH-CVG/lightglue_disk : ETH-CVG/lightglue_disk is a LightGlue variant trained on DISK for fast, accurate local feature and keypoint matching between image pairs, suitable for tasks like pose estimation and homography estimation. It adaptively prunes non-matchable points and adjusts computation based on image-pair difficulty.\nstanfordmimi/synthpose-vitpose-base-hf : SynthPose VitPose Base is a keypoint-detection model that predicts 52 dense human pose markers (17 COCO keypoints plus 35 anatomical markers) for biomechanical and kinematic analysis from images. It uses a VitPose Base backbone and is designed to work after a person detector to localize humans before pose estimation.\nusyd-community/vitpose-base : usyd-community/vitpose-base is a Vision Transformer\u2013based model for human pose estimation, predicting 2D keypoints for people in images given person bounding boxes. It is suitable for applications like action recognition, fitness tracking, and animation where accurate human keypoints are needed."
    },
    "1044": {
        "id": "1044",
        "name": "Question Answering",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Question Answering:\ndeepset/roberta-base-squad2 : deepset/roberta-base-squad2 is an English extractive question answering model based on roberta-base, fine-tuned on SQuAD 2.0 to return answer spans or detect unanswerable questions. It is suitable for building QA systems over documents and supports both Transformers and Haystack integrations.\nmonologg/koelectra-small-v2-distilled-korquad-384 : A Korean ELECTRA-based question answering model fine-tuned on the KorQuAD dataset for extractive QA over Korean context passages.\ndistilbert/distilbert-base-cased-distilled-squad : This is a compact DistilBERT-based extractive question answering model fine-tuned on SQuAD v1.1, optimized for speed and efficiency while retaining strong accuracy. It predicts answer spans within a given context paragraph for English questions.\ndeepset/bert-large-uncased-whole-word-masking-squad2 : An English BERT-large model fine-tuned on SQuAD 2.0 for extractive question answering, returning answer spans (or no-answer) from a given context. It is suitable for building QA systems over documents or passages.\ngoogle-bert/bert-large-uncased-whole-word-masking-finetuned-squad : This is a 24-layer BERT-large uncased model with whole-word masking, fine-tuned on SQuAD for extractive question answering over English text.\ndistilbert/distilbert-base-uncased-distilled-squad : This is a distilled DistilBERT-base-uncased model fine-tuned on SQuAD v1.1 for extractive question answering in English, offering a smaller and faster alternative to BERT with competitive accuracy. It predicts answer spans within a given context passage for user questions.\ndeepset/bert-base-cased-squad2 : This is an English BERT-base cased model fine-tuned on SQuAD 2.0 for extractive question answering, returning answer spans or no-answer when appropriate. It is suitable for QA over passages or document collections via Transformers or Haystack.\ndeepset/tinyroberta-squad2 : deepset/tinyroberta-squad2 is a distilled English extractive question answering model trained on SQuAD 2.0, providing fast span-based answers (or no-answer) from a given context. It offers prediction quality comparable to roberta-base-squad2 while running roughly twice as fast.\ngoogle-bert/bert-large-cased-whole-word-masking-finetuned-squad : BERT-large cased model with whole word masking, fine-tuned on SQuAD for extractive question answering over English text. Given a question and a context passage, it predicts the span of text in the passage that answers the question.\ndeepset/roberta-base-squad2-distilled : This is an English extractive question answering model distilled from deepset/roberta-large-squad2 and trained on SQuAD 2.0, optimized to select answer spans or abstain when no answer is present."
    },
    "1048": {
        "id": "1048",
        "name": "Translation",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Translation:\ngoogle-t5/t5-small : T5-small is a 60M-parameter text-to-text transformer that can perform translation, summarization, question answering, and other NLP tasks by mapping input text to output text in multiple languages (en, fr, ro, de).\ngoogle-t5/t5-base : T5-base is a 220M-parameter text-to-text transformer that can perform translation (and other NLP tasks) between English, French, Romanian, and German by mapping input text strings to output text strings.\nHelsinki-NLP/opus-mt-fr-en : A transformer-based neural machine translation model that translates French text into English, trained on OPUS data with SentencePiece preprocessing and alignment enhancements.\nHelsinki-NLP/opus-mt-tc-big-tr-en : This is a Marian-based neural machine translation model that translates Turkish (tr) text into English (en), trained on OPUS data as part of the OPUS-MT project. It achieves competitive BLEU scores on several standard benchmarks including Tatoeba, FLORES-101, and WMT news test sets.\nutrobinmv/t5_translate_en_ru_zh_small_1024 : Multilingual T5 model for direct machine translation between English, Russian, and Chinese using a text prefix to indicate the target language."
    },
    "1050": {
        "id": "1050",
        "name": "Summarization",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Summarization:\nfacebook/bart-large-cnn : BART-large fine-tuned on CNN/DailyMail for abstractive English news summarization, producing concise summaries of long articles or documents.\nknkarthick/MEETING_SUMMARY : This model is a fine-tuned BART-based abstractive summarizer optimized for meeting transcripts and conversational text, but it also works on general long-form documents. It generates concise summaries of dialogues and discussions from datasets like AMI, SAMSUM, DialogSum, and XSum.\nFalconsai/text_summarization : This model is a fine-tuned T5-small transformer specialized for generating concise, coherent summaries of English text, suitable for documents, articles, and other long-form content. It is optimized for summarization and may not generalize as well to other NLP tasks without additional fine-tuning.\nmrm8488/bert2bert_shared-spanish-finetuned-summarization : Modelo encoder-decoder basado en BERT (BETO) entrenado con MLSUM en espa\u00f1ol para generar res\u00famenes autom\u00e1ticos de noticias y otros textos largos.\neenzeenee/t5-base-korean-summarization : A Korean T5-based sequence-to-sequence model fine-tuned for abstractive summarization of Korean texts from papers, books, and report-style documents. It generates concise summaries from long-form Korean input when prompted with a summarization prefix.\nd0rj/rut5-base-summ : Finetuned ruT5-base model for abstractive summarization of Russian (and some English) texts and dialogues, trained on multiple news and dialogue datasets. It generates concise summaries from longer inputs.\nmrm8488/bert2bert_shared-german-finetuned-summarization : German BERT2BERT EncoderDecoder model fine-tuned on MLSUM DE for abstractive summarization of German news and similar texts.\nFalconsai/medical_summarization : This model is a T5-large variant fine-tuned to generate concise, coherent summaries of long and complex medical texts such as research articles, clinical notes, and case reports. It is optimized to handle specialized medical terminology and extract key clinical findings and conclusions."
    },
    "1052": {
        "id": "1052",
        "name": "Text Generation",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Text Generation:\nopenai-community/gpt2 : GPT-2 is a 124M-parameter English causal language model that predicts the next token in a sequence and is mainly used for open-ended text generation and feature extraction. It can also be fine-tuned for downstream NLP tasks but inherits biases from its web-scraped training data.\nGensyn/Qwen2.5-0.5B-Instruct : Gensyn/Qwen2.5-0.5B-Instruct is a 0.5B-parameter instruction-tuned causal language model based on Qwen2.5, suitable for text generation and chat-style interactions, and designed to be finetuned via Gensyn RL Swarm. Once finetuned, it can be used like the original Qwen2.5-0.5B-Instruct model in standard NLP workflows.\ntrl-internal-testing/tiny-Qwen2ForCausalLM-2.5 : A tiny Qwen2 causal language model intended for unit tests and minimal text generation experiments within the TRL/transformers ecosystem.\ndistilbert/distilgpt2 : DistilGPT2 is a distilled, smaller and faster English GPT-2 variant for general-purpose text generation and language modeling, trained on OpenWebText and supervised by the 124M-parameter GPT-2.\nQwen/Qwen2.5-Coder-0.5B-Instruct : Qwen/Qwen2.5-Coder-0.5B-Instruct is a 0.5B-parameter instruction-tuned code-focused causal language model optimized for code generation, reasoning, and fixing, while retaining general and math capabilities. It supports long-context (32k tokens) coding workflows and chat-style interactions.\nQwen/Qwen2.5-0.5B-Instruct : Qwen/Qwen2.5-0.5B-Instruct is a 0.5B-parameter instruction-tuned causal language model optimized for chat-style text generation, long-context reasoning, and multilingual use. It supports up to 32K context and is suitable for lightweight assistants, coding help, and structured output generation."
    },
    "1054": {
        "id": "1054",
        "name": "Fill-Mask",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Fill-Mask:\ngoogle-bert/bert-base-uncased : BERT base uncased is a bidirectional Transformer pretrained on English with masked language modeling and next sentence prediction, useful for fill-mask and as a general-purpose text encoder. It is typically fine-tuned for downstream NLP tasks like classification, QA, and token labeling.\nFacebookAI/roberta-large : RoBERTa-large is a case-sensitive English transformer model pretrained with a masked language modeling objective, mainly used for fill-mask prediction and as a feature extractor for downstream NLP tasks. It learns bidirectional contextual representations from large-scale text corpora.\ndistilbert/distilbert-base-uncased : DistilBERT base uncased is a smaller, faster distilled version of BERT-base trained on English text for masked language modeling and as a general-purpose encoder for downstream NLP tasks.\nFacebookAI/roberta-base : RoBERTa-base is an English masked language model pretrained with a masked language modeling objective, producing bidirectional contextual embeddings suitable for fill-mask and downstream NLP tasks. It is case-sensitive and optimized for tasks that use full-sentence context, such as classification and question answering.\nFacebookAI/xlm-roberta-base : XLM-RoBERTa-base is a multilingual masked language model trained on 100 languages, useful for fill-mask tasks and as a feature extractor for downstream NLP tasks like classification and QA. It learns bidirectional sentence representations via masked language modeling.\ngoogle-bert/bert-base-multilingual-cased : BERT multilingual base (cased) is a transformer model pretrained on Wikipedia in 104 languages for masked language modeling and next sentence prediction, mainly intended to be fine-tuned for downstream NLP tasks. It is case-sensitive and provides contextual embeddings and fill-mask predictions across many languages.\ngoogle-bert/bert-base-multilingual-uncased : BERT-base multilingual uncased is a transformer model pretrained on Wikipedia in 100+ languages for masked language modeling and next sentence prediction, useful for fill-mask and as a multilingual encoder for downstream tasks.\ngoogle-bert/bert-base-cased : BERT base cased is an English transformer model pretrained with masked language modeling and next sentence prediction, suitable for fill-mask tasks and as a general-purpose encoder for downstream NLP tasks. It is case-sensitive and typically fine-tuned for applications like classification, token labeling, and QA."
    },
    "1056": {
        "id": "1056",
        "name": "Sentence Similarity",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Sentence Similarity:\nsentence-transformers/all-MiniLM-L6-v2 : This sentence-transformers model encodes sentences and short paragraphs into 384-dimensional embeddings suitable for semantic similarity, clustering, and information retrieval tasks.\nsentence-transformers/all-mpnet-base-v2 : This sentence-transformers model encodes sentences and short paragraphs into 768-dimensional embeddings suitable for semantic similarity, clustering, and information retrieval. It is optimized for English and truncates inputs longer than 384 word pieces.\nsentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 : This multilingual MiniLM-based sentence-transformers model maps sentences and paragraphs in many languages to 384-dimensional embeddings for tasks like sentence similarity, clustering, and semantic search.\nsentence-transformers/gtr-t5-base : This sentence-transformers model encodes sentences and paragraphs into 768-dimensional dense vectors optimized for semantic search and sentence similarity tasks. It is based on the encoder of T5-base and was converted from the TensorFlow gtr-base-1 model.\nsentence-transformers/paraphrase-multilingual-mpnet-base-v2 : This multilingual sentence-transformers model maps sentences and paragraphs in many languages to 768-dimensional embeddings for tasks like semantic search, clustering, and sentence similarity.\nsentence-transformers/multi-qa-mpnet-base-dot-v1 : This model is a sentence-transformers MPNet-based encoder that maps queries and passages into 768-dimensional vectors for semantic search and sentence similarity using dot-product scores with CLS pooling.\nAlibaba-NLP/gte-large-en-v1.5 : Alibaba-NLP/gte-large-en-v1.5 is an English text-embedding model optimized for sentence similarity, retrieval, reranking, and related semantic tasks, with up to 8192 token context and 1024-dimensional embeddings. It achieves state-of-the-art performance on MTEB for its size category.\nsentence-transformers/all-MiniLM-L12-v2 : This sentence-transformers model encodes sentences and short paragraphs into 384-dimensional embeddings suitable for semantic similarity, clustering, and retrieval tasks. It is optimized for English and trained on over a billion sentence pairs using a contrastive learning objective.\nsentence-transformers/paraphrase-MiniLM-L6-v2 : This sentence-transformers model maps sentences and paragraphs into 384-dimensional embeddings for tasks like semantic similarity, clustering, and search.\nsentence-transformers/msmarco-distilbert-base-tas-b : This sentence-transformers model maps English sentences and passages into 768-dimensional embeddings optimized for semantic search and sentence similarity, particularly on MS MARCO-style retrieval tasks."
    },
    "1058": {
        "id": "1058",
        "name": "Text Ranking",
        "type": "workflow",
        "content": "Here are a list of available pre-trained models for Text Ranking:\ncross-encoder/ms-marco-MiniLM-L6-v2 : This cross-encoder ranks passages for a given query by directly scoring query\u2013passage pairs, optimized for MS MARCO passage retrieval and re-ranking tasks.\ncross-encoder/ms-marco-TinyBERT-L2-v2 : This cross-encoder ranks passages for a given query by scoring query\u2013passage pairs, optimized for MS MARCO passage reranking with very high throughput. It is suitable for information retrieval pipelines where candidate passages are first retrieved (e.g., via BM25) and then re-ranked.\ncross-encoder/ms-marco-MiniLM-L2-v2 : This cross-encoder ranks passages for a given query by directly scoring query\u2013passage pairs, optimized for MS MARCO passage retrieval and re-ranking tasks.\nmixedbread-ai/mxbai-rerank-xsmall-v1 : This is a small cross-encoder reranker for English text-ranking tasks, designed to rescore and reorder candidate documents given a query, offering strong retrieval quality with low latency and cost.\ncross-encoder/ms-marco-MiniLM-L4-v2 : This cross-encoder ranks passages for a given query by directly scoring query\u2013passage pairs, optimized for MS MARCO passage retrieval and reranking tasks.\ncross-encoder/ms-marco-MiniLM-L12-v2 : This cross-encoder ranks passages for a given query by directly scoring query\u2013passage pairs, optimized for MS MARCO passage retrieval and re-ranking tasks.\nAlibaba-NLP/gte-reranker-modernbert-base : Alibaba-NLP/gte-reranker-modernbert-base is an English cross-encoder reranker built on ModernBERT, designed to score and rerank document or passage candidates for a given query, supporting long contexts up to 8192 tokens. It achieves strong performance on BEIR, LoCo, and COIR retrieval benchmarks.\njinaai/jina-reranker-v2-base-multilingual : jinaai/jina-reranker-v2-base-multilingual is a multilingual cross-encoder reranker that scores query\u2013document pairs for relevance, optimized for information retrieval, long-context reranking, and code/function/text-to-SQL-aware ranking. It supports up to 1024-token contexts with sliding-window chunking and optional flash attention for faster inference.\ncross-encoder/stsb-roberta-base : This cross-encoder model scores the semantic similarity of English sentence pairs on a 0\u20131 scale, suitable for text ranking and STS tasks. It is based on roberta-base and trained on the STS benchmark dataset."
    },
    "1060": {
      "id": "1060",
      "name": "Gesture recognition and Pose estimation",
      "type": "workflow",
      "content": "A good and usefull pretrained model is yolo11l-pose, also yolo11s-pose and yolo11m-pose are available too."
    }
  },
  "edges": [
    {"source": "0", "target": "1"},
    {"source": "0", "target": "9"},
    {"source": "0", "target": "10"},
    {"source": "0", "target": "14"},
    {"source": "0", "target": "17"},
    {"source": "0", "target": "20"},
    {"source": "0", "target": "21"},
    {"source": "0", "target": "22"},
    {"source": "22", "target": "2022"},
    {"source": "9", "target": "18"},
    {"source": "9", "target": "19"},
    {"source": "1", "target": "2"},
    {"source": "1", "target": "5"},
    {"source": "1", "target": "6"},
    {"source": "6", "target": "8"},
    {"source": "5", "target": "7"},
    {"source": "2", "target": "3"},
    {"source": "2", "target": "4"},
    {"source": "10", "target": "11"},
    {"source": "10", "target": "12"},
    {"source": "10", "target": "13"},
    {"source": "14", "target": "15"},
    {"source": "14", "target": "16"},
    {"source": "0", "target": "1000"},
    {"source": "0", "target": "1002"},
    {"source": "0", "target": "1004"},
    {"source": "0", "target": "1006"},
    {"source": "0", "target": "1008"},
    {"source": "0", "target": "1010"},
    {"source": "0", "target": "1012"},
    {"source": "0", "target": "1014"},
    {"source": "0", "target": "1016"},
    {"source": "0", "target": "1018"},
    {"source": "0", "target": "1020"},
    {"source": "0", "target": "1022"},
    {"source": "1022", "target": "21022"},
    {"source": "0", "target": "1024"},
    {"source": "0", "target": "1026"},
    {"source": "0", "target": "1030"},
    {"source": "0", "target": "1034"},
    {"source": "0", "target": "1038"},
    {"source": "0", "target": "1040"},
    {"source": "0", "target": "1044"},
    {"source": "0", "target": "1048"},
    {"source": "0", "target": "1050"},
    {"source": "0", "target": "1052"},
    {"source": "0", "target": "1054"},
    {"source": "0", "target": "1056"},
    {"source": "0", "target": "1058"},
    {"source": "0", "target": "1060"}
]}