"""
–ë–∏–∑–Ω–µ—Å-—Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.

–≠—Ç–æ—Ç —Ñ–∞–π–ª –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –±–∏–∑–Ω–µ—Å-–∫–µ–π—Å—ã:
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º (CMS)
- –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å
- –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞
- –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—á–∏—Å—Ç–∫–∞
- –ü–æ–∏—Å–∫ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

–í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ —Å—Ö–µ–º—ã SemanticChunk.
"""

from chunk_metadata_adapter.chunk_query import ChunkQuery
from chunk_metadata_adapter.data_types import ChunkType, ChunkStatus, LanguageEnum, ChunkRole


def example_content_management_queries():
    """–ó–∞–ø—Ä–æ—Å—ã –¥–ª—è —Å–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º."""
    print("=== –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º (CMS) ===")
    
    # 1. –†–µ–¥–∞–∫—Ç–æ—Ä—Å–∫–∞—è –æ—á–µ—Ä–µ–¥—å - –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –æ–±–∑–æ—Ä–∞
    editorial_data = {
        "status": ChunkStatus.CLEANED.value,  # –û–¥–∏–Ω–æ—á–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        "quality_score": "[0.6,0.85]",  # –ù—É–∂–Ω–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        "is_public": False,  # –ï—â–µ –Ω–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ
        "category": "article"  # –û–¥–∏–Ω–æ—á–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    }
    editorial_query, errors = ChunkQuery.from_dict_with_validation(editorial_data)
    assert errors is None
    print(f"‚úÖ –†–µ–¥–∞–∫—Ç–æ—Ä—Å–∫–∞—è –æ—á–µ—Ä–µ–¥—å: —Å—Ç–∞—Ç—É—Å={editorial_query.status}, –∫–∞—á–µ—Å—Ç–≤–æ={editorial_query.quality_score}")
    
    # 2. –ì–æ—Ç–æ–≤—ã–π –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç
    publish_ready_data = {
        "status": ChunkStatus.RELIABLE.value,
        "quality_score": ">=0.9",
        "coverage": ">=0.85",
        "is_public": False,  # –ì–æ—Ç–æ–≤ –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        "feedback_rejected": "<=1"  # –ú–∏–Ω–∏–º—É–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
    }
    publish_query, errors = ChunkQuery.from_dict_with_validation(publish_ready_data)
    assert errors is None
    print(f"‚úÖ –ì–æ—Ç–æ–≤ –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: –∫–∞—á–µ—Å—Ç–≤–æ={publish_query.quality_score}, –ø–æ–∫—Ä—ã—Ç–∏–µ={publish_query.coverage}")
    
    # 3. –ü–æ–ø—É–ª—è—Ä–Ω—ã–π –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    popular_published_data = {
        "is_public": True,
        "feedback_accepted": ">=10",
        "used_in_generation": True,
        "quality_score": ">=0.8",
        "status": ChunkStatus.RELIABLE.value
    }
    popular_query, errors = ChunkQuery.from_dict_with_validation(popular_published_data)
    assert errors is None
    print(f"‚úÖ –ü–æ–ø—É–ª—è—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç: –æ—Ç–∑—ã–≤—ã={popular_query.feedback_accepted}, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è={popular_query.used_in_generation}")
    
    # 4. –ö–æ–Ω—Ç–µ–Ω—Ç, —Ç—Ä–µ–±—É—é—â–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    needs_update_data = {
        "year": "<2022",  # –°—Ç–∞—Ä—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        "quality_score": "[0.5,0.8]",  # –°—Ä–µ–¥–Ω–µ-–Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        "feedback_rejected": ">2",  # –ï—Å—Ç—å –∫—Ä–∏—Ç–∏–∫–∞
        "category": "tutorial"  # –û–¥–∏–Ω–æ—á–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    }
    update_query, errors = ChunkQuery.from_dict_with_validation(needs_update_data)
    assert errors is None
    print(f"‚úÖ –¢—Ä–µ–±—É–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: –≥–æ–¥={update_query.year}, –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è={update_query.feedback_rejected}")
    
    return [editorial_query, publish_query, popular_query, update_query]


def example_quality_control_queries():
    """–ó–∞–ø—Ä–æ—Å—ã –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    print("\n=== –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ ===")
    
    # 1. –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫
    high_quality_data = {
        "quality_score": ">=0.95",
        "coverage": ">=0.9",
        "cohesion": ">=0.8",
        "feedback_accepted": ">=5",
        "feedback_rejected": "<=1"
    }
    high_quality_query, errors = ChunkQuery.from_dict_with_validation(high_quality_data)
    assert errors is None
    print(f"‚úÖ –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: quality={high_quality_query.quality_score}, cohesion={high_quality_query.cohesion}")
    
    # 2. –ü—Ä–æ–±–ª–µ–º–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, —Ç—Ä–µ–±—É—é—â–∏–π –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    problematic_data = {
        "quality_score": "<0.5",
        "feedback_rejected": ">3",
        "status": ChunkStatus.RAW.value,  # –û–¥–∏–Ω–æ—á–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        "used_in_generation": False  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
    }
    problematic_query, errors = ChunkQuery.from_dict_with_validation(problematic_data)
    assert errors is None
    print(f"‚úÖ –ü—Ä–æ–±–ª–µ–º–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç: –∫–∞—á–µ—Å—Ç–≤–æ={problematic_query.quality_score}, –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è={problematic_query.feedback_rejected}")
    
    # 3. –ö–æ–Ω—Ç–µ–Ω—Ç —Å –ø–ª–æ—Ö–∏–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ (–Ω—É–∂–Ω–∞ –ø–µ—Ä–µ—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è)
    bad_boundaries_data = {
        "boundary_prev": "<0.4",
        "boundary_next": "<0.4",
        "cohesion": "<0.6",
        "type": ChunkType.DOC_BLOCK.value  # –û–¥–∏–Ω–æ—á–Ω—ã–π —Ç–∏–ø
    }
    boundaries_query, errors = ChunkQuery.from_dict_with_validation(bad_boundaries_data)
    assert errors is None
    print(f"‚úÖ –ü–ª–æ—Ö–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã: prev={boundaries_query.boundary_prev}, next={boundaries_query.boundary_next}")
    
    # 4. –ù–µ–¥–æ–æ—Ü–µ–Ω–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (—Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –º–∞–ª–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
    undervalued_data = {
        "quality_score": ">=0.8",
        "coverage": ">=0.7",
        "used_in_generation": False,
        "feedback_accepted": "<=2",
        "is_public": True
    }
    undervalued_query, errors = ChunkQuery.from_dict_with_validation(undervalued_data)
    assert errors is None
    print(f"‚úÖ –ù–µ–¥–æ–æ—Ü–µ–Ω–µ–Ω–Ω—ã–π: –∫–∞—á–µ—Å—Ç–≤–æ={undervalued_query.quality_score}, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è={undervalued_query.used_in_generation}")
    
    return [high_quality_query, problematic_query, boundaries_query, undervalued_query]


def example_analytics_and_reporting_queries():
    """–ó–∞–ø—Ä–æ—Å—ã –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏."""
    print("\n=== –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å ===")
    
    # 1. –û—Ç—á–µ—Ç –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    performance_report_data = {
        "used_in_generation": True,
        "quality_score": ">=0.7",
        "feedback_accepted": ">0",
        "year": ">=2023",
        "is_public": True
    }
    performance_query, errors = ChunkQuery.from_dict_with_validation(performance_report_data)
    assert errors is None
    print(f"‚úÖ –û—Ç—á–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –∞–∫—Ç–∏–≤–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å—é")
    
    # 2. –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ —è–∑—ã–∫–∞–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è (–æ—Ç–¥–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã)
    programming_languages = ["python", "javascript", "typescript", "java", "cpp"]
    language_queries = []
    
    for lang in programming_languages:
        lang_data = {
            "language": lang,
            "type": ChunkType.CODE_BLOCK.value,  # –û–¥–∏–Ω–æ—á–Ω—ã–π —Ç–∏–ø
            "status": ChunkStatus.RELIABLE.value  # –û–¥–∏–Ω–æ—á–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        }
        lang_query, errors = ChunkQuery.from_dict_with_validation(lang_data)
        if errors is None:
            language_queries.append((lang_query, lang))
            print(f"‚úÖ –Ø–∑—ã–∫ {lang}: —Ç–∏–ø={lang_query.type}")
    
    # 3. –ú–µ—Ç—Ä–∏–∫–∏ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    categories = ["documentation", "tutorial", "reference", "example"]
    engagement_queries = []
    
    for category in categories:
        engagement_data = {
            "category": category,
            "feedback_accepted": ">0",
            "used_in_generation": True,
            "quality_score": ">=0.7"
        }
        eng_query, errors = ChunkQuery.from_dict_with_validation(engagement_data)
        if errors is None:
            engagement_queries.append((eng_query, category))
            print(f"‚úÖ –í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å {category}: –æ—Ç–∑—ã–≤—ã={eng_query.feedback_accepted}")
    
    # 4. –ê–Ω–∞–ª–∏–∑ –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–æ—Ç–¥–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã)
    lifecycle_stages = [
        (ChunkStatus.RAW.value, "–°—ã—Ä–æ–π"),
        (ChunkStatus.CLEANED.value, "–û—á–∏—â–µ–Ω–Ω—ã–π"),
        (ChunkStatus.VERIFIED.value, "–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π"),
        (ChunkStatus.VALIDATED.value, "–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π"),
        (ChunkStatus.RELIABLE.value, "–ù–∞–¥–µ–∂–Ω—ã–π")
    ]
    
    lifecycle_queries = []
    for status_value, description in lifecycle_stages:
        lifecycle_data = {"status": status_value}
        lc_query, errors = ChunkQuery.from_dict_with_validation(lifecycle_data)
        if errors is None:
            lifecycle_queries.append((lc_query, description))
            print(f"‚úÖ –≠—Ç–∞–ø –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ '{description}': {lc_query.status}")
    
    return {
        "performance": performance_query,
        "languages": language_queries,
        "engagement": engagement_queries,
        "lifecycle": lifecycle_queries
    }


def example_search_and_discovery_queries():
    """–ó–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    print("\n=== –ü–æ–∏—Å–∫ –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ ===")
    
    # 1. –ü–æ–∏—Å–∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ Python
    python_docs_data = {
        "type": ChunkType.DOC_BLOCK.value,
        "language": LanguageEnum.PYTHON.value,
        "quality_score": ">=0.8",
        "status": ChunkStatus.RELIABLE.value,
        "category": "documentation",  # –û–¥–∏–Ω–æ—á–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
        "is_public": True
    }
    python_docs_query, errors = ChunkQuery.from_dict_with_validation(python_docs_data)
    assert errors is None
    print(f"‚úÖ Python –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: —Ç–∏–ø={python_docs_query.type}, —è–∑—ã–∫={python_docs_query.language}")
    
    # 2. –ü–æ–∏—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–æ–¥–∞ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è
    code_examples_data = {
        "type": ChunkType.CODE_BLOCK.value,  # –û–¥–∏–Ω–æ—á–Ω—ã–π —Ç–∏–ø
        "quality_score": ">=0.7",
        "feedback_accepted": ">=3",  # –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ–º
        "category": "example",  # –û–¥–∏–Ω–æ—á–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
        "is_public": True,
        "used_in_generation": True  # –ü–æ–ª–µ–∑–Ω—ã–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    }
    examples_query, errors = ChunkQuery.from_dict_with_validation(code_examples_data)
    assert errors is None
    print(f"‚úÖ –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞: —Ç–∏–ø—ã={examples_query.type}, –æ—Ç–∑—ã–≤—ã={examples_query.feedback_accepted}")
    
    # 3. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –∂–µ–º—á—É–∂–∏–Ω (–≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –º–∞–ª–æ –∏–∑–≤–µ—Å—Ç–Ω—ã)
    hidden_gems_data = {
        "quality_score": ">=0.85",
        "coverage": ">=0.8",
        "feedback_accepted": "[1,5]",  # –ú–∞–ª–æ, –Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
        "used_in_generation": False,  # –ù–µ–¥–æ–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        "is_public": True,
        "status": ChunkStatus.RELIABLE.value
    }
    gems_query, errors = ChunkQuery.from_dict_with_validation(hidden_gems_data)
    assert errors is None
    print(f"‚úÖ –°–∫—Ä—ã—Ç—ã–µ –∂–µ–º—á—É–∂–∏–Ω—ã: –∫–∞—á–µ—Å—Ç–≤–æ={gems_query.quality_score}, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è={gems_query.used_in_generation}")
    
    # 4. –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ä–æ–ª–∏
    developer_content_data = {
        "role": ChunkRole.DEVELOPER.value,
        "type": ChunkType.CODE_BLOCK.value,  # –û–¥–∏–Ω–æ—á–Ω—ã–π —Ç–∏–ø
        "language": LanguageEnum.PYTHON.value,  # –û–¥–∏–Ω–æ—á–Ω—ã–π —è–∑—ã–∫
        "quality_score": ">=0.7",
        "category": "tutorial"  # –û–¥–∏–Ω–æ—á–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    }
    dev_query, errors = ChunkQuery.from_dict_with_validation(developer_content_data)
    assert errors is None
    print(f"‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤: —Ä–æ–ª—å={dev_query.role}, —è–∑—ã–∫={dev_query.language}")
    
    return [python_docs_query, examples_query, gems_query, dev_query]


def example_maintenance_and_cleanup_queries():
    """–ó–∞–ø—Ä–æ—Å—ã –¥–ª—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –∏ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö."""
    print("\n=== –û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –∏ –æ—á–∏—Å—Ç–∫–∞ ===")
    
    # 1. –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏—è
    archive_candidates_data = {
        "year": "<2020",  # –°—Ç–∞—Ä—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        "used_in_generation": False,  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        "feedback_accepted": "<=2",  # –ù–∏–∑–∫–∞—è –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å
        "quality_score": "<0.6",  # –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        "is_public": False  # –ù–µ –ø—É–±–ª–∏—á–Ω—ã–π
    }
    archive_query, errors = ChunkQuery.from_dict_with_validation(archive_candidates_data)
    assert errors is None
    print(f"‚úÖ –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –∞—Ä—Ö–∏–≤–∞: –≥–æ–¥={archive_query.year}, –∫–∞—á–µ—Å—Ç–≤–æ={archive_query.quality_score}")
    
    # 2. –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–ª–∏ –∏–∑–±—ã—Ç–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    duplicate_candidates_data = {
        "quality_score": "[0.3,0.7]",  # –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        "cohesion": "<0.5",  # –ü–ª–æ—Ö–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å
        "boundary_prev": ">0.8",  # –°–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–π
        "boundary_next": ">0.8",  # –°–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π
        "used_in_generation": False
    }
    duplicate_query, errors = ChunkQuery.from_dict_with_validation(duplicate_candidates_data)
    assert errors is None
    print(f"‚úÖ –í–æ–∑–º–æ–∂–Ω—ã–µ –¥—É–±–ª–∏: —Å–≤—è–∑–Ω–æ—Å—Ç—å={duplicate_query.cohesion}, –≥—Ä–∞–Ω–∏—Ü—ã={duplicate_query.boundary_prev}")
    
    # 3. –ö–æ–Ω—Ç–µ–Ω—Ç, —Ç—Ä–µ–±—É—é—â–∏–π –º–∏–≥—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç—É—Å–∞
    status_migration_data = {
        "status": ChunkStatus.RAW.value,  # –û–¥–∏–Ω–æ—á–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        "quality_score": ">=0.8",  # –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        "feedback_rejected": "<=1",  # –ú–∞–ª–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
        "year": ">=2023"  # –ù–µ–¥–∞–≤–Ω–∏–π
    }
    migration_query, errors = ChunkQuery.from_dict_with_validation(status_migration_data)
    assert errors is None
    print(f"‚úÖ –ì–æ—Ç–æ–≤ –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é —Å—Ç–∞—Ç—É—Å–∞: —Ç–µ–∫—É—â–∏–π={migration_query.status}, –∫–∞—á–µ—Å—Ç–≤–æ={migration_query.quality_score}")
    
    # 4. –ö–æ–Ω—Ç–µ–Ω—Ç —Å —É—Å—Ç–∞—Ä–µ–≤—à–∏–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    outdated_metadata_data = {
        "chunking_version": "",  # –ü—É—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è —á–∞–Ω–∫–∏–Ω–≥–∞
        "source_path": "",  # –ü—É—Å—Ç–æ–π –ø—É—Ç—å –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É
        "category": "",  # –ü—É—Å—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
        "quality_score": ">=0.5"  # –ù–æ –ø—Ä–∏–µ–º–ª–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    }
    outdated_query, errors = ChunkQuery.from_dict_with_validation(outdated_metadata_data)
    assert errors is None
    print(f"‚úÖ –£—Å—Ç–∞—Ä–µ–≤—à–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: –≤–µ—Ä—Å–∏—è='{outdated_query.chunking_version}', –ø—É—Ç—å='{outdated_query.source_path}'")
    
    return [archive_query, duplicate_query, migration_query, outdated_query]


if __name__ == "__main__":
    print("üíº –ë–ò–ó–ù–ï–°-–°–¶–ï–ù–ê–†–ò–ò –¢–ò–ü–ò–ó–ò–†–û–í–ê–ù–ù–´–• –ó–ê–ü–†–û–°–û–í")
    print("=" * 55)
    
    example_content_management_queries()
    example_quality_control_queries()
    example_analytics_and_reporting_queries()
    example_search_and_discovery_queries()
    example_maintenance_and_cleanup_queries()
    
    print("\n" + "=" * 55)
    print("‚úÖ –í—Å–µ –±–∏–∑–Ω–µ—Å-—Å—Ü–µ–Ω–∞—Ä–∏–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
    print("üéØ –ü—Ä–∏–º–µ—Ä—ã –≥–æ—Ç–æ–≤—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö") 