import numpy as np
from tqdm import tqdm

from max_div.internal.benchmarking import benchmark
from max_div.internal.markdown import (
    Report,
    Table,
    TableAggregationType,
    TableElement,
    TablePercentage,
    TableTimeElapsed,
    h2,
)
from max_div.internal.math.modify_p_selectivity import exponential_selectivity, modify_p_selectivity
from max_div.internal.utils import stdout_to_file

MODIFY_P_METHODS = [np.int32(0), np.int32(10), np.int32(20), np.int32(100)]


# =================================================================================================
#  Main benchmark
# =================================================================================================
def benchmark_modify_p_selectivity(speed: float = 0.0, markdown: bool = False, file: bool = False) -> None:
    """
    Benchmarks the modify_p_selectivity function from `max_div.internal.math.modify_p_selectivity`,
      for various different 'method'-values across different sizes of probability arrays.

    Array sizes tested: [2, 4, 8, ..., 4096, 8192]

    For each benchmark iteration, a random modifier value in (0.0, 1.0) is chosen from
    100 pre-generated random values to ensure variability.

    :param speed: value in [0.0, 1.0] (default=0.0); 0.0=accurate but slow; 1.0=fast but less accurate
    :param markdown: If `True`, outputs the results as a Markdown table.
    :param file: If `True`, redirects output to a file instead of console.
    """

    print("Benchmarking `modify_p_selectivity`...")

    # --- speed-dependent settings --------------------
    n_accuracy = round(1000.0 / (100.0**speed))  # 1000 when speed=0, 10 when speed=1
    max_size = round(100_000 / (1_000**speed))
    t_per_run = 0.01 / (1000.0**speed)
    n_warmup = int(8 - 6 * speed)
    n_benchmark = int(25 - 24 * speed)

    # --- compute approximation errors ----------------
    # compute errors by method (by comparing exact power method vs other methods on calibration data)
    error_by_method = {method: compute_accuracy(method, n_accuracy) for method in MODIFY_P_METHODS}

    # --- prepare random modifier values --------------
    # Generate 100 random modifier values in (0.0, 1.0)
    np.random.seed(42)
    random_modifiers = np.random.uniform(0.0, 1.0, 100).astype(np.float32)

    # --- benchmark ------------------------------------
    table = Table(headers=["size"] + [f"method={method}" for method in MODIFY_P_METHODS] + ["exponential"])
    sizes = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000]
    sizes = [size for size in sizes if size <= max_size]

    for size in tqdm(sizes, leave=file):
        table_row: list[TableElement | str] = [str(size)]

        # Generate random p array for benchmarking
        # Use size-dependent seed for reproducibility
        np.random.seed(size + 1000)
        p_in = np.random.rand(size).astype(np.float32)
        p_out = np.empty_like(p_in)

        # test modify_p_selectivity for all defined methods
        for method in MODIFY_P_METHODS:
            # define function to be benchmarked
            def benchmark_fun(_idx: int):
                modify_p_selectivity(p_in, random_modifiers[_idx], method, p_out)

            # run benchmark
            table_row.append(
                TableTimeElapsed.from_benchmark_result(
                    benchmark(
                        f=benchmark_fun,
                        t_per_run=t_per_run,
                        n_warmup=n_warmup,
                        n_benchmark=n_benchmark,
                        silent=True,
                        index_range=100,
                    )
                )
            )

        # test order_based_selectivity
        def benchmark_fun(_idx: int):
            exponential_selectivity(p_in, p_out, random_modifiers[_idx])

        # run benchmark
        table_row.append(
            TableTimeElapsed.from_benchmark_result(
                benchmark(
                    f=benchmark_fun,
                    t_per_run=t_per_run,
                    n_warmup=n_warmup,
                    n_benchmark=n_benchmark,
                    silent=True,
                    index_range=100,
                )
            )
        )

        table.add_row(table_row)

    # --- show results -----------------------------------------

    # --- create final report ---
    table.add_aggregate_row(TableAggregationType.GEOMEAN)
    table.add_row(
        ["**e_approx:**"]
        + [TablePercentage(error_by_method[method], decimals=2) for method in MODIFY_P_METHODS]
        + ["N/A"]
    )

    table.highlight_results(TableTimeElapsed, clr_lowest=Table.GREEN, clr_highest=Table.RED)
    table.highlight_results(TablePercentage, clr_lowest=Table.GREEN, clr_highest=Table.RED)

    report = Report()
    report += [
        "Tested methods:",
        get_methods_table(),
        h2("Benchmark results"),
        table,
    ]

    # --- output ---
    with stdout_to_file(file, "benchmark_modify_p_selectivity.md"):
        report.print(markdown=markdown)


# =================================================================================================
#  Helpers
# =================================================================================================
def get_methods_table() -> Table:
    table = Table(headers=["`name`", "`method(args)`", "Type", "Details"])
    table.add_row(
        [
            "**method=0**",
            "`modify_p_selectivity(method=0)`",
            "Power-based",
            "p**t",
        ]
    )
    table.add_row(
        [
            "**method=10**",
            "`modify_p_selectivity(method=10)`",
            "Power-based",
            "fast_exp2(t * fast_log2(p)) (NOT specifically optimized for this use case)",
        ]
    )
    table.add_row(
        [
            "**method=20**",
            "`modify_p_selectivity(method=20)`",
            "Power-based",
            "fast_pow(p, t) (specifically optimized for this use case)",
        ]
    )
    table.add_row(
        [
            "**method=100**",
            "`modify_p_selectivity(method=100)`",
            "Power-based",
            "2-segment PWL approx. of p**t",
        ]
    )
    table.add_row(
        [
            "**exponential**",
            "`exponential_selectivity()`",
            "Exponential mapping",
            "maps [p_min, p_max] to [1.0, low_value**t]",
        ]
    )

    return table


def compute_accuracy(method: int, n: int) -> float:
    """Computes accuracy of a given method as a fraction in [0.0, 1.0]."""

    total_error = 0.0  # total sum of abs errors
    total_pmod = 0.0  # total sum of target values (wrt which we computed errors)

    for modify in np.linspace(-0.9, 0.9, n):
        t = (1.0 + modify) / (1.0 - modify)

        # construct p_in such that p_out_target is uniformly spaced in [0.0, 1.0],
        # which will focus (for each modify-value) p_in-values in regions where we'll be able to see differences best
        p_out_target = np.linspace(0.0, 1.0, n)
        p_in = (p_out_target ** (1.0 / t)).astype(np.float32)
        p_out = np.empty_like(p_in)
        modify_p_selectivity(p_in, np.float32(modify), np.int32(method), p_out)

        total_error += np.sum(np.abs(p_out_target - p_out))
        total_pmod += np.sum(p_out_target)

    error_frac = total_error / total_pmod
    return error_frac
