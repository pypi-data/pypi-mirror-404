#!/usr/bin/env python3
"""
è®°å¿†è¿ç§»è„šæœ¬

åŠŸèƒ½:
1. å°†ç°æœ‰ memories.json æ‰¹é‡å‘é‡åŒ–åˆ° ChromaDB
2. é‡ç½® MEMORY.md ä¸ºç²¾åæ‘˜è¦æ ¼å¼
3. éªŒè¯è¿ç§»ç»“æœ

ä½¿ç”¨æ–¹æ³•:
    python scripts/migrate_memories.py
    python scripts/migrate_memories.py --dry-run  # ä»…æ£€æŸ¥ï¼Œä¸æ‰§è¡Œ
    python scripts/migrate_memories.py --reset-memory-md  # åªé‡ç½® MEMORY.md
"""

import json
import sys
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))


def load_memories(memories_file: Path) -> list[dict]:
    """åŠ è½½ç°æœ‰è®°å¿†"""
    if not memories_file.exists():
        print(f"âŒ è®°å¿†æ–‡ä»¶ä¸å­˜åœ¨: {memories_file}")
        return []
    
    try:
        with open(memories_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"âœ… åŠ è½½äº† {len(data)} æ¡è®°å¿†")
        return data
    except Exception as e:
        print(f"âŒ åŠ è½½è®°å¿†å¤±è´¥: {e}")
        return []


def migrate_to_vector_store(memories: list[dict], data_dir: Path) -> int:
    """å°†è®°å¿†è¿ç§»åˆ°å‘é‡åº“"""
    try:
        from openakita.memory.vector_store import VectorStore
    except ImportError as e:
        print(f"âŒ å¯¼å…¥ VectorStore å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿å·²å®‰è£…ä¾èµ–: pip install sentence-transformers chromadb")
        return 0
    
    print("\nğŸ“¦ åˆå§‹åŒ–å‘é‡å­˜å‚¨...")
    vector_store = VectorStore(data_dir=data_dir)
    
    if not vector_store.enabled:
        print("âŒ å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥")
        return 0
    
    print(f"âœ… å‘é‡å­˜å‚¨å·²å¯ç”¨ (æ¨¡å‹: {vector_store.model_name})")
    
    # æ‰¹é‡æ·»åŠ 
    print(f"\nğŸ”„ å¼€å§‹è¿ç§» {len(memories)} æ¡è®°å¿†...")
    
    batch_data = []
    for m in memories:
        batch_data.append({
            "id": m.get("id", ""),
            "content": m.get("content", ""),
            "type": m.get("type", "fact"),
            "priority": m.get("priority", "short_term"),
            "importance": m.get("importance_score", 0.5),
            "tags": m.get("tags", []),
        })
    
    added = vector_store.batch_add(batch_data)
    print(f"âœ… æˆåŠŸè¿ç§» {added} æ¡è®°å¿†åˆ°å‘é‡åº“")
    
    # éªŒè¯
    stats = vector_store.get_stats()
    print(f"\nğŸ“Š å‘é‡åº“ç»Ÿè®¡:")
    print(f"   - æ€»è®°å¿†æ•°: {stats['count']}")
    print(f"   - æ¨¡å‹: {stats['model']}")
    print(f"   - è®¾å¤‡: {stats['device']}")
    
    return added


def reset_memory_md(memory_md_path: Path, memories: list[dict]) -> bool:
    """é‡ç½® MEMORY.md ä¸ºç²¾åæ‘˜è¦æ ¼å¼"""
    print(f"\nğŸ“ é‡ç½® MEMORY.md: {memory_md_path}")
    
    # æŒ‰ç±»å‹åˆ†ç»„
    by_type = {
        "preference": [],
        "rule": [],
        "fact": [],
        "skill": [],
    }
    
    for m in memories:
        # åªé€‰å–æ°¸ä¹…æˆ–é•¿æœŸè®°å¿†
        priority = m.get("priority", "short_term")
        if priority not in ("permanent", "long_term"):
            continue
        
        mem_type = m.get("type", "fact").lower()
        if mem_type in by_type:
            by_type[mem_type].append(m)
    
    # æŒ‰é‡è¦æ€§æ’åºï¼Œæ¯ç±»æœ€å¤š 3-5 æ¡
    for key in by_type:
        by_type[key].sort(key=lambda x: x.get("importance_score", 0), reverse=True)
        by_type[key] = by_type[key][:5 if key == "fact" else 3]
    
    # ç”Ÿæˆ Markdown
    lines = [
        "# Core Memory",
        "",
        "> Agent æ ¸å¿ƒè®°å¿†ï¼Œæ¯æ¬¡å¯¹è¯éƒ½ä¼šåŠ è½½ã€‚æ¯æ—¥å‡Œæ™¨è‡ªåŠ¨åˆ·æ–°ã€‚",
        f"> æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        "",
    ]
    
    if by_type["preference"]:
        lines.append("## ç”¨æˆ·åå¥½")
        for m in by_type["preference"]:
            lines.append(f"- {m.get('content', '')}")
        lines.append("")
    
    if by_type["rule"]:
        lines.append("## é‡è¦è§„åˆ™")
        for m in by_type["rule"]:
            lines.append(f"- {m.get('content', '')}")
        lines.append("")
    
    if by_type["fact"]:
        lines.append("## å…³é”®äº‹å®")
        for m in by_type["fact"]:
            lines.append(f"- {m.get('content', '')}")
        lines.append("")
    
    if by_type["skill"]:
        lines.append("## æˆåŠŸæ¨¡å¼")
        for m in by_type["skill"][:2]:
            lines.append(f"- {m.get('content', '')}")
        lines.append("")
    
    if not any(by_type.values()):
        lines.append("## è®°å¿†")
        lines.append("[æš‚æ— æ ¸å¿ƒè®°å¿†]")
        lines.append("")
    
    content = "\n".join(lines)
    
    try:
        # å¤‡ä»½æ—§æ–‡ä»¶
        if memory_md_path.exists():
            backup_path = memory_md_path.with_suffix(".md.bak")
            memory_md_path.rename(backup_path)
            print(f"   å·²å¤‡ä»½æ—§æ–‡ä»¶åˆ°: {backup_path}")
        
        # å†™å…¥æ–°æ–‡ä»¶
        memory_md_path.write_text(content, encoding="utf-8")
        print(f"âœ… MEMORY.md å·²é‡ç½® ({len(content)} å­—ç¬¦)")
        return True
        
    except Exception as e:
        print(f"âŒ é‡ç½® MEMORY.md å¤±è´¥: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description="è®°å¿†è¿ç§»è„šæœ¬")
    parser.add_argument("--dry-run", action="store_true", help="ä»…æ£€æŸ¥ï¼Œä¸æ‰§è¡Œ")
    parser.add_argument("--reset-memory-md", action="store_true", help="åªé‡ç½® MEMORY.md")
    args = parser.parse_args()
    
    print("=" * 60)
    print("OpenAkita è®°å¿†ç³»ç»Ÿè¿ç§»è„šæœ¬")
    print("=" * 60)
    
    # è·¯å¾„é…ç½®
    data_dir = project_root / "data" / "memory"
    memories_file = data_dir / "memories.json"
    memory_md_path = project_root / "identity" / "MEMORY.md"
    
    print(f"\nğŸ“‚ æ•°æ®ç›®å½•: {data_dir}")
    print(f"ğŸ“‚ è®°å¿†æ–‡ä»¶: {memories_file}")
    print(f"ğŸ“‚ MEMORY.md: {memory_md_path}")
    
    # åŠ è½½è®°å¿†
    memories = load_memories(memories_file)
    
    if not memories:
        print("\nâš ï¸ æ²¡æœ‰è®°å¿†éœ€è¦è¿ç§»")
        return
    
    if args.dry_run:
        print("\nğŸ” [DRY RUN] ä»…æ£€æŸ¥ï¼Œä¸æ‰§è¡Œè¿ç§»")
        print(f"   å°†è¿ç§» {len(memories)} æ¡è®°å¿†åˆ°å‘é‡åº“")
        print(f"   å°†é‡ç½® MEMORY.md")
        return
    
    if args.reset_memory_md:
        # åªé‡ç½® MEMORY.md
        reset_memory_md(memory_md_path, memories)
        return
    
    # å®Œæ•´è¿ç§»
    print("\n" + "=" * 60)
    print("å¼€å§‹è¿ç§»...")
    print("=" * 60)
    
    # 1. è¿ç§»åˆ°å‘é‡åº“
    migrated = migrate_to_vector_store(memories, data_dir)
    
    # 2. é‡ç½® MEMORY.md
    reset_memory_md(memory_md_path, memories)
    
    # å®Œæˆ
    print("\n" + "=" * 60)
    print("è¿ç§»å®Œæˆ!")
    print("=" * 60)
    print(f"âœ… å‘é‡åŒ–è®°å¿†: {migrated} æ¡")
    print(f"âœ… MEMORY.md å·²é‡ç½®")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. å¯åŠ¨ OpenAkita éªŒè¯åŠŸèƒ½æ­£å¸¸")
    print("2. æµ‹è¯•å‘é‡æœç´¢: åœ¨å¯¹è¯ä¸­æé—®ç›¸å…³é—®é¢˜")
    print("3. ç­‰å¾…å‡Œæ™¨è‡ªåŠ¨å½’çº³ï¼Œæˆ–æ‰‹åŠ¨æ‰§è¡Œ consolidate_memories")


if __name__ == "__main__":
    main()
