import json
import re
import shutil
from pathlib import Path
from typing import Any, Optional

import tiktoken

from .. import ColumnStrings, logerr, pretty_print
from ..configs.system import AsisTobe
from ..logger import log, logsuc
from ..reader_controller.exceptions import EmptyIdfyNotNullDictException
from ..reader_controller.types import IdfyGoods, IndexedIdfyGoods


print = log


class Tools:
    # –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –≤—Å–µ—Ö Unicode-–ø—Ä–æ–±–µ–ª–æ–≤ (–≤–∫–ª—é—á–∞—è zero-width)
    _UNICODE_SPACES_RE = re.compile(
        r"[\u0009\u000A\u000B\u000C\u000D\u0020\u0085\u00A0\u1680"
        r"\u180E\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\u200B\u2060\uFEFF]+"
    )

    @staticmethod
    def semicolonizer(string: str) -> str:
        string = string.replace(",", ", ")
        return string.replace("  ", " ")

    @staticmethod
    def extract_digits_list(s: str) -> list[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —á–∏—Å–ª–∞ (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ü–∏—Ñ—Ä) –∏–∑ —Å—Ç—Ä–æ–∫–∏.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ (–Ω–µ int, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–µ–¥—É—â–∏–µ –Ω—É–ª–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ).
        """
        return re.findall(r'\d+', s)

    @staticmethod
    def get_dry_brand(brand: str):
        allowed_chars = (
            'abcdefghijklmnopqrstuvwxyz'
            '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è'
            '0123456789&!-\''
        )
        return Tools.get_dry_string(brand, allowed_chars)

    @staticmethod
    def get_dry_string(input_string: str, allows: Optional[str] = None) -> str:
        """–í—ã—Å—É—à–∏–≤–∞–µ—Ç —Å—Ç—Ä–æ–∫—É, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã –≤ lower-—Ñ–æ—Ä–º–∞—Ç–µ"""
        allowed_chars = (
            'abcdefghijklmnopqrstuvwxyz'
            '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è'
            '0123456789'
        )
        if allows is not None:
            allowed_chars = allows
        return ''.join(
            c for c in input_string.lower()
            if c in allowed_chars
        )

    @staticmethod
    def get_dry_string_list(input_string: str) -> list[str]:
        """–í—ã—Å—É—à–∏–≤–∞–µ—Ç —Å—Ç—Ä–æ–∫—É, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã –≤ lower-—Ñ–æ—Ä–º–∞—Ç–µ –≤ —Å–ø–∏—Å–∫–µ"""
        result: list[str] = []
        for word in input_string.split():
            result.append(Tools.get_dry_string(word))
        return result

    @staticmethod
    def get_all_fields(fields: list[str]) -> list[str]:
        """
        –†–∞—Å—à–∏—Ä—è–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π, –¥–æ–±–∞–≤–ª—è—è —Å—É—Ñ—Ñ–∏–∫—Å—ã _asis –∏ _tobe
        :param fields: —Å–ø–∏—Å–æ–∫ –±–∞–∑–æ–≤—ã—Ö –ø–æ–ª–µ–π
        :return: —Å–ø–∏—Å–æ–∫ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
        """
        if not fields:
            raise ValueError("–°–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        extended_fields = []
        for field in fields:
            extended_fields.append(f"{field}_asis")
            extended_fields.append(f"{field}_tobe")
        return extended_fields

    @staticmethod
    def get_relay(fields: list[str]) -> dict[str, str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å asis->tobe —Å–≤—è–∑–∫–æ–π –∏–∑ fields"""
        if not len(fields):
            raise IndexError("–°–ü–ò–°–û–ö –ü–û–õ–ï–ô –î–õ–Ø –§–ò–ù–ê–õ–ò–ó–ê–¶–ò–ò –ù–ï –ú–û–ñ–ï–¢ –ë–´–¢–¨ –ü–£–°–¢–´–ú")
        result: dict[str, str] = {}
        for field in fields:
            result[f"{field}_asis"] = f"{field}_tobe"
        return result

    @staticmethod
    def normalize_spaces(text: str) -> str:
        """
        –ó–∞–º–µ–Ω—è–µ—Ç –≤—Å–µ —Ç–∏–ø—ã –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ Unicode –Ω–∞ –æ–±—ã—á–Ω—ã–π –ø—Ä–æ–±–µ–ª.
        """
        return Tools._UNICODE_SPACES_RE.sub(' ', text)

    @staticmethod
    def string_stripper(text: str) -> str:
        """
        - –£–¥–∞–ª—è–µ—Ç –≤–µ–¥—É—â–∏–µ/–∫–æ–Ω–µ—á–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        - –ó–∞–º–µ–Ω—è–µ—Ç –≤—Å–µ –Ω–µ–≤–∏–¥–∏–º—ã–µ –∏ —Å—Ç—Ä–∞–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–±—ã—á–Ω—ã–µ
        - –°—Ö–ª–æ–ø—ã–≤–∞–µ—Ç –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ –æ–¥–∏–Ω
        """
        if not text:
            return ""
        text = Tools.normalize_spaces(text)
        text = text.strip()
        text = re.sub(r' {2,}', ' ', text)
        return text.strip()

    @staticmethod
    def clean_field(string: str) -> str:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞"""
        string = Tools.string_stripper(string)
        string = Tools.trim_spec(string)
        return Tools.string_stripper(string)

    @staticmethod
    def polysplit(
        text: str,
        separators: Optional[list[str]] = None,
        no_empty: bool = False
    ) -> list[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —É–¥–∞–ª–µ–Ω–∏—è –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫.

        :param text: –ò—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
        :param separators: –°–ø–∏—Å–æ–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
        :param no_empty: –ï—Å–ª–∏ True (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é), —É–¥–∞–ª—è–µ—Ç –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        :return: –°–ø–∏—Å–æ–∫ —á–∞—Å—Ç–µ–π —Å—Ç—Ä–æ–∫–∏
        """
        if separators is None:
            separators = ["/", "\\", "|", "-", "+", " "]
        if not text:
            return []

        result = [text]

        for sep in separators:
            temp = []
            for part in result:
                temp.extend(part.split(sep))
            result = temp

        if no_empty:
            return [part for part in result if part]  # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        return result  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å

    @staticmethod
    def get_words(string: str, use_clean_strip: bool = False) -> list[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –ø–æ –ø—Ä–æ–±–µ–ª—É

        use_clean_strip - –µ—Å–ª–∏ –∑–∞–¥–∞–Ω, –≤–æ–∑—å–º—ë—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥—Ä—É–≥–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        """
        string = Tools.normalize_spaces(string)
        string_list = string.split(" ")
        if use_clean_strip:
            # –£–ë–†–ê–õ–ò "!"
            return [word.strip('.,?:;"\'()-_/|*<>#%^&+=~ ') for word in string_list]
        return string_list

    @staticmethod
    def get_lower_words(words: list[str]) -> list[str]:
        """–ë–µ—Ä–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫, –¥–µ–ª–∞–µ—Ç –∏—Ö lower"""
        return [word.lower() for word in words]

    @staticmethod
    def get_most_longer_word_of_list(words_list):
        """
        –ú–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–±–∏—Ä–∞–µ—Ç —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ —Å–ø–∏—Å–∫–∞.
        –ï—Å–ª–∏ —Ç–∞–∫–∏—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ - —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É.

        :param words_list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤
        :return: –°–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ –ø–µ—Ä–≤–æ–µ –∏–∑ —Å–∞–º—ã—Ö –¥–ª–∏–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ
        """
        if not words_list:
            return None

        # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Å–ª–æ–≤–∞
        max_length = max(len(word) for word in words_list)

        # –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–ª–æ–≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
        longest_words = [word for word in words_list if len(word) == max_length]

        # –ï—Å–ª–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
        if len(longest_words) == 1:
            return longest_words[0]
        else:
            # –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –æ–¥–Ω–æ–π –¥–ª–∏–Ω—ã, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É –∏ –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ
            return sorted(longest_words)[0]

    @staticmethod
    def get_first_of_dict(dictionary: dict, key: bool = False) -> Optional[Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∫–ª—é—á–∞"""
        priority_key = next(iter(dictionary), None)
        if priority_key is None:
            return None
        return priority_key if key else dictionary[priority_key]

    @staticmethod
    def trim_spec(word: str, is_brand: bool = False) -> str:
        """
        –û—á–∏—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –æ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤ –ø–æ –∫—Ä–∞—è–º.
        –ü—Ä–∏ is_brand=True:
          - –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ ‚Äî –æ–¥–Ω–æ —Å–ª–æ–≤–æ –≤ —Å–∫–æ–±–∫–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, "(Samsung)") ‚Üí —É–¥–∞–ª—è–µ—Ç —Å–∫–æ–±–∫–∏.
          - –ï—Å–ª–∏ —Å–ª–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–ª–∏ —Å–∫–æ–±–∫–∏ –Ω–µ –ø–æ –∫—Ä–∞—è–º ‚Üí –æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–∞–∫ –µ—Å—Ç—å.

        Args:
            word (str): –ò—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞.
            is_brand (bool): –§–ª–∞–≥, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π, —á—Ç–æ —Å—Ç—Ä–æ–∫–∞ ‚Äî –±—Ä–µ–Ω–¥ (—Ç—Ä–µ–±—É–µ—Ç –æ—Å–æ–±–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–∫–æ–±–æ–∫).

        Returns:
            str: –û—á–∏—â–µ–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞.
        """
        if not word or not isinstance(word, str):
            return word

        word = word.strip()

        # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—Ä–∞—ë–≤: —É–¥–∞–ª—è–µ–º –≤—Å—ë, –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä, –¥–µ—Ñ–∏—Å–∞, –∞–ø–æ—Å—Ç—Ä–æ—Ñ–∞, —Å–∫–æ–±–æ–∫
        cleaned = re.sub(r'^[^\w\(\)\[\]\{\}\'-]+|[^\w\(\)\[\]\{\}\'-]+$', '', word)

        if not is_brand:
            return cleaned

        # === –õ–æ–≥–∏–∫–∞ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤ ===

        # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —É–∂–µ –Ω–µ –≤ —Å–∫–æ–±–∫–∞—Ö ‚Äî –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
        if not (cleaned.startswith('(') and cleaned.endswith(')')):
            return cleaned

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ "–æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º"
        inner = cleaned[1:-1].strip()  # —É–±–∏—Ä–∞–µ–º —Å–∫–æ–±–∫–∏ –∏ –ø—Ä–æ–±–µ–ª—ã

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ "—Å–ª–æ–≤–∞" (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±—É–∫–≤/—Ü–∏—Ñ—Ä)
        inner_words = re.findall(r'[\w]+', inner, re.UNICODE)

        # –ï—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ 2+ —Å–ª–æ–≤ ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º —Å–∫–æ–±–∫–∏
        if len(inner_words) >= 2:
            return cleaned

        # –ï—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ 1 —Å–ª–æ–≤–æ ‚Äî —É–¥–∞–ª—è–µ–º –≤–Ω–µ—à–Ω–∏–µ —Å–∫–æ–±–∫–∏
        if len(inner_words) == 1:
            return inner

        # –ï—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ 0 —Å–ª–æ–≤ (–ø—É—Å—Ç–æ –∏–ª–∏ —Ç–æ–ª—å–∫–æ —Å–∏–º–≤–æ–ª—ã) ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑ —Å–∫–æ–±–æ–∫
        return inner  # –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º

    @staticmethod
    def get_most_popular(variants_list: list[str], variants_registry: dict[str, int]) -> list[str]:
        """
        –í—ã–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–∑ variants_list
        –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ variants_registry –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è: {
            "–±—Ä–µ–Ω–¥1": <–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π_–≤_–¥–æ–∫—É–º–µ–Ω—Ç–µ>,
            "–±—Ä–µ–Ω–¥2": <–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π_–≤_–¥–æ–∫—É–º–µ–Ω—Ç–µ>,
        }
        –ï—Å–ª–∏ —É –æ–¥–Ω–æ–≥–æ –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–µ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –≤ variants_registry, —Ç–æ –ø—Ä–∏—Ä–∞–≤–Ω—è—Ç—å –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∫ 0
        –ï—Å–ª–∏ —É –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å, –≤–µ—Ä–Ω—É—Ç—å –≤—Å–µ —Ç–∞–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        :param variants_list: —Å–ø–∏—Å–æ–∫ –±—Ä–µ–Ω–¥–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        :param variants_registry: —Å–ª–æ–≤–∞—Ä—å –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –±—Ä–µ–Ω–¥–æ–≤
        :return: —Å–ø–∏—Å–æ–∫ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤
        """
        # –°–Ω–∞—á–∞–ª–∞ –Ω–∞–π–¥–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —á–∞—Å—Ç–æ—Ç—É
        max_frequency = -1
        for brand in variants_list:
            frequency = variants_registry.get(Tools.get_dry_brand(brand), 0)
            if frequency > max_frequency:
                max_frequency = frequency

        # print(f"{max_frequency=}")

        # –¢–µ–ø–µ—Ä—å —Å–æ–±–µ—Ä–µ–º –≤—Å–µ –±—Ä–µ–Ω–¥—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–æ—Ç–æ–π
        popular_brands = []
        for brand in variants_list:
            frequency = variants_registry.get(Tools.get_dry_brand(brand), 0)
            if frequency == max_frequency:
                popular_brands.append(brand)

        return popular_brands

    @staticmethod
    def get_threads_data_parts_by_dict(
        idfy_not_null_dict: IdfyGoods,
        thread_pks: list[int],
        is_use_small_chunks: bool = False,
        small_chunk_size: int = 10,
    ) -> IndexedIdfyGoods:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ —Å–ø–∏—Å–∫—É thread_pks.
        –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö < 10 ‚Äî –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–¥—É—Ç –≤ –ø–µ—Ä–≤—ã–π –ø–æ—Ç–æ–∫ (–ø–æ —Å–ø–∏—Å–∫—É).
        –ò–Ω–∞—á–µ ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ.

        :param idfy_not_null_dict: –ò—Å—Ö–æ–¥–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö (–∫–ª—é—á: int, –∑–Ω–∞—á–µ–Ω–∏–µ: dict)
        :param thread_pks: –°–ø–∏—Å–æ–∫ pk –ø–æ—Ç–æ–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ .values_list('pk', flat=True))
        :param is_use_small_chunks: –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –æ–±—ä—ë–º –¥–∞–Ω–Ω—ã—Ö (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)
        :param small_chunk_size: –ú–∞–∫—Å. –∫–æ–ª-–≤–æ –∑–∞–ø–∏—Å–µ–π, –µ—Å–ª–∏ is_use_small_chunks=True
        :return: dict[thread_pk: dict[int, XLSGood]] ‚Äî –∫–ª—é—á ‚Äî pk –ø–æ—Ç–æ–∫–∞
        :raises ValueError: –ï—Å–ª–∏ thread_pks –ø—É—Å—Ç
        :raises EmptyIdfyNotNullDictException: –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö
        """
        log("–ì–µ–Ω–µ—Ä–∏—Ä—É—é —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤")

        if not thread_pks:
            raise ValueError("–°–ø–∏—Å–æ–∫ thread_pks –ø—É—Å—Ç")

        if not idfy_not_null_dict:
            raise EmptyIdfyNotNullDictException("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        items = list(idfy_not_null_dict.items())
        if is_use_small_chunks:
            items = items[:small_chunk_size]
        total_items = len(items)
        total_threads = len(thread_pks)

        result: IndexedIdfyGoods = {}

        # üîπ –õ–æ–≥–∏–∫–∞: –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ ‚Äî –≤—Å—ë –≤ –ø–µ—Ä–≤—ã–π –ø–æ—Ç–æ–∫
        if total_items < 10:
            first_pk = thread_pks[0]
            log(f"–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ({total_items} < 10). –í—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ –ø–æ—Ç–æ–∫ {first_pk}")
            result[first_pk] = dict(items)
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ ‚Äî –ø—É—Å—Ç—ã–µ
            for pk in thread_pks[1:]:
                result[pk] = {}
        else:
            # –û–±—ã—á–Ω–æ–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            base_size, remainder = divmod(total_items, total_threads)
            index = 0

            for pk in thread_pks:
                chunk_size = base_size + (1 if remainder > 0 else 0)
                chunk = dict(items[index:index + chunk_size])
                result[pk] = chunk
                index += chunk_size
                remainder -= 1

        avg_per_thread = total_items // total_threads if total_threads else 0
        logsuc(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–æ–≤–∞—Ä—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. ~ –∑–∞–ø–∏—Å–µ–π –≤ –ø–æ—Ç–æ–∫–µ: {avg_per_thread}")
        return result

    @staticmethod
    def get_dict_filtered(
        data: dict[Any, dict[str, Any]],
        include_by_rules: list[dict[str, Any]],
        strict: bool = False
    ) -> dict[Any, dict[str, Any]]:
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤–∞—Ä–µ–π –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
          - –ù–µ—Å–∫–æ–ª—å–∫–æ —É—Å–ª–æ–≤–∏–π –Ω–∞ –æ–¥–Ω–æ –ø–æ–ª–µ ‚Üí –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –∫–∞–∫ OR
          - –£—Å–ª–æ–≤–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã–µ –ø–æ–ª—è ‚Üí –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –∫–∞–∫ AND

        –ü—Ä–∏–º–µ—Ä:
            rules = [
                {'status': 'active'},
                {'status': 'pending'},
                {'category': 'premium'}
            ]
            ‚Üí (status == 'active' OR status == 'pending') AND category == 'premium'

        :param data: —Å–ª–æ–≤–∞—Ä—å, –≥–¥–µ –∑–Ω–∞—á–µ–Ω–∏—è ‚Äî —Å–ª–æ–≤–∞—Ä–∏
        :param include_by_rules: —Å–ø–∏—Å–æ–∫ —Ñ–∏–ª—å—Ç—Ä–æ–≤ (—É—Å–ª–æ–≤–∏—è –Ω–∞ –ø–æ–ª—è)
        :param strict: –µ—Å–ª–∏ True, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∫–ª—é—á–∞ = –æ—à–∏–±–∫–∞; –∏–Ω–∞—á–µ ‚Äî –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        :return: –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
        """
        if not isinstance(data, dict):
            raise ValueError("data –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä—ë–º")
        if not isinstance(include_by_rules, list):
            raise ValueError("include_by_rules –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")

        # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–∞–≤–∏–ª ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        if not include_by_rules:
            return {k: v for k, v in data.items() if isinstance(v, dict)}

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª–∞ –ø–æ –∫–ª—é—á–∞–º: field -> set(expected_values)
        conditions: dict[str, set[Any]] = {}
        for rule in include_by_rules:
            if not isinstance(rule, dict):
                continue
            for key, value in rule.items():
                if key not in conditions:
                    conditions[key] = set()
                conditions[key].add(value)

        result = {}

        for key, item in data.items():
            if not isinstance(item, dict):
                continue

            match = True
            for field, allowed_values in conditions.items():
                if field not in item:
                    # –ö–ª—é—á –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                    if strict:
                        match = False
                    else:
                        match = False  # –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç (–∫–ª—é—á–∞ –Ω–µ—Ç)
                    break

                actual_value = item[field]
                if actual_value not in allowed_values:
                    match = False
                    break  # –Ω–µ –ø—Ä–æ—à–ª–æ —É—Å–ª–æ–≤–∏–µ –ø–æ –ø–æ–ª—é

            if match:
                result[key] = item

        return result

    @staticmethod
    def skip_n_rows(data_dict: IdfyGoods, n_of_skip_rows: int) -> IdfyGoods:
        """–ü—Ä–æ–ø—É—Å–∫ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤"""
        return dict(list(data_dict.items())[n_of_skip_rows:])

    @staticmethod
    def no_repeats_of_list(lst: list) -> list:
        """
        –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤.

        :param lst: –ò—Å—Ö–æ–¥–Ω—ã–π —Å–ø–∏—Å–æ–∫
        :return: –°–ø–∏—Å–æ–∫ –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        """
        seen = set()
        filtered = [x for x in lst if not (x in seen or seen.add(x))]
        return [x for x in filtered if x is not None]

    @staticmethod
    def find_original_substring(source: str, word: str) -> Optional[str]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –ø–æ–¥—Å—Ç—Ä–æ–∫—É –≤ –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –æ–±—Ä–∞–∑—Ü—É (—Å —É—á–µ—Ç–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞ –∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–∞–ø–∏—Å–∞–Ω–∏—è).

        :param source: –ò—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        :param word: –ü–æ–¥—Å—Ç—Ä–æ–∫–∞-–æ–±—Ä–∞–∑–µ—Ü (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ)
        :return: –ù–∞–π–¥–µ–Ω–Ω–∞—è –ø–æ–¥—Å—Ç—Ä–æ–∫–∞ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –≤–∏–¥–µ –∏–ª–∏ None
        """
        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –≤ –æ–±—Ä–∞–∑—Ü–µ –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        pattern_parts = [re.escape(part) for part in word.split()]

        # –°–æ–∑–¥–∞–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ:
        # - –ò—â–µ–º —Å–ª–æ–≤–∞ –æ–±—Ä–∞–∑—Ü–∞ –≤ –ª—é–±–æ–º –ø–æ—Ä—è–¥–∫–µ
        # - –ú–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –¥—Ä—É–≥–∏–µ —Å–∏–º–≤–æ–ª—ã
        # - –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–π –ø–æ–∏—Å–∫
        regex_pattern = r'(?i)(?=.*?\b{}\b)'.format(r'\b)(?=.*?\b'.join(pattern_parts))
        regex_pattern += r'([^\n]*?{}[^\n]*)'.format(r'[^\n]*?'.join(pattern_parts))

        match = re.search(regex_pattern, source)
        if not match:
            return None

        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ—á–Ω—É—é –ø–æ–¥—Å—Ç—Ä–æ–∫—É, —Å–æ–¥–µ—Ä–∂–∞—â—É—é –≤—Å–µ —Å–ª–æ–≤–∞ –æ–±—Ä–∞–∑—Ü–∞
        matched_text = match.group(1)

        # –£—Ç–æ—á–Ω—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        words = word.lower().split()
        start_pos = 0
        end_pos = len(matched_text)

        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞
        first_word_pattern = re.compile(re.escape(words[0]), re.IGNORECASE)
        first_match = first_word_pattern.search(matched_text)
        if first_match:
            start_pos = first_match.start()

        # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ–≤–∞
        last_word_pattern = re.compile(re.escape(words[-1]), re.IGNORECASE)
        last_matches = list(last_word_pattern.finditer(matched_text))
        if last_matches:
            end_pos = last_matches[-1].end()

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–¥—Å—Ç—Ä–æ–∫—É —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –Ω–∞–ø–∏—Å–∞–Ω–∏–µ–º
        return matched_text[start_pos:end_pos].strip(" ,.-")

    @staticmethod
    def get_normal_word_from_source_by_castrat_word(
        source: str,
        word: str
    ):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É, –Ω–∞–π–¥–µ–Ω–Ω–æ–º—É –ø–æ –∫–∞—Å—Ç—Ä–∞—Ç—É –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–µ
        :param source: example = "–°—Ç–∏—Ä–∞–ª—å–Ω–∞—è –º–∞—à–∏–Ω–∞ AEG L7WBE68SI White"
        :param word: example = "white"
        :return: example = "White"
        """
        # debug: bool = False
        _debug: bool = True

        if _debug: logerr("<get_normal_word_from_source_by_castrat_word>")
        if _debug: print(f"{source=}")
        if _debug: print(f"{word=}")
        source_list: list[str] = source.split()
        if _debug: pretty_print(source_list, title=f"source_list", m2d=False)
        source_lower: str = source.lower()
        if _debug: print(f"{source_lower=}")

        source_lower_list: list[str] = source_lower.split()
        source_lower_list = [Tools.trim_spec(color) for color in source_lower_list]
        if _debug: pretty_print(source_lower_list, title=f"source_lower_list", m2d=False)

        index_main: int = 0
        for index, word_part in enumerate(source_lower_list):
            if word not in word_part:
                continue
            else:
                index_main = index
                break
        if _debug: print(f"{index_main=}")

        if not index_main:
            return None
        return source_list[index_main]

    @staticmethod
    def completely_nulled(fields: list[str], data_dict: dict) -> bool:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –≤—Å–µ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –ø–æ–ª—è –≤ data_dict —Ä–∞–≤–Ω—ã —Å—Ç—Ä–æ–∫–µ "0".
        :param fields: —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        :param data_dict: —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏
        :return: True, –µ—Å–ª–∏ –≤—Å–µ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –ø–æ–ª—è —Ä–∞–≤–Ω—ã "0", –∏–Ω–∞—á–µ False
        """
        return all(data_dict.get(field) == "0" for field in fields)

    @staticmethod
    def get_stripped_words(words: list[str], chars: int = 3) -> list[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ words –∏ —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –≤ –Ω–∞—á–∞–ª–µ —Å–ª–æ–≤–∞ chars,
        –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è —Å–ø–∏—Å–æ–∫ –æ–±—Ä–µ–∑–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
        :param words:
        :param chars:
        :return:
        """
        if chars < 0:
            raise ValueError("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º")
        return [word[:chars] for word in words]

    @classmethod
    def get_stripped_data_by_fields(cls, data: dict, required_fields: list[str]):
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏. –ü—Ä–æ–±–µ–≥–∞–µ—Ç—Å—è –ø–æ –ø–æ–ª—é –∏–∑ required_field. –ß–∏—Å—Ç–∏—Ç –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤
        :param data:
        :param required_fields:
        :return:
        """
        result: dict[str, str] = {}
        for field, value in data.items():
            if field not in required_fields:
                result[field] = value
                continue
            result[field] = cls.string_stripper(data[field])

        return result

    @staticmethod
    def get_desymbolization_string(string: str, allowed_symbols: list):
        """–û—Å—Ç–∞–≤–ª—è–µ–º –±—É–∫–≤—ã-—Ü–∏—Ñ—Ä—ã –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã"""
        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –≤–∫–ª—é—á–µ–Ω–∏—è –≤ —Ä–µ–≥—É–ª—è—Ä–∫—É
        escaped = ''.join(re.escape(sym) for sym in allowed_symbols)
        # –†–µ–≥—É–ª—è—Ä–∫–∞ –æ—Å—Ç–∞–≤–ª—è–µ—Ç A-z, –ê-—è, 0-9 –∏ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        allowed_pattern = fr"[^a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9{escaped}]"
        return re.sub(allowed_pattern, " ", string)

    @staticmethod
    def ireplace(
        string: str,
        substr: str,
        value: str = ""
    ) -> str:
        """
        –£–¥–∞–ª—è–µ—Ç –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ `substring` –∏–∑ —Å—Ç—Ä–æ–∫–∏ `string` –±–µ–∑ —É—á—ë—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞.
        –ù–∞–ø—Ä–∏–º–µ—Ä: string="HelloWorldWorld", substring="world" ‚Üí "Hello"
        """
        pattern = re.compile(re.escape(substr), re.IGNORECASE)
        return pattern.sub(value, string)

    @staticmethod
    def get_sliced_after(string: str, phrase: str) -> str:
        """
        –ú–µ—Ç–æ–¥ –∏—â–µ—Ç –≤ —Å—Ç—Ä–æ–∫–µ string –Ω–∞—á–∞–ª–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—è —Ñ—Ä–∞–∑—ã phrase.
        –ï—Å–ª–∏ –Ω–∞—Ö–æ–¥–∏—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∞—Å—Ç—å —Å—Ç—Ä–æ–∫–∏ –î–û —Ñ—Ä–∞–∑—ã.
        –ï—Å–ª–∏ –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É.

        –ü—Ä–∏–º–µ—Ä:
            string="abc", phrase="b" ‚Üí return "a"

        :param string: –ò—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
        :param phrase: –§—Ä–∞–∑–∞, –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ –æ–±—Ä–µ–∑–∞—Ç—å
        :return: –û–±—Ä–µ–∑–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
        """
        string_lower = string.lower()
        phrase_lower = phrase.lower()

        index = string_lower.find(phrase_lower)
        if index != -1:
            return string[:index]
        return string  # –µ—Å–ª–∏ phrase –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –≤–µ—Ä–Ω—É—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π

    @staticmethod
    def ifound(
        string: str, substring: str, is_all: bool = False
    ) -> bool | list[int]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–π –ø–æ–∏—Å–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ `substring` –≤ —Å—Ç—Ä–æ–∫–µ `string`.

        :param string: –°—Ç—Ä–æ–∫–∞, –≤ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ–∏—Å–∫
        :param substring: –ü–æ–¥—Å—Ç—Ä–æ–∫–∞, –∫–æ—Ç–æ—Ä—É—é –∏—â–µ–º
        :param is_all: –ï—Å–ª–∏ True ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –≤—Ö–æ–∂–¥–µ–Ω–∏—è;
                       –ï—Å–ª–∏ False ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç True/False –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–ª–∏—á–∏—è –≤—Ö–æ–∂–¥–µ–Ω–∏—è
        :return: bool –∏–ª–∏ list[int]
        """
        string_lower = string.lower()
        substring_lower = substring.lower()

        if not is_all:
            return substring_lower in string_lower

        # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –≤—Ö–æ–∂–¥–µ–Ω–∏–π
        indices = []
        start = 0
        while True:
            index = string_lower.find(substring_lower, start)
            if index == -1:
                break
            indices.append(index)
            start = index + 1  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏—Å–∫–∞—Ç—å —Å–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–∏–º–≤–æ–ª–∞

        return indices

    @staticmethod
    def try_to_int(data: Optional) -> bool:
        try:
            int(data)
            return True
        except (ValueError, TypeError):
            return False

    @staticmethod
    def get_all_files_from_dir(
        dir_path: Path,
        exts_list: list[str] | None = None,
        exclude_file_with_list: list[str] | None = None,
        sort_files: bool = True
    ) -> list[Path]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏

        :param dir_path: –ø—É—Ç—å –¥–æ –ø–∞–ø–∫–∏
        :param exts_list: —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è (–±–µ–∑ —Ç–æ—á–∫–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä ['jpg', 'png'])
        :param exclude_file_with_list: –∏—Å–∫–ª—é—á–∏—Ç—å —Ñ–∞–π–ª—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –≤ –∏–º–µ–Ω–∏
        :param sort_files: —Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ª–∏ —Ñ–∞–π–ª—ã –ø–æ –∏–º–µ–Ω–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        :return: –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ Path –æ–±—ä–µ–∫—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤
        """
        exts_list = exts_list or ['*']
        exclude_file_with_list = exclude_file_with_list or ['~']

        if not dir_path.is_dir():
            raise NotADirectoryError(f"–ü—É—Ç—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π: {dir_path}")

        found_files: list[Path] = []

        for ext in exts_list:
            pattern = f"*.{ext}" if ext != '*' else "*"
            found_files.extend(
                file_path
                for file_path in dir_path.glob(pattern)
                if file_path.is_file()
                and not any(
                    exclude_str in file_path.name
                    for exclude_str in exclude_file_with_list
                )
            )

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        unique_files = list(set(found_files))
        return sorted(unique_files, key=lambda x: x.name) if sort_files else unique_files

    @staticmethod
    def sort_dict_by_keys(dictionary: dict, reverse=False):
        """
        –°–æ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø–æ –∫–ª—é—á–∞–º.

        :param dictionary: –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏.
        :param reverse: –§–ª–∞–≥ –¥–ª—è –≤—ã–±–æ—Ä–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ (False - –ø—Ä—è–º–æ–π –ø–æ—Ä—è–¥–æ–∫, True - –æ–±—Ä–∞—Ç–Ω—ã–π).
        :return: –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å.
        """
        sorted_items = sorted(dictionary.items(), key=lambda x: x[0], reverse=reverse)
        return dict(sorted_items)

    @staticmethod
    def sequential_combinations(
        words_list: list[str],
        use_dry: bool = True,
        get_string: bool = True,
        is_brand: bool = False
    ):
        """
        –°–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å–ª–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –∏—Ö –ø–æ —É–±—ã–≤–∞–Ω–∏—é –¥–ª–∏–Ω—ã.
        –ï—Å–ª–∏ get_string=True, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≤–º–µ—Å—Ç–æ —Å–ø–∏—Å–∫–æ–≤.

        :param is_brand:
        :param use_dry:
        :param words_list: –í—Ö–æ–¥–Ω–æ–π —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤
        :param get_string: –§–ª–∞–≥, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–π —Ñ–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ (—Å–ø–∏—Å–∫–∏ –∏–ª–∏ —Å—Ç—Ä–æ–∫–∏)
        :return: –°–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —É–±—ã–≤–∞–Ω–∏—é –¥–ª–∏–Ω—ã
        """
        result = []
        length = len(words_list)

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
        for start_idx in range(length):
            for end_idx in range(start_idx + 1, length + 1):
                combo = words_list[start_idx:end_idx]
                result.append(combo)

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –¥–ª–∏–Ω—ã
        result.sort(key=len, reverse=True)

        if get_string:
            result = [' '.join(combo) for combo in result]

        if use_dry:
            if is_brand:
                result = [Tools.get_dry_brand(combo) for combo in result]
            else:
                result = [Tools.get_dry_string(combo) for combo in result]

        return result

    @staticmethod
    def get_no_entity_no_brand_source_name(
        data_item: dict,
        no_entity: bool = True,
        no_brand: bool = True
    ) -> str:
        source_name: str = data_item.get(ColumnStrings.DATA_SOURCE_NAME)
        if no_entity:
            source_name = source_name.replace(data_item.get(ColumnStrings.DATA_ENTITY_ASIS), "")
        if no_brand:
            source_name = source_name.replace(data_item.get(ColumnStrings.DATA_BRAND_ASIS), "")
        return source_name.strip()

    @staticmethod
    def find_full_match(source_name: str, pattern: str) -> str:
        """
        –ú–µ—Ç–æ–¥ –Ω–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–Ω–æ–µ –ø–æ–ª–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ.
        –û–∫–æ–Ω—á–∞–Ω–∏–µ–º —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø—Ä–æ–±–µ–ª, –∑–∞–ø—è—Ç–∞—è, –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ –∫–æ–Ω–µ—Ü —Å—Ç—Ä–æ–∫–∏.

        :param source_name: –ò—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
        :param pattern: –ß–∞—Å—Ç—å —Å—Ç—Ä–æ–∫–∏, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ –∏—Å–∫–∞—Ç—å
        :return: –ü–æ–ª–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∏—Å–∫–æ–º–æ–π —Å—Ç—Ä–æ–∫–∏
        """
        pos = source_name.find(pattern)
        if pos == -1:
            return ""

        # –ù–∞—á–∏–Ω–∞–µ–º –¥–≤–∏–∂–µ–Ω–∏–µ –≤–ø—Ä–∞–≤–æ –æ—Ç –º–µ—Å—Ç–∞ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        end_pos = pos + len(pattern)
        while end_pos < len(source_name) and source_name[end_pos].isalnum():  # –ü–æ–∫–∞ –±—É–∫–≤–∞ –∏–ª–∏ —Ü–∏—Ñ—Ä–∞
            end_pos += 1

        # –í—ã—Ä–µ–∑–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
        full_match = source_name[pos:end_pos].strip(", \n\r\t")
        return full_match

    @staticmethod
    def get_smallest(*lists):
        """–í—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–º–µ–Ω—å—à–µ–µ –Ω–µ–Ω—É–ª–µ–≤–æ–µ —á–∏—Å–ª–æ –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤.
        –ï—Å–ª–∏ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω—É–ª–µ–≤—ã–µ –∏–ª–∏ —Å–ø–∏—Å–∫–∏ –ø—É—Å—Ç—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None."""
        # # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–µ–Ω—É–ª–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ –≤—Å–µ—Ö —Å–ø–∏—Å–∫–æ–≤
        # non_zero_values = []
        # for lst in lists:
        #     if lst:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ø–∏—Å–æ–∫ –Ω–µ –ø—É—Å—Ç–æ–π
        #         for value in lst:
        #             if value != 0:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω—É–ª–∏
        #                 non_zero_values.append(value)
        non_zero_values = [v for lst in lists if lst for v in lst if v != 0]
        return min(non_zero_values) if non_zero_values else None

    @staticmethod
    def get_chunks_data_by_dict(
        idfy_not_null_dict: dict,
        chunk_size: int,
        is_use_small_chunks: bool = False,
        small_chunk_size: int = 10
    ) -> list[dict[Any, Any]]:
        """–î–µ–ª–∏—Ç –°–õ–û–í–ê–†–¨ —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–∞ —á–∞–Ω–∫–∏ —Å –¥–∞–Ω–Ω—ã–º–∏"""
        if chunk_size <= 0:
            raise ValueError("–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º")

        if is_use_small_chunks:
            chunk_size = small_chunk_size

        items = list(idfy_not_null_dict.items())
        return [
            dict(items[i:i + chunk_size])
            for i in range(0, len(items), chunk_size)
        ]

    @staticmethod
    def split_into_chunks(big_list, chunk_size=100):
        """
        –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ –°–ü–ò–°–ö–ê –Ω–∞ –º–µ–Ω—å—à–∏–µ –∫—É—Å–æ—á–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.

        :param big_list: –û—Å–Ω–æ–≤–Ω–æ–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –¥–µ–ª–µ–Ω–∏—è.
        :param chunk_size: –†–∞–∑–º–µ—Ä –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 100).
        :return: –°–ø–∏—Å–æ–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.
        """
        return [big_list[i:i + chunk_size] for i in range(0, len(big_list), chunk_size)]

    @staticmethod
    def filter_list(a: list, b: list) -> list:
        """
        –û—Å—Ç–∞–≤–ª—è–µ—Ç –≤ —Å–ø–∏—Å–∫–µ A —Ç–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Å–ø–∏—Å–∫–µ B

        :param a: –û—Å–Ω–æ–≤–Ω–æ–π —Å–ø–∏—Å–æ–∫ (–±—É–¥–µ—Ç –∏–∑–º–µ–Ω–µ–Ω)
        :param b: –°–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        :return: –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ A
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        set_b = set(b)
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–µ –∏–∑ B
        return [x for x in a if x not in set_b]

    @staticmethod
    def get_field_structure(field: str):
        if AsisTobe.ASIS in field:
            return {
                "name": field.replace(f"_{AsisTobe.ASIS}", ""),
                "mod": AsisTobe.ASIS
            }
        elif AsisTobe.TOBE in field:
            return {
                "name": field.replace(f"_{AsisTobe.TOBE}", ""),
                "mod": AsisTobe.TOBE
            }
        else:
            return {
                "name": field,
                "mod": None
            }

    @staticmethod
    def num_tokens_from_messages(messages, model="gpt-4o-mini"):
        """Return the number of tokens used by a list of messages."""
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            print("Warning: model not found. Using o200k_base encoding.")
            encoding = tiktoken.get_encoding("o200k_base")
        if model in {
            "gpt-3.5-turbo-0125",
            "gpt-4-0314",
            "gpt-4-32k-0314",
            "gpt-4-0613",
            "gpt-4-32k-0613",
            "gpt-4o-mini-2024-07-18",
            "gpt-4o-2024-08-06"
        }:
            tokens_per_message = 3
            tokens_per_name = 1
        elif "gpt-3.5-turbo" in model:
            print(
                "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125."
            )
            return Tools.num_tokens_from_messages(messages, model="gpt-3.5-turbo-0125")
        elif "gpt-4o-mini" in model:
            print(
                "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18."
            )
            return Tools.num_tokens_from_messages(messages, model="gpt-4o-mini-2024-07-18")
        elif "gpt-4o" in model:
            print(
                "Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06."
            )
            return Tools.num_tokens_from_messages(messages, model="gpt-4o-2024-08-06")
        elif "gpt-4" in model:
            print("Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.")
            return Tools.num_tokens_from_messages(messages, model="gpt-4-0613")
        else:
            raise NotImplementedError(
                f"""num_tokens_from_messages() is not implemented for model {model}."""
            )
        num_tokens = 0
        for message in messages:
            num_tokens += tokens_per_message
            for key, value in message.items():
                num_tokens += len(encoding.encode(value))
                if key == "name":
                    num_tokens += tokens_per_name
        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>
        return num_tokens

    @staticmethod
    def remove_duplicate_words(text: str) -> str:
        """
        –£–¥–∞–ª—è–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞ –≤ —Å—Ç—Ä–æ–∫–µ, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫ –∏ —Ä–µ–≥–∏—Å—Ç—Ä –ø–µ—Ä–≤–æ–≥–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—è.

        :param text: –ò—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
        :return: –°—Ç—Ä–æ–∫–∞ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–ª–æ–≤
        """
        seen_words = set()
        result = []

        for word in text.split():
            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            lower_word = word.lower()

            if lower_word not in seen_words:
                seen_words.add(lower_word)
                result.append(word)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ —Å —Ä–µ–≥–∏—Å—Ç—Ä–æ–º

        return ' '.join(result)

    @staticmethod
    def clear_directory_contents(path: Path, with_dir: bool = False) -> None:
        """
        –ü–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–∏ (–≤–∫–ª—é—á–∞—è –ø–æ–¥–ø–∞–ø–∫–∏ –∏ —Ñ–∞–π–ª—ã).
        –ù–µ —É–¥–∞–ª—è–µ—Ç —Å–∞–º—É –ø–∞–ø–∫—É, —Ç–æ–ª—å–∫–æ –µ—ë —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ.

        Args:
            path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –æ—á–∏—Å—Ç–∏—Ç—å (Path –æ–±—ä–µ–∫—Ç)

        Raises:
            ValueError: –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –ø—É—Ç—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π
            OSError: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
            :param path:
            :param with_dir: –≤–º–µ—Å—Ç–µ —Å –ø–∞–ø–∫–æ–π
        """
        if not path.is_dir():
            raise ValueError(f"–ü—É—Ç—å {path} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π")

        # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
        for item in path.iterdir():
            try:
                if item.is_file() or item.is_symlink():
                    item.unlink()  # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã –∏ —Å–∏–º–ª–∏–Ω–∫–∏
                elif item.is_dir():
                    shutil.rmtree(item)  # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —É–¥–∞–ª—è–µ–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            except Exception as e:
                raise OSError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {item}: {e}")

    @staticmethod
    def parse_filename(filename: str, is_filters: bool = False) -> tuple[int, str]:
        """
        –ü–∞—Ä—Å–∏—Ç –∏–º—è —Ñ–∞–π–ª–∞ —Ñ–æ—Ä–º–∞—Ç–∞ "2417_–õ–µ—Å—Ç–Ω–∏—Ü—ã_–∏_—Å—Ç—Ä–µ–º—è–Ω–∫–∏_20250529_–Ω–∞_—Ä–∞–∑–º–µ—Ç–∫—É.xlsx"
        –∏–ª–∏ "–§–∏–ª—å—Ç—Ä—ã_12990_–ë–æ–∫–∞–ª—ã_–∏_—Å—Ç–∞–∫–∞–Ω—ã_20250922_–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ.xlsx"

        Args:
            filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            is_filters: —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ñ–∏–ª—å—Ç—Ä–∞

        Returns:
            tuple: (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, –Ω–∞–∑–≤–∞–Ω–∏–µ_–∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
        """
        # –£–¥–∞–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        name_without_ext = filename.rsplit('.', 1)[0]

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å–∏–º–≤–æ–ª—É –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
        parts = name_without_ext.split('_')

        if is_filters:
            # –î–ª—è —Ñ–∏–ª—å—Ç—Ä–æ–≤: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å "–§–∏–ª—å—Ç—Ä—ã" –∏ –±–µ—Ä–µ–º —Å–ª–µ–¥—É—é—â—É—é —á–∞—Å—Ç—å –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            if len(parts) < 2:
                raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∞—Å—Ç–µ–π –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ —Ñ–∏–ª—å—Ç—Ä–∞")

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º "–§–∏–ª—å—Ç—Ä—ã" –∏ –±–µ—Ä–µ–º —Å–ª–µ–¥—É—é—â—É—é —á–∞—Å—Ç—å –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            category_part = parts[1]
            start_index = 2  # –Ω–∞—á–∏–Ω–∞–µ–º —Å —á–∞—Å—Ç–∏ –ø–æ—Å–ª–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        else:
            # –û–±—ã—á–Ω—ã–π —Å–ª—É—á–∞–π: –ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å - –∫–∞—Ç–µ–≥–æ—Ä–∏—è
            if not parts:
                raise ValueError("–ü—É—Å—Ç–æ–µ –∏–º—è —Ñ–∞–π–ª–∞")

            category_part = parts[0]
            start_index = 1  # –Ω–∞—á–∏–Ω–∞–µ–º —Å–æ –≤—Ç–æ—Ä–æ–π —á–∞—Å—Ç–∏

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ü–∏—Ñ—Ä
        if not category_part.isdigit():
            raise ValueError(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–æ–ª–∂–Ω–∞ —Å–æ—Å—Ç–æ—è—Ç—å –∏–∑ —Ü–∏—Ñ—Ä, –ø–æ–ª—É—á–µ–Ω–æ: {category_part}")

        category = int(category_part)

        # –ò—â–µ–º –∏–Ω–¥–µ–∫—Å –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏, —Å–æ–¥–µ—Ä–∂–∞—â–µ–π –¥–∞—Ç—É (—Ñ–æ—Ä–º–∞—Ç YYYYMMDD)
        date_pattern = re.compile(r'^\d{8}$')
        date_index = None

        for i, part in enumerate(parts[start_index:], start=start_index):
            if date_pattern.match(part):
                date_index = i
                break

        if date_index is None:
            # –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –±–µ—Ä–µ–º –≤—Å–µ —á–∞—Å—Ç–∏ –ø–æ—Å–ª–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            name_parts = parts[start_index:]
        else:
            name_parts = parts[start_index:date_index]

        # –°–æ–±–∏—Ä–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∑–∞–º–µ–Ω—è—è –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
        category_name = ' '.join(name_parts)

        return category, category_name

    @staticmethod
    def idfy_by_field(
        data: dict | list,
        key_field: str,
        save_repeats: bool = True,
        repeat_char: str = "_"
    ):
        """
        –ú–µ—Ç–æ–¥ –ø—Ä–æ–≤–æ–¥–∏—Ç —Å–ª–æ–≤–∞—Ä–∏–∑–∞—Ü–∏—é —Å–ª–æ–≤–∞—Ä—è/—Å–ø–∏—Å–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º –∏–∑ –ø–æ–ª—è
        :param repeat_char: —Å–∏–º–≤–æ–ª, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞–≤–∏—Ç—Å—è –ø–æ—Å–ª–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –ø–æ–ª—è, –µ—Å–ª–∏ —Ç–∞–∫–æ–π —ç–ª–µ–º–µ–Ω—Ç —É–∂–µ –µ—Å—Ç—å
        :param save_repeats: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ø–æ–≤—Ç–æ—Ä—ã?
        :param data:
        :param key_field: –ø–æ–ª–µ, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –±—É–¥–µ—Ç –±—Ä–∞—Ç—å—Å—è –∫–ª—é—á –¥–ª—è —Å–ª–æ–≤–∞—Ä—è
        :return:
        """
        if not isinstance(data, list):
            raise TypeError("–î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")

        result: dict[Any, Any] = {}
        item: dict
        for item in data:
            if not isinstance(item, dict):
                raise TypeError("–≠–ª–µ–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä—ë–º")
            if key_field not in item:
                raise AttributeError(f"–ü–æ–ª–µ ¬´{key_field}¬ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —ç–ª–µ–º–µ–Ω—Ç–µ")

            key_value: Optional[Any] = item.get(key_field)
            if not key_value:
                raise ValueError(f"–í –∫–∞—á–µ—Å—Ç–≤–µ –∫–ª—é—á–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ö—ç—à–∏—Ä—É–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {key_value}")

            while True:
                if key_value not in result:
                    result[key_value] = item
                    break
                if save_repeats:
                    key_value = f"{repeat_char}{key_value}"
        return result

    @staticmethod
    def is_valid_json(s):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä–æ–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–Ω–æ–µ json-—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            json.loads(s)
            return True
        except (json.JSONDecodeError, TypeError, ValueError):
            return False

    @staticmethod
    def str2json(text: str) -> Optional[list | dict]:
        try:
            json_data = json.loads(text)
        except json.JSONDecodeError:
            return None
        return json_data

    @staticmethod
    def get_clean_words(words: list[str]) -> list[str]:
        """–ë–µ—Ä–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫, –¥–µ–ª–∞–µ—Ç –∏—Ö lower"""
        words_lower = [word.lower() for word in words]
        return [Tools.clean_field(word) for word in words_lower]

    @staticmethod
    def get_fields_pairs(fields: list[str]) -> dict[str, dict[str, str]]:
        if not len(fields):
            raise IndexError("–°–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π –¥–ª—è –ø–∞—Ä–∏—Ä–∞—Ü–∏–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        result: dict[str, dict[str, str]] = {}
        field: str
        for field in fields:
            result[field] = {
                AsisTobe.ASIS: f"{field}_{AsisTobe.ASIS}",
                AsisTobe.TOBE: f"{field}_{AsisTobe.TOBE}",
            }
        return result

    @staticmethod
    def get_word_by_index(
        words_list: list[str],
        words_indexes: list[int],
        *,
        ignore_errors: bool = True,
        unique_only: bool = True
    ) -> list[str]:
        """
        –£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –ø–æ—Ä—è–¥–∫–∞.
        ...
        """
        if not words_list or not words_indexes:
            return []

        result = []
        seen_words = set()

        for i in words_indexes:
            if 0 <= i < len(words_list):
                word = words_list[i]
                if unique_only:
                    if word not in seen_words:
                        result.append(word)
                        seen_words.add(word)
                else:
                    result.append(word)
            elif not ignore_errors:
                raise IndexError(f"–ò–Ω–¥–µ–∫—Å {i} –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã —Å–ø–∏—Å–∫–∞ (–¥–ª–∏–Ω–∞: {len(words_list)})")

        return result
