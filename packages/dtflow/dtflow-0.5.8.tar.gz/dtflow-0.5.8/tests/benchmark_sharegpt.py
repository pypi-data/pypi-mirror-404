#!/usr/bin/env python
"""
ShareGPT æ•°æ®é›†æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨

ä½¿ç”¨ data/sharegpt_all.json (261MB, 75,532æ¡) è¿›è¡ŒçœŸå®æ•°æ®æ€§èƒ½æµ‹è¯•
"""

import os
import sys
import time
import tempfile
import json
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Any, Callable
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from dtflow.cli.sample import sample, head, tail
from dtflow.cli.stats import stats
from dtflow.cli.io_ops import concat, diff
from dtflow.cli.clean import dedupe, clean
from dtflow.cli.transform import transform
from dtflow.storage.io import load_data, save_data


@dataclass
class BenchmarkResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""
    name: str
    elapsed: float
    input_size: int
    output_size: int = 0
    throughput: float = 0.0  # æ¡/ç§’
    notes: str = ""


class PerformanceBenchmark:
    """æ€§èƒ½æµ‹è¯•å™¨"""

    def __init__(self, data_file: str):
        self.data_file = Path(data_file)
        self.file_size_mb = self.data_file.stat().st_size / (1024 * 1024)
        self.results: List[BenchmarkResult] = []
        self.temp_dir = tempfile.mkdtemp(prefix="benchmark_")

        # é¢„åŠ è½½æ•°æ®è·å–æ¡æ•°
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {self.data_file}")
        start = time.perf_counter()
        self.data = load_data(str(self.data_file))
        load_time = time.perf_counter() - start
        self.data_count = len(self.data)

        print(f"   æ–‡ä»¶å¤§å°: {self.file_size_mb:.1f} MB")
        print(f"   æ•°æ®æ¡æ•°: {self.data_count:,}")
        print(f"   åŠ è½½è€—æ—¶: {load_time:.2f}s")
        print(f"   ä¸´æ—¶ç›®å½•: {self.temp_dir}")
        print()

        # è®°å½•åŠ è½½æ€§èƒ½
        self.results.append(BenchmarkResult(
            name="load_data (JSON)",
            elapsed=load_time,
            input_size=self.data_count,
            throughput=self.data_count / load_time,
            notes=f"{self.file_size_mb:.1f}MB JSON æ–‡ä»¶"
        ))

        # ä¿å­˜ä¸º JSONL æ ¼å¼ç”¨äºåç»­æµ‹è¯•
        self.jsonl_file = Path(self.temp_dir) / "sharegpt.jsonl"
        print(f"ğŸ“ è½¬æ¢ä¸º JSONL æ ¼å¼...")
        start = time.perf_counter()
        save_data(self.data, str(self.jsonl_file))
        save_time = time.perf_counter() - start
        jsonl_size = self.jsonl_file.stat().st_size / (1024 * 1024)
        print(f"   JSONL å¤§å°: {jsonl_size:.1f} MB")
        print(f"   ä¿å­˜è€—æ—¶: {save_time:.2f}s")
        print()

        self.results.append(BenchmarkResult(
            name="save_data (JSONL)",
            elapsed=save_time,
            input_size=self.data_count,
            throughput=self.data_count / save_time,
            notes=f"è¾“å‡º {jsonl_size:.1f}MB JSONL"
        ))

    def run(self, name: str, func: Callable, input_size: int = 0, notes: str = "") -> BenchmarkResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        print(f"  â± {name}...", end=" ", flush=True)
        start = time.perf_counter()
        try:
            result = func()
            elapsed = time.perf_counter() - start
            output_size = result if isinstance(result, int) else 0
            throughput = (input_size or self.data_count) / elapsed if elapsed > 0 else 0
            print(f"{elapsed:.3f}s ({throughput:,.0f} æ¡/ç§’)")

            br = BenchmarkResult(
                name=name,
                elapsed=elapsed,
                input_size=input_size or self.data_count,
                output_size=output_size,
                throughput=throughput,
                notes=notes
            )
            self.results.append(br)
            return br
        except Exception as e:
            elapsed = time.perf_counter() - start
            print(f"å¤±è´¥: {e}")
            br = BenchmarkResult(
                name=name,
                elapsed=elapsed,
                input_size=input_size or self.data_count,
                notes=f"é”™è¯¯: {e}"
            )
            self.results.append(br)
            return br

    def benchmark_sample(self):
        """é‡‡æ ·å‘½ä»¤æ€§èƒ½æµ‹è¯•"""
        print("\nğŸ“Š Sample å‘½ä»¤æ€§èƒ½æµ‹è¯•")
        print("-" * 50)

        # head é‡‡æ ·
        output = Path(self.temp_dir) / "sample_head.jsonl"
        self.run(
            "head 1000æ¡",
            lambda: head(str(self.jsonl_file), num=1000, output=str(output)),
            notes="ä»å¤´éƒ¨é‡‡æ ·"
        )

        # head å¤§é‡é‡‡æ ·
        output = Path(self.temp_dir) / "sample_head_large.jsonl"
        self.run(
            "head 10000æ¡",
            lambda: head(str(self.jsonl_file), num=10000, output=str(output)),
            notes="ä»å¤´éƒ¨é‡‡æ ·"
        )

        # tail é‡‡æ ·
        output = Path(self.temp_dir) / "sample_tail.jsonl"
        self.run(
            "tail 1000æ¡",
            lambda: tail(str(self.jsonl_file), num=1000, output=str(output)),
            notes="ä»å°¾éƒ¨é‡‡æ ·"
        )

        # éšæœºé‡‡æ ·
        output = Path(self.temp_dir) / "sample_random.jsonl"
        self.run(
            "random 5000æ¡",
            lambda: sample(str(self.jsonl_file), num=5000, type="random", output=str(output), seed=42),
            notes="éšæœºé‡‡æ ·"
        )

    def benchmark_stats(self):
        """ç»Ÿè®¡å‘½ä»¤æ€§èƒ½æµ‹è¯•"""
        print("\nğŸ“Š Stats å‘½ä»¤æ€§èƒ½æµ‹è¯•")
        print("-" * 50)

        # å¿«é€Ÿç»Ÿè®¡
        self.run(
            "stats å¿«é€Ÿæ¨¡å¼",
            lambda: stats(str(self.jsonl_file), full=False),
            notes="åªç»Ÿè®¡è¡Œæ•°å’Œå­—æ®µç»“æ„"
        )

        # å®Œæ•´ç»Ÿè®¡ï¼ˆè¾ƒæ…¢ï¼‰
        self.run(
            "stats å®Œæ•´æ¨¡å¼",
            lambda: stats(str(self.jsonl_file), full=True),
            notes="å®Œæ•´å€¼åˆ†å¸ƒç»Ÿè®¡"
        )

    def benchmark_clean(self):
        """æ¸…æ´—å‘½ä»¤æ€§èƒ½æµ‹è¯•"""
        print("\nğŸ“Š Clean å‘½ä»¤æ€§èƒ½æµ‹è¯•")
        print("-" * 50)

        # strip æ¸…æ´—
        output = Path(self.temp_dir) / "clean_strip.jsonl"
        self.run(
            "clean --strip",
            lambda: clean(str(self.jsonl_file), strip=True, output=str(output)),
            notes="å»é™¤å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½"
        )

        # drop-empty æ¸…æ´—
        output = Path(self.temp_dir) / "clean_drop_empty.jsonl"
        self.run(
            "clean --drop-empty=system",
            lambda: clean(str(self.jsonl_file), drop_empty="system", output=str(output)),
            notes="åˆ é™¤ system ä¸ºç©ºçš„è®°å½•"
        )

        # keep å­—æ®µ
        output = Path(self.temp_dir) / "clean_keep.jsonl"
        self.run(
            "clean --keep=conversations",
            lambda: clean(str(self.jsonl_file), keep="conversations", output=str(output)),
            notes="åªä¿ç•™ conversations å­—æ®µ"
        )

    def benchmark_dedupe(self):
        """å»é‡å‘½ä»¤æ€§èƒ½æµ‹è¯•"""
        print("\nğŸ“Š Dedupe å‘½ä»¤æ€§èƒ½æµ‹è¯•")
        print("-" * 50)

        # å…¨é‡å»é‡
        output = Path(self.temp_dir) / "dedupe_full.jsonl"
        self.run(
            "dedupe å…¨é‡ç²¾ç¡®å»é‡",
            lambda: dedupe(str(self.jsonl_file), output=str(output)),
            notes="åŸºäºå®Œæ•´å†…å®¹å“ˆå¸Œ"
        )

        # æŒ‰å­—æ®µå»é‡
        output = Path(self.temp_dir) / "dedupe_system.jsonl"
        self.run(
            "dedupe --key=system",
            lambda: dedupe(str(self.jsonl_file), key="system", output=str(output)),
            notes="æŒ‰ system å­—æ®µå»é‡"
        )

    def benchmark_io(self):
        """IO å‘½ä»¤æ€§èƒ½æµ‹è¯•"""
        print("\nğŸ“Š IO å‘½ä»¤æ€§èƒ½æµ‹è¯•")
        print("-" * 50)

        # æ‹†åˆ†æ–‡ä»¶ç”¨äº concat æµ‹è¯•
        part1 = Path(self.temp_dir) / "part1.jsonl"
        part2 = Path(self.temp_dir) / "part2.jsonl"
        save_data(self.data[:30000], str(part1))
        save_data(self.data[30000:60000], str(part2))

        # concat
        output = Path(self.temp_dir) / "concat_result.jsonl"
        self.run(
            "concat 2ä¸ªæ–‡ä»¶ (å„30000æ¡)",
            lambda: concat(str(part1), str(part2), output=str(output)),
            input_size=60000,
            notes="åˆå¹¶ä¸¤ä¸ªæ–‡ä»¶"
        )

        # diff
        self.run(
            "diff 2ä¸ªæ–‡ä»¶ (å„30000æ¡)",
            lambda: diff(str(part1), str(part2)),
            input_size=60000,
            notes="å¯¹æ¯”ä¸¤ä¸ªæ–‡ä»¶"
        )

    def benchmark_transform(self):
        """è½¬æ¢å‘½ä»¤æ€§èƒ½æµ‹è¯•"""
        print("\nğŸ“Š Transform å‘½ä»¤æ€§èƒ½æµ‹è¯•")
        print("-" * 50)

        # ä½¿ç”¨ sharegpt é¢„è®¾
        output = Path(self.temp_dir) / "transform_sharegpt.jsonl"
        self.run(
            "transform --preset=sharegpt",
            lambda: transform(str(self.jsonl_file), preset="sharegpt", output=str(output)),
            notes="ShareGPT æ ¼å¼è½¬æ¢"
        )

        # ä½¿ç”¨ openai_chat é¢„è®¾ï¼ˆé™åˆ¶æ•°é‡ï¼‰
        # å…ˆåˆ›å»ºå°æ•°æ®é›†
        small_file = Path(self.temp_dir) / "small_10000.jsonl"
        save_data(self.data[:10000], str(small_file))

        output = Path(self.temp_dir) / "transform_openai.jsonl"
        self.run(
            "transform --preset=openai_chat (10000æ¡)",
            lambda: transform(str(small_file), preset="openai_chat", output=str(output)),
            input_size=10000,
            notes="è½¬æ¢ä¸º OpenAI Chat æ ¼å¼"
        )

    def generate_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        report = []
        report.append("=" * 70)
        report.append("dtflow CLI æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 70)
        report.append(f"æµ‹è¯•æ—¶é—´: {now}")
        report.append(f"æµ‹è¯•æ–‡ä»¶: {self.data_file}")
        report.append(f"æ–‡ä»¶å¤§å°: {self.file_size_mb:.1f} MB")
        report.append(f"æ•°æ®æ¡æ•°: {self.data_count:,}")
        report.append("")

        # æŒ‰ç±»åˆ«åˆ†ç»„
        categories = {}
        for r in self.results:
            cat = r.name.split()[0]
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(r)

        # ç”Ÿæˆè¯¦ç»†ç»“æœè¡¨æ ¼
        report.append("-" * 70)
        report.append(f"{'æµ‹è¯•é¡¹':<35} {'è€—æ—¶(s)':<10} {'ååé‡(æ¡/s)':<15} {'å¤‡æ³¨'}")
        report.append("-" * 70)

        for cat, items in categories.items():
            for r in items:
                throughput_str = f"{r.throughput:,.0f}" if r.throughput > 0 else "-"
                report.append(f"{r.name:<35} {r.elapsed:<10.3f} {throughput_str:<15} {r.notes}")

        report.append("-" * 70)

        # æ±‡æ€»ç»Ÿè®¡
        total_time = sum(r.elapsed for r in self.results)
        report.append("")
        report.append("ğŸ“Š æ±‡æ€»ç»Ÿè®¡")
        report.append("-" * 40)
        report.append(f"æ€»æµ‹è¯•é¡¹: {len(self.results)}")
        report.append(f"æ€»è€—æ—¶: {total_time:.2f}s")

        # æ‰¾å‡ºæœ€å¿«å’Œæœ€æ…¢çš„æ“ä½œ
        sorted_results = sorted(self.results, key=lambda x: x.throughput, reverse=True)
        fastest = [r for r in sorted_results if r.throughput > 0][:3]
        slowest = [r for r in sorted_results if r.throughput > 0][-3:]

        report.append("")
        report.append("ğŸš€ æœ€å¿«æ“ä½œ (ååé‡):")
        for r in fastest:
            report.append(f"   {r.name}: {r.throughput:,.0f} æ¡/ç§’")

        report.append("")
        report.append("ğŸ¢ æœ€æ…¢æ“ä½œ (ååé‡):")
        for r in reversed(slowest):
            report.append(f"   {r.name}: {r.throughput:,.0f} æ¡/ç§’")

        report.append("")
        report.append("=" * 70)

        return "\n".join(report)

    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        import shutil
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
            print(f"\nğŸ§¹ å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {self.temp_dir}")


def main():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    data_file = "data/sharegpt_all.json"

    if not os.path.exists(data_file):
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {data_file}")
        sys.exit(1)

    print("=" * 70)
    print("dtflow CLI æ€§èƒ½æµ‹è¯•")
    print("=" * 70)
    print()

    benchmark = PerformanceBenchmark(data_file)

    try:
        # è¿è¡Œå„ç±»æµ‹è¯•
        benchmark.benchmark_sample()
        benchmark.benchmark_stats()
        benchmark.benchmark_clean()
        benchmark.benchmark_dedupe()
        benchmark.benchmark_io()
        benchmark.benchmark_transform()

        # ç”ŸæˆæŠ¥å‘Š
        report = benchmark.generate_report()
        print("\n" + report)

        # ä¿å­˜æŠ¥å‘Š
        report_file = "benchmark_report.txt"
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report)
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    finally:
        benchmark.cleanup()


if __name__ == "__main__":
    main()
