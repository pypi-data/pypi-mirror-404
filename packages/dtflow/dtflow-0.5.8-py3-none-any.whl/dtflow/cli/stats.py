"""
CLI æ•°æ®ç»Ÿè®¡ç›¸å…³å‘½ä»¤
"""

from pathlib import Path
from typing import Any, Dict, List, Optional

import orjson

from ..storage.io import load_data
from ..utils.field_path import get_field_with_spec
from .common import (
    _check_file_format,
    _infer_type,
    _is_numeric,
    _pad_to_width,
    _truncate,
)


def stats(
    filename: str,
    top: int = 10,
    full: bool = False,
    fields: Optional[List[str]] = None,
    expand_fields: Optional[List[str]] = None,
) -> None:
    """
    æ˜¾ç¤ºæ•°æ®æ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯ã€‚

    é»˜è®¤å¿«é€Ÿæ¨¡å¼ï¼šåªç»Ÿè®¡è¡Œæ•°å’Œå­—æ®µç»“æ„ã€‚
    å®Œæ•´æ¨¡å¼ï¼ˆ--fullï¼‰ï¼šç»Ÿè®¡å€¼åˆ†å¸ƒã€å”¯ä¸€å€¼ã€é•¿åº¦ç­‰è¯¦ç»†ä¿¡æ¯ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        top: æ˜¾ç¤ºé¢‘ç‡æœ€é«˜çš„å‰ N ä¸ªå€¼ï¼Œé»˜è®¤ 10ï¼ˆä»…å®Œæ•´æ¨¡å¼ï¼‰
        full: å®Œæ•´æ¨¡å¼ï¼Œç»Ÿè®¡å€¼åˆ†å¸ƒã€å”¯ä¸€å€¼ç­‰è¯¦ç»†ä¿¡æ¯
        fields: æŒ‡å®šç»Ÿè®¡çš„å­—æ®µåˆ—è¡¨ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        expand_fields: å±•å¼€ list å­—æ®µç»Ÿè®¡çš„å­—æ®µåˆ—è¡¨

    Examples:
        dt stats data.jsonl            # å¿«é€Ÿæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        dt stats data.jsonl --full     # å®Œæ•´æ¨¡å¼
        dt stats data.csv -f --top=5   # å®Œæ•´æ¨¡å¼ï¼Œæ˜¾ç¤º Top 5
        dt stats data.jsonl --full --field=category  # æŒ‡å®šå­—æ®µ
        dt stats data.jsonl --full --expand=tags     # å±•å¼€ list å­—æ®µ
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # å¿«é€Ÿæ¨¡å¼ï¼šå¿½ç•¥ --field å’Œ --expand å‚æ•°
    if not full:
        if fields or expand_fields:
            print("âš ï¸  è­¦å‘Š: --field å’Œ --expand å‚æ•°ä»…åœ¨å®Œæ•´æ¨¡å¼ (--full) ä¸‹ç”Ÿæ•ˆ")
        _quick_stats(filepath)
        return

    # åŠ è½½æ•°æ®
    try:
        data = load_data(str(filepath))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    if not data:
        print("æ–‡ä»¶ä¸ºç©º")
        return

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total = len(data)
    field_stats = _compute_field_stats(data, top, fields, expand_fields)

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    _print_stats(filepath.name, total, field_stats)


def _quick_stats(filepath: Path) -> None:
    """
    å¿«é€Ÿç»Ÿè®¡æ¨¡å¼ï¼šåªç»Ÿè®¡è¡Œæ•°å’Œå­—æ®µç»“æ„ï¼Œä¸éå†å…¨éƒ¨æ•°æ®ã€‚

    ç‰¹ç‚¹:
    - ä½¿ç”¨æµå¼è®¡æ•°ï¼Œä¸åŠ è½½å…¨éƒ¨æ•°æ®åˆ°å†…å­˜
    - åªè¯»å–å‰å‡ æ¡æ•°æ®æ¥æ¨æ–­å­—æ®µç»“æ„
    - ä¸è®¡ç®—å€¼åˆ†å¸ƒã€å”¯ä¸€å€¼ç­‰è€—æ—¶ç»Ÿè®¡
    """
    from ..streaming import _count_rows_fast

    ext = filepath.suffix.lower()
    file_size = filepath.stat().st_size

    # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
    def format_size(size: int) -> str:
        for unit in ["B", "KB", "MB", "GB"]:
            if size < 1024:
                return f"{size:.1f} {unit}"
            size /= 1024
        return f"{size:.1f} TB"

    # å¿«é€Ÿç»Ÿè®¡è¡Œæ•°
    total = _count_rows_fast(str(filepath))
    if total is None:
        # å›é€€ï¼šæ‰‹åŠ¨è®¡æ•°
        total = 0
        try:
            with open(filepath, "rb") as f:
                for line in f:
                    if line.strip():
                        total += 1
        except Exception:
            total = -1

    # è¯»å–å‰å‡ æ¡æ•°æ®æ¨æ–­å­—æ®µç»“æ„
    sample_data = []
    sample_size = 5
    try:
        if ext == ".jsonl":
            with open(filepath, "rb") as f:
                for i, line in enumerate(f):
                    if i >= sample_size:
                        break
                    line = line.strip()
                    if line:
                        sample_data.append(orjson.loads(line))
        elif ext == ".csv":
            import polars as pl

            df = pl.scan_csv(str(filepath)).head(sample_size).collect()
            sample_data = df.to_dicts()
        elif ext == ".parquet":
            import polars as pl

            df = pl.scan_parquet(str(filepath)).head(sample_size).collect()
            sample_data = df.to_dicts()
        elif ext in (".arrow", ".feather"):
            import polars as pl

            df = pl.scan_ipc(str(filepath)).head(sample_size).collect()
            sample_data = df.to_dicts()
        elif ext == ".json":
            with open(filepath, "rb") as f:
                data = orjson.loads(f.read())
                if isinstance(data, list):
                    sample_data = data[:sample_size]
    except Exception:
        pass

    # åˆ†æå­—æ®µç»“æ„
    fields = []
    if sample_data:
        all_keys = set()
        for item in sample_data:
            all_keys.update(item.keys())

        for key in sorted(all_keys):
            # ä»é‡‡æ ·æ•°æ®ä¸­æ¨æ–­ç±»å‹
            sample_values = [item.get(key) for item in sample_data if key in item]
            non_null = [v for v in sample_values if v is not None]
            if non_null:
                field_type = _infer_type(non_null)
            else:
                field_type = "unknown"
            fields.append({"field": key, "type": field_type})

    # è¾“å‡º
    try:
        from rich.console import Console
        from rich.panel import Panel
        from rich.table import Table

        console = Console()

        # æ¦‚è§ˆ
        console.print(
            Panel(
                f"[bold]æ–‡ä»¶:[/bold] {filepath.name}\n"
                f"[bold]å¤§å°:[/bold] {format_size(file_size)}\n"
                f"[bold]æ€»æ•°:[/bold] {total:,} æ¡\n"
                f"[bold]å­—æ®µ:[/bold] {len(fields)} ä¸ª",
                title="ğŸ“Š å¿«é€Ÿç»Ÿè®¡",
                expand=False,
            )
        )

        if fields:
            table = Table(title="ğŸ“‹ å­—æ®µç»“æ„", show_header=True, header_style="bold cyan")
            table.add_column("#", style="dim", justify="right")
            table.add_column("å­—æ®µ", style="green")
            table.add_column("ç±»å‹", style="yellow")

            for i, f in enumerate(fields, 1):
                table.add_row(str(i), f["field"], f["type"])

            console.print(table)

    except ImportError:
        # æ²¡æœ‰ richï¼Œä½¿ç”¨æ™®é€šæ‰“å°
        print(f"\n{'=' * 40}")
        print("ğŸ“Š å¿«é€Ÿç»Ÿè®¡")
        print(f"{'=' * 40}")
        print(f"æ–‡ä»¶: {filepath.name}")
        print(f"å¤§å°: {format_size(file_size)}")
        print(f"æ€»æ•°: {total:,} æ¡")
        print(f"å­—æ®µ: {len(fields)} ä¸ª")

        if fields:
            print("\nğŸ“‹ å­—æ®µç»“æ„:")
            for i, f in enumerate(fields, 1):
                print(f"  {i}. {f['field']} ({f['type']})")


def _extract_with_wildcard(item: dict, field_spec: str) -> List[Any]:
    """å¤„ç†åŒ…å« [*] çš„å­—æ®µè·¯å¾„ï¼Œè¿”å›æ‰€æœ‰å€¼"""
    if "[*]" not in field_spec:
        # æ—  [*]ï¼Œç›´æ¥è¿”å›å•ä¸ªå€¼çš„åˆ—è¡¨
        value = get_field_with_spec(item, field_spec)
        return [value] if value is not None else []

    # åˆ†å‰²è·¯å¾„ï¼šmessages[*].role -> ("messages", ".role")
    before, after = field_spec.split("[*]", 1)
    after = after.lstrip(".")  # ç§»é™¤å¼€å¤´çš„ç‚¹

    # è·å–æ•°ç»„
    array = get_field_with_spec(item, before) if before else item
    if not isinstance(array, list):
        return []

    # æå–æ¯ä¸ªå…ƒç´ çš„åç»­è·¯å¾„
    results = []
    for elem in array:
        if after:
            val = get_field_with_spec(elem, after)
        else:
            val = elem
        if val is not None:
            results.append(val)

    return results


def _extract_field_values(
    data: List[Dict],
    field_spec: str,
    expand: bool = False,
) -> List[Any]:
    """
    ä»æ•°æ®ä¸­æå–å­—æ®µå€¼ã€‚

    Args:
        data: æ•°æ®åˆ—è¡¨
        field_spec: å­—æ®µè·¯å¾„è§„æ ¼ï¼ˆå¦‚ "messages[*].role"ï¼‰
        expand: æ˜¯å¦å±•å¼€ list

    Returns:
        å€¼åˆ—è¡¨ï¼ˆå±•å¼€æˆ–ä¸å±•å¼€ï¼‰
    """
    all_values = []

    for item in data:
        if "[*]" in field_spec or expand:
            # ä½¿ç”¨é€šé…ç¬¦æå–æ‰€æœ‰å€¼
            values = _extract_with_wildcard(item, field_spec)

            if expand and len(values) == 1 and isinstance(values[0], list):
                # å±•å¼€æ¨¡å¼ï¼šå¦‚æœè¿”å›å•ä¸ªåˆ—è¡¨ï¼Œå±•å¼€å…¶å…ƒç´ 
                all_values.extend(values[0])
            elif expand and values and isinstance(values[0], list):
                # å¤šä¸ªåˆ—è¡¨ï¼Œå…¨éƒ¨å±•å¼€
                for v in values:
                    if isinstance(v, list):
                        all_values.extend(v)
                    else:
                        all_values.append(v)
            else:
                # ä¸å±•å¼€æˆ–éåˆ—è¡¨å€¼
                all_values.extend(values)
        else:
            # æ™®é€šå­—æ®µè·¯å¾„
            value = get_field_with_spec(item, field_spec)
            if expand and isinstance(value, list):
                # å±•å¼€ list
                all_values.extend(value)
            else:
                all_values.append(value)

    return all_values


def _compute_field_stats(
    data: List[Dict],
    top: int,
    fields: Optional[List[str]] = None,
    expand_fields: Optional[List[str]] = None,
) -> List[Dict[str, Any]]:
    """
    å•æ¬¡éå†è®¡ç®—æ¯ä¸ªå­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯ã€‚

    ä¼˜åŒ–ï¼šå°†å¤šæ¬¡éå†åˆå¹¶ä¸ºå•æ¬¡éå†ï¼Œåœ¨éå†è¿‡ç¨‹ä¸­åŒæ—¶æ”¶é›†æ‰€æœ‰ç»Ÿè®¡æ•°æ®ã€‚

    Args:
        data: æ•°æ®åˆ—è¡¨
        top: Top N å€¼æ•°é‡
        fields: æŒ‡å®šç»Ÿè®¡çš„å­—æ®µåˆ—è¡¨
        expand_fields: å±•å¼€ list å­—æ®µç»Ÿè®¡çš„å­—æ®µåˆ—è¡¨
    """
    from collections import Counter, defaultdict

    if not data:
        return []

    total = len(data)

    # å¦‚æœæ²¡æœ‰æŒ‡å®šå­—æ®µï¼Œç»Ÿè®¡æ‰€æœ‰é¡¶å±‚å­—æ®µï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    if not fields and not expand_fields:
        # å•æ¬¡éå†æ”¶é›†æ‰€æœ‰å­—æ®µçš„å€¼å’Œç»Ÿè®¡ä¿¡æ¯
        field_values = defaultdict(list)  # å­˜å‚¨æ¯ä¸ªå­—æ®µçš„æ‰€æœ‰å€¼
        field_counters = defaultdict(Counter)  # å­˜å‚¨æ¯ä¸ªå­—æ®µçš„å€¼é¢‘ç‡ï¼ˆç”¨äº top Nï¼‰

        for item in data:
            for k, v in item.items():
                field_values[k].append(v)
                # å¯¹å€¼è¿›è¡Œæˆªæ–­åè®¡æ•°ï¼ˆç”¨äº top N æ˜¾ç¤ºï¼‰
                displayable = _truncate(v if v is not None else "", 30)
                field_counters[k][displayable] += 1

        # æ ¹æ®æ”¶é›†çš„æ•°æ®è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats_list = []
        for field in sorted(field_values.keys()):
            values = field_values[field]
            non_null = [v for v in values if v is not None and v != ""]
            non_null_count = len(non_null)

            # æ¨æ–­ç±»å‹ï¼ˆä»ç¬¬ä¸€ä¸ªéç©ºå€¼ï¼‰
            field_type = _infer_type(non_null)

            # åŸºç¡€ç»Ÿè®¡
            stat = {
                "field": field,
                "non_null": non_null_count,
                "null_rate": f"{non_null_count / total * 100:.1f}%",
                "type": field_type,
            }

            # ç±»å‹ç‰¹å®šç»Ÿè®¡
            if non_null:
                # å”¯ä¸€å€¼è®¡æ•°ï¼ˆå¯¹å¤æ‚ç±»å‹ä½¿ç”¨ hash èŠ‚çœå†…å­˜ï¼‰
                stat["unique"] = _count_unique(non_null, field_type)

                # å­—ç¬¦ä¸²ç±»å‹ï¼šè®¡ç®—é•¿åº¦ç»Ÿè®¡
                if field_type == "str":
                    lengths = [len(str(v)) for v in non_null]
                    stat["len_min"] = min(lengths)
                    stat["len_max"] = max(lengths)
                    stat["len_avg"] = sum(lengths) / len(lengths)

                # æ•°å€¼ç±»å‹ï¼šè®¡ç®—æ•°å€¼ç»Ÿè®¡
                elif field_type in ("int", "float"):
                    nums = [float(v) for v in non_null if _is_numeric(v)]
                    if nums:
                        stat["min"] = min(nums)
                        stat["max"] = max(nums)
                        stat["avg"] = sum(nums) / len(nums)

                # åˆ—è¡¨ç±»å‹ï¼šè®¡ç®—é•¿åº¦ç»Ÿè®¡
                elif field_type == "list":
                    lengths = [len(v) if isinstance(v, list) else 0 for v in non_null]
                    stat["len_min"] = min(lengths)
                    stat["len_max"] = max(lengths)
                    stat["len_avg"] = sum(lengths) / len(lengths)

                # Top N å€¼ï¼ˆå·²åœ¨éå†æ—¶æ”¶é›†ï¼‰
                stat["top_values"] = field_counters[field].most_common(top)

            stats_list.append(stat)

        return stats_list

    # æŒ‡å®šäº†å­—æ®µï¼šæ”¶é›†æŒ‡å®šå­—æ®µçš„ç»Ÿè®¡
    stats_list = []
    expand_set = set(expand_fields) if expand_fields else set()

    # åˆå¹¶å­—æ®µåˆ—è¡¨
    all_fields = set(fields) if fields else set()
    all_fields.update(expand_set)

    for field_spec in sorted(all_fields):
        is_expanded = field_spec in expand_set

        # æå–å­—æ®µå€¼
        values = _extract_field_values(data, field_spec, expand=is_expanded)

        # è¿‡æ»¤ None å’Œç©ºå€¼
        non_null = [v for v in values if v is not None and v != ""]
        non_null_count = len(non_null)

        # æ¨æ–­ç±»å‹
        field_type = _infer_type(non_null)

        # åŸºç¡€ç»Ÿè®¡
        if is_expanded:
            # å±•å¼€æ¨¡å¼ï¼šæ˜¾ç¤ºå…ƒç´ æ€»æ•°å’Œå¹³å‡æ•°ï¼Œè€Œééç©ºç‡
            stat = {
                "field": field_spec,
                "non_null": non_null_count,
                "null_rate": f"æ€»å…ƒç´ : {len(values)}",
                "type": field_type,
                "is_expanded": is_expanded,
            }
        else:
            # æ™®é€šæ¨¡å¼ï¼šæ˜¾ç¤ºéç©ºç‡
            stat = {
                "field": field_spec,
                "non_null": non_null_count,
                "null_rate": f"{non_null_count / total * 100:.1f}%",
                "type": field_type,
                "is_expanded": is_expanded,
            }

        # ç±»å‹ç‰¹å®šç»Ÿè®¡
        if non_null:
            # å”¯ä¸€å€¼è®¡æ•°
            stat["unique"] = _count_unique(non_null, field_type)

            # å­—ç¬¦ä¸²ç±»å‹ï¼šè®¡ç®—é•¿åº¦ç»Ÿè®¡
            if field_type == "str":
                lengths = [len(str(v)) for v in non_null]
                stat["len_min"] = min(lengths)
                stat["len_max"] = max(lengths)
                stat["len_avg"] = sum(lengths) / len(lengths)

            # æ•°å€¼ç±»å‹ï¼šè®¡ç®—æ•°å€¼ç»Ÿè®¡
            elif field_type in ("int", "float"):
                nums = [float(v) for v in non_null if _is_numeric(v)]
                if nums:
                    stat["min"] = min(nums)
                    stat["max"] = max(nums)
                    stat["avg"] = sum(nums) / len(nums)

            # åˆ—è¡¨ç±»å‹ï¼šè®¡ç®—é•¿åº¦ç»Ÿè®¡
            elif field_type == "list":
                lengths = [len(v) if isinstance(v, list) else 0 for v in non_null]
                stat["len_min"] = min(lengths)
                stat["len_max"] = max(lengths)
                stat["len_avg"] = sum(lengths) / len(lengths)

            # Top N å€¼ï¼ˆéœ€è¦é‡æ–°è®¡æ•°ï¼‰
            counter = Counter()
            for v in non_null:
                displayable = _truncate(v if v is not None else "", 30)
                counter[displayable] += 1
            stat["top_values"] = counter.most_common(top)

        stats_list.append(stat)

    return stats_list


def _count_unique(values: List[Any], field_type: str) -> int:
    """
    è®¡ç®—å”¯ä¸€å€¼æ•°é‡ã€‚

    å¯¹äºç®€å•ç±»å‹ç›´æ¥æ¯”è¾ƒï¼Œå¯¹äº list/dict æˆ–æ··åˆç±»å‹ä½¿ç”¨ hashã€‚
    """
    if field_type in ("list", "dict"):
        return _count_unique_by_hash(values)
    else:
        # ç®€å•ç±»å‹ï¼šå°è¯•ç›´æ¥æ¯”è¾ƒï¼Œå¤±è´¥åˆ™å›é€€åˆ° hash æ–¹å¼
        try:
            return len(set(values))
        except TypeError:
            # æ··åˆç±»å‹ï¼ˆå¦‚å­—æ®µä¸­æ—¢æœ‰ str åˆæœ‰ dictï¼‰ï¼Œå›é€€åˆ° hash
            return _count_unique_by_hash(values)


def _count_unique_by_hash(values: List[Any]) -> int:
    """ä½¿ç”¨ orjson åºåˆ—åŒ–åè®¡ç®— hash æ¥ç»Ÿè®¡å”¯ä¸€å€¼"""
    import hashlib

    seen = set()
    for v in values:
        try:
            h = hashlib.md5(orjson.dumps(v, option=orjson.OPT_SORT_KEYS)).digest()
            seen.add(h)
        except TypeError:
            # æ— æ³•åºåˆ—åŒ–çš„å€¼ï¼Œç”¨ repr å…œåº•
            seen.add(repr(v))
    return len(seen)


def _print_stats(filename: str, total: int, field_stats: List[Dict[str, Any]]) -> None:
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    try:
        from rich.console import Console
        from rich.panel import Panel
        from rich.table import Table

        console = Console()

        # æ¦‚è§ˆ
        console.print(
            Panel(
                f"[bold]æ–‡ä»¶:[/bold] {filename}\n"
                f"[bold]æ€»æ•°:[/bold] {total:,} æ¡\n"
                f"[bold]å­—æ®µ:[/bold] {len(field_stats)} ä¸ª",
                title="ğŸ“Š æ•°æ®æ¦‚è§ˆ",
                expand=False,
            )
        )

        # å­—æ®µç»Ÿè®¡è¡¨
        table = Table(title="ğŸ“‹ å­—æ®µç»Ÿè®¡", show_header=True, header_style="bold cyan")
        table.add_column("å­—æ®µ", style="green")
        table.add_column("ç±»å‹", style="yellow")
        table.add_column("éç©ºç‡", justify="right")
        table.add_column("å”¯ä¸€å€¼", justify="right")
        table.add_column("ç»Ÿè®¡", style="dim")

        for stat in field_stats:
            # ä½¿ç”¨ stat ä¸­çš„ null_rateï¼ˆæ”¯æŒå±•å¼€æ¨¡å¼çš„ç‰¹æ®Šæ˜¾ç¤ºï¼‰
            if "null_rate" in stat:
                non_null_rate = stat["null_rate"]
            else:
                non_null_rate = f"{stat['non_null'] / total * 100:.0f}%"
            unique = str(stat.get("unique", "-"))

            # å­—æ®µåï¼ˆæ·»åŠ å±•å¼€æ ‡è®°ï¼‰
            field_name = stat["field"]
            if stat.get("is_expanded"):
                field_name += " (å±•å¼€)"

            # æ„å»ºç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²
            extra = []
            if "len_avg" in stat:
                extra.append(
                    f"é•¿åº¦: {stat['len_min']}-{stat['len_max']} (avg {stat['len_avg']:.0f})"
                )
            if "avg" in stat:
                if stat["type"] == "int":
                    extra.append(
                        f"èŒƒå›´: {int(stat['min'])}-{int(stat['max'])} (avg {stat['avg']:.1f})"
                    )
                else:
                    extra.append(
                        f"èŒƒå›´: {stat['min']:.2f}-{stat['max']:.2f} (avg {stat['avg']:.2f})"
                    )

            table.add_row(
                field_name,
                stat["type"],
                non_null_rate,
                unique,
                "; ".join(extra) if extra else "-",
            )

        console.print(table)

        # Top å€¼ç»Ÿè®¡ï¼ˆä»…æ˜¾ç¤ºæœ‰æ„ä¹‰çš„å­—æ®µï¼‰
        for stat in field_stats:
            top_values = stat.get("top_values", [])
            if not top_values:
                continue

            # è·³è¿‡æ•°å€¼ç±»å‹ï¼ˆmin/max/avg å·²è¶³å¤Ÿï¼‰
            if stat["type"] in ("int", "float"):
                continue

            # è·³è¿‡å”¯ä¸€å€¼è¿‡å¤šçš„å­—æ®µï¼ˆåŸºæœ¬éƒ½æ˜¯å”¯ä¸€çš„ï¼‰
            unique_ratio = stat.get("unique", 0) / total if total > 0 else 0
            if unique_ratio > 0.9 and stat.get("unique", 0) > 100:
                continue

            # å­—æ®µåï¼ˆæ·»åŠ å±•å¼€æ ‡è®°ï¼‰
            field_display = stat["field"]
            if stat.get("is_expanded"):
                field_display += " (å±•å¼€)"

            console.print(
                f"\n[bold cyan]{field_display}[/bold cyan] å€¼åˆ†å¸ƒ (Top {len(top_values)}):"
            )
            max_count = max(c for _, c in top_values) if top_values else 1
            # å±•å¼€æ¨¡å¼ä¸‹ä½¿ç”¨ non_nullï¼ˆå…ƒç´ æ€»æ•°ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ totalï¼ˆæ•°æ®æ¡æ•°ï¼‰
            base_count = stat["non_null"] if stat.get("is_expanded") else total
            for value, count in top_values:
                pct = count / base_count * 100 if base_count > 0 else 0
                bar_len = int(count / max_count * 20)  # æŒ‰ç›¸å¯¹æ¯”ä¾‹ï¼Œæœ€é•¿ 20 å­—ç¬¦
                bar = "â–ˆ" * bar_len
                display_value = value if value else "[ç©º]"
                # ä½¿ç”¨æ˜¾ç¤ºå®½åº¦å¯¹é½ï¼ˆå¤„ç†ä¸­æ–‡å­—ç¬¦ï¼‰
                padded_value = _pad_to_width(display_value, 32)
                console.print(f"  {padded_value} {count:>6} ({pct:>5.1f}%) {bar}")

    except ImportError:
        # æ²¡æœ‰ richï¼Œä½¿ç”¨æ™®é€šæ‰“å°
        print(f"\n{'=' * 50}")
        print("ğŸ“Š æ•°æ®æ¦‚è§ˆ")
        print(f"{'=' * 50}")
        print(f"æ–‡ä»¶: {filename}")
        print(f"æ€»æ•°: {total:,} æ¡")
        print(f"å­—æ®µ: {len(field_stats)} ä¸ª")

        print(f"\n{'=' * 50}")
        print("ğŸ“‹ å­—æ®µç»Ÿè®¡")
        print(f"{'=' * 50}")
        print(f"{'å­—æ®µ':<20} {'ç±»å‹':<8} {'éç©ºç‡':<8} {'å”¯ä¸€å€¼':<8}")
        print("-" * 50)

        for stat in field_stats:
            non_null_rate = f"{stat['non_null'] / total * 100:.0f}%"
            unique = str(stat.get("unique", "-"))
            print(f"{stat['field']:<20} {stat['type']:<8} {non_null_rate:<8} {unique:<8}")


def token_stats(
    filename: str,
    field: str = "messages",
    model: str = "cl100k_base",
    detailed: bool = False,
    workers: Optional[int] = None,
) -> None:
    """
    ç»Ÿè®¡æ•°æ®é›†çš„ Token ä¿¡æ¯ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„
        field: è¦ç»Ÿè®¡çš„å­—æ®µï¼ˆé»˜è®¤ messagesï¼‰ï¼Œæ”¯æŒåµŒå¥—è·¯å¾„è¯­æ³•
        model: åˆ†è¯å™¨: cl100k_base (é»˜è®¤), qwen2.5, llama3, gpt-4 ç­‰
        detailed: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
        workers: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ŒNone è‡ªåŠ¨æ£€æµ‹ï¼Œ1 ç¦ç”¨å¹¶è¡Œ

    Examples:
        dt token-stats data.jsonl
        dt token-stats data.jsonl --field=text --model=qwen2.5
        dt token-stats data.jsonl --field=conversation.messages
        dt token-stats data.jsonl --field=messages[-1].content   # ç»Ÿè®¡æœ€åä¸€æ¡æ¶ˆæ¯
        dt token-stats data.jsonl --detailed
        dt token-stats data.jsonl --workers=4   # ä½¿ç”¨ 4 è¿›ç¨‹
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {filepath}")
    try:
        data = load_data(str(filepath))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    if not data:
        print("æ–‡ä»¶ä¸ºç©º")
        return

    total = len(data)
    print(f"   å…± {total:,} æ¡æ•°æ®")

    # æ£€æŸ¥å­—æ®µç±»å‹å¹¶é€‰æ‹©åˆé€‚çš„ç»Ÿè®¡æ–¹æ³•ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
    sample = data[0]
    field_value = get_field_with_spec(sample, field)

    # å°è¯•ä½¿ç”¨ rich è¿›åº¦æ¡
    try:
        from rich.progress import BarColumn, Progress, SpinnerColumn, TaskProgressColumn, TextColumn

        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]ç»Ÿè®¡ Token"),
            BarColumn(),
            TaskProgressColumn(),
            TextColumn(f"(æ¨¡å‹: {model})"),
        ) as progress:
            task = progress.add_task("", total=total)

            def update_progress(current: int, total_count: int):
                progress.update(task, completed=current)

            if isinstance(field_value, list) and field_value and isinstance(field_value[0], dict):
                from ..tokenizers import messages_token_stats

                stats_result = messages_token_stats(
                    data,
                    messages_field=field,
                    model=model,
                    progress_callback=update_progress,
                    workers=workers,
                )
                _print_messages_token_stats(stats_result, detailed)
            else:
                from ..tokenizers import token_stats as compute_token_stats

                stats_result = compute_token_stats(
                    data,
                    fields=field,
                    model=model,
                    progress_callback=update_progress,
                    workers=workers,
                )
                _print_text_token_stats(stats_result, detailed)

    except ImportError:
        # æ²¡æœ‰ richï¼Œæ˜¾ç¤ºç®€å•è¿›åº¦
        print(f"ğŸ”¢ ç»Ÿè®¡ Token (æ¨¡å‹: {model}, å­—æ®µ: {field})...")
        try:
            if isinstance(field_value, list) and field_value and isinstance(field_value[0], dict):
                from ..tokenizers import messages_token_stats

                stats_result = messages_token_stats(
                    data, messages_field=field, model=model, workers=workers
                )
                _print_messages_token_stats(stats_result, detailed)
            else:
                from ..tokenizers import token_stats as compute_token_stats

                stats_result = compute_token_stats(data, fields=field, model=model, workers=workers)
                _print_text_token_stats(stats_result, detailed)
        except ImportError as e:
            print(f"é”™è¯¯: {e}")
            return
        except Exception as e:
            print(f"é”™è¯¯: ç»Ÿè®¡å¤±è´¥ - {e}")
            import traceback

            traceback.print_exc()


def _print_messages_token_stats(stats: Dict[str, Any], detailed: bool) -> None:
    """æ‰“å° messages æ ¼å¼çš„ token ç»Ÿè®¡"""
    try:
        from rich.console import Console
        from rich.panel import Panel
        from rich.table import Table

        console = Console()

        # æ¦‚è§ˆ
        std = stats.get("std_tokens", 0)
        overview = (
            f"[bold]æ€»æ ·æœ¬æ•°:[/bold] {stats['count']:,}\n"
            f"[bold]æ€» Token:[/bold] {stats['total_tokens']:,}\n"
            f"[bold]å¹³å‡ Token:[/bold] {stats['avg_tokens']:,} (std: {std:.1f})\n"
            f"[bold]èŒƒå›´:[/bold] {stats['min_tokens']:,} - {stats['max_tokens']:,}"
        )
        console.print(Panel(overview, title="ğŸ“Š Token ç»Ÿè®¡æ¦‚è§ˆ", expand=False))

        # ç™¾åˆ†ä½æ•°è¡¨æ ¼
        table = Table(title="ğŸ“ˆ åˆ†å¸ƒç»Ÿè®¡")
        table.add_column("ç™¾åˆ†ä½", style="cyan", justify="center")
        table.add_column("Token æ•°", justify="right")
        percentiles = [
            ("Min", stats["min_tokens"]),
            ("P25", stats.get("p25", "-")),
            ("P50 (ä¸­ä½æ•°)", stats.get("median_tokens", "-")),
            ("P75", stats.get("p75", "-")),
            ("P90", stats.get("p90", "-")),
            ("P95", stats.get("p95", "-")),
            ("P99", stats.get("p99", "-")),
            ("Max", stats["max_tokens"]),
        ]
        for name, val in percentiles:
            table.add_row(name, f"{val:,}" if isinstance(val, int) else str(val))
        console.print(table)

        if detailed:
            # åˆ†è§’è‰²ç»Ÿè®¡
            role_table = Table(title="ğŸ“‹ åˆ†è§’è‰²ç»Ÿè®¡")
            role_table.add_column("è§’è‰²", style="cyan")
            role_table.add_column("Token æ•°", justify="right")
            role_table.add_column("å æ¯”", justify="right")

            total = stats["total_tokens"]
            for role, key in [
                ("User", "user_tokens"),
                ("Assistant", "assistant_tokens"),
                ("System", "system_tokens"),
            ]:
                tokens = stats.get(key, 0)
                pct = tokens / total * 100 if total > 0 else 0
                role_table.add_row(role, f"{tokens:,}", f"{pct:.1f}%")

            console.print(role_table)
            console.print(f"\nå¹³å‡å¯¹è¯è½®æ•°: {stats.get('avg_turns', 0)}")

    except ImportError:
        # æ²¡æœ‰ richï¼Œä½¿ç”¨æ™®é€šæ‰“å°
        std = stats.get("std_tokens", 0)
        print(f"\n{'=' * 40}")
        print("ğŸ“Š Token ç»Ÿè®¡æ¦‚è§ˆ")
        print(f"{'=' * 40}")
        print(f"æ€»æ ·æœ¬æ•°: {stats['count']:,}")
        print(f"æ€» Token: {stats['total_tokens']:,}")
        print(f"å¹³å‡ Token: {stats['avg_tokens']:,} (std: {std:.1f})")
        print(f"èŒƒå›´: {stats['min_tokens']:,} - {stats['max_tokens']:,}")

        print("\nğŸ“ˆ ç™¾åˆ†ä½åˆ†å¸ƒ:")
        print(f"  P25: {stats.get('p25', '-'):,}  P50: {stats.get('median_tokens', '-'):,}")
        print(f"  P75: {stats.get('p75', '-'):,}  P90: {stats.get('p90', '-'):,}")
        print(f"  P95: {stats.get('p95', '-'):,}  P99: {stats.get('p99', '-'):,}")

        if detailed:
            print(f"\n{'=' * 40}")
            print("ğŸ“‹ åˆ†è§’è‰²ç»Ÿè®¡")
            print(f"{'=' * 40}")
            total = stats["total_tokens"]
            for role, key in [
                ("User", "user_tokens"),
                ("Assistant", "assistant_tokens"),
                ("System", "system_tokens"),
            ]:
                tokens = stats.get(key, 0)
                pct = tokens / total * 100 if total > 0 else 0
                print(f"{role}: {tokens:,} ({pct:.1f}%)")
            print(f"\nå¹³å‡å¯¹è¯è½®æ•°: {stats.get('avg_turns', 0)}")


def _print_text_token_stats(stats: Dict[str, Any], detailed: bool) -> None:
    """æ‰“å°æ™®é€šæ–‡æœ¬çš„ token ç»Ÿè®¡"""
    try:
        from rich.console import Console
        from rich.panel import Panel
        from rich.table import Table

        console = Console()

        std = stats.get("std_tokens", 0)
        overview = (
            f"[bold]æ€»æ ·æœ¬æ•°:[/bold] {stats['count']:,}\n"
            f"[bold]æ€» Token:[/bold] {stats['total_tokens']:,}\n"
            f"[bold]å¹³å‡ Token:[/bold] {stats['avg_tokens']:.1f} (std: {std:.1f})\n"
            f"[bold]èŒƒå›´:[/bold] {stats['min_tokens']:,} - {stats['max_tokens']:,}"
        )
        console.print(Panel(overview, title="ğŸ“Š Token ç»Ÿè®¡", expand=False))

        # ç™¾åˆ†ä½æ•°è¡¨æ ¼
        table = Table(title="ğŸ“ˆ åˆ†å¸ƒç»Ÿè®¡")
        table.add_column("ç™¾åˆ†ä½", style="cyan", justify="center")
        table.add_column("Token æ•°", justify="right")
        percentiles = [
            ("Min", stats["min_tokens"]),
            ("P25", stats.get("p25", "-")),
            ("P50 (ä¸­ä½æ•°)", stats.get("median_tokens", "-")),
            ("P75", stats.get("p75", "-")),
            ("P90", stats.get("p90", "-")),
            ("P95", stats.get("p95", "-")),
            ("P99", stats.get("p99", "-")),
            ("Max", stats["max_tokens"]),
        ]
        for name, val in percentiles:
            table.add_row(name, f"{val:,}" if isinstance(val, int) else str(val))
        console.print(table)

    except ImportError:
        std = stats.get("std_tokens", 0)
        print(f"\n{'=' * 40}")
        print("ğŸ“Š Token ç»Ÿè®¡")
        print(f"{'=' * 40}")
        print(f"æ€»æ ·æœ¬æ•°: {stats['count']:,}")
        print(f"æ€» Token: {stats['total_tokens']:,}")
        print(f"å¹³å‡ Token: {stats['avg_tokens']:.1f} (std: {std:.1f})")
        print(f"èŒƒå›´: {stats['min_tokens']:,} - {stats['max_tokens']:,}")

        print("\nğŸ“ˆ ç™¾åˆ†ä½åˆ†å¸ƒ:")
        print(f"  P25: {stats.get('p25', '-'):,}  P50: {stats.get('median_tokens', '-'):,}")
        print(f"  P75: {stats.get('p75', '-'):,}  P90: {stats.get('p90', '-'):,}")
        print(f"  P95: {stats.get('p95', '-'):,}  P99: {stats.get('p99', '-'):,}")
