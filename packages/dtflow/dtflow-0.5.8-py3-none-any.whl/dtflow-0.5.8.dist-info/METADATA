Metadata-Version: 2.4
Name: dtflow
Version: 0.5.8
Summary: A flexible data transformation tool for ML training formats (SFT, RLHF, Pretrain)
Project-URL: Homepage, https://github.com/yourusername/DataTransformer
Project-URL: Documentation, https://github.com/yourusername/DataTransformer#readme
Project-URL: Repository, https://github.com/yourusername/DataTransformer
Project-URL: Issues, https://github.com/yourusername/DataTransformer/issues
Project-URL: Changelog, https://github.com/yourusername/DataTransformer/blob/main/CHANGELOG.md
Author-email: Your Name <your.email@example.com>
Maintainer-email: Your Name <your.email@example.com>
License-Expression: MIT
Keywords: ai,data-processing,data-transformation,machine-learning,nlp,pretrain,rlhf,sft
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Text Processing
Requires-Python: >=3.8
Requires-Dist: numpy>=1.20.0
Requires-Dist: orjson>=3.9.0
Requires-Dist: polars>=0.20.0
Requires-Dist: pyyaml>=5.4.0
Requires-Dist: rich>=10.0.0
Requires-Dist: tiktoken>=0.5.0
Requires-Dist: typer>=0.9.0
Provides-Extra: converters
Requires-Dist: datasets>=2.0.0; extra == 'converters'
Provides-Extra: dev
Requires-Dist: black>=21.0; extra == 'dev'
Requires-Dist: datasets>=2.0.0; extra == 'dev'
Requires-Dist: datasketch>=1.5.0; extra == 'dev'
Requires-Dist: flake8>=3.9.0; extra == 'dev'
Requires-Dist: huggingface-hub>=0.20.0; extra == 'dev'
Requires-Dist: isort>=5.9.0; extra == 'dev'
Requires-Dist: mypy>=0.910; extra == 'dev'
Requires-Dist: pyarrow; extra == 'dev'
Requires-Dist: pytest-cov>=2.12.0; extra == 'dev'
Requires-Dist: pytest>=6.0.0; extra == 'dev'
Requires-Dist: rich>=10.0.0; extra == 'dev'
Requires-Dist: scikit-learn>=0.24.0; extra == 'dev'
Requires-Dist: tiktoken>=0.5.0; extra == 'dev'
Requires-Dist: tokenizers>=0.15.0; extra == 'dev'
Requires-Dist: toolong>=1.5.0; extra == 'dev'
Provides-Extra: display
Provides-Extra: docs
Requires-Dist: myst-parser>=0.15.0; extra == 'docs'
Requires-Dist: sphinx-rtd-theme>=0.5.0; extra == 'docs'
Requires-Dist: sphinx>=4.0.0; extra == 'docs'
Provides-Extra: full
Requires-Dist: datasets>=2.0.0; extra == 'full'
Requires-Dist: datasketch>=1.5.0; extra == 'full'
Requires-Dist: huggingface-hub>=0.20.0; extra == 'full'
Requires-Dist: pyarrow; extra == 'full'
Requires-Dist: rich>=10.0.0; extra == 'full'
Requires-Dist: scikit-learn>=0.24.0; extra == 'full'
Requires-Dist: tiktoken>=0.5.0; extra == 'full'
Requires-Dist: tokenizers>=0.15.0; extra == 'full'
Requires-Dist: toolong>=1.5.0; extra == 'full'
Provides-Extra: logs
Requires-Dist: toolong>=1.5.0; extra == 'logs'
Provides-Extra: similarity
Requires-Dist: datasketch>=1.5.0; extra == 'similarity'
Requires-Dist: scikit-learn>=0.24.0; extra == 'similarity'
Provides-Extra: storage
Requires-Dist: pyarrow; extra == 'storage'
Provides-Extra: tokenizers
Requires-Dist: tiktoken>=0.5.0; extra == 'tokenizers'
Provides-Extra: tokenizers-hf
Requires-Dist: huggingface-hub>=0.20.0; extra == 'tokenizers-hf'
Requires-Dist: tiktoken>=0.5.0; extra == 'tokenizers-hf'
Requires-Dist: tokenizers>=0.15.0; extra == 'tokenizers-hf'
Description-Content-Type: text/markdown

# dtflow

ç®€æ´çš„æ•°æ®æ ¼å¼è½¬æ¢å·¥å…·ï¼Œä¸“ä¸ºæœºå™¨å­¦ä¹ è®­ç»ƒæ•°æ®è®¾è®¡ã€‚

## å®‰è£…

```bash
pip install dtflow

# å¯é€‰ä¾èµ–
pip install tiktoken          # Token ç»Ÿè®¡ï¼ˆOpenAI æ¨¡å‹ï¼‰
pip install transformers      # Token ç»Ÿè®¡ï¼ˆHuggingFace æ¨¡å‹ï¼‰
pip install datasets          # HuggingFace Dataset è½¬æ¢
```

## ğŸ¤– Claude Code é›†æˆ

dtflow å†…ç½®äº† [Claude Code](https://docs.anthropic.com/en/docs/claude-code) skillï¼š

```bash
dt install-skill      # å®‰è£… skill
dt skill-status       # æŸ¥çœ‹çŠ¶æ€
```

å®‰è£…ååœ¨ Claude Code ä¸­è¾“å…¥ `/dtflow`ï¼ŒClaude å°†æŒæ¡ dtflow çš„å®Œæ•´ç”¨æ³•ï¼Œå¯ç›´æ¥ååŠ©ä½ å®Œæˆæ•°æ®å¤„ç†ä»»åŠ¡ã€‚

## å¿«é€Ÿå¼€å§‹

```python
from dtflow import DataTransformer

# åŠ è½½æ•°æ®
dt = DataTransformer.load("data.jsonl")

# é“¾å¼æ“ä½œï¼šè¿‡æ»¤ -> è½¬æ¢ -> ä¿å­˜
(dt.filter(lambda x: x.score > 0.8)
   .to(lambda x: {"q": x.question, "a": x.answer})
   .save("output.jsonl"))
```

## æ ¸å¿ƒåŠŸèƒ½

### æ•°æ®åŠ è½½ä¸ä¿å­˜

```python
# æ”¯æŒ JSONLã€JSONã€CSVã€Parquetã€Arrowï¼ˆä½¿ç”¨ Polars å¼•æ“ï¼Œæ¯” Pandas å¿« 3xï¼‰
dt = DataTransformer.load("data.jsonl")
dt.save("output.jsonl")

# ä»åˆ—è¡¨åˆ›å»º
dt = DataTransformer([{"q": "é—®é¢˜", "a": "ç­”æ¡ˆ"}])
```

### æ•°æ®è¿‡æ»¤

```python
# Lambda è¿‡æ»¤
dt.filter(lambda x: x.score > 0.8)

# æ”¯æŒå±æ€§è®¿é—®
dt.filter(lambda x: x.language == "zh")
```

### æ•°æ®éªŒè¯

```python
# ç®€å•éªŒè¯ï¼Œè¿”å›ä¸é€šè¿‡çš„è®°å½•åˆ—è¡¨
errors = dt.validate(lambda x: len(x.messages) >= 2)

if errors:
    for e in errors[:5]:
        print(f"ç¬¬ {e.index} è¡Œ: {e.error}")
```

### Schema éªŒè¯

ä½¿ç”¨ Schema è¿›è¡Œç»“æ„åŒ–æ•°æ®éªŒè¯ï¼š

```python
from dtflow import Schema, Field, openai_chat_schema

# ä½¿ç”¨é¢„è®¾ Schema
result = dt.validate_schema(openai_chat_schema)
print(result)  # ValidationResult(valid=950, invalid=50, errors=[...])

# è‡ªå®šä¹‰ Schema
schema = Schema({
    "messages": Field(type="list", required=True, min_length=1),
    "messages[*].role": Field(type="str", choices=["user", "assistant", "system"]),
    "messages[*].content": Field(type="str", min_length=1),
    "score": Field(type="float", min=0, max=1),
})

result = dt.validate_schema(schema)

# è¿‡æ»¤å‡ºæœ‰æ•ˆæ•°æ®
valid_dt = dt.validate_schema(schema, filter_invalid=True)
valid_dt.save("valid.jsonl")
```

**é¢„è®¾ Schema**ï¼š

| Schema åç§° | ç”¨é€” |
|------------|------|
| `openai_chat_schema` | OpenAI messages æ ¼å¼éªŒè¯ |
| `alpaca_schema` | Alpaca instruction/output æ ¼å¼ |
| `sharegpt_schema` | ShareGPT conversations æ ¼å¼ |
| `dpo_schema` | DPO prompt/chosen/rejected æ ¼å¼ |

**Field å‚æ•°**ï¼š

| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `type` | ç±»å‹éªŒè¯ | `"str"`, `"int"`, `"float"`, `"bool"`, `"list"`, `"dict"` |
| `required` | æ˜¯å¦å¿…å¡« | `True` / `False` |
| `min` / `max` | æ•°å€¼èŒƒå›´ | `min=0, max=1` |
| `min_length` / `max_length` | é•¿åº¦èŒƒå›´ | `min_length=1` |
| `choices` | æšä¸¾å€¼ | `choices=["user", "assistant"]` |
| `pattern` | æ­£åˆ™åŒ¹é… | `pattern=r"^\d{4}-\d{2}-\d{2}$"` |
| `custom` | è‡ªå®šä¹‰éªŒè¯ | `custom=lambda x: x > 0` |

### æ•°æ®è½¬æ¢

```python
# è‡ªå®šä¹‰è½¬æ¢
dt.to(lambda x: {"question": x.q, "answer": x.a})

# ä½¿ç”¨é¢„è®¾æ¨¡æ¿
dt.to(preset="openai_chat", user_field="q", assistant_field="a")
```

### é¢„è®¾æ¨¡æ¿

| é¢„è®¾åç§° | è¾“å‡ºæ ¼å¼ |
|---------|---------|
| `openai_chat` | `{"messages": [{"role": "user", ...}, {"role": "assistant", ...}]}` |
| `alpaca` | `{"instruction": ..., "input": ..., "output": ...}` |
| `sharegpt` | `{"conversations": [{"from": "human", ...}, {"from": "gpt", ...}]}` |
| `dpo_pair` | `{"prompt": ..., "chosen": ..., "rejected": ...}` |
| `simple_qa` | `{"question": ..., "answer": ...}` |

### Token ç»Ÿè®¡

```python
from dtflow import count_tokens, token_counter, token_filter, token_stats

# è®¡ç®— token æ•°é‡
count = count_tokens("Hello world", model="gpt-4")

# æ·»åŠ  token_count å­—æ®µ
dt.transform(token_counter("text")).save("with_tokens.jsonl")

# æŒ‰ token é•¿åº¦è¿‡æ»¤
dt.filter(token_filter("text", max_tokens=2048))
dt.filter(token_filter(["question", "answer"], min_tokens=10, max_tokens=4096))

# ç»Ÿè®¡ token åˆ†å¸ƒ
stats = token_stats(dt.data, "text")
# {"total_tokens": 12345, "avg_tokens": 123, "min_tokens": 5, "max_tokens": 500, ...}
```

æ”¯æŒ `tiktoken`ï¼ˆOpenAIï¼Œé»˜è®¤ï¼‰å’Œ `transformers` åç«¯ï¼Œ**è‡ªåŠ¨æ£€æµ‹**ï¼š

```python
# OpenAI æ¨¡å‹ -> è‡ªåŠ¨ä½¿ç”¨ tiktoken
count_tokens("Hello", model="gpt-4")

# HuggingFace/æœ¬åœ°æ¨¡å‹ -> è‡ªåŠ¨ä½¿ç”¨ transformers
count_tokens("Hello", model="Qwen/Qwen2-7B")
count_tokens("Hello", model="/home/models/qwen")
```

### Messages Token ç»Ÿè®¡

ä¸“ä¸ºå¤šè½®å¯¹è¯è®¾è®¡çš„ token ç»Ÿè®¡åŠŸèƒ½ï¼š

```python
from dtflow import messages_token_counter, messages_token_filter, messages_token_stats

# ä¸ºæ¯æ¡æ•°æ®æ·»åŠ  token ç»Ÿè®¡
dt.transform(messages_token_counter(model="gpt-4"))  # ç®€å•æ¨¡å¼ï¼Œè¾“å‡ºæ€»æ•°
dt.transform(messages_token_counter(model="gpt-4", detailed=True))  # è¯¦ç»†æ¨¡å¼
# è¯¦ç»†æ¨¡å¼è¾“å‡º: {"total": 500, "user": 200, "assistant": 280, "system": 20, "turns": 5, ...}

# æŒ‰ token æ•°å’Œè½®æ•°è¿‡æ»¤
dt.filter(messages_token_filter(min_tokens=100, max_tokens=4096))
dt.filter(messages_token_filter(min_turns=2, max_turns=10))

# ç»Ÿè®¡æ•´ä¸ªæ•°æ®é›†
stats = messages_token_stats(dt.data, model="gpt-4")
# {"count": 1000, "total_tokens": 500000, "user_tokens": 200000, "assistant_tokens": 290000, ...}
```

### æ ¼å¼è½¬æ¢å™¨

```python
from dtflow import (
    to_hf_dataset, from_hf_dataset,    # HuggingFace Dataset
    to_openai_batch, from_openai_batch, # OpenAI Batch API
    to_llama_factory,                   # LLaMA-Factory Alpaca æ ¼å¼
    to_axolotl,                         # Axolotl æ ¼å¼
    messages_to_text,                   # messages è½¬çº¯æ–‡æœ¬
)

# HuggingFace Dataset äº’è½¬
ds = to_hf_dataset(dt.data)
ds.push_to_hub("my-dataset")

data = from_hf_dataset("tatsu-lab/alpaca", split="train")

# OpenAI Batch API
batch_input = dt.to(to_openai_batch(model="gpt-4o"))
results = from_openai_batch(batch_output)

# messages è½¬çº¯æ–‡æœ¬ï¼ˆæ”¯æŒ chatml/llama2/simple æ¨¡æ¿ï¼‰
dt.transform(messages_to_text(template="chatml"))
```

### LLaMA-Factory æ ¼å¼

å®Œæ•´æ”¯æŒ LLaMA-Factory çš„ SFT è®­ç»ƒæ ¼å¼ï¼š

```python
from dtflow import (
    to_llama_factory,              # Alpaca æ ¼å¼ï¼ˆå•è½®ï¼‰
    to_llama_factory_sharegpt,     # ShareGPT æ ¼å¼ï¼ˆå¤šè½®å¯¹è¯ï¼‰
    to_llama_factory_vlm,          # VLM Alpaca æ ¼å¼
    to_llama_factory_vlm_sharegpt, # VLM ShareGPT æ ¼å¼
)

# Alpaca æ ¼å¼
dt.transform(to_llama_factory()).save("alpaca.jsonl")
# è¾“å‡º: {"instruction": "...", "input": "", "output": "..."}

# ShareGPT æ ¼å¼ï¼ˆå¤šè½®å¯¹è¯ï¼‰
dt.transform(to_llama_factory_sharegpt()).save("sharegpt.jsonl")
# è¾“å‡º: {"conversations": [{"from": "human", "value": "..."}, {"from": "gpt", "value": "..."}], "system": "..."}

# VLM æ ¼å¼ï¼ˆå›¾ç‰‡/è§†é¢‘ï¼‰
dt.transform(to_llama_factory_vlm(images_field="images")).save("vlm.jsonl")
# è¾“å‡º: {"instruction": "...", "output": "...", "images": ["/path/to/img.jpg"]}

dt.transform(to_llama_factory_vlm_sharegpt(images_field="images", videos_field="videos"))
# è¾“å‡º: {"conversations": [...], "images": [...], "videos": [...]}
```

### ms-swift æ ¼å¼

æ”¯æŒ ModelScope ms-swift çš„è®­ç»ƒæ ¼å¼ï¼š

```python
from dtflow import (
    to_swift_messages,        # æ ‡å‡† messages æ ¼å¼
    to_swift_query_response,  # query-response æ ¼å¼
    to_swift_vlm,             # VLM æ ¼å¼
)

# messages æ ¼å¼
dt.transform(to_swift_messages()).save("swift_messages.jsonl")
# è¾“å‡º: {"messages": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}

# query-response æ ¼å¼ï¼ˆè‡ªåŠ¨æå– historyï¼‰
dt.transform(to_swift_query_response(query_field="messages")).save("swift_qr.jsonl")
# è¾“å‡º: {"query": "...", "response": "...", "system": "...", "history": [["q1", "a1"], ...]}

# VLM æ ¼å¼
dt.transform(to_swift_vlm(images_field="images")).save("swift_vlm.jsonl")
# è¾“å‡º: {"messages": [...], "images": ["/path/to/img.jpg"]}
```

### è®­ç»ƒæ¡†æ¶ä¸€é”®å¯¼å‡º

å°†æ•°æ®å¯¼å‡ºä¸ºç›®æ ‡è®­ç»ƒæ¡†æ¶å¯ç›´æ¥ä½¿ç”¨çš„æ ¼å¼ï¼Œè‡ªåŠ¨ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼š

```python
from dtflow import DataTransformer

dt = DataTransformer.load("data.jsonl")

# 1. æ£€æŸ¥æ¡†æ¶å…¼å®¹æ€§
result = dt.check_compatibility("llama-factory")
print(result)
# âœ… å…¼å®¹ - LLaMA-Factory (openai_chat)
# æˆ–
# âŒ ä¸å…¼å®¹ - é”™è¯¯: xxx

# 2. ä¸€é”®å¯¼å‡ºåˆ° LLaMA-Factory
files = dt.export_for("llama-factory", "./llama_ready/")
# ç”Ÿæˆæ–‡ä»¶:
# - ./llama_ready/custom_dataset.json      # æ•°æ®æ–‡ä»¶
# - ./llama_ready/dataset_info.json        # æ•°æ®é›†é…ç½®
# - ./llama_ready/train_args.yaml          # è®­ç»ƒå‚æ•°æ¨¡æ¿

# 3. å¯¼å‡ºåˆ° ms-swift
files = dt.export_for("swift", "./swift_ready/")
# ç”Ÿæˆ: data.jsonl + train_swift.sh

# 4. å¯¼å‡ºåˆ° Axolotl
files = dt.export_for("axolotl", "./axolotl_ready/")
# ç”Ÿæˆ: data.jsonl + config.yaml

# æŒ‡å®šæ•°æ®é›†åç§°
dt.export_for("llama-factory", "./output/", dataset_name="my_sft_data")
```

**æ”¯æŒçš„æ¡†æ¶**ï¼š

| æ¡†æ¶ | å¯¼å‡ºå†…å®¹ | ä½¿ç”¨æ–¹å¼ |
|------|---------|---------|
| `llama-factory` | data.json + dataset_info.json + train_args.yaml | `llamafactory-cli train train_args.yaml` |
| `swift` | data.jsonl + train_swift.sh | `bash train_swift.sh` |
| `axolotl` | data.jsonl + config.yaml | `accelerate launch -m axolotl.cli.train config.yaml` |

**è‡ªåŠ¨æ ¼å¼æ£€æµ‹**ï¼š

| æ£€æµ‹åˆ°çš„æ ¼å¼ | æ•°æ®ç»“æ„ |
|------------|---------|
| `openai_chat` | `{"messages": [{"role": "user", ...}]}` |
| `alpaca` | `{"instruction": ..., "output": ...}` |
| `sharegpt` | `{"conversations": [{"from": "human", ...}]}` |
| `dpo` | `{"prompt": ..., "chosen": ..., "rejected": ...}` |

### å…¶ä»–æ“ä½œ

```python
# é‡‡æ ·
dt.sample(100)           # éšæœºé‡‡æ · 100 æ¡
dt.head(10)              # å‰ 10 æ¡
dt.tail(10)              # å 10 æ¡

# åˆ†å‰²
train, test = dt.split(ratio=0.8, shuffle=True, seed=42)

# ç»Ÿè®¡
stats = dt.stats()       # æ€»æ•°ã€å­—æ®µä¿¡æ¯
count = dt.count(lambda x: x.score > 0.9)

# æ‰“ä¹±
dt.shuffle(seed=42)
```

## CLI å‘½ä»¤

```bash
# æ•°æ®é‡‡æ ·
dt sample data.jsonl --num=10
dt sample data.csv --num=100 --sample_type=head
dt sample data.jsonl 1000 --by=category           # åˆ†å±‚é‡‡æ ·
dt sample data.jsonl 1000 --by=meta.source        # æŒ‰åµŒå¥—å­—æ®µåˆ†å±‚é‡‡æ ·
dt sample data.jsonl 1000 --by=messages.#         # æŒ‰æ¶ˆæ¯æ•°é‡åˆ†å±‚é‡‡æ ·
dt sample data.jsonl --where="category=tech"      # ç­›é€‰åé‡‡æ ·
dt sample data.jsonl --where="messages.#>=2"      # å¤šæ¡ä»¶ç­›é€‰

# æ•°æ®è½¬æ¢ - é¢„è®¾æ¨¡å¼
dt transform data.jsonl --preset=openai_chat
dt transform data.jsonl --preset=alpaca

# æ•°æ®è½¬æ¢ - é…ç½®æ–‡ä»¶æ¨¡å¼
dt transform data.jsonl                    # é¦–æ¬¡è¿è¡Œç”Ÿæˆé…ç½®æ–‡ä»¶
# ç¼–è¾‘ .dt/data.py åå†æ¬¡è¿è¡Œ
dt transform data.jsonl --num=100          # æ‰§è¡Œè½¬æ¢

# Pipeline æ‰§è¡Œï¼ˆå¯å¤ç°çš„æ•°æ®å¤„ç†æµç¨‹ï¼‰
dt run pipeline.yaml
dt run pipeline.yaml --input=new_data.jsonl --output=result.jsonl

# Token ç»Ÿè®¡
dt token-stats data.jsonl --field=messages --model=gpt-4
dt token-stats data.jsonl --field=messages[-1].content   # ç»Ÿè®¡æœ€åä¸€æ¡æ¶ˆæ¯
dt token-stats data.jsonl --field=text --detailed
dt token-stats data.jsonl --workers=4                    # å¤šè¿›ç¨‹åŠ é€Ÿï¼ˆæ•°æ®é‡å¤§æ—¶è‡ªåŠ¨å¯ç”¨ï¼‰

# æ•°æ®å¯¹æ¯”
dt diff v1/train.jsonl v2/train.jsonl
dt diff a.jsonl b.jsonl --key=id
dt diff a.jsonl b.jsonl --key=meta.uuid    # æŒ‰åµŒå¥—å­—æ®µåŒ¹é…

# æ•°æ®æ¸…æ´—
dt clean data.jsonl --drop-empty                    # åˆ é™¤ä»»æ„ç©ºå€¼è®°å½•
dt clean data.jsonl --drop-empty=text,answer        # åˆ é™¤æŒ‡å®šå­—æ®µä¸ºç©ºçš„è®°å½•
dt clean data.jsonl --drop-empty=meta.source        # åˆ é™¤åµŒå¥—å­—æ®µä¸ºç©ºçš„è®°å½•
dt clean data.jsonl --min-len=text:10               # text å­—æ®µæœ€å°‘ 10 å­—ç¬¦
dt clean data.jsonl --min-len=messages.#:2          # è‡³å°‘ 2 æ¡æ¶ˆæ¯
dt clean data.jsonl --max-len=messages[-1].content:500  # æœ€åä¸€æ¡æ¶ˆæ¯æœ€å¤š 500 å­—ç¬¦
dt clean data.jsonl --keep=question,answer          # åªä¿ç•™è¿™äº›å­—æ®µ
dt clean data.jsonl --drop=metadata                 # åˆ é™¤æŒ‡å®šå­—æ®µ
dt clean data.jsonl --strip                         # å»é™¤å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½

# æ•°æ®å»é‡
dt dedupe data.jsonl                            # å…¨é‡ç²¾ç¡®å»é‡
dt dedupe data.jsonl --key=text                 # æŒ‰å­—æ®µç²¾ç¡®å»é‡
dt dedupe data.jsonl --key=meta.id              # æŒ‰åµŒå¥—å­—æ®µå»é‡
dt dedupe data.jsonl --key=messages[0].content  # æŒ‰ç¬¬ä¸€æ¡æ¶ˆæ¯å†…å®¹å»é‡
dt dedupe data.jsonl --key=text --similar=0.8   # ç›¸ä¼¼åº¦å»é‡

# æ–‡ä»¶æ‹¼æ¥
dt concat a.jsonl b.jsonl -o merged.jsonl

# æ•°æ®ç»Ÿè®¡
dt stats data.jsonl                                       # å¿«é€Ÿæ¨¡å¼
dt stats data.jsonl --full                                # å®Œæ•´æ¨¡å¼ï¼ˆå«å€¼åˆ†å¸ƒï¼‰
dt stats data.jsonl --full --field=category               # æŒ‡å®šå­—æ®µç»Ÿè®¡
dt stats data.jsonl --full --expand=tags                  # å±•å¼€ list å­—æ®µç»Ÿè®¡å…ƒç´ åˆ†å¸ƒ
dt stats data.jsonl --full --expand='messages[*].role'    # å±•å¼€åµŒå¥— list å­—æ®µ

# Claude Code Skill å®‰è£…
dt install-skill                              # å®‰è£…åˆ° ~/.claude/skills/
dt skill-status                               # æŸ¥çœ‹å®‰è£…çŠ¶æ€

# æ•°æ®éªŒè¯
dt validate data.jsonl --preset=openai_chat           # ä½¿ç”¨é¢„è®¾ schema éªŒè¯
dt validate data.jsonl --preset=alpaca --verbose      # è¯¦ç»†è¾“å‡º
dt validate data.jsonl --preset=sharegpt --filter-invalid -o valid.jsonl  # è¿‡æ»¤å‡ºæœ‰æ•ˆæ•°æ®
dt validate data.jsonl --preset=dpo --max-errors=100  # é™åˆ¶é”™è¯¯è¾“å‡ºæ•°é‡
dt validate data.jsonl --preset=openai_chat --workers=4  # å¤šè¿›ç¨‹åŠ é€Ÿ
```

### å­—æ®µè·¯å¾„è¯­æ³•

CLI å‘½ä»¤ä¸­çš„å­—æ®µå‚æ•°æ”¯æŒåµŒå¥—è·¯å¾„è¯­æ³•ï¼Œå¯è®¿é—®æ·±å±‚åµŒå¥—çš„æ•°æ®ï¼š

| è¯­æ³• | å«ä¹‰ | ç¤ºä¾‹ |
|------|------|------|
| `a.b.c` | åµŒå¥—å­—æ®µ | `meta.source` |
| `a[0].b` | æ•°ç»„ç´¢å¼• | `messages[0].role` |
| `a[-1].b` | è´Ÿç´¢å¼• | `messages[-1].content` |
| `a.#` | æ•°ç»„é•¿åº¦ | `messages.#` |
| `a[*].b` | å±•å¼€æ‰€æœ‰å…ƒç´  | `messages[*].role` |
| `a[*].b:join` | å±•å¼€å¹¶ç”¨ `\|` æ‹¼æ¥ | `messages[*].role:join` |
| `a[*].b:unique` | å±•å¼€å»é‡åæ‹¼æ¥ | `messages[*].role:unique` |

æ”¯æŒå­—æ®µè·¯å¾„çš„å‘½ä»¤å‚æ•°ï¼š

| å‘½ä»¤ | å‚æ•° | ç¤ºä¾‹ |
|------|------|------|
| `sample` | `--by=`, `--where=` | `--by=meta.source`ã€`--where=messages.#>=2` |
| `dedupe` | `--key=` | `--key=meta.id`ã€`--key=messages[0].content` |
| `clean` | `--drop-empty=` | `--drop-empty=meta.source` |
| `clean` | `--min-len=` | `--min-len=messages.#:2` |
| `clean` | `--max-len=` | `--max-len=messages[-1].content:500` |
| `token-stats` | `--field=` | `--field=messages[-1].content` |
| `diff` | `--key=` | `--key=meta.uuid` |

`--where` æ”¯æŒçš„æ“ä½œç¬¦ï¼š

| æ“ä½œç¬¦ | å«ä¹‰ | ç¤ºä¾‹ |
|--------|------|------|
| `=` | ç­‰äº | `--where="category=tech"` |
| `!=` | ä¸ç­‰äº | `--where="source!=wiki"` |
| `~=` | åŒ…å« | `--where="content~=æœºå™¨å­¦ä¹ "` |
| `>` | å¤§äº | `--where="score>0.8"` |
| `>=` | å¤§äºç­‰äº | `--where="messages.#>=2"` |
| `<` | å°äº | `--where="length<1000"` |
| `<=` | å°äºç­‰äº | `--where="turns<=10"` |

ç¤ºä¾‹æ•°æ®ï¼š
```json
{"meta": {"source": "wiki"}, "messages": [{"role": "user", "content": "hi"}, {"role": "assistant", "content": "hello"}]}
```

- `meta.source` â†’ `"wiki"`
- `messages[0].role` â†’ `"user"`
- `messages[-1].content` â†’ `"hello"`
- `messages.#` â†’ `2`
- `messages[*].role` â†’ `"user"` (é»˜è®¤å–ç¬¬ä¸€ä¸ª)
- `messages[*].role:join` â†’ `"user|assistant"`

### Pipeline é…ç½®

ä½¿ç”¨ YAML é…ç½®æ–‡ä»¶å®šä¹‰å¯å¤ç°çš„æ•°æ®å¤„ç†æµç¨‹ï¼š

```yaml
# pipeline.yaml
version: "1.0"
seed: 42
input: raw_data.jsonl
output: processed.jsonl

steps:
  - type: filter
    condition: "score > 0.5"

  - type: filter
    condition: "len(text) > 10"

  - type: transform
    preset: openai_chat
    params:
      user_field: q
      assistant_field: a

  - type: dedupe
    key: text
```

æ”¯æŒçš„æ­¥éª¤ç±»å‹ï¼š

| æ­¥éª¤ | å‚æ•° | è¯´æ˜ |
|------|------|------|
| `filter` | `condition` | æ¡ä»¶è¿‡æ»¤ï¼š`score > 0.5`, `len(text) > 10`, `field is not empty` |
| `transform` | `preset`, `params` | æ ¼å¼è½¬æ¢ï¼Œä½¿ç”¨é¢„è®¾æ¨¡æ¿ |
| `dedupe` | `key`, `similar` | å»é‡ï¼Œæ”¯æŒç²¾ç¡®å’Œç›¸ä¼¼åº¦å»é‡ |
| `sample` | `num`, `seed` | éšæœºé‡‡æ · |
| `head` | `num` | å–å‰ N æ¡ |
| `tail` | `num` | å–å N æ¡ |
| `shuffle` | `seed` | æ‰“ä¹±é¡ºåº |
| `split` | `ratio`, `seed` | æ•°æ®é›†åˆ†å‰² |

æ‰§è¡Œ Pipelineï¼š

```bash
dt run pipeline.yaml
dt run pipeline.yaml --input=new_data.jsonl  # è¦†ç›–è¾“å…¥æ–‡ä»¶
```

### æ•°æ®è¡€ç¼˜è¿½è¸ª

è®°å½•æ•°æ®å¤„ç†çš„å®Œæ•´å†å²ï¼Œæ”¯æŒå¯å¤ç°å’Œé—®é¢˜è¿½æº¯ï¼š

```python
# å¯ç”¨è¡€ç¼˜è¿½è¸ª
dt = DataTransformer.load("raw.jsonl", track_lineage=True)

# æ­£å¸¸è¿›è¡Œæ•°æ®å¤„ç†
result = (dt
    .filter(lambda x: x.score > 0.5)
    .transform(lambda x: {"q": x.q, "a": x.a})
    .dedupe("q")
)

# ä¿å­˜æ—¶è®°å½•è¡€ç¼˜
result.save("processed.jsonl", lineage=True)
# è‡ªåŠ¨ç”Ÿæˆ processed.jsonl.lineage.json
```

æŸ¥çœ‹è¡€ç¼˜å†å²ï¼š

```bash
dt history processed.jsonl
# è¾“å‡ºï¼š
# ğŸ“Š æ•°æ®è¡€ç¼˜æŠ¥å‘Š: processed.jsonl
# â””â”€ ç‰ˆæœ¬ 1
#    æ¥æº: raw.jsonl
#    æ“ä½œé“¾:
#      â”œâ”€ filter: 1000 â†’ 800
#      â”œâ”€ transform: 800 â†’ 800
#      â””â”€ dedupe: 800 â†’ 750
#    è¾“å‡ºæ•°é‡: 750

dt history processed.jsonl --json  # JSON æ ¼å¼è¾“å‡º
```

### æ—¥å¿—æŸ¥çœ‹

dtflow å†…ç½®äº† [toolong](https://github.com/Textualize/toolong) æ—¥å¿—æŸ¥çœ‹å™¨ï¼š

```bash
pip install dtflow[logs]    # å®‰è£…æ—¥å¿—å·¥å…·

tl app.log                  # äº¤äº’å¼ TUI æŸ¥çœ‹
tl --tail app.log           # å®æ—¶è·Ÿè¸ªï¼ˆç±»ä¼¼ tail -fï¼‰
dt logs                     # æŸ¥çœ‹ä½¿ç”¨è¯´æ˜
```

### å¤§æ–‡ä»¶æµå¼å¤„ç†

ä¸“ä¸ºè¶…å¤§æ–‡ä»¶è®¾è®¡çš„æµå¼å¤„ç†æ¥å£ï¼Œå†…å­˜å ç”¨ O(1)ï¼Œæ”¯æŒ JSONLã€CSVã€Parquetã€Arrow æ ¼å¼ï¼š

```python
from dtflow import load_stream, load_sharded

# æµå¼åŠ è½½å’Œå¤„ç†ï¼ˆ100GB æ–‡ä»¶ä¹Ÿåªç”¨å¸¸é‡å†…å­˜ï¼‰
(load_stream("huge_100gb.jsonl")
    .filter(lambda x: x["score"] > 0.5)
    .transform(lambda x: {"text": x["content"]})
    .save("output.jsonl"))

# è·¨æ ¼å¼è½¬æ¢ï¼ˆCSV â†’ Parquetï¼‰
(load_stream("data.csv")
    .filter(lambda x: x["score"] > 0.5)
    .save("output.parquet"))

# åˆ†ç‰‡æ–‡ä»¶åŠ è½½ï¼ˆæ”¯æŒå¤šæ ¼å¼ï¼‰
(load_sharded("data/train_*.parquet")
    .filter(lambda x: len(x["text"]) > 10)
    .save("merged.jsonl"))

# åˆ†ç‰‡ä¿å­˜
(load_stream("huge.jsonl")
    .transform(lambda x: {"q": x["question"], "a": x["answer"]})
    .save_sharded("output/", shard_size=100000))
# ç”Ÿæˆ: output/part-00000.jsonl, output/part-00001.jsonl, ...

# æ‰¹æ¬¡å¤„ç†ï¼ˆé€‚åˆéœ€è¦æ‰¹é‡è°ƒç”¨ API çš„åœºæ™¯ï¼‰
for batch in load_stream("data.jsonl").batch(1000):
    results = call_api(batch)  # æ‰¹é‡å¤„ç†
```

ç‰¹ç‚¹ï¼š
- **æƒ°æ€§æ‰§è¡Œ**ï¼šfilter/transform ä¸ä¼šç«‹å³æ‰§è¡Œï¼Œåªåœ¨ save/collect æ—¶æ‰è§¦å‘
- **O(1) å†…å­˜**ï¼šæ— è®ºæ–‡ä»¶å¤šå¤§ï¼Œå†…å­˜å ç”¨æ’å®šï¼ˆè¯»å–ä¾§ï¼‰
- **å¤šæ ¼å¼æ”¯æŒ**ï¼šJSONLã€CSVã€Parquetã€Arrow å‡æ”¯æŒæµå¼å¤„ç†
- **è·¨æ ¼å¼è½¬æ¢**ï¼šå¯ç›´æ¥ä» CSV è¯»å–å¹¶ä¿å­˜ä¸º Parquet ç­‰
- **åˆ†ç‰‡æ”¯æŒ**ï¼šæ”¯æŒ glob æ¨¡å¼åŠ è½½å¤šä¸ªåˆ†ç‰‡ï¼Œè‡ªåŠ¨åˆå¹¶å¤„ç†

## é”™è¯¯å¤„ç†

```python
# è·³è¿‡é”™è¯¯é¡¹ï¼ˆé»˜è®¤ï¼‰
dt.to(transform_func, on_error="skip")

# æŠ›å‡ºå¼‚å¸¸
dt.to(transform_func, on_error="raise")

# ä¿ç•™åŸå§‹æ•°æ®
dt.to(transform_func, on_error="keep")

# è¿”å›é”™è¯¯ä¿¡æ¯
result, errors = dt.to(transform_func, return_errors=True)
```

## è®¾è®¡å“²å­¦

### å‡½æ•°å¼ä¼˜äºç±»ç»§æ‰¿

ä¸éœ€è¦å¤æ‚çš„ OOP æŠ½è±¡ï¼Œç›´æ¥ç”¨å‡½æ•°è§£å†³é—®é¢˜ï¼š

```python
# âœ… ç®€å•ç›´æ¥
dt.to(lambda x: {"q": x.question, "a": x.answer})

# âŒ ä¸éœ€è¦è¿™ç§è®¾è®¡
class MyFormatter(BaseFormatter):
    def format(self, item): ...
```

### é¢„è®¾æ˜¯ä¾¿åˆ©å±‚ï¼Œä¸æ˜¯æ ¸å¿ƒæŠ½è±¡

90% çš„éœ€æ±‚ç”¨ `transform(lambda x: ...)` å°±èƒ½è§£å†³ã€‚é¢„è®¾åªæ˜¯å¸¸è§åœºæ™¯çš„å¿«æ·æ–¹å¼ï¼š

```python
# é¢„è®¾ï¼šå¸¸è§åœºæ™¯çš„ä¾¿åˆ©å‡½æ•°
dt.to(preset="openai_chat")

# è‡ªå®šä¹‰ï¼šå®Œå…¨æ§åˆ¶è½¬æ¢é€»è¾‘
dt.to(lambda x: {
    "messages": [
        {"role": "user", "content": x.q},
        {"role": "assistant", "content": x.a}
    ]
})
```

### KISS åŸåˆ™

- ä¸€ä¸ªæ ¸å¿ƒç±» `DataTransformer` æå®šæ‰€æœ‰æ“ä½œ
- é“¾å¼ APIï¼Œä»£ç åƒè‡ªç„¶è¯­è¨€
- å±æ€§è®¿é—® `x.field` ä»£æ›¿ `x["field"]`
- ä¸è¿‡åº¦è®¾è®¡ï¼Œä¸è¿½æ±‚"å¯æ‰©å±•æ¡†æ¶"

### å®ç”¨ä¸»ä¹‰

ä¸è¿½æ±‚å­¦æœ¯ä¸Šçš„å®Œç¾æŠ½è±¡ï¼Œåªæä¾›**è¶³å¤Ÿå¥½ç”¨çš„å·¥å…·**ã€‚

## License

MIT
