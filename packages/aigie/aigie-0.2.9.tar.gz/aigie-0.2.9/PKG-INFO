Metadata-Version: 2.4
Name: aigie
Version: 0.2.9
Summary: Enterprise-grade AI agent reliability monitoring and autonomous remediation
Home-page: https://github.com/aigie/aigie-sdk
Author: Aigie Team
Author-email: support@aigie.io
Project-URL: Documentation, https://docs.aigie.io
Project-URL: Source, https://github.com/aigie/aigie-sdk
Project-URL: Tracker, https://github.com/aigie/aigie-sdk/issues
Project-URL: Changelog, https://github.com/aigie/aigie-sdk/blob/main/CHANGELOG.md
Keywords: ai agent monitoring observability llm reliability remediation
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: httpx>=0.25.0
Provides-Extra: compression
Requires-Dist: zstandard>=0.22.0; extra == "compression"
Provides-Extra: openai
Requires-Dist: openai>=1.0.0; extra == "openai"
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.18.0; extra == "anthropic"
Provides-Extra: gemini
Requires-Dist: google-generativeai>=0.3.0; extra == "gemini"
Provides-Extra: langchain
Requires-Dist: langchain-core>=0.1.0; extra == "langchain"
Provides-Extra: langgraph
Requires-Dist: langgraph>=0.0.20; extra == "langgraph"
Requires-Dist: langchain-core>=0.1.0; extra == "langgraph"
Provides-Extra: browser-use
Requires-Dist: browser-use>=0.1.0; extra == "browser-use"
Requires-Dist: Pillow>=10.0.0; extra == "browser-use"
Provides-Extra: crewai
Requires-Dist: crewai>=0.28.0; extra == "crewai"
Provides-Extra: autogen
Requires-Dist: pyautogen>=0.2.0; extra == "autogen"
Provides-Extra: llamaindex
Requires-Dist: llama-index>=0.10.0; extra == "llamaindex"
Provides-Extra: openai-agents
Requires-Dist: openai-agents>=0.0.3; extra == "openai-agents"
Provides-Extra: dspy
Requires-Dist: dspy-ai>=2.4.0; extra == "dspy"
Provides-Extra: claude-agent-sdk
Requires-Dist: claude-agent-sdk>=0.0.10; extra == "claude-agent-sdk"
Provides-Extra: strands
Requires-Dist: strands-agents>=0.1.0; extra == "strands"
Provides-Extra: google-adk
Requires-Dist: google-adk>=1.0.0; extra == "google-adk"
Provides-Extra: instructor
Requires-Dist: instructor>=1.0.0; extra == "instructor"
Provides-Extra: semantic-kernel
Requires-Dist: semantic-kernel>=1.0.0; extra == "semantic-kernel"
Provides-Extra: pinecone
Requires-Dist: pinecone-client>=3.0.0; extra == "pinecone"
Provides-Extra: qdrant
Requires-Dist: qdrant-client>=1.7.0; extra == "qdrant"
Provides-Extra: chromadb
Requires-Dist: chromadb>=0.4.0; extra == "chromadb"
Provides-Extra: weaviate
Requires-Dist: weaviate-client>=4.0.0; extra == "weaviate"
Provides-Extra: vectordbs
Requires-Dist: pinecone-client>=3.0.0; extra == "vectordbs"
Requires-Dist: qdrant-client>=1.7.0; extra == "vectordbs"
Requires-Dist: chromadb>=0.4.0; extra == "vectordbs"
Requires-Dist: weaviate-client>=4.0.0; extra == "vectordbs"
Provides-Extra: opentelemetry
Requires-Dist: opentelemetry-api>=1.20.0; extra == "opentelemetry"
Requires-Dist: opentelemetry-sdk>=1.20.0; extra == "opentelemetry"
Provides-Extra: pandas
Requires-Dist: pandas>=1.5.0; extra == "pandas"
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: pytest-httpx>=0.30.0; extra == "dev"
Requires-Dist: pytest-benchmark>=4.0.0; extra == "dev"
Requires-Dist: pytest-mock>=3.12.0; extra == "dev"
Requires-Dist: respx>=0.20.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: mypy>=1.5.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=7.0.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.3.0; extra == "docs"
Requires-Dist: sphinx-autodoc-typehints>=1.24.0; extra == "docs"
Provides-Extra: all
Requires-Dist: zstandard>=0.22.0; extra == "all"
Requires-Dist: openai>=1.0.0; extra == "all"
Requires-Dist: anthropic>=0.18.0; extra == "all"
Requires-Dist: google-generativeai>=0.3.0; extra == "all"
Requires-Dist: langchain-core>=0.1.0; extra == "all"
Requires-Dist: langgraph>=0.0.20; extra == "all"
Requires-Dist: browser-use>=0.1.0; extra == "all"
Requires-Dist: Pillow>=10.0.0; extra == "all"
Requires-Dist: crewai>=0.28.0; extra == "all"
Requires-Dist: pyautogen>=0.2.0; extra == "all"
Requires-Dist: llama-index>=0.10.0; extra == "all"
Requires-Dist: openai-agents>=0.0.3; extra == "all"
Requires-Dist: dspy-ai>=2.4.0; extra == "all"
Requires-Dist: claude-agent-sdk>=0.0.10; extra == "all"
Requires-Dist: strands-agents>=0.1.0; extra == "all"
Requires-Dist: google-adk>=1.0.0; extra == "all"
Requires-Dist: instructor>=1.0.0; extra == "all"
Requires-Dist: semantic-kernel>=1.0.0; extra == "all"
Requires-Dist: opentelemetry-api>=1.20.0; extra == "all"
Requires-Dist: opentelemetry-sdk>=1.20.0; extra == "all"
Provides-Extra: integrations
Requires-Dist: langchain-core>=0.1.0; extra == "integrations"
Requires-Dist: langgraph>=0.0.20; extra == "integrations"
Requires-Dist: browser-use>=0.1.0; extra == "integrations"
Requires-Dist: Pillow>=10.0.0; extra == "integrations"
Requires-Dist: crewai>=0.28.0; extra == "integrations"
Requires-Dist: pyautogen>=0.2.0; extra == "integrations"
Requires-Dist: llama-index>=0.10.0; extra == "integrations"
Requires-Dist: openai-agents>=0.0.3; extra == "integrations"
Requires-Dist: dspy-ai>=2.4.0; extra == "integrations"
Requires-Dist: claude-agent-sdk>=0.0.10; extra == "integrations"
Requires-Dist: strands-agents>=0.1.0; extra == "integrations"
Requires-Dist: google-adk>=1.0.0; extra == "integrations"
Requires-Dist: instructor>=1.0.0; extra == "integrations"
Requires-Dist: semantic-kernel>=1.0.0; extra == "integrations"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Aigie SDK

Production-grade Python SDK for integrating Aigie monitoring into your AI agent workflows.

## ‚ú® Features

- üöÄ **Event Buffering**: 10-100x performance improvement with batch uploads
- üéØ **Decorator Support**: 50%+ less boilerplate code
- ‚öôÔ∏è **Flexible Configuration**: Config class with sensible defaults
- üîÑ **Automatic Retries**: Exponential backoff with configurable policies
- üîó **LangChain Integration**: Seamless callback handler
- üìä **Production Ready**: Handles network failures, race conditions, and more

## Quick Start

### Installation

```bash
pip install aigie
```

### Basic Usage

#### Option 1: Context Manager (Traditional)
```python
from aigie import Aigie

aigie = Aigie()
await aigie.initialize()

async with aigie.trace("My Workflow") as trace:
    async with trace.span("operation", type="llm") as span:
        result = await do_work()
        span.set_output({"result": result})
```

#### Option 2: Decorator (Recommended - 50% less code!)
```python
from aigie import Aigie

aigie = Aigie()
await aigie.initialize()

@aigie.trace(name="my_workflow")
async def my_workflow():
    @aigie.span(name="operation", type="llm")
    async def operation():
        return await do_work()
    return await operation()
```

#### Option 3: With Configuration
```python
from aigie import Aigie, Config

config = Config(
    api_url="https://api.aigie.com",
    api_key="your-key",
    batch_size=100,  # Buffer 100 events before sending
    flush_interval=5.0  # Or flush every 5 seconds
)
aigie = Aigie(config=config)
await aigie.initialize()
```

## Configuration

### Environment Variables
```bash
export AIGIE_API_URL=http://your-aigie-instance:8000/api
export AIGIE_API_KEY=your-api-key-here
export AIGIE_BATCH_SIZE=100
export AIGIE_FLUSH_INTERVAL=5.0
```

### Config Object
```python
from aigie import Config

config = Config(
    api_url="https://api.aigie.com",
    api_key="your-key",
    batch_size=100,
    flush_interval=5.0,
    enable_buffering=True,  # Default: True
    max_retries=3
)
```

## Performance

### Before (No Buffering)
- 1000 spans = 1000+ API calls
- ~30 seconds total time
- High network overhead

### After (With Buffering)
- 1000 spans = 2-10 API calls
- ~0.5 seconds total time
- **99%+ reduction in API calls**

## Advanced Features

### OpenTelemetry Integration

Works with any OpenTelemetry-compatible tool (Datadog, New Relic, Jaeger, etc.):

```python
from aigie import Aigie
from aigie.opentelemetry import setup_opentelemetry

aigie = Aigie()
await aigie.initialize()

# One-line setup
setup_opentelemetry(aigie, service_name="my-service")

# Now all OTel spans automatically go to Aigie!
from opentelemetry import trace
tracer = trace.get_tracer(__name__)

with tracer.start_as_current_span("operation"):
    # Automatically traced
    pass
```

### Synchronous API

For non-async codebases:

```python
from aigie import AigieSync

aigie = AigieSync()
aigie.initialize()  # Blocking

with aigie.trace("workflow") as trace:
    with trace.span("operation") as span:
        result = do_work()  # Sync code
        span.set_output({"result": result})
```

## Installation

### Basic
```bash
pip install aigie
```

### With OpenTelemetry
```bash
pip install aigie[opentelemetry]
```

### With LangChain
```bash
pip install aigie[langchain]
```

### All Features
```bash
pip install aigie[all]
```

## Advanced Features (Phase 3)

### W3C Trace Context Propagation

Distributed tracing across microservices:

```python
# Extract from incoming request
context = aigie.extract_trace_context(request.headers)

async with aigie.trace("workflow") as trace:
    trace.set_trace_context(context)
    
    # Propagate to downstream service
    headers = trace.get_trace_headers()
    response = await httpx.get("https://api.example.com", headers=headers)
```

### Prompt Management

Create, version, and track prompts:

```python
# Create prompt
prompt = await aigie.prompts.create(
    name="customer_support",
    template="You are a helpful assistant. Customer: {customer_name}",
    version="1.0"
)

# Use in trace
async with aigie.trace("support") as trace:
    trace.set_prompt(prompt)
    rendered = prompt.render(customer_name="John")
    response = await llm.ainvoke(rendered)
```

### Evaluation Hooks

Automatic quality monitoring:

```python
from aigie import EvaluationHook, ScoreType

hook = EvaluationHook(
    name="accuracy",
    evaluator=accuracy_evaluator,
    score_type=ScoreType.ACCURACY
)

async with aigie.trace("workflow") as trace:
    trace.add_evaluation_hook(hook)
    result = await do_work()
    await trace.run_evaluations(expected, result)
```

### Streaming Support

Real-time span updates:

```python
async with aigie.trace("workflow") as trace:
    async with trace.span("llm_call", stream=True) as span:
        async for chunk in llm.astream("Hello"):
            span.append_output(chunk)  # Update in real-time
            yield chunk
```

## Documentation

- [SDK Improvement Analysis](./SDK_IMPROVEMENT_ANALYSIS.md) - Comprehensive analysis
- [Examples](./EXAMPLES_IMPROVED.md) - Before/after code examples
- [Comparison Table](./COMPARISON_TABLE.md) - Feature comparison with market leaders
- [Phase 2 Features](./PHASE2_FEATURES.md) - OpenTelemetry, Sync API, Type Hints
- [Phase 3 Features](./PHASE3_FEATURES.md) - W3C Context, Prompts, Evaluations, Streaming


