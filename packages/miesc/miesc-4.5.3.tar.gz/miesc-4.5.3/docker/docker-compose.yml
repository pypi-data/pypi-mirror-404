# MIESC v4.3.7 - Docker Compose Configuration
# Multi-layer Intelligent Evaluation for Smart Contracts
# Now with ML Pipeline: FP filtering, severity prediction, clustering
# Plus: Ollama LLM integration for AI-powered report interpretation
# SmartLLM: Uses deepseek-coder for code analysis, mistral for general interpretation
#
# Usage:
#   Build and run:     docker-compose up --build
#   Run tests:         docker-compose run miesc-test
#   Run API server:    docker-compose up miesc-api
#   Run ML CLI:        docker-compose run miesc-ml
#   Interactive shell: docker-compose run miesc-shell
#
# LLM-Enhanced Reports:
#   # Start Ollama with both models
#   docker-compose --profile llm up -d ollama ollama-init
#
#   # Or use the setup script
#   ./scripts/docker-setup.sh
#
#   # Run audit with LLM interpretation
#   docker-compose run miesc-shell
#   miesc audit full /app/contracts/MyContract.sol -o /data/results.json
#   miesc report /data/results.json -t professional --llm-interpret -o /data/report.md
#   miesc report /data/results.json -t premium --llm-interpret -o /data/premium-report.md

version: '3.8'

services:
  # Main MIESC service (runs tests by default)
  miesc:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: miesc:4.3.7
    container_name: miesc-main
    environment:
      - MIESC_VERSION=4.3.7
      - MIESC_ENV=docker
      - PYTHONUNBUFFERED=1
      # Ollama configuration (when using LLM features)
      - OLLAMA_HOST=http://ollama:11434
      - MIESC_LLM_MODEL=mistral:latest
    volumes:
      # Mount source code for development (optional, comment out for production)
      # - ./src:/app/src:ro
      # - ./tests:/app/tests:ro
      # Mount data directory for analysis results
      - miesc-data:/data
      # Mount contracts directory (if analyzing local contracts)
      - ./contracts:/app/contracts:ro
    networks:
      - miesc-network
    healthcheck:
      test: ["CMD", "python", "-c", "from src.agents.base_agent import BaseAgent; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # MIESC ML CLI (ML-enhanced analysis)
  miesc-ml:
    build:
      context: .
      dockerfile: Dockerfile
    image: miesc:4.3.7
    container_name: miesc-ml
    environment:
      - MIESC_VERSION=4.3.7
      - MIESC_ENV=docker-ml
      - PYTHONUNBUFFERED=1
      - OLLAMA_HOST=http://ollama:11434
      - MIESC_LLM_MODEL=mistral:latest
    command: python -m src.miesc_ml_cli --help
    volumes:
      - miesc-data:/data
      - ./contracts:/app/contracts:ro
    networks:
      - miesc-network
    profiles:
      - ml

  # MIESC Test Runner (explicitly runs test suite)
  miesc-test:
    build:
      context: .
      dockerfile: Dockerfile
    image: miesc:4.3.7
    container_name: miesc-test
    environment:
      - MIESC_VERSION=4.3.7
      - MIESC_ENV=docker-test
      - PYTHONUNBUFFERED=1
    command: >
      sh -c "echo '=== MIESC v4.3.6 - Test Suite ===' &&
             echo 'Running comprehensive tests including ML pipeline...' &&
             python -m pytest tests/ -v --tb=short --maxfail=5 --cov=src --cov-report=term-missing"
    volumes:
      - miesc-data:/data
    networks:
      - miesc-network
    profiles:
      - test

  # MIESC API Server (FastAPI)
  miesc-api:
    build:
      context: .
      dockerfile: Dockerfile
    image: miesc:4.3.7
    container_name: miesc-api
    environment:
      - MIESC_VERSION=4.3.7
      - MIESC_ENV=docker-api
      - PYTHONUNBUFFERED=1
      - OLLAMA_HOST=http://ollama:11434
      - MIESC_LLM_MODEL=mistral:latest
    command: uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload
    ports:
      - "8000:8000"
    volumes:
      - miesc-data:/data
      - ./contracts:/app/contracts:ro
    networks:
      - miesc-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    profiles:
      - api

  # MIESC Interactive Shell (for development)
  miesc-shell:
    build:
      context: .
      dockerfile: Dockerfile
    image: miesc:4.3.7
    container_name: miesc-shell
    environment:
      - MIESC_VERSION=4.3.7
      - MIESC_ENV=docker-shell
      - PYTHONUNBUFFERED=1
      - OLLAMA_HOST=http://ollama:11434
      - MIESC_LLM_MODEL=mistral:latest
    command: /bin/bash
    stdin_open: true
    tty: true
    volumes:
      - miesc-data:/data
      - ./contracts:/app/contracts
    networks:
      - miesc-network
    profiles:
      - dev

  # ===========================================================================
  # MIESC FULL - Complete image with ALL 30+ security tools
  # Includes: Mythril, Manticore, Echidna, Halmos, PyTorch for ML
  # Image size: ~8GB | Memory required: 8GB+
  # ===========================================================================
  miesc-full:
    build:
      context: .
      dockerfile: Dockerfile.full
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: miesc:4.3.7-full
    container_name: miesc-full
    environment:
      - MIESC_VERSION=4.3.7-full
      - MIESC_ENV=docker-full
      - PYTHONUNBUFFERED=1
      - OLLAMA_HOST=http://ollama:11434
      - MIESC_LLM_MODEL=mistral:latest
    command: /bin/bash
    stdin_open: true
    tty: true
    volumes:
      - miesc-data:/data
      - ./contracts:/app/contracts
    networks:
      - miesc-network
    profiles:
      - full

  # MIESC Analyzer (analyze specific contract)
  miesc-analyzer:
    build:
      context: .
      dockerfile: Dockerfile
    image: miesc:4.3.7
    container_name: miesc-analyzer
    environment:
      - MIESC_VERSION=4.3.7
      - MIESC_ENV=docker-analyzer
      - PYTHONUNBUFFERED=1
      - CONTRACT_PATH=${CONTRACT_PATH:-/app/contracts/example.sol}
      - OLLAMA_HOST=http://ollama:11434
      - MIESC_LLM_MODEL=mistral:latest
    command: >
      sh -c "echo '=== MIESC v4.3.6 - Contract Analyzer ===' &&
             echo 'Analyzing: ${CONTRACT_PATH}' &&
             python -m src.cli.miesc_cli analyze ${CONTRACT_PATH} --output /data/report.json"
    volumes:
      - miesc-data:/data
      - ./contracts:/app/contracts:ro
    networks:
      - miesc-network
    profiles:
      - analyze

  # ===========================================================================
  # Ollama - Local LLM Server (for AI-powered report interpretation)
  # ===========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: miesc-ollama
    ports:
      # Expose Ollama API (optional, for external access)
      - "11434:11434"
    volumes:
      # Persist downloaded models
      - ollama-models:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:11434/api/tags || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    networks:
      - miesc-network
    restart: unless-stopped
    profiles:
      - llm
    # GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # ===========================================================================
  # Ollama Model Initializer
  # Automatically pulls required models on first run
  # SmartLLM uses: deepseek-coder (code analysis) + mistral (general interpretation)
  # ===========================================================================
  ollama-init:
    image: ollama/ollama:latest
    container_name: miesc-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=http://ollama:11434
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "=== MIESC Ollama Model Initializer ==="
        echo "SmartLLM requires two models for optimal analysis:"
        echo "  - deepseek-coder:6.7b (Solidity/code analysis)"
        echo "  - mistral:latest (general interpretation)"
        echo ""

        # Model 1: deepseek-coder for code analysis
        echo "[1/2] Checking deepseek-coder:6.7b..."
        if ! ollama list | grep -q "deepseek-coder:6.7b"; then
          echo "Pulling deepseek-coder:6.7b (~3.8GB)..."
          ollama pull deepseek-coder:6.7b
          echo "✓ deepseek-coder ready!"
        else
          echo "✓ deepseek-coder already available."
        fi

        # Model 2: mistral for general interpretation
        echo "[2/2] Checking mistral:latest..."
        if ! ollama list | grep -q "mistral:latest"; then
          echo "Pulling mistral:latest (~4GB)..."
          ollama pull mistral:latest
          echo "✓ mistral ready!"
        else
          echo "✓ mistral already available."
        fi

        echo ""
        echo "=== Both models ready! ==="
        echo "Available models:"
        ollama list
        echo ""
        echo "You can now use --llm-interpret with miesc report command."
        echo "SmartLLM will automatically select the best model for each task."
    networks:
      - miesc-network
    restart: "no"
    profiles:
      - llm

networks:
  miesc-network:
    driver: bridge
    name: miesc-network

volumes:
  miesc-data:
    driver: local
    name: miesc-data
  ollama-models:
    driver: local
    name: miesc-ollama-models

# Usage Examples:
# ================
#
# 1. Build and run default (tests):
#    docker-compose up --build
#
# 2. Run test suite explicitly:
#    docker-compose run miesc-test
#
# 3. Run API server:
#    docker-compose --profile api up miesc-api
#
# 4. Interactive shell for development:
#    docker-compose --profile dev run miesc-shell
#
# 5. Analyze a specific contract:
#    CONTRACT_PATH=/app/contracts/MyToken.sol docker-compose --profile analyze run miesc-analyzer
#
# 6. Start Ollama for LLM-powered reports:
#    docker-compose --profile llm up -d ollama ollama-init
#
# 7. Generate report with LLM interpretation:
#    docker-compose --profile dev run miesc-shell
#    # Inside container:
#    miesc audit full /app/contracts/MyContract.sol -o /data/results.json
#    miesc report /data/results.json -t professional --llm-interpret -o /data/report.md
#
# 8. Clean up:
#    docker-compose down -v
#    docker rmi miesc:4.3.7
