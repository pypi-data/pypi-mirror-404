# CrewPlus

[![PyPI version](https://badge.fury.io/py/crewplus.svg)](https://badge.fury.io/py/crewplus)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python Version](https://img.shields.io/pypi/pyversions/crewplus.svg)](https://pypi.org/project/crewplus)
[![Build Status](https://img.shields.io/travis/com/your-org/crewplus-base.svg)](https://travis-ci.com/your-org/crewplus-base)

**CrewPlus** provides the foundational services and core components for building advanced AI applications. It is the heart of the CrewPlus ecosystem, designed for scalability, extensibility, and seamless integration.

## Overview

This repository, `crewplus-base`, contains the core `crewplus` Python package. It includes essential building blocks for interacting with large language models, managing vector databases, and handling application configuration. Whether you are building a simple chatbot or a complex multi-agent system, CrewPlus offers the robust foundation you need.

## The CrewPlus Ecosystem

CrewPlus is designed as a modular and extensible ecosystem of packages. This allows you to adopt only the components you need for your specific use case.

-   **`crewplus` (This package):** The core package containing foundational services for chat, model load balancing, and vector stores.
-   **`crewplus-agent`:** crewplus agent core: agentic task planner and executor, with context-aware memory.
-   **`crewplus-ingestion`:** Provides robust pipelines for knowledge ingestion and data processing.
-   **`crewplus-memory`:** Provides agent memory services for Crewplus AI Agents.
-   **`crewplus-integrations`:** A collection of third-party integrations to connect CrewPlus with other services and platforms.

## Features

-   **Chat Services:** A unified interface for interacting with various chat models:
    -   `GeminiChatModel` - Google Gemini models via Google AI or Vertex AI
    -   `ClaudeChatModel` - Anthropic Claude models via Google Vertex AI
    -   `TracedAzureChatOpenAI` - Azure OpenAI with built-in tracing
-   **Model Load Balancer:** Intelligently distribute requests across multiple LLM endpoints.
-   **Vector DB Services:** working with popular vector stores (e.g. Milvus, Zilliz Cloud) for retrieval-augmented generation (RAG) and agent memory.
-   **Observability & Tracing:** Automatic integration with tracing tools like Langfuse, with an extensible design for adding others (e.g., Helicone, ...).


## Documentation

For detailed guides and API references, please see the `docs/` folder.

-   **[GeminiChatModel Documentation](./docs/GeminiChatModel.md)**: A comprehensive guide to using the `GeminiChatModel` for text, image, and video understanding.
-   **[ClaudeChatModel Documentation](./docs/ClaudeChatModel.md)**: A comprehensive guide to using Claude models via Google Vertex AI with full LangChain compatibility.

## Installation

To install the core `crewplus` package, run the following command:

```bash
pip install crewplus
```

## Getting Started

### Using GeminiChatModel

Here is a simple example of how to use the `GeminiChatModel` to start a conversation with Google Gemini.

```python
from crewplus.services import GeminiChatModel

# Initialize the model
llm = GeminiChatModel(
    model_name="gemini-2.0-flash",
    google_api_key="your-google-api-key"
)

# Start a conversation
response = llm.invoke("Hello, what is CrewPlus?")
print(response.content)
```

### Using ClaudeChatModel

Here is an example of how to use the `ClaudeChatModel` to interact with Claude via Google Vertex AI.

```python
from crewplus.services import ClaudeChatModel

# Authenticate with GCP first: gcloud auth application-default login

# Initialize the model
llm = ClaudeChatModel(
    model_name="claude-opus-4-5",
    project_id="your-gcp-project-id",
    region="global",  # or "us-east1", "europe-west1" for regional endpoints
    max_tokens=1024
)

# Start a conversation
response = llm.invoke("Hello, what is CrewPlus?")
print(response.content)

# Streaming example
for chunk in llm.stream("Tell me about AI agents"):
    print(chunk.content, end="", flush=True)
```

## Project Structure

The `crewplus-base` repository is organized to separate core logic, tests, and documentation. 

```
crewplus-base/                    # GitHub repo name
├── pyproject.toml
├── README.md
├── LICENSE
├── CHANGELOG.md
├── crewplus/                 # PyPI package name
│   └──  __init__.py
│   └──  services/
│       └──  __init__.py
│       └──  gemini_chat_model.py
│       └──  claude_chat_model.py
│       └──  azure_chat_model.py
│       └──  model_load_balancer.py
│       └──  tracing_manager.py
│       └──  ...
│   └──  vectorstores/milvus
│       └──  __init__.py
│       └──  schema_milvus.py
│       └──  vdb_service.py
│   └──  utils/
│       └──  __init__.py
│       └──  schema_action.py
│       └──  ...
├── tests/
│   └── ...
├── docs/
│   └── ...
└── notebooks/
    └── ...

```

## Version Update

**0.2.91**
- Add `ClaudeChatModel` for Anthropic Claude models via Google Vertex AI
- Full support for Claude Opus 4.5, Sonnet 4.5, and Haiku 4.5
- Async operations support with `AsyncAnthropicVertex`
- Streaming support for both sync and async modes
- Automatic Langfuse tracing integration

**0.2.80**
- Add `FeedbackManager` to support LangSmith-style feedback with Langfuse score

**0.2.50**
- Add async `aget_vector_store` to enable async vector search

## Deploy to PyPI

Clean Previous Build Artifacts:
Remove the dist/, build/, and *.egg-info/ directories to ensure that no old files are included in the new build.

rm -rf dist build *.egg-info

### install deployment tool
pip install twine

### build package
python -m build

### deploy to TestPyPI (Test first)
python -m twine upload --repository testpypi dist/*

### install from TestPyPI 
pip install -i https://test.pypi.org/simple/ crewplus

### Deploy to official PyPI
python -m twine upload dist/*
